@Article{Eldar2010,
  author    = {Eldar, Y. and Kuppinger, P. and B{\"o}lcskei, H.},
  title     = {Block-sparse signals: Uncertainty relations and efficient recovery},
  number    = {6},
  pages     = {3042--3054},
  volume    = {58},
  annote    = {read http://www.nari.ee.ethz.ch/commth//pubs/p/Block2009 block-coherence IEEE Transactions on Signal Processing},
  journal   = {IEEE Trans. Signal Process.},
  keywords  = {Basis pursuit,Block-sparsity,compressed sensing,matching pursuit},
  month     = jun,
  owner     = {Fardin},
  timestamp = {2016-09-30T13:34:19Z},
  year      = {2010},
}

@Article{Velmurugan2016,
  author    = {Velmurugan, J. and Sinha, Sanjib and Nagappa, Madhu and Mariyappa, N. and Bindu, P. S. and Ravi, G. S. and Hazra, Nandita and Thennarasu, K. and Ravi, V. and Taly, A. B. and Satishchandra, P.},
  title     = {Combined {{MEG}}\textendash{}{{EEG}} Source Localisation in Patients with Sub-Acute Sclerosing Pan-Encephalitis},
  doi       = {10.1007/s10072-016-2571-4},
  issn      = {1590-1874, 1590-3478},
  language  = {en},
  number    = {8},
  pages     = {1221--1231},
  urldate   = {2016-10-21},
  volume    = {37},
  abstract  = {To study the genesis and propagation patterns of periodic complexes (PCs) associated with myoclonic jerks in sub-acute sclerosing pan-encephalitis (SSPE) using magnetoencephalography (MEG) and electroencephalography (EEG). Simultaneous recording of MEG (306 channels) and EEG (64 channels) in five patients of SSPE (M:F = 3:2; age 10.8 $\pm$ 3.2 years; symptom-duration 6.2 $\pm$ 10 months) was carried out using Elekta Neuromag\textregistered{} TRIUX\texttrademark{} system. Qualitative analysis of 80\textendash{}160 PCs per patient was performed. Ten isomorphic classical PCs with significant field topography per patient were analysed at the `onset' and at `earliest significant peak' of the burst using discrete and distributed source imaging methods. MEG background was asymmetrical in 2 and slow in 3 patients. Complexes were periodic (3) or quasi-periodic (2), occurring every 4\textendash{}16 s and varied in morphology among patients. Mean source localization at onset of bursts using discrete and distributed source imaging in magnetic source imaging (MSI) was in thalami and or insula (50 and 50 \%, respectively) and in electric source imaging (ESI) was also in thalami and or insula (38 and 46 \%, respectively). Mean source localization at the earliest rising phase of peak in MSI was in peri-central gyrus (49 and 42 \%) and in ESI it was in frontal cortex (52 and 56 \%). Further analysis revealed that PCs were generated in thalami and or insula and thereafter propagated to anterolateral surface of the cortices (viz. sensori-motor cortex and frontal cortex) to same side as that of the onset. This novel MEG\textendash{}EEG based case series of PCs provides newer insights for understanding the plausible generators of myoclonus in SSPE and patterns of their propagation.},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RAGTPM6E\\s10072-016-2571-4.html:text/html},
  journal   = {Neurological Sciences},
  month     = apr,
  timestamp = {2016-10-21T14:04:40Z},
  year      = {2016},
}

@Article{Heath2006,
  author    = {Heath, R. W. and Strohmer, T. and Paulraj, A. J.},
  title     = {On Quasi-Orthogonal Signatures for {{CDMA}} Systems},
  doi       = {10.1109/TIT.2005.864469},
  issn      = {0018-9448},
  number    = {3},
  pages     = {1217--1226},
  volume    = {52},
  abstract  = {Sum capacity optimal signatures in synchronous code-division multiple-access (CDMA) systems are functions of the codebook length as well as the number of active users. A new signature set must be assigned every time the number of active users changes. This correspondence considers signature sets that are less sensitive to changes in the number of active users. Equiangular signature sequences are proven to solve a certain max-min signal-to-interference-plus-noise problem, which results from their interference invariance. Unions of orthonormal bases have subsets that come close to satisfying the Welch bound. Bounds on the maximum number of bases with minimum maximum correlation are derived and a new construction algorithm is provided. Connections are made between these signature design problems, Grassmannian line packing, frame theory, and algebraic geometry},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\WTJCSP3V\\articleDetails.html:text/html},
  journal   = {IEEE Transactions on Information Theory},
  keywords  = {algebraic geometry,Circuits,codebook length,code division multiple access,code-division multiple-access,Code-division multiple-access (CDMA) systems,Code division multiplexing,Colored noise,Encoding,equiangular signature sequence,Fading,frame theory,Gaussian channels,Geometry,Grassmannian line packing,Interference,interference (signal),Iterative algorithms,minimax techniques,minimum maximum correlation,Multiaccess communication,number theory,quasiorthogonal signature,sequences,signal-to-interference-plus-noise problem,sum capacity,synchronous CDMA system,Walsh sequences,Welch bound,White noise},
  month     = mar,
  timestamp = {2016-09-30T13:50:26Z},
  year      = {2006},
}

@Article{Cand`es2008a,
  author    = {Cand{\`e}s, Emmanuel J.},
  title     = {The restricted isometry property and its implications for compressed sensing},
  doi       = {http://dx.doi.org/10.1016/j.crma.2008.03.014},
  issn      = {1631-073X},
  number    = {9-10},
  pages     = {589--592},
  volume    = {346},
  abstract  = {It is now well-known that one can reconstruct sparse or compressible
	signals accurately from a very limited number of measurements, possibly
	contaminated with noise. This technique known as “compressed sensing?
	or “compressive sampling? relies on properties of the sensing
	matrix such as the restricted isometry property. In this Note, we
	establish new results about the accuracy of the reconstruction from
	undersampled measurements which improve on earlier estimates, and
	have the advantage of being more elegant. To cite this article: E.J.
	Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008).},
  annote    = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  journal   = {Comptes Rendus Mathematique},
  owner     = {Fardin},
  timestamp = {2017-06-23T13:04:18Z},
  year      = {2008},
}

@Article{Cotter2005,
  author    = {Cotter, S.F. and Rao, B.D. and Engan, K. and Kreutz-Delgado, K.},
  title     = {Sparse solutions to linear inverse problems with multiple measurement vectors},
  doi       = {10.1109/TSP.2005.849172},
  issn      = {1053-587X},
  number    = {7},
  pages     = {2477--2488},
  volume    = {53},
  abstract  = {We address the problem of finding sparse solutions to an underdetermined
	system of equations when there are multiple measurement vectors having
	the same, but unknown, sparsity structure. The single measurement
	sparse solution problem has been extensively studied in the past.
	Although known to be NP-hard, many single-measurement suboptimal
	algorithms have been formulated that have found utility in many different
	applications. Here, we consider in depth the extension of two classes
	of algorithms-Matching Pursuit (MP) and FOCal Underdetermined System
	Solver (FOCUSS)-to the multiple measurement case so that they may
	be used in applications such as neuromagnetic imaging, where multiple
	measurement vectors are available, and solutions with a common sparsity
	structure must be computed. Cost functions appropriate to the multiple
	measurement problem are developed, and algorithms are derived based
	on their minimization. A simulation study is conducted on a test-case
	dictionary to show how the utilization of more than one measurement
	vector improves the performance of the MP and FOCUSS classes of algorithm,
	and their performances are compared.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {algorithms-matching pursuit,Computational modeling,Cost function,Dictionaries,Equations,focal underdetermined system solver,Focusing,inverse problems,linear inverse problem,measurement vector,Minimization methods,neuromagnetic imaging,Pursuit algorithms,signal processing,suboptimal algorithm,Testing,Vectors},
  month     = jul,
  owner     = {Fardin},
  timestamp = {2016-09-30T11:19:42Z},
  year      = {2005},
}

@Article{Mishali2011a,
  author     = {Mishali, M. and Eldar, Y. C. and Dounaevsky, O. and Shoshan, E.},
  title      = {Xampling: {{Analog}} to Digital at Sub-{{Nyquist}} Rates},
  doi        = {10.1049/iet-cds.2010.0147},
  issn       = {1751-858X},
  number     = {1},
  pages      = {8--20},
  volume     = {5},
  abstract   = {The authors present a sub-Nyquist analog-to-digital converter of wideband inputs. The circuit realises the recently proposed modulated wideband converter, which is a flexible platform for sampling signals according to their actual bandwidth occupation. The theoretical work enables, for example, a sub-Nyquist wideband communication receiver, which has no prior information on the transmitter carrier positions. The present design supports input signals with 2 GHz Nyquist rate and 120 MHz spectrum occupancy, with arbitrary transmission frequencies. The sampling rate is as low as 280 MHz. To the best of the authors knowledge, this is the first reported hardware that performs sub-Nyquist sampling and reconstruction of wideband signals. The authors describe the various circuit design considerations, with an emphasis on the non-ordinary challenges the converter introduces: mixing a signal with a multiple set of sinusoids, rather than a single local oscillator, and generation of highly transient periodic waveforms, with transient intervals on the order of the Nyquist rate. Hardware experiments validate the design and demonstrate sub-Nyquist sampling and signal reconstruction.},
  file       = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MH4SWW6J\\5692791.html:text/html},
  journal    = {IET Circuits, Devices Systems},
  keywords   = {analog-to-digital converter,analogue-digital conversion,circuit design considerations,frequency 2 GHz,frequency 120 MHz,frequency 280 MHz,integrated circuit design,signal reconstruction,single local oscillator,subNyquist wideband communication receiver,transient intervals,transient periodic waveforms,wideband signals,Xampling},
  month      = jan,
  shorttitle = {Xampling},
  timestamp  = {2016-09-30T11:17:32Z},
  year       = {2011},
}

@Book{Hari2017,
  author = {Riitta Hari and Aina Puce},
  title  = {MEG-EEG Primer},
  year   = {2017},
}

@Article{Mallat1993,
  author    = {Mallat, S.G. and Zhang, Z.},
  title     = {Matching pursuits with time-frequency dictionaries},
  doi       = {10.1109/78.258082},
  issn      = {1053-587X},
  number    = {12},
  pages     = {3397--3415},
  volume    = {41},
  abstract  = {The authors introduce an algorithm, called matching pursuit, that
	decomposes any signal into a linear expansion of waveforms that are
	selected from a redundant dictionary of functions. These waveforms
	are chosen in order to best match the signal structures. Matching
	pursuits are general procedures to compute adaptive signal representations.
	With a dictionary of Gabor functions a matching pursuit defines an
	adaptive time-frequency transform. They derive a signal energy distribution
	in the time-frequency plane, which does not include interference
	terms, unlike Wigner and Cohen class distributions. A matching pursuit
	isolates the signal structures that are coherent with respect to
	a given dictionary. An application to pattern extraction from noisy
	signals is described. They compare a matching pursuit decomposition
	with a signal expansion over an optimized wavepacket orthonormal
	basis, selected with the algorithm of Coifman and Wickerhauser see
	(IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {adaptive signal representations,adaptive time-frequency transform,Dictionaries,Fourier transforms,Gabor functions,Interference,linear waveform expansion,matching pursuit algorithm,Matching pursuit algorithms,matching pursuit decomposition,Natural languages,noisy signals,optimized wavepacket orthonormal basis,pattern extraction,Pursuit algorithms,signal energy distribution,signal expansion,signal processing,Signal processing algorithms,signal representations,signal structures,time-frequency analysis,Time frequency analysis,time-frequency dictionaries,time-frequency plane,Vocabulary,wavelet transforms},
  month     = dec,
  owner     = {afdidehf},
  timestamp = {2016-09-29T14:53:46Z},
  year      = {1993},
}

@Article{Hurley2009,
  author    = {Hurley, N. and Rickard, Scott},
  title     = {Comparing Measures of Sparsity},
  doi       = {10.1109/TIT.2009.2027527},
  issn      = {0018-9448},
  number    = {10},
  pages     = {4723--4741},
  volume    = {55},
  abstract  = {Sparsity of representations of signals has been shown to be a key
	concept of fundamental importance in fields such as blind source
	separation, compression, sampling and signal analysis. The aim of
	this paper is to compare several commonly-used sparsity measures
	based on intuitive attributes. Intuitively, a sparse representation
	is one in which a small number of coefficients contain a large proportion
	of the energy. In this paper, six properties are discussed: (Robin
	Hood, Scaling, Rising Tide, Cloning, Bill Gates, and Babies), each
	of which a sparsity measure should have. The main contributions of
	this paper are the proofs and the associated summary table which
	classify commonly-used sparsity measures based on whether or not
	they satisfy these six propositions. Only two of these measures satisfy
	all six: the pq-mean with p les 1, q > 1 and the Gini index.},
  annote    = {read Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {adaptive signal processing,blind source separation,Cloning,compression analysis,Gini index,image coding,information theory,Machine learning,Measures of sparsity,measuring sparsity,sampling analysis,Sampling methods,Sea measurements,Signal analysis,source separation,sparse distribution,sparse representation,Sparsity,sparsity measures,Tides},
  month     = oct,
  owner     = {Fardin},
  timestamp = {2016-09-30T10:42:08Z},
  year      = {2009},
}

@InProceedings{Berg1999,
  author    = {Berg, A.P. and Mikhael, W.B.},
  booktitle = {Circuits and Systems, 1999. ISCAS '99. Proceedings of the 1999 IEEE International Symposium on},
  title     = {A survey of mixed transform techniques for speech and image coding},
  doi       = {10.1109/ISCAS.1999.779953},
  pages     = {106--109},
  volume    = {4},
  abstract  = {The goal of transform based coding is to build a representation of
	a signal using the smallest number of weighted basis functions possible,
	while maintaining the ability to reconstruct the signal with adequate
	fidelity. Mixed transform techniques, which employ subsets of non-orthogonal
	basis functions chosen from two or more transform domains, have been
	shown to consistently yield more efficient signal representations
	than those based on one transform. This paper provides a survey of
	mixed transform techniques, also known as multitransforms or mixed
	basis representations, which have been developed for speech and image
	coding},
  keywords  = {Baseband,Compaction,data compression,Dictionaries,discrete cosine transforms,Discrete transforms,Fourier transforms,image coding,Image reconstruction,mixed basis representations,mixed transform techniques,multitransforms,nonorthogonal basis functions,Prototypes,signal representation,signal representations,speech coding,Speech coding,transform based coding,Transform coding,transforms,weighted basis functions},
  month     = jul,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:28:53Z},
  year      = {1999},
}

@Article{Cotter2002,
  author    = {Cotter, S.F. and Rao, B.D.},
  title     = {Sparse channel estimation via matching pursuit with application to equalization},
  doi       = {10.1109/26.990897},
  issn      = {0090-6778},
  number    = {3},
  pages     = {374--377},
  volume    = {50},
  abstract  = {Channels with a sparse impulse response arise in a number of communication
	applications. Exploiting the sparsity of the channel, we show how
	an estimate of the channel may be obtained using a matching pursuit
	(MP) algorithm. This estimate is compared to thresholded variants
	of the least squares (LS) channel estimate. Among these sparse channel
	estimates, the MP estimate is computationally much simpler to implement
	and a shorter training sequence is required to form an accurate channel
	estimate leading to greater information throughput},
  journal   = {Communications, IEEE Transactions on},
  keywords  = {Broadband communication,Broadband communication,channel estimation,Channel estimation,decision feedback equalisers,decision feedback equalisers,decision feedback equalizer,decision feedback equalizer,Delay estimation,Delay estimation,DFE,DFE,HDTV,HDTV,information throughput,information throughput,intersymbol interference,intersymbol interference,intersymbol interference,ISI,ISI,Least squares approximation,Least squares approximation,least squares approximations,Least squares approximations,least squares channel estimate,least squares channel estimate,matching pursuit algorithm,matching pursuit algorithm,Matching pursuit algorithms,Matching pursuit algorithms,parameter estimation,parameter estimation,Pursuit algorithms,Pursuit algorithms,sparse channel estimation,sparse channel estimation,sparse impulse response,sparse impulse response,telecommunication channels,telecommunication channels,Throughput,Throughput,training sequence,training sequence,transient response,transient response,TV,TV,White noise,White noise},
  month     = mar,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:29:32Z},
  year      = {2002},
}

@Book{Boyd2004,
  author    = {Boyd, Stephen and Vandenberghe, Lieven},
  title     = {Convex Optimization},
  publisher = {{Cambridge, U.K.: Cambridge Univ. Press}},
  abstract  = {Convex optimization problems arise frequently in many different fields.
	A comprehensive introduction to the subject, this book shows in detail
	how such problems can be solved numerically with great efficiency.
	The focus is on recognizing convex optimization problems and then
	finding the most appropriate technique for solving them. The text
	contains many worked examples and homework exercises and will appeal
	to students, researchers and practitioners in fields such as engineering,
	computer science, mathematics, statistics, finance, and economics.
	More material can be found at the web sites for EE364A (Stanford)
	or EE236B (UCLA), and our own web pages. Source code for almost all
	examples and figures in part 2 of the book is available in CVX (in
	the examples directory), in CVXOPT (in the book examples directory),
	and in CVXPY. Source code for examples in Chapters 9, 10, and 11
	can be found here. Instructors can obtain complete solutions to exercises
	by email request to us; please give us the URL of the course you
	are teaching. If you find an error not listed in our errata list,
	please do let us know about it.},
  owner     = {afdidehf},
  timestamp = {2016-07-08T12:04:41Z},
  year      = {2004},
}

@Article{Gribonval2004a,
  author    = {Gribonval, R{\'e}mi and Nielsen, Morten},
  title     = {On the Strong Uniqueness of Highly Sparse Representations from Redundant Dictionaries},
  pages     = {201--208},
  urldate   = {2016-05-16},
  volume    = {3195},
  annote    = {Independent Component Analysis and Blind Signal Separation},
  journal   = {Int. Conf. Ind. Compon. Anal. Blind Signal Sep.},
  timestamp = {2017-04-19T15:29:02Z},
  year      = {2004},
}

@Article{Blumensath2009a,
  author    = {Blumensath, Thomas and Davies, Mike E.},
  title     = {Iterative hard thresholding for compressed sensing},
  doi       = {http://dx.doi.org/10.1016/j.acha.2009.04.002},
  issn      = {1063-5203},
  number    = {3},
  pages     = {265 - 274},
  volume    = {27},
  abstract  = {Compressed sensing is a technique to sample compressible signals below
	the Nyquist rate, whilst still allowing near optimal reconstruction
	of the signal. In this paper we present a theoretical analysis of
	the iterative hard thresholding algorithm when applied to the compressed
	sensing recovery problem. We show that the algorithm has the following
	properties (made more precise in the main text of the paper)• It
	gives near-optimal error guarantees. • It is robust to observation
	noise. • It succeeds with a minimum number of observations. •
	It can be used with any sampling operator for which the operator
	and its adjoint can be computed. • The memory requirement is linear
	in the problem size. • Its computational complexity per iteration
	is of the same order as the application of the measurement operator
	or its adjoint. • It requires a fixed number of iterations depending
	only on the logarithm of a form of signal to noise ratio of the signal.
	• Its performance guarantees are uniform in that they only depend
	on properties of the sampling operator and signal sparsity.},
  journal   = {Applied and Computational Harmonic Analysis},
  keywords  = {Algorithms},
  owner     = {afdidehf},
  timestamp = {2016-07-09T19:45:21Z},
  year      = {2009},
}

@Article{Ben-Haim2011,
  author    = {Ben-Haim, Z. and Eldar, Y.C.},
  title     = {Near-Oracle Performance of Greedy Block-Sparse Estimation Techniques From Noisy Measurements},
  doi       = {10.1109/JSTSP.2011.2160250},
  issn      = {1932-4553},
  number    = {5},
  pages     = {1032-1047},
  volume    = {5},
  abstract  = {This paper examines the ability of greedy algorithms to estimate a
	block sparse parameter vector from noisy measurements. In particular,
	block sparse versions of the orthogonal matching pursuit and thresholding
	algorithms are analyzed under both adversarial and Gaussian noise
	models. In the adversarial setting, it is shown that estimation accuracy
	comes within a constant factor of the noise power. Under Gaussian
	noise, the Crame?r-Rao bound is derived, and it is shown that the
	greedy techniques come close to this bound at high signal-to-noise
	ratio. The guarantees are numerically compared with the actual performance
	of block and non-block algorithms, identifying situations in which
	block sparse techniques improve upon the scalar sparsity approach.
	Specifically, we show that block sparse methods are particularly
	successful when the atoms within each block are nearly orthogonal.},
  journal   = {Selected Topics in Signal Processing, IEEE Journal of},
  keywords  = {Atomic measurements,block sparse parameter vector,block sparsity,Coherence,Crame?r-Rao bound,Dictionaries,Estimation,Gaussian noise,greedy block-sparse estimation technique,greedy block-sparse estimation technique,iterative methods,Matching pursuit algorithms,Matching pursuit algorithms,near-oracle performance,near-oracle performance,noisy measurement,orthogonal matching pursuit,orthogonal matching pursuit,orthogonal matching pursuit,performance guarantees,scalar sparsity approach,scalar sparsity approach,signal processing,signal processing,Signal-To-Noise Ratio,thresholding,thresholding algorithm,thresholding algorithm},
  month     = sep,
  owner     = {afdidehf},
  timestamp = {2016-07-10T06:48:47Z},
  year      = {2011},
}

@Article{Plonsey1967,
  author    = {Plonsey, Robert and Heppner, Dennis B.},
  title     = {Considerations of Quasi-Stationarity in Electrophysiological Systems},
  doi       = {10.1007/BF02476917},
  issn      = {0007-4985, 1522-9602},
  language  = {en},
  number    = {4},
  pages     = {657--664},
  urldate   = {2017-08-21},
  volume    = {29},
  abstract  = {Conditions under which a time varying electromagnetic field problem (such as arises in electrophysiology, electrocardiography, etc.) can be reduced to the conventional quasistatic problem are summarized. These conditions are discussed for typical physiological parameters.},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\6Z4EXCM7\\BF02476917.html:text/html},
  journal   = {The bulletin of mathematical biophysics},
  month     = dec,
  timestamp = {2017-08-21T15:00:59Z},
  year      = {1967},
}

@Article{Elad2001,
  author    = {Elad, M. and Bruckstein, A.M.},
  title     = {On sparse signal representations},
  doi       = {10.1109/ICIP.2001.958936},
  pages     = {3-6},
  volume    = {1},
  abstract  = {An elementary proof of a basic uncertainty principle concerning pairs
	of representations of ?N vectors in different orthonormal bases is
	provided. The result, slightly stronger than stated before, has a
	direct impact on the uniqueness property of the sparse representation
	of such vectors using pairs of orthonormal bases as overcomplete
	dictionaries. The main contribution in this paper is the improvement
	of an important result due to Donoho and Huo (1999) concerning the
	replacement of the l0 optimization problem by a linear programming
	minimization when searching for the unique sparse representation},
  journal   = {Image Processing, 2001. Proceedings. 2001 International Conference on},
  keywords  = {Cities and towns,Computer science,Dictionaries,Equations,linear programming,linear programming minimization,minimisation,optimization problem,orthonormal bases,overcomplete dictionaries,Signal generators,signal processing,signal representation,signal representations,sparse representation,Uncertainty,uncertainty principle,uniqueness property,unique sparse representation search,Vectors},
  owner     = {Fardin},
  timestamp = {2016-07-10T07:14:04Z},
  year      = {2001},
}

@Misc{Oostenveld2011,
  author       = {Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs},
  title        = {{{FieldTrip}}: {{Open Source Software}} for {{Advanced Analysis}} of {{MEG}}, {{EEG}}, and {{Invasive Electrophysiological Data}}},
  doi          = {10.1155/2011/156869},
  howpublished = {\url{https://www.hindawi.com/journals/cin/2011/156869/}},
  language     = {en},
  type         = {Research article},
  urldate      = {2017-12-25},
  abstract     = {This paper describes FieldTrip, an open source software package that we developed for the analysis of MEG, EEG, and other electrophysiological data. The software is implemented as a MATLAB toolbox and includes a complete set of consistent and user-friendly high-level functions that allow experimental neuroscientists to analyze experimental data. It includes algorithms for simple and advanced analysis, such as time-frequency analysis using multitapers, source reconstruction using dipoles, distributed sources and beamformers, connectivity analysis, and nonparametric statistical permutation tests at the channel and source level. The implementation as toolbox allows the user to perform elaborate and structured analyses of large data sets using the MATLAB command line and batch scripting. Furthermore, users and developers can easily extend the functionality and implement new algorithms. The modular design facilitates the reuse in other software packages.},
  file         = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XGP3I2D4\\156869.html:application/xhtml+xml},
  journal      = {Computational Intelligence and Neuroscience},
  pmid         = {21253357},
  shorttitle   = {{{FieldTrip}}},
  timestamp    = {2017-12-25T16:41:37Z},
  year         = {2011},
}

@Article{Wright2009a,
  author    = {Wright, J. and Yang, A. Y. and Ganesh, A. and Sastry, S. S. and Ma, Y.},
  title     = {Robust Face Recognition via Sparse Representation},
  doi       = {10.1109/TPAMI.2008.79},
  issn      = {0162-8828},
  number    = {2},
  pages     = {210--227},
  volume    = {31},
  abstract  = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords  = {Algorithms,algorithms,Artificial Intelligence,Artificial Intelligence,Automated,Biometry,Biometry,Classification algorithms,Classification algorithms,Classification algorithms,Classifier design and evaluation,Classifier design and evaluation,Cluster Analysis,cluster analysis,compressed sensing,compressed sensing,compressed sensing,Computer-Assisted,downsampled images,downsampled images,eigenfaces,eigenfaces,ell^1–minimization,ell^1–minimization,Face,Face,Face and gesture recognition,Face and gesture recognition,Face and gesture recognition,face recognition,face recognition,Feature evaluation and selection,Feature evaluation and selection,feature extraction,feature extraction,Feature Extraction,Humans,Humans,illumination,illumination,image-based object recognition,image-based object recognition,Image Enhancement,Image Enhancement,Image Enhancement,Image Interpretation,Image Interpretation; Computer-Assisted,Image recognition,Image recognition,Laplacianfaces,Laplacianfaces,Lighting,lighting,lightning,lightning,Linear regression,Linear regression,Linear regression,multiple linear regression model,multiple linear regression model,object recognition,Object Recognition,occlusion,occlusion,occlusion and corruption,occlusion and corruption,occlusion and corruption,Outlier rejection,Outlier rejection,Pattern Recognition,Pattern Recognition; Automated,random processes,random processes,random projections,random projections,random projections,regression analysis,regression analysis,Reproducibility of Results,Reproducibility of Results,robust face recognition,robust face recognition,robust face recognition,Robustness,Robustness,Sensitivity and Specificity,Sensitivity and Specificity,signal representation,signal representation,signal representations,signal representations,Signal representations,Spare representation,Spare representation,sparse representation,sparse representation,sparse signal representation,sparse signal representation,sparse signal representation,Subtraction Technique,Subtraction Technique,validation and outlier rejection.,validation and outlier rejection.,validation and outlier rejection.},
  month     = feb,
  timestamp = {2016-09-30T11:28:15Z},
  year      = {2009},
}

@Article{Davis1997,
  author    = {Davis, G. and Mallat, S. and Avellaneda, M.},
  title     = {Adaptive greedy approximations},
  doi       = {10.1007/BF02678430},
  issn      = {0176-4276},
  language  = {English},
  number    = {1},
  pages     = {57--98},
  volume    = {13},
  abstract  = {The problem of optimally approximating a function with a linear expansion
	over a redundant dictionary of waveforms is NP-hard. The greedy matching
	pursuit algorithm and its orthogonalized variant produce suboptimal
	function expansions by iteratively choosing dictionary waveforms
	that best match the function's structures. A matching pursuit provides
	a means of quickly computing compact, adaptive function approximations.
	Numerical experiments show that the approximation errors from matching
	pursuits initially decrease rapidly, but the asymptotic decay rate
	of the errors is slow. We explain this behavior by showing that matching
	pursuits are chaotic, ergodic maps. The statistical properties of
	the approximation errors of a pursuit can be obtained from the invariant
	measure of the pursuit. We characterize these measures using group
	symmetries of dictionaries and by constructing a stochastic differential
	equation model. We derive a notion of the coherence of a signal with
	respect to a dictionary from our characterization of the approximation
	errors of a pursuit. The dictionary elements selected during the
	initial iterations of a pursuit correspond to a function's coherent
	structures. The tail of the expansion, on the other hand, corresponds
	to a noise which is characterized by the invariant measure of the
	pursuit map. When using a suitable dictionary, the expansion of a
	function into its coherent structures yields a compact approximation.
	We demonstrate a denoising algorithm based on coherent function expansions.},
  journal   = {Constructive Approximation},
  keywords  = {41A10,Adaptive approximations,denoising,Greedy algorithms,matching pursuit,overcomplete signal representation,Time frequency analysis},
  owner     = {afdidehf},
  timestamp = {2016-09-30T10:44:14Z},
  year      = {1997},
}

@Article{Sarvas1987,
  author    = {Sarvas, J.},
  title     = {Basic Mathematical and Electromagnetic Concepts of the Biomagnetic Inverse Problem},
  doi       = {10.1088/0031-9155/32/1/004},
  issn      = {0031-9155},
  language  = {en},
  number    = {1},
  pages     = {11},
  urldate   = {2016-10-14},
  volume    = {32},
  abstract  = {Basic mathematical and physical concepts of the biomagnetic inverse problem are reviewed with some new approaches. The forward problem is discussed for both homogeneous and inhomogeneous media. Geselowitz' formulae and a surface integral equation are presented to handle a piecewise homogeneous conductor. The special cases of a spherically symmetric conductor and a horizontally layered medium are discussed in detail. The non-uniqueness of the solution of the magnetic inverse problem is discussed and the difficulty caused by the contribution of the electric potential to the magnetic field outside the conductor is studied. As practical methods of solving the inverse problem, a weighted least-squares search with confidence limits and the method of minimum norm estimate are discussed.},
  journal   = {Physics in Medicine and Biology},
  timestamp = {2016-10-14T12:01:39Z},
  year      = {1987},
}

@Article{Davies2009a,
  author    = {Davies, M. E. and Gribonval, R.},
  title     = {lp minimization and sparse approximation failure for compressible signals},
  journal   = {SAMPTA},
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:07:47Z},
  year      = {2009},
}

@Article{Donoho2006a,
  author    = {Donoho, D.L. and Elad, M. and Temlyakov, V.N.},
  title     = {Stable recovery of sparse overcomplete representations in the presence of noise},
  doi       = {10.1109/TIT.2005.860430},
  issn      = {0018-9448},
  number    = {1},
  pages     = {6--18},
  volume    = {52},
  abstract  = {Overcomplete representations are attracting interest in signal processing
	theory, particularly due to their potential to generate sparse representations
	of signals. However, in general, the problem of finding sparse representations
	must be unstable in the presence of noise. This paper establishes
	the possibility of stable recovery under a combination of sufficient
	sparsity and favorable structure of the overcomplete system. Considering
	an ideal underlying signal that has a sufficiently sparse representation,
	it is assumed that only a noisy version of it can be observed. Assuming
	further that the overcomplete system is incoherent, it is shown that
	the optimally sparse approximation to the noisy data differs from
	the optimally sparse decomposition of the ideal noiseless signal
	by at most a constant multiple of the noise level. As this optimal-sparsity
	method requires heavy (combinatorial) computational effort, approximation
	algorithms are considered. It is shown that similar stability is
	also available using the basis and the matching pursuit algorithms.
	Furthermore, it is shown that these methods result in sparse approximation
	of the noisy data that contains only terms also appearing in the
	unique sparsest representation of the ideal noiseless sparse signal.},
  annote    = {read},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {approximation theory,Basis pursuit,Dictionaries,greedy approximation,greedy approximation algorithm,incoherent dictionary,iterative methods,Kruskal rank,Linear algebra,matching pursuit,Matching pursuit algorithms,Noise generators,Noise level,noisy data,optimal sparse decomposition,overcomplete representation,signal denoising,signal processing,Signal processing algorithms,signal processing theory,signal representation,signal representations,sparse overcomplete representation,sparse representation,Stability,stable recovery,stepwise regression,superresolution,superresolution signal,time-frequency analysis,Vectors},
  month     = jan,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:17:03Z},
  year      = {2006},
}

@Article{Afdideh2016,
  author    = {Afdideh, F. and Phlypo, R. and Jutten, C.},
  title     = {Recovery Guarantees for Mixed Norm $\ell_{p_1,p_2}$ Block Sparse Representations},
  doi       = {10.1109/EUSIPCO.2016.7760274},
  pages     = {378-382},
  abstract  = {In this work, we propose theoretical and algorithmic-independent recovery
	conditions which guarantee the uniqueness of block sparse recovery
	in general dictionaries through a general mixed norm optimization
	problem. These conditions are derived using the proposed block uncertainty
	principles and block null space property, based on some newly defined
	characterizations of block spark, and (p, p)-block mutual incoherence.
	We show that there is improvement in the recovery condition when
	exploiting the block structure of the representation. In addition,
	the proposed recovery condition extends the similar results for block
	sparse setting by generalizing the criterion for determining the
	active blocks, generalizing the block sparse recovery condition,
	and relaxing some constraints on blocks such as linear independency
	of the columns.},
  annote    = {2016 24th European Signal Processing Conference (EUSIPCO)},
  journal   = {24th Eur. Signal Process. Conf.},
  keywords  = {algorithmic-independent recovery condition,block mutual incoherence constant,Block Mutual Incoherence Constant (BMIC),block null space property,Block Spark,block spark characterization,block sparse recovery condition,Block-sparse recovery conditions,Block-sparsity,block uncertainty,Block Uncertainty Principle (BUP),Dictionaries,Europe,mixed norm lp1-p2 block sparse representation,mixed norm optimization problem,optimisation,Optimization,signal processing,Signal processing algorithms,signal representation,Sparks,Uncertainty},
  month     = aug,
  timestamp = {2017-06-23T11:39:51Z},
  year      = {2016},
}

@InProceedings{Donoho2006d,
  author    = {Donoho, D. L. and Tanner, J.},
  booktitle = {2006 40th Annual Conference on Information Sciences and Systems},
  title     = {Thresholds for the Recovery of Sparse Solutions via L1 Minimization},
  doi       = {10.1109/CISS.2006.286462},
  pages     = {202--206},
  abstract  = {The ubiquitous least squares method for systems of linear equations returns solutions which typically have all non-zero entries. However, solutions with the least number of non-zeros allow for greater insight. An exhaustive search for the sparsest solution is intractable, NP-hard. Recently, a great deal of research showed that linear programming can find the sparsest solution for certain 'typical' systems of equations, provided the solution is sufficiently sparse. In this note we report recent progress determining conditions under which the sparsest solution to large systems is available by linear programming. Our work shows that there are sharp thresholds on sparsity below which these methods will succeed and above which they fail; it evaluates those thresholds precisely and hints at several interesting applications.},
  keywords  = {Cities and towns,Cities and towns,compressed sensing,compressed sensing,Equations,Equations,L1 minimization,l1 minimization,least squares approximations,Least squares approximations,Least squares methods,Least squares methods,linear programming,linear programming,Mathematics,Mathematics,Minimization methods,Minimization methods,NP-hard,NP-hard,Sampling methods,Sampling methods,sparse matrices,Sparse matrices,sparse solution,sparse solution,Statistics,Statistics,ubiquitous least squares method,ubiquitous least squares method},
  month     = mar,
  timestamp = {2016-09-30T10:46:41Z},
  year      = {2006},
}

@PhdThesis{Dossal2005,
  author = {Dossal, C.},
  title  = {Estimation de fonctions g{\'e}om{\'e}triques et d{\'e}convolution},
  year   = {2005},
}

@Article{Gribonval2003a,
  author    = {Gribonval, R. and Nielsen, M.},
  title     = {Sparse representations in unions of bases},
  doi       = {10.1109/TIT.2003.820031},
  issn      = {0018-9448},
  number    = {12},
  pages     = {3320--3325},
  volume    = {49},
  abstract  = {The purpose of this correspondence is to generalize a result by Donoho
	and Huo and Elad and Bruckstein on sparse representations of signals
	in a union of two orthonormal bases for RN. We consider general (redundant)
	dictionaries for RN, and derive sufficient conditions for having
	unique sparse representations of signals in such dictionaries. The
	special case where the dictionary is given by the union of L?2 orthonormal
	bases for RN is studied in more detail. In particular, it is proved
	that the result of Donoho and Huo, concerning the replacement of
	the ?0 optimization problem with a linear programming problem when
	searching for sparse representations, has an analog for dictionaries
	that may be highly redundant.},
  annote    = {read Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Dictionaries,Grassmannian frame,indeterminancy,linear programming,minimisation,mutually incoherent base,nonlinear approximation,NSP,null space property,redundant dictionary,redundant dictionary,sparse matrices,sparse matrices,sparse representation,Sufficient conditions,Vectors},
  month     = dec,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:24:26Z},
  year      = {2003},
}

@InProceedings{Ziaei2010,
  author    = {Ziaei, A. and Pezeshki, A. and Bahmanpour, S. and Azimi-Sadjadi, M.R.},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2010 Conference Record of the Forty Fourth Asilomar Conference on},
  title     = {Compressed sensing of different size block-sparse signals: Efficient recovery},
  doi       = {10.1109/ACSSC.2010.5757679},
  pages     = {818-821},
  abstract  = {This paper considers compressed sensing of different size block-sparse
	signals, i.e. signals with nonzero elements occurring in blocks with
	different lengths. A new sufficient condition for mixed l2/l1-optimization
	algorithm is derived to successfully recover k-sparse signals. We
	show that if the signal possesses k-block sparse structure, then
	via mixed l2/l1-optimization algorithm, a better reconstruction results
	can be achieved in comparison with the conventional l1-optimization
	algorithm and fixed-size mixed l2/l1-optimization algorithm. The
	significance of the results presented in this paper lies in the fact
	that making explicit use of different block-sparsity can yield better
	reconstruction properties than treating the signal as being sparse
	in the conventional sense, thereby ignoring the structure in the
	signal.},
  annote    = {read},
  keywords  = {block-sparse signals,Block-sparsity,Coherence,compressed sensing,compressed sensing,Dictionaries,Error analysis,Matching pursuit algorithms,Minimization,mixed l2/l1-optimization algorithm,mixed-optimization algorithm,signal processing,sparse matrices},
  month     = nov,
  owner     = {afdidehf},
  timestamp = {2016-07-08T11:56:47Z},
  year      = {2010},
}

@InProceedings{Acar2003,
  author    = {Acar, C.E. and Gencer, N.G.},
  booktitle = {Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE},
  title     = {Sensitivity of EEG and MEG to conductivity perturbations},
  doi       = {10.1109/IEMBS.2003.1280508},
  pages     = {2834-2837 Vol.3},
  volume    = {3},
  abstract  = {Solution of the electro-magnetic source imaging (EMSI) problem requires
	an accurate representation of the head using a numerical model. Some
	of the errors in source estimation are due to the differences between
	this model and the actual head. This study investigates the effects
	of conductivity perturbations, that is, changing the conductivity
	of a small region by a small amount, on the EEG and MEG measurements.
	By computing the change in measurements for perturbations throughout
	the volume, it is possible to obtain a spatial distribution of sensitivity.
	Using this information, it is possible, for a given source configuration,
	to identify the regions to which a measurement is most sensitive.
	In this work, two mathematical expressions for efficient computation
	of the sensitivity distribution are presented. The formulation is
	implemented for a numerical head model using the finite element method
	(FEM). 3D sensitivity distributions are computed and analyzed for
	selected dipoles and sensors. It was observed that the voltage measurements
	are sensitive to the skull, the regions near the dipole and the electrodes.
	The magnetic field measurements are mostly sensitive to regions near
	the dipole. It could also be possible to use the computed sensitivity
	matrices to estimate or update the conductivity of the tissues from
	EEG and MEG measurements.},
  keywords  = {bioelectric phenomena,biological tissues,biomedical electrodes,biosensors,Brain modeling,Conductivity measurement,conductivity perturbations,dipoles,Distributed computing,EEG,electroencephalography,electro-magnetic source imaging,Estimation error,finite element analysis,finite element method,Finite element methods,magnetic field measurement,magnetic field measurements,Magnetic heads,magnetoencephalography,medical image processing,MEG,numerical head model,Numerical models,physiological models,sensitivity distribution,Sensors,source estimation,voltage measurement,voltage measurements,Volume measurement},
  month     = sep,
  owner     = {afdidehf},
  timestamp = {2016-07-10T08:10:37Z},
  year      = {2003},
}

@Article{Zelnik-Manor2012,
  author    = {Zelnik-Manor, L. and Rosenblum, K. and Eldar, Y.C.},
  title     = {Dictionary Optimization for Block-Sparse Representations},
  doi       = {10.1109/TSP.2012.2187642},
  issn      = {1053-587X},
  number    = {5},
  pages     = {2386-2395},
  volume    = {60},
  abstract  = {Recent work has demonstrated that using a carefully designed dictionary
	instead of a predefined one, can improve the sparsity in jointly
	representing a class of signals. This has motivated the derivation
	of learning methods for designing a dictionary which leads to the
	sparsest representation for a given set of signals. In some applications,
	the signals of interest can have further structure, so that they
	can be well approximated by a union of a small number of subspaces
	(e.g., face recognition and motion segmentation). This implies the
	existence of a dictionary which enables block-sparse representations
	of the input signals once its atoms are properly sorted into blocks.
	In this paper, we propose an algorithm for learning a block-sparsifying
	dictionary of a given set of signals. We do not require prior knowledge
	on the association of signals into groups (subspaces). Instead, we
	develop a method that automatically detects the underlying block
	structure given the maximal size of those groups. This is achieved
	by iteratively alternating between updating the block structure of
	the dictionary and updating the dictionary atoms to better fit the
	data. Our experiments show that for block-sparse data the proposed
	algorithm significantly improves the dictionary recovery ability
	and lowers the representation error compared to dictionary learning
	methods that do not employ block structure.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {Algorithm design and analysis,block-sparse representation,block-sparsifying dictionary,block sparsity,Cost function,Dictionaries,dictionary design,dictionary learning method,dictionary optimization,Learning systems,Matching pursuit algorithms,optimisation,signal reconstruction,signal representation,Sparse coding,Vectors},
  month     = may,
  owner     = {afdidehf},
  timestamp = {2016-07-08T12:11:27Z},
  year      = {2012},
}

@Article{Donoho1989,
  author    = {Donoho, D.L. and Stark, P.B.},
  title     = {Uncertainty Principles and Signal Recovery},
  number    = {3},
  pages     = {906--931},
  volume    = {49},
  abstract  = {The uncertainty principle can easily be generalized to cases where
	the sets of concentration are not intervals. Such generalizations
	are presented for continuous and discrete-time functions, and for
	several measures of concentration (e.g., $L_2 $ and $L_1 $
	measures). The generalizations explain interesting phenomena in signal
	recovery problems where there is an interplay of missing data, sparsity,
	and bandlimiting. Read More: http://epubs.siam.org/doi/abs/10.1137/0149053},
  journal   = {SIAM J. Appl. Math.},
  keywords  = {bandlimiting timelimiting,L1-methods,signal recovery,sparse spike trains,stable recovery,uncertainty principle,unique recovery},
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:12:09Z},
  year      = {1989},
}

@Article{Yan1991,
  author     = {Yan, Y. and Nunez, P. L. and Hart, R. T.},
  title      = {Finite-Element Model of the Human Head: Scalp Potentials Due to Dipole Sources},
  doi        = {10.1007/BF02442317},
  issn       = {0140-0118, 1741-0444},
  language   = {en},
  number     = {5},
  pages      = {475--481},
  urldate    = {2017-08-22},
  volume     = {29},
  abstract   = {Three-dimensional finite-element models provide a method to study the relationship between human scalp potentials and neural current sources inside the brain. A new formulation of dipole-like current sources is developed here. Finiteelement analyses based on this formulation are carried out for both a threeconcentric-spheres model and a human-head model. Differences in calculated scalp potentials between these two models are studied in the context of the forward and inverse problems in EEG. The effects of the eye orbit structure on surface potential distribution are also studied.},
  file       = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DHKDQG8Q\\BF02442317.html:text/html},
  journal    = {Medical and Biological Engineering and Computing},
  month      = sep,
  shorttitle = {Finite-Element Model of the Human Head},
  timestamp  = {2017-08-22T09:45:07Z},
  year       = {1991},
}

@Article{Natarajan1995,
  author    = {Natarajan, B. K.},
  title     = {Sparse Approximate Solutions To Linear Systems},
  number    = {2},
  pages     = {227--234},
  volume    = {24},
  abstract  = {The following problem is considered: given a matrix A in Rm,
	(m rows and n columns), a vector b in Rm, and 6 > 0, compute a vector
	x satisfying IIAx bl[2 <_ 6 if such exists, such that x has the fewest
	number of non-zero entries over all such vectors. It is shown that
	the problem is NP-hard, but that the well-known greedy heuristic
	is good in that it computes a solution with at most [18 Opt(6/Z)llA+
	I1 ln(llbl12/6)] non-zero entries, where Opt(6/2) is the optimum
	number of nonzero entries at error 6/2, A is the matrix obtained
	by normalizing each column of A with respect to the L2 norm, and
	A+ is its pseudo-inverse.},
  journal   = {SIAM J. Comput.},
  keywords  = {Linear systems,sparse solutions},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:18:32Z},
  year      = {1995},
}

@Article{Blumensath2009,
  author    = {Blumensath, T. and Davies, M.E.},
  title     = {Sampling Theorems for Signals From the Union of Finite-Dimensional Linear Subspaces},
  doi       = {10.1109/TIT.2009.2013003},
  issn      = {0018-9448},
  number    = {4},
  pages     = {1872--1882},
  volume    = {55},
  abstract  = {Compressed sensing is an emerging signal acquisition technique that
	enables signals to be sampled well below the Nyquist rate, given
	that the signal has a sparse representation in an orthonormal basis.
	In fact, sparsity in an orthonormal basis is only one possible signal
	model that allows for sampling strategies below the Nyquist rate.
	In this paper, we consider a more general signal model and assume
	signals that live on or close to the union of linear subspaces of
	low dimension. We present sampling theorems for this model that are
	in the same spirit as the Nyquist-Shannon sampling theorem in that
	they connect the number of required samples to certain model parameters.
	Contrary to the Nyquist-Shannon sampling theorem, which gives a necessary
	and sufficient condition for the number of required samples as well
	as a simple linear algorithm for signal reconstruction, the model
	studied here is more complex. We therefore concentrate on two aspects
	of the signal model, the existence of one to one maps to lower dimensional
	observation spaces and the smoothness of the inverse map. We show
	that almost all linear maps are one to one when the observation space
	is at least of the same dimension as the largest dimension of the
	convex hull of the union of any two subspaces in the model. However,
	we also show that in order for the inverse map to have certain smoothness
	properties such as a given finite Lipschitz constant, the required
	observation dimension necessarily depends logarithmically on the
	number of subspaces in the signal model. In other words, while unique
	linear sampling schemes require a small number of samples depending
	only on the dimension of the subspaces involved, in order to have
	stable sampling methods, the number of samples depends necessarily
	logarithmically on the number of subspaces in the model. These results
	are then applied to two examples, the standard compressed sensing
	signal model i- - n which the signal has a sparse representation
	in an orthonormal basis and to a sparse signal model with additional
	tree structure.},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {additional tree structure,Bandwidth,compressed sensing,compressed sensing signal model,Councils,embedding and restricted isometry,finite-dimensional linear subspaces,Focusing,Image reconstruction,linear sampling schemes,Nyquist criterion,Nyquist rate,Nyquist-Shannon sampling theorem,Sampling methods,sampling theorems,signal acquisition,signal processing,signal reconstruction,signal sampling,sparse representation,Sufficient conditions,Tree data structures,trees (mathematics),unions of linear subspaces},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:25:16Z},
  year      = {2009},
}

@InCollection{Vaiter2015,
  author    = {Vaiter, Samuel and Peyr{\'e}, Gabriel and Fadili, Jalal},
  booktitle = {Sampling {{Theory}}, a {{Renaissance}}},
  title     = {Low {{Complexity Regularization}} of {{Linear Inverse Problems}}},
  doi       = {10.1007/978-3-319-19749-4_3},
  editor    = {Pfander, G{\"o}tz E.},
  isbn      = {978-3-319-19748-7 978-3-319-19749-4},
  language  = {en},
  pages     = {103--153},
  publisher = {{Springer International Publishing}},
  series    = {Applied and Numerical Harmonic Analysis},
  urldate   = {2016-09-30},
  abstract  = {Inverse problems and regularization theory is a central theme in imaging sciences, statistics, and machine learning. The goal is to reconstruct an unknown vector from partial indirect, and possibly noisy, measurements of it. A now standard method for recovering the unknown vector is to solve a convex optimization problem that enforces some prior knowledge about its structure. This chapter delivers a review of recent advances in the field where the regularization prior promotes solutions conforming to some notion of simplicity/low complexity. These priors encompass as popular examples sparsity and group sparsity (to capture the compressibility of natural signals and images), total variation and analysis sparsity (to promote piecewise regularity), and low rank (as natural extension of sparsity to matrix-valued data). Our aim is to provide a unified treatment of all these regularizations under a single umbrella, namely the theory of partial smoothness. This framework is very general and accommodates all low complexity regularizers just mentioned, as well as many others. Partial smoothness turns out to be the canonical way to encode low-dimensional models that can be linear spaces or more general smooth manifolds. This review is intended to serve as a one stop shop toward the understanding of the theoretical properties of the so-regularized solutions. It covers a large spectrum including (i) recovery guarantees and stability to noise, both in terms of $\mathscr{l}$ 2-stability and model (manifold) identification; (ii) sensitivity analysis to perturbations of the parameters involved (in particular the observations), with applications to unbiased risk estimation; (iii) convergence properties of the forward-backward proximal splitting scheme that is particularly well suited to solve the corresponding large-scale regularized optimization problem.},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HFCPJVQT\\978-3-319-19749-4_3.html:text/html},
  keywords  = {Appl.Mathematics/Computational Methods of Engineering,Approximations and Expansions,Functions of a Complex Variable,Information and Communication; Circuits,Signal; Image and Speech Processing},
  timestamp = {2016-10-07T14:20:53Z},
  year      = {2015},
}

@Article{Amaldi1998,
  author    = {Amaldi, Edoardo and Kann, Viggo},
  title     = {On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems},
  doi       = {10.1016/S0304-3975(97)00115-1},
  issn      = {0304-3975},
  number    = {1–2},
  pages     = {237--260},
  urldate   = {2016-03-30},
  volume    = {209},
  abstract  = {We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (Min ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (Min RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both Min ULR and Min RVLS the four basic types of relational operators =, ⩾, &gt; and ≠ are considered. While Min RVLS with equations was mentioned to be NP-hard in (Garey and Johnson, 1979), we established in (Amaldi; 1992; Amaldi and Kann, 1995) that min ULR with equalities and inequalities are NP-hard even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in (Arora et al., 1993). In this paper we determine strong bounds on the approximability of various variants of Min RVLS and min ULR, including constrained ones where the variables are restricted to take binary values or where some relations are mandatory while others are optional. The various NP-hard versions turn out to have different approximability properties depending on the type of relations and the additional constraints, but none of them can be approximated within any constant factor, unless P = NP. Particular attention is devoted to two interesting special cases that occur in discriminant analysis and machine learning. In particular, we disprove a conjecture of van Horn and Martinez (1992) regarding the existence of a polynomial-time algorithm to design linear classifiers (or perceptrons) that involve a close-to-minimum number of features.},
  file      = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Q9NIJNQ8\\S0304397597001151.html:text/html},
  journal   = {Theoretical Computer Science},
  keywords  = {Approximability bounds,Approximability bounds,Designing linear classifiers,Designing linear classifiers,Linear systems,Linear systems,Nonzero variables,Nonzero variables,Unsatisfied relations,Unsatisfied relations},
  month     = dec,
  timestamp = {2016-09-30T10:42:52Z},
  year      = {1998},
}

@Article{Gencer2004,
  author    = {Gen{\c c}er, Nevzat G. and Acar, Can E.},
  title     = {Sensitivity of {{EEG}} and {{MEG}} Measurements to Tissue Conductivity},
  doi       = {10.1088/0031-9155/49/5/004},
  issn      = {0031-9155},
  language  = {en},
  number    = {5},
  pages     = {701},
  urldate   = {2016-11-22},
  volume    = {49},
  abstract  = {Monitoring the electrical activity inside the human brain using electrical and magnetic field measurements requires a mathematical head model. Using this model the potential distribution in the head and magnetic fields outside the head are computed for a given source distribution. This is called the forward problem of the electro-magnetic source imaging. Accurate representation of the source distribution requires a realistic geometry and an accurate conductivity model. Deviation from the actual head is one of the reasons for the localization errors. In this study, the mathematical basis for the sensitivity of voltage and magnetic field measurements to perturbations from the actual conductivity model is investigated. Two mathematical expressions are derived relating the changes in the potentials and magnetic fields to conductivity perturbations. These equations show that measurements change due to secondary sources at the perturbation points. A finite element method (FEM) based formulation is developed for computing the sensitivity of measurements to tissue conductivities efficiently. The sensitivity matrices are calculated for both a concentric spheres model of the head and a realistic head model. The rows of the sensitivity matrix show that the sensitivity of a voltage measurement is greater to conductivity perturbations on the brain tissue in the vicinity of the dipole, the skull and the scalp beneath the electrodes. The sensitivity values for perturbations in the skull and brain conductivity are comparable and they are, in general, greater than the sensitivity for the scalp conductivity. The effects of the perturbations on the skull are more pronounced for shallow dipoles, whereas, for deep dipoles, the measurements are more sensitive to the conductivity of the brain tissue near the dipole. The magnetic measurements are found to be more sensitive to perturbations near the dipole location. The sensitivity to perturbations in the brain tissue is much greater when the primary source is tangential and it decreases as the dipole depth increases. The resultant linear system of equations can be used to update the initially assumed conductivity distribution for the head. They may be further exploited to image the conductivity distribution of the head from EEG and/or MEG measurements. This may be a fast and promising new imaging modality.},
  journal   = {Physics in Medicine and Biology},
  timestamp = {2016-11-22T12:42:32Z},
  year      = {2004},
}

@Article{Blumensath2008,
  author    = {Blumensath, Thomas and Davies, Mike E.},
  title     = {Iterative {{Thresholding}} for {{Sparse Approximations}}},
  doi       = {10.1007/s00041-008-9035-z},
  issn      = {1069-5869, 1531-5851},
  language  = {en},
  number    = {5-6},
  pages     = {629--654},
  urldate   = {2016-10-07},
  volume    = {14},
  abstract  = {Sparse signal expansions represent or approximate a signal using a small number of elements from a large collection of elementary waveforms. Finding the optimal sparse expansion is known to be NP hard in general and non-optimal strategies such as Matching Pursuit, Orthogonal Matching Pursuit, Basis Pursuit and Basis Pursuit De-noising are often called upon. These methods show good performance in practical situations, however, they do not operate on the $\mathscr{l}$0 penalised cost functions that are often at the heart of the problem. In this paper we study two iterative algorithms that are minimising the cost functions of interest. Furthermore, each iteration of these strategies has computational complexity similar to a Matching Pursuit iteration, making the methods applicable to many real world problems. However, the optimisation problem is non-convex and the strategies are only guaranteed to find local solutions, so good initialisation becomes paramount. We here study two approaches. The first approach uses the proposed algorithms to refine the solutions found with other methods, replacing the typically used conjugate gradient solver. The second strategy adapts the algorithms and we show on one example that this adaptation can be used to achieve results that lie between those obtained with Matching Pursuit and those found with Orthogonal Matching Pursuit, while retaining the computational complexity of the Matching Pursuit algorithm.},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NS8PACQI\\s00041-008-9035-z.html:text/html},
  journal   = {Journal of Fourier Analysis and Applications},
  month     = sep,
  timestamp = {2016-10-07T11:27:57Z},
  year      = {2008},
}

@Article{Hadamard1902,
  author    = {Hadamard, Jacques},
  title     = {Sur les probl{\`e}mes aux d{\'e}riv{\'e}s partielles et leur signification physique},
  pages     = {49--52},
  volume    = {13},
  journal   = {Princeton Univ.},
  keywords  = {Ill-posed,problem},
  timestamp = {2017-04-19T14:24:06Z},
  year      = {1902},
}

@InProceedings{Elhamifar2011,
  author    = {Elhamifar, E. and Vidal, R.},
  booktitle = {2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Robust classification using structured sparse representation},
  doi       = {10.1109/CVPR.2011.5995664},
  pages     = {1873--1879},
  abstract  = {In many problems in computer vision, data in multiple classes lie in multiple low-dimensional subspaces of a high-dimensional ambient space. However, most of the existing classification methods do not explicitly take this structure into account. In this paper, we consider the problem of classification in the multi-sub space setting using sparse representation techniques. We exploit the fact that the dictionary of all the training data has a block structure where the training data in each class form few blocks of the dictionary. We cast the classification as a structured sparse recovery problem where our goal is to find a representation of a test example that uses the minimum number of blocks from the dictionary. We formulate this problem using two different classes of non-convex optimization programs. We propose convex relaxations for these two non-convex programs and study conditions under which the relaxations are equivalent to the original problems. In addition, we show that the proposed optimization programs can be modified properly to also deal with corrupted data. To evaluate the proposed algorithms, we consider the problem of automatic face recognition. We show that casting the face recognition problem as a structured sparse recovery problem can improve the results of the state-of-the-art face recognition algorithms, especially when we have relatively small number of training data for each class. In particular, we show that the new class of convex programs can improve the state-of-the-art face recognition results by 10% with only 25% of the training data. In addition, we show that the algorithms are robust to occlusion, corruption, and disguise.},
  keywords  = {automatic face recognition,automatic face recognition,computer vision,computer vision,concave programming,concave programming,convex relaxations,convex relaxations,convex relaxations,Dictionaries,Dictionaries,Face,Face,face recognition,face recognition,image classification,image classification,image representation,image representation,image representation,multi-sub space setting,multi-sub space setting,nonconvex optimization programs,nonconvex optimization programs,Optimization,Optimization,robust classification,robust classification,robust classification,structured sparse representation,structured sparse representation,Training data,Training data,Vectors,Vectors},
  month     = jun,
  timestamp = {2016-09-30T11:45:25Z},
  year      = {2011},
}

@Article{Zhao2015a,
  author    = {Zhao, Junxi and Song, Rongfang and Zhao, Jie and Zhu, Wei-Ping},
  title     = {New Conditions for Uniformly Recovering Sparse Signals via Orthogonal Matching Pursuit},
  doi       = {10.1016/j.sigpro.2014.06.010},
  issn      = {0165-1684},
  pages     = {106--113},
  urldate   = {2016-09-23},
  volume    = {106},
  abstract  = {Recently, lots of work has been done on conditions of guaranteeing sparse signal recovery using orthogonal matching pursuit (OMP). However, none of the existing conditions is both necessary and sufficient in terms of the so-called restricted isometric property, coherence, cumulative coherence (Babel function), or other verifiable quantities in the literature. Motivated by this observation, we propose a new measure of a matrix, named as union cumulative coherence, and present both sufficient and necessary conditions under which the OMP algorithm can uniformly recover sparse signals for all sensing matrices. The proposed condition guarantees a uniform recovery of sparse signals using OMP, and reveals the capability of OMP in sparse recovery. We demonstrate by examples that the proposed condition can be used to more effectively determine the recoverable sparse signals via OMP than the conditions existing in the literature. Furthermore, sparse recovery from noisy measurements is also considered in terms of the proposed union cumulative coherence.},
  annote    = {read},
  file      = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\EWZKTPKK\\S016516841400276X.html:text/html},
  journal   = {Signal Processing},
  keywords  = {compressive sensing,Cumulative coherence,orthogonal matching pursuit,Restricted isometric property,sparse signal},
  month     = jan,
  timestamp = {2016-09-30T13:51:59Z},
  year      = {2015},
}

@InProceedings{Elhamifar2010,
  author    = {Elhamifar, E. and Vidal, R.},
  booktitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  title     = {Clustering disjoint subspaces via sparse representation},
  doi       = {10.1109/ICASSP.2010.5495317},
  pages     = {1926--1929},
  abstract  = {Given a set of data points drawn from multiple low-dimensional linear subspaces of a high-dimensional space, we consider the problem of clustering these points according to the subspaces they belong to. Our approach exploits the fact that each data point can be written as a sparse linear combination of all the other points. When the subspaces are independent, the sparse coefficients can be found by solving a linear program. However, when the subspaces are disjoint, but not independent, the problem becomes more challenging. In this paper, we derive theoretical bounds relating the principal angles between the subspaces and the distribution of the data points across all the subspaces under which the coefficients are guaranteed to be sparse. The clustering of the data is then easily obtained from the sparse coefficients. We illustrate the validity of our results through simulation experiments.},
  keywords  = {Application software,Application software,Clustering methods,Clustering methods,computer vision,computer vision,data clustering,data clustering,disjoint subspace clustering,disjoint subspace clustering,disjoint subspace clustering,disjoint subspaces,disjoint subspaces,Image Processing,Image processing,Image segmentation,Image segmentation,pattern clustering,pattern clustering,pattern clustering,signal processing,signal processing,sparse coefficients,sparse coefficients,sparse linear combination,sparse linear combination,sparse matrices,sparse matrices,Sparse matrices,sparse representation,sparse representation,Sparsity,sparsity,Statistical analysis,statistical analysis,subspace angles,subspace angles,subspace angles,Subspace clustering,subspace clustering,Video sequences,video sequences},
  month     = mar,
  timestamp = {2016-09-30T13:27:54Z},
  year      = {2010},
}

@Article{Donoho2005b,
  author    = {Donoho, David L.},
  title     = {High-Dimensional Centrally Symmetric Polytopes with Neighborliness Proportional to Dimension},
  doi       = {10.1007/s00454-005-1220-0},
  issn      = {0179-5376, 1432-0444},
  language  = {en},
  number    = {4},
  pages     = {617--652},
  urldate   = {2016-05-15},
  volume    = {35},
  journal   = {Discrete and Computational Geometry},
  keywords  = {Combinatorics,Combinatorics,Computational Mathematics and Numerical Analysis,Computational Mathematics and Numerical Analysis},
  month     = dec,
  timestamp = {2016-10-07T14:42:32Z},
  year      = {2005},
}

@Article{Elhamifar2013,
  author     = {Elhamifar, E. and Vidal, R.},
  title      = {Sparse Subspace Clustering: Algorithm, Theory, and Applications},
  doi        = {10.1109/TPAMI.2013.57},
  issn       = {0162-8828},
  number     = {11},
  pages      = {2765--2781},
  volume     = {35},
  abstract   = {Many real-world problems deal with collections of high-dimensional data, such as images, videos, text, and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efficient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.},
  journal    = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords   = {$(ell_1)$-minimization,$(ell_1)$-minimization,Algorithms,algorithms,Artificial Intelligence,Artificial Intelligence,Automated,Biometry,Biometry,clustering,clustering,Clustering algorithms,Clustering algorithms,Clustering algorithms,computational complexity,computational complexity,Computer-Assisted,computer vision,computer vision,convex programming,convex programming,convex programming,convex relaxation,convex relaxation,data point clustering,data point clustering,data point representation,data point representation,data structures,data structures,data structures,Face,Face,face clustering,face clustering,general NP-hard problem,general NP-hard problem,High-dimensional data,high-dimensional data,high-dimensional data,high-dimensional data collection,high-dimensional data collection,Humans,Humans,Image Interpretation,Image Interpretation; Computer-Assisted,intrinsic low-dimensionality,intrinsic low-dimensionality,intrinsic low-dimensionality,minimisation,minimisation,minimization program,minimization program,motion segmentation,motion segmentation,Noise,Noise,Optimization,Optimization,pattern clustering,pattern clustering,pattern clustering,Pattern Recognition,Pattern Recognition; Automated,principal angles,principal angles,Sample Size,Sample Size,sparse matrices,sparse matrices,Sparse matrices,sparse optimization program,sparse optimization program,sparse representation,sparse representation,sparse subspace clustering algorithm,sparse subspace clustering algorithm,sparse subspace clustering algorithm,spectral clustering,spectral clustering,spectral clustering framework,spectral clustering framework,spectral clustering framework,subspaces,subspaces,synthetic data,synthetic data,Vectors,Vectors},
  month      = nov,
  shorttitle = {Sparse Subspace Clustering},
  timestamp  = {2016-09-30T13:27:00Z},
  year       = {2013},
}

@Article{Chen2006,
  author    = {Chen, Jie and Huo, X.},
  title     = {Theoretical Results on Sparse Representations of Multiple-Measurement Vectors},
  doi       = {10.1109/TSP.2006.881263},
  issn      = {1053-587X},
  number    = {12},
  pages     = {4634--4643},
  volume    = {54},
  abstract  = {The sparse representation of a multiple-measurement vector (MMV) is
	a relatively new problem in sparse representation. Efficient methods
	have been proposed. Although many theoretical results that are available
	in a simple case-single-measurement vector (SMV)-the theoretical
	analysis regarding MMV is lacking. In this paper, some known results
	of SMV are generalized to MMV. Some of these new results take advantages
	of additional information in the formulation of MMV. We consider
	the uniqueness under both an lscr0-norm-like criterion and an lscr1-norm-like
	criterion. The consequent equivalence between the lscr0-norm approach
	and the lscr1-norm approach indicates a computationally efficient
	way of finding the sparsest representation in a redundant dictionary.
	For greedy algorithms, it is proven that under certain conditions,
	orthogonal matching pursuit (OMP) can find the sparsest representation
	of an MMV with computational efficiency, just like in SMV. Simulations
	show that the predictions made by the proved theorems tend to be
	very conservative; this is consistent with some recent advances in
	probabilistic analysis based on random matrix theory. The connections
	will be discussed},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {Analytical models,Basis pursuit,Computational efficiency,Computational modeling,Dictionaries,Equations,Greedy algorithms,iterative methods,l0-norm-like criterion,l1-norm-like criterion,Magnetic analysis,Matching pursuit algorithms,matrix algebra,multiple-measurement vector (MMV),multiple-measurement vectors,orthogonal matching pursuit,orthogonal matching pursuit (OMP),Predictive models,probabilistic analysis,random matrix theory,redundant dictionary,signal representation,single-measurement vector,sparse matrices,sparse representation,sparse representations,statistical analysis,time-frequency analysis},
  month     = dec,
  owner     = {Fardin},
  timestamp = {2016-09-30T11:20:03Z},
  year      = {2006},
}

@InProceedings{Kwon2012,
  author    = {Kwon, H. and Rao, B.D.},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on},
  title     = {On the benefits of the block-sparsity structure in sparse signal recovery},
  doi       = {10.1109/ICASSP.2012.6288716},
  pages     = {3685-3688},
  abstract  = {We study the problem of support recovery of block-sparse signals,
	where nonzero entries occur in clusters, via random noisy measurements.
	By drawing analogy between the problem of block-sparse signal recovery
	and the problem of communication over Gaussian multi-input and single-output
	multiple access channel, we derive the sufficient and necessary condition
	under which exact support recovery is possible. Based on the results,
	we show that block-sparse signals can reduce the number of measurements
	required for exact support recovery, by at least `1/(block size)',
	compared to conventional or scalar-sparse signals. The minimum gain
	is guaranteed by increased signal to noise power ratio (SNR) and
	reduced effective number of entries (i.e., not individual elements
	but blocks) that are dominant at low SNR and at high SNR, respectively.
	When the correlation between the elements of each nonzero block is
	low, a larger gain than `1/(block size)' is expected due to, so called,
	diversity effect, especially in the moderate and low SNR regime.},
  annote    = {read},
  keywords  = {Block-sparse signal recovery,block-sparse signals,block-sparsity structure,Brain modeling,channel capacity,Gaussian channels,Gaussian multiinput and single-output multiple access channel,MIMO communication,MISO-MAC channel capacity,MISOMAC channel capacity,MISOMAC channel capacity,MISO-MAC channel capacity,multi-access systems,Noise measurement,random noisy measurements,Receivers,scalar-sparse signals,signal reconstruction,signal to noise power ratio,Signal to noise ratio,Size measurement,SNR,support recovery,Vectors},
  month     = mar,
  owner     = {afdidehf},
  timestamp = {2016-07-10T07:15:35Z},
  year      = {2012},
}

@PhdThesis{Elhamifar2012,
  author    = {Elhamifar, Ehsan},
  title     = {Sparse modeling for high-dimensional multi-manifold data analysis},
  abstract  = {High-dimensional data are ubiquitous in many areas of science and
	engineering, such as machine learning, signal and image processing,
	computer vision, pattern recognition, bioinformatics, etc. Often,
	high-dimensional data are not distributed uniformly in the ambient
	space; instead they lie in or close to a union of low-dimensional
	manifolds. Recovering such low-dimensional structures in the data
	helps to not only significantly reduce the computational cost and
	memory requirements of algorithms that deal with the data, but also
	reduce the effect of the high-dimensional noise in the data and improve
	the performance of inference and learning tasks. There are three
	fundamental tasks related to the multi-manifold data: clustering,
	dimensionality reduction, and classification. While the area of machine
	learning has seen great advances in these areas, the applicability
	of current algorithms are limited due to several challenges. First,
	in many problems, manifolds are spatially close or even intersect,
	while existing methods work only when manifolds are sufficiently
	separated. Second, most algorithms require to know the dimensions
	or the number of manifolds a priori, while in real-world problems
	such quantities are often unknown. Third, most existing algorithms
	have difficulty in effectively dealing with data nuisances, such
	as noise, outliers, and missing entries, as well as manifolds of
	different intrinsic dimensions. In this thesis, we present new frameworks
	based on sparse representation techniques for the problems of clustering,
	dimensionality reduction and classification of multi-manifold data
	that effectively address the aforementioned challenges. The key idea
	behind the proposed algorithms is what we call the self-expressiveness
	property of the data. This property states that in an appropriate
	dictionary formed from the given data points in multiple manifolds,
	a sparse representation of a data point corresponds to selecting
	other points from the same manifold. Our goal is then to search for
	such sparse representations and use them in appropriate frameworks
	to cluster, embed, and classify multi-manifold data. We propose sparse
	optimization programs to find such desired representations and develop
	theoretical guarantees for the success of the proposed algorithms.
	By extensive experiments on synthetic and real data, we demonstrate
	that the proposed algorithms significantly improve the state-of-the-art
	results.},
  address   = {United States -- Maryland},
  annote    = {read},
  keywords  = {Applied sciences,block-sparse recovery,block-sparse representation,embedding,face clustering,face recognition,Manifold learning,motion segmentation,multi-manifold classification,multi-manifold clustering,Multi-manifold data,multi-manifold dimensionality reduction,multi-manifold embedding,Pure sciences,real data,Sparse Manifold Clustering,sparse representation,sparse subspace clustering,subspace clustering,Subspace segmentation,Subspace-sparse recovery theory,synthetic data},
  owner     = {Fardin},
  school    = {The Johns Hopkins University},
  timestamp = {2016-07-11T16:49:03Z},
  year      = {2012},
}

@Article{Foucart2009a,
  author    = {Foucart, Simon and Lai, Ming-Jun},
  title     = {Sparsest solutions of underdetermined linear systems via $\ell_q $-minimization for $0 < q \leq 1$},
  doi       = {http://dx.doi.org/10.1016/j.acha.2008.09.001},
  issn      = {1063-5203},
  number    = {3},
  pages     = {395--407},
  volume    = {26},
  abstract  = {We present a condition on the matrix of an underdetermined linear
	system which guarantees that the solution of the system with minimal
	l q -quasinorm is also the sparsest one. This generalizes, and slightly
	improves, a similar result for the l 1 -norm. We then introduce a
	simple numerical scheme to compute solutions with minimal l q -quasinorm,
	and we study its convergence. Finally, we display the results of
	some experiments which indicate that the l q -method performs better
	than other available methods.},
  annote    = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882 Applied and Computational Harmonic Analysis},
  journal   = {App. Comput. Harmonic Anal.},
  owner     = {Fardin},
  timestamp = {2016-09-29T16:34:37Z},
  year      = {2009},
}

@Article{Ling2013,
  author    = {Ling, Qing and Wen, Zaiwen and Yin, Wotao},
  title     = {Decentralized Jointly Sparse Optimization by Reweighted $\ell _{q}$ Minimization},
  doi       = {10.1109/TSP.2012.2236830},
  issn      = {1053-587X},
  number    = {5},
  pages     = {1165--1170},
  volume    = {61},
  abstract  = {A set of vectors (or signals) are jointly sparse if all their nonzero
	entries are found on a small number of rows (or columns). Consider
	a network of agents {i} that collaboratively recover a set of jointly
	sparse vectors {x(i)} from their linear measurements {y(i)}. Assume
	that every agent i collects its own measurement y(i) and aims to
	recover its own vector x(i) taking advantages of the joint sparsity
	structure. This paper proposes novel decentralized algorithms to
	recover these vectors in a way that every agent runs a recovery algorithm
	and exchanges with its neighbors only the estimated joint support
	of the vectors. The agents will obtain their solutions through collaboration
	while keeping their vectors' values and measurements private. As
	such, the proposed approach finds applications in distributed human
	action recognition, cooperative spectrum sensing, decentralized event
	detection, as well as collaborative data mining. We use a non-convex
	minimization model and propose algorithms that alternate between
	support consensus and vector update. The latter step is based on
	reweighted ?q iterations, where q can be 1 or 2. We numerically compare
	the proposed decentralized algorithms with existing centralized and
	decentralized algorithms. Simulation results demonstrate that the
	proposed decentralized approaches have strong recovery performance
	and converge reasonably fast.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {cognitive radio,compressed sensing,convex programming,Data mining,Data mining,Decentralized algorithm,decentralized jointly sparse optimization,distributed human action recognition,jointly sparse optimization,Joints,joint support estimation,linear measurements,minimisation,Minimization,nonconvex minimization model,non-convex model,Optimization,reweighted ?q minimization,Sensors,support consensus,Support vector machines,Vectors},
  month     = mar,
  owner     = {Fardin},
  timestamp = {2016-09-30T11:24:07Z},
  year      = {2013},
}

@Article{Eldar2010b,
  author        = {Eldar, Y.C. and Kuppinger, P. and B{\"o}lcskei, H.},
  title         = {Compressed {{Sensing}} of {{Block}}-{{Sparse Signals}}: {{Uncertainty Relations}} and {{Efficient Recovery}}},
  doi           = {10.1109/TSP.2010.2044837},
  eprint        = {0906.3173},
  eprinttype    = {arxiv},
  issn          = {1053-587X, 1941-0476},
  number        = {6},
  pages         = {3042--3054},
  urldate       = {2016-09-30},
  volume        = {58},
  abstract      = {We consider compressed sensing of block-sparse signals, i.e., sparse signals that have nonzero coefficients occurring in clusters. An uncertainty relation for block-sparse signals is derived, based on a block-coherence measure, which we introduce. We then show that a block-version of the orthogonal matching pursuit algorithm recovers block \$k\$-sparse signals in no more than \$k\$ steps if the block-coherence is sufficiently small. The same condition on block-coherence is shown to guarantee successful recovery through a mixed \$$\backslash$ell\_2/$\backslash$ell\_1\$-optimization approach. This complements previous recovery results for the block-sparse case which relied on small block-restricted isometry constants. The significance of the results presented in this paper lies in the fact that making explicit use of block-sparsity can provably yield better reconstruction properties than treating the signal as being sparse in the conventional sense, thereby ignoring the additional structure in the problem.},
  annote        = {Comment: Submitted to the IEEE Trans. on Signal Processing, version 2 has updated figures},
  archiveprefix = {arXiv},
  file          = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3NEW3R3M\\0906.html:text/html},
  journal       = {IEEE Transactions on Signal Processing},
  keywords      = {Computer Science - Information Theory},
  month         = jun,
  shorttitle    = {Compressed {{Sensing}} of {{Block}}-{{Sparse Signals}}},
  timestamp     = {2016-09-30T13:36:29Z},
  year          = {2010},
}

@Article{Baraniuk2008,
  author    = {Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
  title     = {A Simple Proof of the Restricted Isometry Property for Random Matrices},
  doi       = {10.1007/s00365-007-9003-x},
  issn      = {0176-4276},
  language  = {English},
  number    = {3},
  pages     = {253--263},
  volume    = {28},
  abstract  = {We give a simple technique for verifying the Restricted Isometry Property
	(as introduced by Candes and Tao) for random matrices that underlies
	Compressed Sensing. Our approach has two main ingredients: (i) concentration
	inequalities for random inner products that have recently provided
	algorithmically simple proofs of the JohnsonLindenstrauss lemma;
	and (ii) covering numbers for finite-dimensional balls in Euclidean
	space. This leads to an elementary proof of the Restricted Isometry
	Property and brings out connections between Compressed Sensing and
	the Johnson Lindenstrauss lemma. As a result, we obtain simple
	and direct proofs of Kashins theorems on widths of finite balls
	in Euclidean space (and their improvements due to Gluskin) and proofs
	of the existence of optimal Compressed Sensing measurement matrices.
	In the process, we also prove that these measurements have a certain
	universality with respect to the sparsity-inducing basis.},
  journal   = {Constructive Approximation},
  keywords  = {15A52,15N2,60F10,94A12,94A20,compressed sensing,Concentration inequalities,random matrices,Sampling},
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:03:35Z},
  year      = {2008},
}

@Article{Eldar2010a,
  author    = {Eldar, Y.C. and Rauhut, H.},
  title     = {Average Case Analysis of Multichannel Sparse Recovery Using Convex Relaxation},
  doi       = {10.1109/TIT.2009.2034789},
  issn      = {0018-9448},
  number    = {1},
  pages     = {505--519},
  volume    = {56},
  abstract  = {This paper considers recovery of jointly sparse multichannel signals
	from incomplete measurements. Several approaches have been developed
	to recover the unknown sparse vectors from the given observations,
	including thresholding, simultaneous orthogonal matching pursuit
	(SOMP), and convex relaxation based on a mixed matrix norm. Typically,
	worst case analysis is carried out in order to analyze conditions
	under which the algorithms are able to recover any jointly sparse
	set of vectors. However, such an approach is not able to provide
	insights into why joint sparse recovery is superior to applying standard
	sparse reconstruction methods to each channel individually. Previous
	work considered an average case analysis of thresholding and SOMP
	by imposing a probability model on the measured signals. Here, the
	main focus is on analysis of convex relaxation techniques. In particular,
	the mixed l 2,1 approach to multichannel recovery is investigated.
	Under a very mild condition on the sparsity and on the dictionary
	characteristics, measured for example by the coherence, it is shown
	that the probability of recovery failure decays exponentially in
	the number of channels. This demonstrates that most of the time,
	multichannel sparse recovery is indeed superior to single channel
	methods. The probability bounds are valid and meaningful even for
	a small number of signals. Using the tools developed to analyze the
	convex relaxation technique, also previous bounds for thresholding
	and SOMP recovery are tightened.},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {Algorithm design and analysis,average case analysis,Average performance,convex relaxation techniques,Dictionaries,incomplete measurements,jointly sparse multichannel signals,joint sparse recovery,Matching pursuit algorithms,Mathematics,mixed matrix norm,mixed-norm optimization,multichannel recovery,multichannel sparse recovery,probability,probability bounds,probability model,Radar signal processing,Reconstruction algorithms,recovery failure decays,relaxation theory,Signal analysis,Signal processing algorithms,signal reconstruction,simultaneous orthogonal matching pursuit,simultaneous orthogonal matching pursuit,single channel methods,sparse matrices,sparse vectors,standard sparse reconstruction methods,thresholding,Vectors},
  month     = jan,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:23:15Z},
  year      = {2010},
}

@Article{Cand`es2006b,
  author    = {Cand{\`e}s, E.J. and Tao, T.},
  title     = {Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?},
  doi       = {10.1109/TIT.2006.885507},
  issn      = {0018-9448},
  number    = {12},
  pages     = {5406--5425},
  volume    = {52},
  abstract  = {Suppose we are given a vector f in a class FsubeRopfN , e.g., a class
	of digital signals or digital images. How many linear measurements
	do we need to make about f to be able to recover f to within precision
	epsi in the Euclidean (lscr2) metric? This paper shows that if the
	objects of interest are sparse in a fixed basis or compressible,
	then it is possible to reconstruct f to within very high accuracy
	from a small number of random measurements by solving a simple linear
	program. More precisely, suppose that the nth largest entry of the
	vector f (or of its coefficients in a fixed basis) obeys ,
	where R0 and p0. Suppose that we take measurements yk=langf ,Xkrang,k=1,...,K,
	where the Xk are N-dimensional Gaussian vectors with independent
	standard normal entries. Then for each f obeying the decay estimate
	above for some 0<p<1 and with overwhelming probability, our reconstruction
	ft, defined as the solution to the constraints yk=langf ,Xkrang
	with minimal lscr1 norm, obeys parf-fparlscr2lesCp middotRmiddot(K/logN)-r,
	r=1/p-1/2. There is a sense in which this result is optimal; it is
	generally impossible to obtain a higher accuracy from any set of
	K measurements whatsoever. The methodology extends to various other
	random measurement ensembles; for example, we show that similar results
	hold if one observes a few randomly sampled Fourier coefficients
	of f. In fact, the results are quite general and require only two
	hypotheses on the measurement ensemble which are detailed},
  annote    = {Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Concentration of measure,Concrete,convex optimization,Digital images,duality in optimization,encoding,Geometry,image coding,Image reconstruction,linear measurement,linear program,linear programming,Mathematics,Measurement standards,N-dimensional Gaussian vector,Random matrices,random projection,random projections,signal reconstruction,signal recovery,singular values of random matrices,singular values of random matrices,Sparsity,trigonometric expansions,uncertainty principle,universal encoding strategy,Vectors},
  month     = dec,
  owner     = {Fardin},
  timestamp = {2017-06-23T13:28:23Z},
  year      = {2006},
}

@Article{Welch1974,
  author    = {Welch, L.},
  title     = {Lower Bounds on the Maximum Cross Correlation of Signals ({{Corresp}}.)},
  doi       = {10.1109/TIT.1974.1055219},
  issn      = {0018-9448},
  number    = {3},
  pages     = {397--399},
  volume    = {20},
  abstract  = {Some communication systems require sets of signals with impulse-like autocorrelation functions and small cross correlation. There is considerable literature on signals with impulse-like autocorrelation functions hut little on sets of signals with small cross correlation. A possible reason is that designers put too severe a restriction on cross correlation magnitudes. This correspondence establishes lower bounds on how small the cross correlation and autocorrelation can simultaneously be.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5K9JIBV6\\articleDetails.html:text/html},
  journal   = {IEEE Transactions on Information Theory},
  keywords  = {Communication systems,Correlation functions,Detectors,Matched filters,Sampling methods,Signal design,Timing},
  month     = may,
  timestamp = {2016-09-30T13:50:43Z},
  year      = {1974},
}

@Article{Lustig2007,
  author    = {Lustig, Michael and Donoho, David and Pauly, John M.},
  title     = {Sparse MRI: The application of compressed sensing for rapid MR imaging},
  number    = {6},
  pages     = {1182-95},
  volume    = {53},
  abstract  = {The sparsity which is implicit in MR images is exploited to significantly
	undersample k-space. Some MR images such as angiograms are already
	sparse in the pixel representation; other, more complicated images
	have a sparse representation in some transform domain-for example,
	in terms of spatial finite-differences or their wavelet coefficients.
	According to the recently developed mathematical theory of compressed-sensing,
	images with a sparse representation can be recovered from randomly
	undersampled k-space data, provided an appropriate nonlinear recovery
	scheme is used. Intuitively, artifacts due to random undersampling
	add as noise-like interference. In the sparse transform domain the
	significant coefficients stand out above the interference. A nonlinear
	thresholding scheme can recover the sparse coefficients, effectively
	recovering the image itself. In this article, practical incoherent
	undersampling schemes are developed and analyzed by means of their
	aliasing interference. Incoherence is introduced by pseudo-random
	variable-density undersampling of phase-encodes. The reconstruction
	is performed by minimizing the l(1) norm of a transformed image,
	subject to data fidelity constraints. Examples demonstrate improved
	spatial resolution and accelerated acquisition for multislice fast
	spin-echo brain imaging and 3D contrast enhanced angiography},
  journal   = {Magnetic Resonance in Medicine},
  owner     = {afdidehf},
  timestamp = {2016-07-11T16:49:31Z},
  year      = {2007},
}

@Article{Figueiredo2003,
  author    = {Figueiredo, M. A. T. and Nowak, R. D.},
  title     = {An {{EM}} Algorithm for Wavelet-Based Image Restoration},
  doi       = {10.1109/TIP.2003.814255},
  issn      = {1057-7149},
  number    = {8},
  pages     = {906--916},
  volume    = {12},
  abstract  = {This paper introduces an expectation-maximization (EM) algorithm for image restoration (deconvolution) based on a penalized likelihood formulated in the wavelet domain. Regularization is achieved by promoting a reconstruction with low-complexity, expressed in the wavelet coefficients, taking advantage of the well known sparsity of wavelet representations. Previous works have investigated wavelet-based restoration but, except for certain special cases, the resulting criteria are solved approximately or require demanding optimization methods. The EM algorithm herein proposed combines the efficient image representation offered by the discrete wavelet transform (DWT) with the diagonalization of the convolution operator obtained in the Fourier domain. Thus, it is a general-purpose approach to wavelet-based image restoration with computational complexity comparable to that of standard wavelet denoising schemes or of frequency domain deconvolution methods. The algorithm alternates between an E-step based on the fast Fourier transform (FFT) and a DWT-based M-step, resulting in an efficient iterative process requiring O(NlogN) operations per iteration. The convergence behavior of the algorithm is investigated, and it is shown that under mild conditions the algorithm converges to a globally optimal restoration. Moreover, our new approach performs competitively with, in some cases better than, the best existing methods in benchmark tests.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\52XNQU8P\\1217267.html:text/html},
  journal   = {IEEE Transactions on Image Processing},
  keywords  = {benchmark tests,computational complexity,convolution operator,Deconvolution,Discrete Fourier transforms,discrete wavelet transform,Discrete wavelet transforms,EM algorithm,expectation-maximization algorithm,fast Fourier transform,fast Fourier transforms,frequency domain deconvolution methods,globally optimal restoration,image reconstruction,image representation,Image restoration,Iterative algorithms,iterative process,Optimization methods,penalized likelihood,sparsity,wavelet-based image restoration,wavelet coefficients,wavelet denoising schemes,Wavelet domain},
  month     = aug,
  timestamp = {2016-10-14T11:37:22Z},
  year      = {2003},
}

@Article{Cai2009,
  author    = {Cai, T.T. and Xu, Guangwu and Zhang, Jun},
  title     = {On Recovery of Sparse Signals Via $\ell_1$ Minimization},
  doi       = {10.1109/TIT.2009.2021377},
  issn      = {0018-9448},
  number    = {7},
  pages     = {3388--3397},
  volume    = {55},
  abstract  = {This paper considers constrained lscr1 minimization methods in a unified
	framework for the recovery of high-dimensional sparse signals in
	three settings: noiseless, bounded error, and Gaussian noise. Both
	lscr1 minimization with an lscrinfin constraint (Dantzig selector)
	and lscr1 minimization under an llscr2 constraint are considered.
	The results of this paper improve the existing results in the literature
	by weakening the conditions and tightening the error bounds. The
	improvement on the conditions shows that signals with larger support
	can be recovered accurately. In particular, our results illustrate
	the relationship between lscr1 minimization with an llscr2 constraint
	and lscr1 minimization with an lscrinfin constraint. This paper also
	establishes connections between restricted isometry property and
	the mutual incoherence property. Some results of Candes, Romberg,
	and Tao (2006), Candes and Tao (2007), and Donoho, Elad, and Temlyakov
	(2006) are extended.},
  annote    = {read},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {compressed sensing,constrained minimization methods,Dantzig selector,Dantzig selector$ell _{1} $ minimization,Equations,error bounds,Gaussian noise,Gaussian noise,Helium,isometry property,Least squares methods,Linear regression,minimisation,Minimization methods,mutual incoherence property,Noise measurement,restricted isometry property,signal processing,sparse recovery,sparse signal recovery,Sparsity,Statistics,Vectors},
  month     = jul,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:35:27Z},
  year      = {2009},
}

@Article{Baraniuk2010,
  author    = {Baraniuk, R.G. and Cevher, V. and Duarte, M.F. and Hegde, C.},
  title     = {Model-Based Compressive Sensing},
  doi       = {10.1109/TIT.2010.2040894},
  issn      = {0018-9448},
  number    = {4},
  pages     = {1982-2001},
  volume    = {56},
  abstract  = {Compressive sensing (CS) is an alternative to Shannon/Nyquist sampling
	for the acquisition of sparse or compressible signals that can be
	well approximated by just K ¿ N elements from an N -dimensional
	basis. Instead of taking periodic samples, CS measures inner products
	with M < N random vectors and then recovers the signal via a sparsity-seeking
	optimization or greedy algorithm. Standard CS dictates that robust
	signal recovery is possible from M = O(K log(N/K)) measurements.
	It is possible to substantially decrease M without sacrificing robustness
	by leveraging more realistic signal models that go beyond simple
	sparsity and compressibility by including structural dependencies
	between the values and locations of the signal coefficients. This
	paper introduces a model-based CS theory that parallels the conventional
	theory and provides concrete guidelines on how to create model-based
	recovery algorithms with provable performance guarantees. A highlight
	is the introduction of a new class of structured compressible signals
	along with a new sufficient condition for robust structured compressible
	signal recovery that we dub the restricted amplification property,
	which is the natural counterpart to the restricted isometry property
	of conventional CS. Two examples integrate two relevant signal models-wavelet
	trees and block sparsity-into two state-of-the-art CS recovery algorithms
	and prove that they offer robust recovery from just M = O(K) measurements.
	Extensive numerical simulations demonstrate the validity and applicability
	of our new theory and algorithms.},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {block sparsity,compressibility,compressible signals,compressive sensing,Concrete,Costs,greedy algorithm,Greedy algorithms,Guidelines,image coding,information theory,Measurement standards,model-based compressive sensing,model-based recovery algorithms,Nyquist sampling,realistic signal models,restricted amplification property,restricted isometry property,Robustness,robust structured compressible signal recovery,Sampling methods,Shannon sampling,signal coefficients,signal model,sparse matrices,Sparsity,sparsity-seeking optimization,Sufficient conditions,Transform coding,union of subspaces,wavelet tree},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2016-07-09T20:13:49Z},
  year      = {2010},
}

@Book{Foucart2013,
  author    = {Foucart, Simon and Rauhut, Holger},
  title     = {A Mathematical Introduction to Compressive Sensing},
  publisher = {{Birkh{\"a}user}},
  annote    = {ISBN: 978-0-8176-4947-0 (Print) 978-0-8176-4948-7 (Online)},
  owner     = {Fardin},
  timestamp = {2016-10-07T14:19:20Z},
  year      = {2013},
}

@Article{Erickson2005,
  author    = {Erickson, Stephen and Sabatti, Chiara},
  title     = {Empirical Bayes Estimation of a Sparse Vector of Gene Expression Changes},
  doi       = {10.2202/1544-6115.1132},
  issn      = {1544-6115},
  language  = {eng},
  number    = {1},
  pages     = {22},
  volume    = {4},
  abstract  = {Gene microarray technology is often used to compare the expression of thousand of genes in two different cell lines. Typically, one does not expect measurable changes in transcription amounts for a large number of genes; furthermore, the noise level of array experiments is rather high in relation to the available number of replicates. For the purpose of statistical analysis, inference on the "population'' difference in expression for genes across the two cell lines is often cast in the framework of hypothesis testing, with the null hypothesis being no change in expression. Given that thousands of genes are investigated at the same time, this requires some multiple comparison correction procedure to be in place. We argue that hypothesis testing, with its emphasis on type I error and family analogues, may not address the exploratory nature of most microarray experiments. We instead propose viewing the problem as one of estimation of a vector known to have a large number of zero components. In a Bayesian framework, we describe the prior knowledge on expression changes using mixture priors that incorporate a mass at zero, and we choose a loss function that favors the selection of sparse solutions. We consider two different models applicable to the microarray problem, depending on the nature of replicates available, and show how to explore the posterior distributions of the parameters using MCMC. Simulations show an interesting connection between this Bayesian estimation framework and false discovery rate (FDR) control. Finally, two empirical examples illustrate the practical advantages of this Bayesian estimation paradigm.},
  journal   = {Statistical Applications in Genetics and Molecular Biology},
  pmid      = {16646840},
  timestamp = {2016-09-29T14:51:01Z},
  year      = {2005},
}

@Article{Chen2001,
  author    = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
  title     = {Atomic Decomposition by Basis Pursuit},
  number    = {1},
  pages     = {129--159},
  volume    = {43},
  abstract  = {The time-frequency and time-scale communities have recently developed
	a large number of overcomplete waveform dictionaries stationary
	wavelets, wavelet packets, cosine packets, chirplets, and warplets,
	to name a few. Decomposition into overcomplete systems is not unique,
	and several methods for decomposition have been proposed, including
	the method of frames (MOF), matching pursuit (MP), and, for special
	dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP)
	is a principle for decomposing a signal into an optimal superposition
	of dictionary elements, where optimal means having the smallest l1
	norm of coefficients among all such decompositions. We give examples
	exhibiting several advantages over MOF, MP, and BOB, including better
	sparsity and superresolution. BP has interesting relations to ideas
	in areas as diverse as ill-posed problems, abstract harmonic analysis,
	total variation denoising, and multiscale edge denoising. BP in highly
	overcomplete dictionaries leads to large-scale optimization problems.
	With signals of length 8192 and a wavelet packet dictionary, one
	gets an equivalent linear program of size 8192 by 212,992. Such problems
	can be attacked successfully only because of recent advances in linear
	and quadratic programming by interior-point methods. We obtain reasonable
	success with a primal-dual logarithmic barrier method and conjugategradient
	solver.},
  annote    = {SIAM Journal on Scientific Computing},
  journal   = {SIAM J. Sci. Comput.},
  keywords  = {cosine packets,denoising,interior-point methods for linear programming,l1 norm optimization,matching pursuit,MATLAB code,multiscale edges,overcomplete signal representation,time-frequency analysis,time-scale analysis,total variation denoising,wavelet packets,wavelets},
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:08:19Z},
  year      = {2001},
}

@Article{Meinshausen2009,
  author    = {Meinshausen, Nicolai and Yu, Bin},
  title     = {Lasso-type recovery of sparse representations for high-dimensional data},
  doi       = {10.1214/07-AOS582},
  number    = {1},
  pages     = {246--270},
  volume    = {37},
  abstract  = {The Lasso is an attractive technique for regularization and variable
	selection for high-dimensional data, where the number of predictor
	variables pn is potentially much larger than the number of samples
	n. However, it was recently discovered that the sparsity pattern
	of the Lasso estimator can only be asymptotically identical to the
	true sparsity pattern if the design matrix satisfies the so-called
	irrepresentable condition. The latter condition can easily be violated
	in the presence of highly correlated variables. Here we examine the
	behavior of the Lasso estimators if the irrepresentable condition
	is relaxed. Even though the Lasso cannot recover the correct sparsity
	pattern, we show that the estimator is still consistent in the 2-norm
	sense for fixed designs under conditions on (a) the number sn of
	nonzero components of the vector ?n and (b) the minimal singular
	values of design matrices that are induced by selecting small subsets
	of variables. Furthermore, a rate of convergence result is obtained
	on the 2 error with an appropriate choice of the smoothing parameter.
	The rate is shown to be optimal under the condition of bounded maximal
	and minimal sparse eigenvalues. Our results imply that, with high
	probability, all important variables are selected. The set of selected
	variables is a meaningful reduction on the original set of variables.
	Finally, our results are illustrated with the detection of closely
	adjacent frequencies, a problem encountered in astrophysics.},
  annote    = {read},
  journal   = {The Annals of Statistics},
  keywords  = {high-dimensional data,Lasso,Shrinkage estimation,Sparsity},
  owner     = {Fardin},
  timestamp = {2016-09-30T10:51:46Z},
  year      = {2009},
}

@InProceedings{Elhamifar2009,
  author    = {Elhamifar, E. and Vidal, R.},
  booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  title     = {Sparse subspace clustering},
  doi       = {10.1109/CVPR.2009.5206547},
  pages     = {2790--2797},
  abstract  = {We propose a method based on sparse representation (SR) to cluster
	data drawn from multiple low-dimensional linear or affine subspaces
	embedded in a high-dimensional space. Our method is based on the
	fact that each point in a union of subspaces has a SR with respect
	to a dictionary formed by all other data points. In general, finding
	such a SR is NP hard. Our key contribution is to show that, under
	mild assumptions, the SR can be obtained `exactly' by using l1 optimization.
	The segmentation of the data is obtained by applying spectral clustering
	to a similarity matrix built from this SR. Our method can handle
	noise, outliers as well as missing data. We apply our subspace clustering
	algorithm to the problem of segmenting multiple motions in video.
	Experiments on 167 video sequences show that our approach significantly
	outperforms state-of-the-art methods.},
  keywords  = {Clustering algorithms,Dictionaries,image coding,image motion analysis,Image segmentation,information theory,iterative methods,matrix algebra,motion segmentation,optimisation,pattern clustering,Polynomials,Principal component analysis,sparse representation,sparse subspace clustering,spectral clustering,Strontium,video sequences},
  month     = jun,
  owner     = {Fardin},
  timestamp = {2016-09-30T11:48:04Z},
  year      = {2009},
}

@Article{Strohmer2003,
  author    = {Strohmer, Thomas and Jr, Robert W. Heath},
  title     = {Grassmannian frames with applications to coding and communication},
  doi       = {http://dx.doi.org/10.1016/S1063-5203(03)00023-X},
  issn      = {1063-5203},
  number    = {3},
  pages     = {257--275},
  volume    = {14},
  abstract  = {For a given class F of unit norm frames of fixed redundancy we define
	a Grassmannian frame as one that minimizes the maximal correlation
	|〈fk,fl〉| among all frames {fk}k∈I∈F. We first analyze finite-dimensional
	Grassmannian frames. Using links to packings in Grassmannian spaces
	and antipodal spherical codes we derive bounds on the minimal achievable
	correlation for Grassmannian frames. These bounds yield a simple
	condition under which Grassmannian frames coincide with unit norm
	tight frames. We exploit connections to graph theory, equiangular
	line sets, and coding theory in order to derive explicit constructions
	of Grassmannian frames. Our findings extend recent results on unit
	norm tight frames. We then introduce infinite-dimensional Grassmannian
	frames and analyze their connection to unit norm tight frames for
	frames which are generated by group-like unitary systems. We derive
	an example of a Grassmannian Gabor frame by using connections to
	sphere packing theory. Finally we discuss the application of Grassmannian
	frames to wireless communication and to multiple description coding.},
  journal   = {Applied and Computational Harmonic Analysis},
  keywords  = {Conference matrix,Equiangular line sets,frame,Gabor frame,Grassmannian spaces,Multiple description coding,Spherical codes,Unitary system,Unit norm tight frame},
  owner     = {afdidehf},
  timestamp = {2016-09-29T16:10:09Z},
  year      = {2003},
}

@Article{Gedalyahu2010,
  author    = {Gedalyahu, K. and Eldar, Y.C.},
  title     = {Time-Delay Estimation From Low-Rate Samples: A Union of Subspaces Approach},
  doi       = {10.1109/TSP.2010.2044253},
  issn      = {1053-587X},
  number    = {6},
  pages     = {3017--3031},
  volume    = {58},
  abstract  = {Time-delay estimation arises in many applications in which a multipath
	medium has to be identified from pulses transmitted through the channel.
	Previous methods for time delay recovery either operate on the analog
	received signal, or require sampling at the Nyquist rate of the transmitted
	pulse. In this paper, we develop a unified approach to time delay
	estimation from low-rate samples. This problem can be formulated
	in the broader context of sampling over an infinite union of subspaces.
	Although sampling over unions of subspaces has been receiving growing
	interest, previous results either focus on unions of finite-dimensional
	subspaces, or finite unions. The framework we develop here leads
	to perfect recovery of the multipath delays from samples of the channel
	output at the lowest possible rate, even in the presence of overlapping
	transmitted pulses, and allows for a variety of different sampling
	methods. The sampling rate depends only on the number of multipath
	components and the transmission rate, but not on the bandwidth of
	the probing signal. This result can be viewed as a sampling theorem
	over an infinite union of infinite dimensional subspaces. By properly
	manipulating the low-rate samples, we show that the time delays can
	be recovered using the well-known ESPRIT algorithm. Combining results
	from sampling theory with those obtained in the context of direction
	of arrival estimation, we develop sufficient conditions on the transmitted
	pulse and the sampling functions in order to ensure perfect recovery
	of the channel parameters at the minimal possible rate.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {analog received signal,direction-of-arrival estimation,Direction of arrival estimation,ESPRIT algorithm,finite-dimensional subspaces,low-rate samples,multipath delays,Nyquist rate,Sampling methods,sampling theory,signal sampling,sub-Nyquist sampling,time-delay estimation,time delay recovery,time delay recovery,union of subspaces,union of subspaces},
  month     = jun,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:27:39Z},
  year      = {2010},
}

@Article{Vershik1992,
  author    = {Vershik, A. M. and Sporyshev, P. V.},
  title     = {Asymptotic behavior of the number of faces of random polyhedra and the neighborliness problem},
  issn      = {0272-9903},
  number    = {2},
  pages     = {181--201},
  urldate   = {2016-05-16},
  volume    = {11},
  abstract  = {Asymptotic behavior of the number of faces of random polyhedra and
	the neighborliness problem on ResearchGate, the professional network
	for scientists.},
  journal   = {Sel Math Soviet},
  month     = jan,
  timestamp = {2016-09-30T11:11:41Z},
  year      = {1992},
}

@TechReport{Donoho2004a,
  author      = {Donoho, David L.},
  institution = {Comm. Pure Appl. Math},
  title       = {For most large underdetermined systems of equations, the minimal $\ell^1$-norm near-solution approximates the sparsest near-solution},
  abstract    = {We consider inexact linear equations y  where y is a given vector
	in Rn,  is a given n by m matrix, and we wish to find an  which
	is sparse and gives an approximate solution, obeying ky ?  In general this requires combinatorial optimization and so is
	considered intractable. On the other hand, the `1 minimization problem
	min k1 subject to ky ?  is convex, and is considered tractable.
	We show that for most  the solution ) of this
	problem is quite generally a good approximation for . We suppose
	that the columns of  are normalized to unit `2 norm 1 and we place
	uniform measure on such . We study the underdetermined case where
	m  An, A > 1 and prove the existence of (A) and C > 0 so that
	for large n, and for all s except a negligible fraction, the
	following approximate sparse solution property of holds: For every
	y having an approximation ky ?  by a coefficient vector0
	2 Rm with fewer than n nonzeros, we have k
	C. This has two implications. First: for most , whenever the
	combinatorial optimization result  would be very sparse, 
	is a good approximation to. Second: suppose we are given noisy
	data obeying y = 0 + z where the unknown 0 is known to be sparse
	and the noise kzk2. For most , noise-tolerant `1-minimization
	will stably recover 0 from y in the presence of noise z. We study
	also the barely-determined case m = n and reach parallel conclusions
	by slightly different arguments. The techniques include the use of
	almost-spherical sections in Banach space theory and concentration
	of measure for eigenvalues of random matrices.},
  keywords    = {Almost-Spherical,Approximate,Banach,Eigenvalues,equations.,linear,Matrices.,of,Random,Sections,Solution,Spaces.,Sparse,Systems.,underdetermined},
  month       = aug,
  owner       = {Fardin},
  timestamp   = {2017-06-23T09:46:01Z},
  year        = {2004},
}

@InProceedings{Cormode2006,
  author    = {Cormode, G. and Muthukrishnan, S.},
  booktitle = {2006 40th Annual Conference on Information Sciences and Systems},
  title     = {Combinatorial Algorithms for Compressed Sensing},
  doi       = {10.1109/CISS.2006.286461},
  pages     = {198--201},
  abstract  = {In sparse approximation theory, the fundamental problem is to reconstruct a signal AisinRn from linear measurements (A,psii) with respect to a dictionary of psii's. Recently, there is focus on the novel direction of Compressed Sensing where the reconstruction can be done with very few-O(klogn)-linear measurements over a modified dictionary if the signal is compressible, that is, its information is concentrated in k coefficients with the original dictionary. In particular, the results prove that there exists a single O(klogn)timesn measurement matrix such that any such signal can be reconstructed from these measurements, with error at most O(1) times the worst case error for the class of such signals. Compressed sensing has generated tremendous excitement both because of the sophisticated underlying mathematics and because of its potential applications. In this paper, we address outstanding open problems in Compressed Sensing. Our main result is an explicit construction of a non-adaptive measurement matrix and the corresponding reconstruction algorithm so that with a number of measurements polynomial in k, logn, 1/epsiv, we can reconstruct compressible signals. This is the first known polynomial time explicit construction of any such measurement matrix. In addition, our result improves the error guarantee from O(1) to 1+epsiv and improves the reconstruction time from poly(n) to poly (klogn). Our second result is a randomized construction of O(kpolylog(n)) measurements that work for each signal with high probability and gives per-instance approximation guarantees rather than over the class of all signals. Previous work on compressed sensing does not provide such per-instance approximation guarantees; our result improves the best known number of measurements known from prior work in other areas including learning theory, streaming algorithms and complexity theory for this case. Our approach is combinatorial. In particular, we use two p- arallel sets of group tests, one to filter and the other to certify and estimate; the resulting algorithms are quite simple to implement.},
  keywords  = {Approximation methods,Approximation methods,approximation theory,approximation theory,Area measurement,Area measurement,combinatorial algorithm,combinatorial algorithm,compressed sensing,compressed sensing,computational complexity,computational complexity,Dictionaries,Dictionaries,Mathematics,Mathematics,non-adaptive measurement matrix,non-adaptive measurement matrix,Particle measurements,Particle measurements,Polynomials,Polynomials,Pressure measurement,Pressure measurement,Reconstruction algorithms,Reconstruction algorithms,signal reconstruction,signal reconstruction,sparse approximation theory,sparse approximation theory,sparse matrices,Sparse matrices,Time measurement,Time measurement},
  month     = mar,
  timestamp = {2016-09-30T10:47:49Z},
  year      = {2006},
}

@InProceedings{Haueisen2014,
  author    = {Haueisen, J. and Lau, S. and Flemming, L. and Sonntag, H. and Maess, B. and G{\"u}llmar, D.},
  booktitle = {General {{Assembly}} and {{Scientific Symposium}} ({{URSI GASS}}), 2014 {{XXXIth URSI}}},
  title     = {Influence of Volume Conductor Modeling on Source Reconstruction in Magnetoencephalography and Electroencephalography},
  doi       = {10.1109/URSIGASS.2014.6930127},
  pages     = {1--2},
  abstract  = {Summary form only given. The function and structure of the human brain is immensely complex and, at the same time, the key to understanding human behavior and many of today's prevailing diseases. In most cases, this system cannot be investigated directly, but only non-invasively from outside the head. Although several non-invasive measurement modalities are available, only magnetoencephalography (MEG) and electroencephalography (EEG) provide information with a high temporal resolution. In order to reconstruct the neuronal activity underlying measured EEG and MEG data both the forward problem (computing the electromagnetic field due to given sources) and the inverse problem (finding the best fitting sources to explain given data) have to be solved. The forward problem involves a source model and a model with the conductivities of the head. The conductivity model can be as simple as a homogeneously conducting sphere or as complex as a finite element model consisting of millions of elements, each with a different anisotropic conductivity tensor. The question is addressed how complex the employed forward model should be, and, more specifically, the influence of anisotropic volume conduction and the influence of conductivity inhomogeneities are evaluated. For this purpose high resolution finite element models of the rabbit and the human head are employed in combination with individual conductivity tensors to quantify the influence of white matter anisotropy on the solution of the forward and inverse problem in EEG and MEG. Although the current state of the art in the analysis of this influence of brain tissue anisotropy on source reconstruction does not yet allow a final conclusion, the results available indicate that the expected average source localization error due to anisotropic white matter conductivity might be within the principal accuracy limits of current inverse procedures. However, in some percent of the cases a considerably larger localization error might o- cur. In contrast, dipole orientation and dipole strength estimation are influenced significantly by anisotropy. Skull conductivity inhomogeneities such as the spongy bone structure embedded in the compact bone or surgical holes or fontanels in infants have a non-negligible effect on the EEG and MEG forward and inverse problem solution. Especially when source positions are expected to be in the vicinity of the conductivity inhomogeneity and when a large difference with respect to the skull conductivity is indicated, the modeling approach should take the inhomogeneities into account. In conclusion, models taking into account tissue anisotropy and conductivity inhomogeneities information are expected to improve source estimation procedures. Depending on the question addressed, the complexity of the forward and inverse solution approach has to be chosen.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RDRV8MU6\\6930127.html:text/html},
  keywords  = {bioelectric phenomena,Brain modeling,compact bone,Conductivity,conductivity inhomogeneity,conductivity model,conductivity tensors,Educational institutions,EEG,Electroencephalography,finite element analysis,high resolution finite element model,human brain,human head,infant fontanels,Magnetic heads,Magnetoencephalography,medical signal processing,MEG,Nonhomogeneous media,noninvasive measurement,rabbit head,signal reconstruction,skull conductivity,Source reconstruction,spongy bone structure,surgical holes,volume conductor modeling,white matter anisotropy},
  month     = aug,
  timestamp = {2016-10-20T12:52:22Z},
  year      = {2014},
}

@Article{Cand`es2007a,
  author    = {Cand{\`e}s, Emmanuel and Tao, Terence},
  title     = {The Dantzig selector: statistical estimation when p is much larger than n},
  doi       = {10.1214/009053606000001523},
  number    = {6},
  pages     = {2313--2351},
  volume    = {35},
  abstract  = {In many important statistical applications, the number of variables
	or parameters p is much larger than the number of observations n.
	Suppose then that we have observations y = X? +z, where ? ? Rp is
	a parameter vector of interest, X is a data matrix with possibly
	far fewer rows than columns, np, and the zi s are i.i.d. N(0,?2).
	Is it possible to estimate ? reliably based on the noisy data y?
	To estimate ?, we introduce a new estimatorwe call it the Dantzig
	selector which is a solution to the 1-regularization problem
	where r is the residual vector y ? X  ? and t is a positive scalar.
	We show that if X obeys a uniform uncertainty principle (with unit-normed
	columns) and if the true parameter vector ? is sufficiently sparse
	(which here roughly guarantees that the model is identifiable), then
	with very large probability, Our results are nonasymptotic and we
	give values for the constant C. Even though nmay be much smaller
	than p, our estimator achieves a loss within a logarithmic factor
	of the ideal mean squared error one would achieve with an oracle
	which would supply perfect information about which coordinates are
	nonzero, and which were above the noise level. In multivariate regression
	and from a model selection viewpoint, our result says that it is
	possible nearly to select the best subset of variables by solving
	a very simple convex program, which, in fact, can easily be recast
	as a convenient linear program (LP).},
  annote    = {read},
  journal   = {The Annals of Statistics},
  keywords  = {1-minimization,geometry in high dimensions,ideal estimation,linear programming,Model selection,oracle inequalities,random matrices.,restricted orthonormality,sparse solutions to underdetermined systems,Statistical linear model},
  owner     = {Fardin},
  timestamp = {2017-06-23T13:10:04Z},
  year      = {2007},
}

@Online{pahio2005,
  author    = {{pahio}},
  title     = {Summed Numerator and Summed Denominator},
  url       = {https://planetmath.org/SummedNumeratorAndSummedDenominator},
  timestamp = {2017-02-17T10:23:51Z},
  year      = {2005},
}

@Book{Walter2014,
  author    = {Walter, {\'E}ric},
  title     = {Numerical {{Methods}} and {{Optimization}} - {{A Consumer Guide}}},
  publisher = {{Springer}},
  urldate   = {2017-06-08},
  abstract  = {Initial training in pure and applied sciences tends to present problem-solving as the process of elaborating explicit closed-form solutions from basic...},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3X99GKGU\\9783319076706.html:text/html},
  timestamp = {2017-06-08T15:02:25Z},
  year      = {2014},
}

@Article{Geer2004,
  author    = {Geer, Sara A. Van De and Houwelingen, Hans C. Van},
  title     = {High-dimensional data: $p \gg n$ in mathematical statistics and bio-medical applications},
  number    = {6},
  pages     = {939--942},
  volume    = {10},
  journal   = {Bernoulli},
  owner     = {afdidehf},
  timestamp = {2016-09-29T14:53:23Z},
  year      = {2004},
}

@InProceedings{Zelinski2008,
  author    = {Zelinski, A. C. and Goyal, V. K. and Adalsteinsson, E. and Wald, L. L.},
  booktitle = {42nd {{Annual Conference}} on {{Information Sciences}} and {{Systems}}, 2008. {{CISS}} 2008},
  title     = {Sparsity in {{MRI RF}} Excitation Pulse Design},
  doi       = {10.1109/CISS.2008.4558531},
  pages     = {252--257},
  abstract  = {Magnetic resonance imaging (MRI) may be viewed as a two-stage experiment that yields a non-invasive spatial mapping of hydrogen nuclei in living subjects. Nuclear spins within a subject are first excited using a radio-frequency (RF) excitation pulse and proportions of excited spins are then detected using a resonant coil; images are then reconstructed from this data. Excitation pulses need to be tailored to a user's specific needs and in most applications need to be as short as possible, due to spin relaxation, tissue heating, signal-to-noise ratio (SNR), and data readout limitations. The design of short-duration excitation pulses is an important topic and the focus of our work. One may show that RF excitation pulse design, under certain conditions, involves choosing to deposit energy in a continuous, 3-D, Fourier-like domain ("excitation k-space") in order to form some desired excitation in the spatial domain. Energy may only be deposited along a 1-D contour, and there are limitations on where and how it may be placed; the most important fact is that excitation pulse duration directly corresponds to the length of the chosen contour and the rate it is traversed. The problem then is to find a sparse "trajectory" (and corresponding energy deposition) within this k-space such that a high-fidelity version of the desired excitation is formed in the spatial domain. We show how sparsity and simultaneous sparsity are applicable to 2-D and 3-D excitation pulse design and present a novel instance where simultaneous sparsity is desirable. We then discuss how to apply sparse approximation concepts to produce RF pulses. These "sparsity-enforced" designs, generated via convex relaxation techniques, significantly outperform conventional pulses: for fixed pulse duration, sparsity-enforced pulses always produce higher-fidelity excitations.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\G6UCJEV5\\articleDetails.html:text/html},
  keywords  = {1D contour,biological tissues,biomedical MRI,Coils,cone programming,continuous 3D Fourier-like domain,convex relaxation,data readout limitation,energy deposition,excitation k-space,excitation pulse duration,excited spin,Focusing,Fourier transforms,Heating,Hydrogen,hydrogen nuclei,image reconstruction,living subjects,magnetic resonance imaging,medical image processing,MRI RF excitation pulse,MRI RF pulse sequence design,noninvasive spatial mapping,Nuclear magnetic resonance,nuclear spin,Pulse generation,Radio frequency,radiofrequency excitation pulse,resonant coil,second-order cone programming,signal-to-noise ratio,Signal to noise ratio,simultaneous sparsity,sparse approximation,sparse trajectory,sparsity,spin relaxation,tissue heating},
  month     = mar,
  timestamp = {2016-09-30T13:30:06Z},
  year      = {2008},
}

@Book{HornR.A.2012,
  author    = {Horn R.A., Johnson C.R.},
  title     = {Matrix Analysis},
  edition   = {2ed.},
  isbn      = {978-0-521-54823-6},
  publisher = {{New York, NY: Cambridge Press}},
  owner     = {Fardin},
  timestamp = {2016-07-09T20:08:30Z},
  year      = {2012},
}

@Book{Bertsekas1999,
  author    = {Bertsekas, Dimitri P.},
  title     = {{Nonlinear Programming}},
  edition   = {2},
  isbn      = {978-1-886529-00-7},
  language  = {Anglais},
  publisher = {{Athena Scientific}},
  month     = sep,
  timestamp = {2016-10-05T13:52:08Z},
  year      = {1999},
}

@InProceedings{Gemmeke2008a,
  author    = {Gemmeke, J. F. and Cranen, B.},
  booktitle = {Signal Processing Conference, 2008 16th European},
  title     = {Using sparse representations for missing data imputation in noise robust speech recognition},
  pages     = {1--5},
  abstract  = {Noise robustness of automatic speech recognition benefits from using
	missing data imputation: Prior to recognition the parts of the spectrogram
	dominated by noise are replaced by clean speech estimates. Especially
	at low SNRs each frame contains at best only a few uncorrupted coefficients.
	This makes frame-by-frame restoration of corrupted feature vectors
	error-prone, and recognition accuracy will mostly be sub-optimal.
	In this paper we present a novel imputation technique working on
	entire words. A word is sparsely represented in an overcomplete basis
	of exemplar (clean) speech signals using only the uncorrupted time-frequency
	elements of the word. The corrupted elements are replaced by estimates
	obtained by projecting the sparse representation in the basis. We
	achieve recognition accuracies of 92\% at SNR -5 dB using oracle
	masks on AURORA-2 as compared to 61\% using a conventional frame-based
	approach. The performance obtained with estimated masks can be directly
	related to the proportion of correctly identified uncorrupted coefficients.},
  keywords  = {Accuracy,AURORA-2,clean speech estimation,corrupted feature vector error-prone,exemplar speech signals,frame-by-frame restoration,missing data imputation,noise robust automatic speech recognition accuracy,oracle masks,Reliability,signal restoration,Signal to noise ratio,SNR,sparse representations,spectrogram,Speech,speech recognition,time-frequency analysis,uncorrupted time-frequency elements,Vectors},
  month     = aug,
  timestamp = {2016-09-30T11:47:19Z},
  year      = {2008},
}

@Book{Gill1990,
  author    = {Gill, Philip E. and Murray, Walter and Wright, Margaret H.},
  title     = {Numerical {{Linear Algebra}} and {{Optimization}}},
  isbn      = {978-0-201-12649-5},
  language  = {English},
  publisher = {{Perseus Books}},
  volume    = {1},
  address   = {Redwood City, Calif.},
  month     = sep,
  timestamp = {2016-10-05T13:49:27Z},
  year      = {1990},
}

@Article{Hosek1978,
  author    = {Hosek, R. S. and Sances, A. and Jodat, R. W. and Larson, S. J.},
  title     = {The {{Contributions}} of {{Intracerebral Currents}} to the {{EEG}} and {{Evoked Potentials}}},
  doi       = {10.1109/TBME.1978.326337},
  issn      = {0018-9294},
  number    = {5},
  pages     = {405--413},
  volume    = {BME-25},
  abstract  = {Scalp and cortical potentials due to implanted, dipole current sources were measured in the monkey. A four region spherical model of the head was developed and scalp potentials due to theoretical radial dipoles were computed and compared with experimental results. Dipole source locations were chosen to correspond to points along the somatosensory projection pathways to permit comparison of findings with clinical cortical and scalp evoked potential records. Data yielded by the theoretical head model compare well with those obtained experimentally. The results suggest that depth cerebral bioelectric sources can contribute to scalp recorded activity when averaging techniques are used.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\A3TRPMBS\\4122870.html:text/html},
  journal   = {IEEE Transactions on Biomedical Engineering},
  keywords  = {Animals,Audio recording,bioelectric phenomena,Brain modeling,Cerebral cortex,Delay,Electric Conductivity,Electric potential,Electrodes; Implanted,Electroencephalography,Evoked Potentials,Haplorhini,Humans,Magnetic heads,Scalp,Signal generators},
  month     = sep,
  timestamp = {2017-08-22T09:33:45Z},
  year      = {1978},
}

@Article{Tropp2006,
  author    = {Tropp, J.A.},
  title     = {Just relax: convex programming methods for identifying sparse signals in noise},
  doi       = {10.1109/TIT.2005.864420},
  issn      = {0018-9448},
  number    = {3},
  pages     = {1030--1051},
  volume    = {52},
  abstract  = {This paper studies a difficult and fundamental problem that arises
	throughout electrical engineering, applied mathematics, and statistics.
	Suppose that one forms a short linear combination of elementary signals
	drawn from a large, fixed collection. Given an observation of the
	linear combination that has been contaminated with additive noise,
	the goal is to identify which elementary signals participated and
	to approximate their coefficients. Although many algorithms have
	been proposed, there is little theory which guarantees that these
	algorithms can accurately and efficiently solve the problem. This
	paper studies a method called convex relaxation, which attempts to
	recover the ideal sparse signal by solving a convex program. This
	approach is powerful because the optimization can be completed in
	polynomial time with standard scientific software. The paper provides
	general conditions which ensure that convex relaxation succeeds.
	As evidence of the broad impact of these results, the paper describes
	how convex relaxation can be used for several concrete signal recovery
	problems. It also describes applications to channel coding, linear
	regression, and numerical analysis},
  annote    = {PDF \& PPT},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {Additive noise,Algorithms,Application software,Approximation methods,Basis Pursuit,channel coding,Concrete,convex program,convex programming,convex programming method,Electrical engineering,iterative methods,linear codes,Linear regression,Mathematics,numerical analysis,Optimization methods,orthogonal matching pursuit,Polynomials,polynomial time,regression analysis,short linear signal combination,signal denoising,signal detection,signal processing,signal representation,Software standards,sparse representations,sparse signal identification,standard scientific software,Statistics,time-frequency analysis},
  month     = mar,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:43:14Z},
  year      = {2006},
}

@Article{Tropp2004,
  author    = {Tropp, J.A.},
  title     = {Greed is good: algorithmic results for sparse approximation},
  doi       = {10.1109/TIT.2004.834793},
  issn      = {0018-9448},
  number    = {10},
  pages     = {2231--2242},
  volume    = {50},
  abstract  = {This article presents new results on using a greedy algorithm, orthogonal
	matching pursuit (OMP), to solve the sparse approximation problem
	over redundant dictionaries. It provides a sufficient condition under
	which both OMP and Donoho's basis pursuit (BP) paradigm can recover
	the optimal representation of an exactly sparse signal. It leverages
	this theory to show that both OMP and BP succeed for every sparse
	input signal from a wide class of dictionaries. These quasi-incoherent
	dictionaries offer a natural generalization of incoherent dictionaries,
	and the cumulative coherence function is introduced to quantify the
	level of incoherence. This analysis unifies all the recent results
	on BP and extends them to OMP. Furthermore, the paper develops a
	sufficient condition under which OMP can identify atoms from an optimal
	approximation of a nonsparse signal. From there, it argues that OMP
	is an approximation algorithm for the sparse problem over a quasi-incoherent
	dictionary. That is, for every input signal, OMP calculates a sparse
	approximant whose error is only a small factor worse than the minimal
	error that can be attained with the same number of terms.},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Algorithms,algorithm theory,Approximation algorithms,Approximation methods,Approximation methods,approximation theory,atoms identification,Basis Pursuit,BP,BP paradigm,cumulative coherence function,Dictionaries,Donoho's basis pursuit,greedy algorithm,Greedy algorithms,Iterative algorithms,iterative method,iterative methods,linear programming,Matching pursuit algorithms,nonsparse signal,OMP,optimal approximation,orthogonal matching pursuit,quasiincoherent dictionary,redundant dictionary,redundant number systems,signal processing,sparse approximation problem,sparse matrices,Sufficient conditions},
  month     = oct,
  owner     = {Fardin},
  timestamp = {2017-04-19T14:26:20Z},
  year      = {2004},
}

@Article{Parvaresh2008a,
  author    = {Parvaresh, F. and Vikalo, H. and Misra, S. and Hassibi, B.},
  title     = {Recovering Sparse Signals Using Sparse Measurement Matrices in Compressed DNA Microarrays},
  doi       = {10.1109/JSTSP.2008.924384},
  issn      = {1932-4553},
  number    = {3},
  pages     = {275--285},
  volume    = {2},
  abstract  = {Microarrays (DNA, protein, etc.) are massively parallel affinity-based
	biosensors capable of detecting and quantifying a large number of
	different genomic particles simultaneously. Among them, DNA microarrays
	comprising tens of thousands of probe spots are currently being employed
	to test multitude of targets in a single experiment. In conventional
	microarrays, each spot contains a large number of copies of a single
	probe designed to capture a single target, and, hence, collects only
	a single data point. This is a wasteful use of the sensing resources
	in comparative DNA microarray experiments, where a test sample is
	measured relative to a reference sample. Typically, only a fraction
	of the total number of genes represented by the two samples is differentially
	expressed, and, thus, a vast number of probe spots may not provide
	any useful information. To this end, we propose an alternative design,
	the so-called compressed microarrays, wherein each spot contains
	copies of several different probes and the total number of spots
	is potentially much smaller than the number of targets being tested.
	Fewer spots directly translates to significantly lower costs due
	to cheaper array manufacturing, simpler image acquisition and processing,
	and smaller amount of genomic material needed for experiments. To
	recover signals from compressed microarray measurements, we leverage
	ideas from compressive sampling. For sparse measurement matrices,
	we propose an algorithm that has significantly lower computational
	complexity than the widely used linear-programming-based methods,
	and can also recover signals with less sparsity.},
  annote    = {Selected Topics in Signal Processing, IEEE Journal of},
  journal   = {IEEE J. Sel. Topics Signal Process.},
  keywords  = {biocomputing,bioinformatics,biosensors,compressed DNA microarrays,compressive sampling,computational complexity,Costs,DNA,DNA microarrays,genetics,genomic particles,Genomics,image acquisition,image coding,Image Processing,linear programming,Probes,Proteins,signal processing,sparse matrices,sparse measurement matrices,sparse measurements,sparse signals,Testing},
  month     = jun,
  owner     = {afdidehf},
  timestamp = {2016-09-29T14:52:30Z},
  year      = {2008},
}

@Article{Yuan2006,
  author               = {Yuan, Ming and Lin, Yi},
  title                = {Model selection and estimation in regression with grouped variables},
  doi                  = {10.1111/j.1467-9868.2005.00532.x},
  issn                 = {1369-7412},
  number               = {1},
  pages                = {49-67},
  volume               = {68},
  abstract             = {{Summary. We consider the problem of selecting grouped variables (factors)
	for accurate prediction in regression. Such a problem arises naturally
	in many practical situations with the multifactor analysis-of-variance
	problem as the most important and well-known example. Instead of
	selecting factors by stepwise backward elimination, we focus on the
	accuracy of estimation and consider extensions of the lasso, the
	LARS algorithm and the non-negative garrotte for factor selection.
	The lasso, the LARS algorithm and the non-negative garrotte are recently
	proposed regression methods that can be used to select individual
	variables. We study and propose efficient algorithms for the extensions
	of these methods for factor selection and show that these extensions
	give superior performance to the traditional stepwise backward elimination
	method in factor selection problems. We study the similarities and
	the differences between these methods. Simulations and real examples
	are used to illustrate the methods.}},
  annote               = {read},
  citeulike-article-id = {448082},
  citeulike-linkout-0  = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
  citeulike-linkout-1  = {http://dx.doi.org/10.1111/j.1467-9868.2005.00532.x},
  citeulike-linkout-2  = {http://www.ingentaconnect.com/content/bpl/rssb/2006/00000068/00000001/art00004},
  citeulike-linkout-3  = {http://www3.interscience.wiley.com/cgi-bin/abstract/118566968/ABSTRACT},
  day                  = {1},
  journal              = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords             = {Analysis of variance,feature-selection,Lasso,least angle regression,Non-negative garrotte,Piecewise linear solution path},
  month                = feb,
  owner                = {afdidehf},
  posted-at            = {2008-02-27 06:49:54},
  priority             = {2},
  timestamp            = {2016-07-09T20:14:26Z},
  year                 = {2006},
}

@Article{Casazza2004,
  author    = {Casazza, P. G. and Kutyniok, G.},
  title     = {Frames of subspaces},
  pages     = {87--113},
  volume    = {345},
  abstract  = {One approach to ease the construction of frames is to first construct
	local components and then build a global frame from these. In this
	paper we will show that the study of the relation between a frame
	and its local components leads to the definition of a frame of subspaces.
	We introduce this new notion and prove that it provides us with the
	link we need. It will also turn out that frames of subspaces behave
	as a generalization of frames. In particular, we can define an analysis,
	a synthesis and a frame operator for a frame of subspaces, which
	even yield a reconstruction formula. Also concepts such as completeness,
	minimality, and exactness are introduced and investigated. We further
	study several constructions of frames of subspaces, and also of frames
	and Riesz frames using the theory of frames of subspaces. An important
	special case are harmonic frames of subspaces which generalize harmonic
	frames. We show that wavelet subspaces coming from multiresolution
	analysis belong to this class.},
  journal   = {Wavelets, Frames, and Operator Theory, ser. Contemp. Math. . Providence, RI: Amer. Math. Soc.},
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:24:26Z},
  year      = {2004},
}

@Article{Lu2008a,
  author    = {Lu, Y.M. and Do, M.N.},
  title     = {Sampling Signals from a Union of Subspaces},
  doi       = {10.1109/MSP.2007.914999},
  issn      = {1053-5888},
  number    = {2},
  pages     = {41--47},
  volume    = {25},
  abstract  = {The single linear vector space assumption is widely used in modeling
	the signal classes, mainly due to its simplicity and mathematical
	tractability. In certain signals, a union of subspaces can be a more
	appropriate model. This paper provides a new perspective for signal
	sampling by considering signals from a union of subspaces instead
	of a single space.},
  journal   = {Signal Processing Magazine, IEEE},
  keywords  = {Delay,Filtering,Geophysics,Low pass filters,mathematical model,Radar applications,Sampling methods,signal processing,signal sampling,subspace union,Vectors},
  month     = mar,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:25:00Z},
  year      = {2008},
}

@Article{Donoho2003a,
  author    = {Donoho, D.L. and Elad, M.},
  title     = {Maximal Sparsity Representation via $l_1$ Minimization},
  number    = {50},
  pages     = {2197--2202},
  volume    = {100},
  annote    = {read},
  journal   = {Proceedings of the National Academy of Sciences of the United States of America},
  timestamp = {2016-10-05T11:41:30Z},
  year      = {2003},
}

@InProceedings{Koh2007,
  author    = {Koh, Kwangmoo and Kim, Seung-Jean and Boyd, S.},
  booktitle = {Information Theory and Applications Workshop, 2007},
  title     = {An Efficient Method for Large-Scale l1-Regularized Convex Loss Minimization},
  doi       = {10.1109/ITA.2007.4357584},
  pages     = {223--230},
  abstract  = {Convex loss minimization with lscr1 regularization has been proposed
	as a promising method for feature selection in classification (e.g.,
	lscr1-regularized logistic regression) and regression (e.g., lscr1-regularized
	least squares). In this paper we describe an efficient interior-point
	method for solving large-scale lscr1-regularized convex loss minimization
	problems that uses a preconditioned conjugate gradient method to
	compute the search step. The method can solve very large problems.
	For example, the method can solve an lscr1-regularized logistic regression
	problem with a million features and examples (e.g., the 20 Newsgroups
	data set), in a few minutes, on a PC.},
  keywords  = {compressed sensing,conjugate gradient method,conjugate gradient methods,convex loss minimization method,gradient methods,interior-point method,large-scale lscr1-regularization,Large-scale systems,Least squares methods,Logistics,lscr1-regularized least squares,lscr1-regularized logistic regression,minimisation,Minimization methods,Optimization methods,Predictive models,regression analysis,signal processing,Vectors},
  month     = jan,
  owner     = {afdidehf},
  timestamp = {2016-09-30T10:51:04Z},
  year      = {2007},
}

@Book{Mallat2008,
  author    = {Mallat, Stéphane},
  title     = {A Wavelet Tour of Signal Processing},
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:13:36Z},
  year      = {2008},
}

@Article{Gorodnitsky1997,
  author    = {Gorodnitsky, I.F. and Rao, B.D.},
  title     = {Sparse signal reconstruction from limited data using FOCUSS: a re-weighted minimum norm algorithm},
  doi       = {10.1109/78.558475},
  issn      = {1053-587X},
  number    = {3},
  pages     = {600--616},
  volume    = {45},
  abstract  = {We present a nonparametric algorithm for finding localized energy
	solutions from limited data. The problem we address is underdetermined,
	and no prior knowledge of the shape of the region on which the solution
	is nonzero is assumed. Termed the FOcal Underdetermined System Solver
	(FOCUSS), the algorithm has two integral parts: a low-resolution
	initial estimate of the real signal and the iteration process that
	refines the initial estimate to the final localized energy solution.
	The iterations are based on weighted norm minimization of the dependent
	variable with the weights being a function of the preceding iterative
	solutions. The algorithm is presented as a general estimation tool
	usable across different applications. A detailed analysis laying
	the theoretical foundation for the algorithm is given and includes
	proofs of global and local convergence and a derivation of the rate
	of convergence. A view of the algorithm as a novel optimization method
	which combines desirable characteristics of both classical optimization
	and learning-based algorithms is provided. Mathematical results on
	conditions for uniqueness of sparse solutions are also given. Applications
	of the algorithm are illustrated on problems in direction-of-arrival
	(DOA) estimation and neuromagnetic imaging},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {Brain,Convergence,convergence of numerical methods,Cost function,direction-of-arrival estimation,direction of arrival estimation,Direction-of-arrival estimation,Direction of arrival estimation,DOA estimation,final localized energy solution,final localized energy solution,focal underdetermined system solver,Focusing,FOCUSS,Image reconstruction,iteration process,Iterative algorithms,iterative methods,learning-based algorithms,limited data,localized energy solutions,low-resolution initial estimate,magnetoencephalography,medical image processing,neuromagnetic imaging,nonparametric algorithm,optimisation,optimization method,Optimization methods,re-weighted minimum norm algorithm,Sensor arrays,Shape,signal processing,Signal processing algorithms,signal reconstruction,signal reconstruction,sparse signal reconstruction},
  month     = mar,
  owner     = {Fardin},
  timestamp = {2016-09-30T13:52:32Z},
  year      = {1997},
}

@Article{Chartrand2007,
  author    = {Chartrand, R.},
  title     = {Exact Reconstruction of Sparse Signals via Nonconvex Minimization},
  doi       = {10.1109/LSP.2007.898300},
  issn      = {1070-9908},
  number    = {10},
  pages     = {707--710},
  volume    = {14},
  abstract  = {Several authors have shown recently that It is possible to reconstruct
	exactly a sparse signal from fewer linear measurements than would
	be expected from traditional sampling theory. The methods used involve
	computing the signal of minimum lscr1 norm among those having the
	given measurements. We show that by replacing the lscr1 norm with
	the lscrp norm with p < 1, exact reconstruction is possible with
	substantially fewer measurements. We give a theorem in this direction,
	and many numerical examples, both in one complex dimension, and larger-scale
	examples in two real dimensions.},
  annote    = {read},
  journal   = {IEEE Signal Process. Letters},
  keywords  = {compressed sensing,concave programming,Frequency measurement,Gaussian distribution,image coding,Image reconstruction,image sampling,image sampling,minimisation,nonconvex minimization,nonconvex minimization,nonconvex optimization,Sampling methods,Sampling methods,signal reconstruction,sparse signal exact reconstruction,sparse signal exact reconstruction,Surges,Terminology},
  month     = oct,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:32:35Z},
  year      = {2007},
}

@Article{Gorodnitsky1995,
  author     = {Gorodnitsky, I. F. and George, J. S. and Rao, B. D.},
  title      = {Neuromagnetic Source Imaging with {{FOCUSS}}: A Recursive Weighted Minimum Norm Algorithm},
  issn       = {0013-4694},
  language   = {eng},
  number     = {4},
  pages      = {231--251},
  volume     = {95},
  abstract   = {The paper describes a new algorithm for tomographic source reconstruction in neural electromagnetic inverse problems. Termed FOCUSS (FOCal Underdetermined System Solution), this algorithm combines the desired features of the two major approaches to electromagnetic inverse procedures. Like multiple current dipole modeling methods, FOCUSS produces high resolution solutions appropriate for the highly localized sources often encountered in electromagnetic imaging. Like linear estimation methods, FOCUSS allows current sources to assume arbitrary shapes and it preserves the generality and ease of application characteristic of this group of methods. It stands apart from standard signal processing techniques because, as an initialization-dependent algorithm, it accommodates the non-unique set of feasible solutions that arise from the neuroelectric source constraints. FOCUSS is based on recursive, weighted norm minimization. The consequence of the repeated weighting procedure is, in effect, to concentrate the solution in the minimal active regions that are essential for accurately reproducing the measurements. The FOCUSS algorithm is introduced and its properties are illustrated in the context of a number of simulations, first using exact measurements in 2- and 3-D problems, and then in the presence of noise and modeling errors. The results suggest that FOCUSS is a powerful algorithm with considerable utility for tomographic current estimation.},
  journal    = {Electroencephalogr. Clin. Neurophysiol.},
  keywords   = {algorithms,Brain Mapping,Humans,Image Processing; Computer-Assisted,Magnetoencephalography,Models; Neurological},
  month      = oct,
  pmid       = {8529554},
  shorttitle = {Neuromagnetic Source Imaging with {{FOCUSS}}},
  timestamp  = {2017-04-19T14:31:47Z},
  year       = {1995},
}

@Article{Starck2003,
  author    = {Starck, J.-L. and Elad, M. and Donoho, D. L.},
  title     = {Image Decomposition: Separation of Texture from Piecewise Smooth Content},
  abstract  = {This paper presents a novel method for separating images into texture
	and piecewise smooth parts. The proposed approach is based on a combination
	of the Basis Pursuit Denoising (BPDN) algorithm and the Total-Variation
	(TV) regularization scheme. The basic idea promoted in this paper
	is the use of two appropriate dictionaries, one for the representation
	of textures, and the other for the natural scene parts. Each dictionary
	is designed for sparse representation of a particular type of image-content
	(either texture or piecewise smooth). The use of BPDN with the two
	augmented dictionaries leads to the desired separation, along with
	noise removal as a by-product. As the need to choose a proper dictionary
	for natural scene is very hard, a TV regularization is employed to
	better direct the separation process. Experimental results validate
	the algorithm's performance.},
  journal   = {SPIE Meeting},
  keywords  = {Basis pursuit denoising,Curvelet.,Local DCT,piecewise smooth,ridgelet,sparse representations,sparse representations,Texture,total variation,wavelet},
  month     = aug,
  owner     = {Fardin},
  timestamp = {2016-07-08T12:46:25Z},
  year      = {2003},
}

@Article{Starck2002,
  author    = {Starck, J.-L. and Candes, E.J. and Donoho, D.L.},
  title     = {The curvelet transform for image denoising},
  doi       = {10.1109/TIP.2002.1014998},
  issn      = {1057-7149},
  number    = {6},
  pages     = {670--684},
  volume    = {11},
  abstract  = {We describe approximate digital implementations of two new mathematical
	transforms, namely, the ridgelet transform and the curvelet transform.
	Our implementations offer exact reconstruction, stability against
	perturbations, ease of implementation, and low computational complexity.
	A central tool is Fourier-domain computation of an approximate digital
	Radon transform. We introduce a very simple interpolation in the
	Fourier space which takes Cartesian samples and yields samples on
	a rectopolar grid, which is a pseudo-polar sampling set based on
	a concentric squares geometry. Despite the crudeness of our interpolation,
	the visual performance is surprisingly good. Our ridgelet transform
	applies to the Radon transform a special overcomplete wavelet pyramid
	whose wavelets have compact support in the frequency domain. Our
	curvelet transform uses our ridgelet transform as a component step,
	and implements curvelet subbands using a filter bank of a` trous
	wavelet filters. Our philosophy throughout is that transforms should
	be overcomplete, rather than critically sampled. We apply these digital
	transforms to the denoising of some standard images embedded in white
	noise. In the tests reported here, simple thresholding of the curvelet
	coefficients is very competitive with "state of the art" techniques
	based on wavelets, including thresholding of decimated or undecimated
	wavelet transforms and also including tree-based Bayesian posterior
	mean methods. Moreover, the curvelet reconstructions exhibit higher
	perceptual quality than wavelet-based reconstructions, offering visually
	sharper images and, in particular, higher quality recovery of edges
	and of faint linear and curvilinear features. Existing theory for
	curvelet and ridgelet transforms suggests that these new approaches
	can outperform wavelet methods in certain image reconstruction problems.
	The empirical results reported here are in encouraging agreement},
  journal   = {Image Processing, IEEE Transactions on},
  keywords  = {approximate digital implementations,approximate digital Radon transform,Cartesian samples,channel bank filters,computational complexity,concentric squares geometry,curvelet coefficients,curvelet transform,decimated wavelet transforms,exact reconstruction,Filter bank,filtering theory,filtering theory,Fourier-domain,Fourier space,Fourier space,Fourier transforms,frequency domain,frequency domain,image denoising,Image reconstruction,interpolation,low computational complexity,overcomplete wavelet pyramid,pseudo-polar sampling set,Radon transforms,rectopolar grid,ridgelet transform,Sampling methods,Stability,tree-based Bayesian posterior mean methods,trous wavelet filters,undecimated wavelet transforms,visual performance,wavelet-based image reconstruction,Wavelet domain,wavelet transforms,White noise,White noise},
  month     = jun,
  owner     = {afdidehf},
  timestamp = {2016-09-29T16:09:32Z},
  year      = {2002},
}

@Article{Malmivuo2011,
  author    = {Malmivuo, Jaakko},
  title     = {Comparison of the {{Properties}} of {{EEG}} and {{MEG}} in {{Detecting}} the {{Electric Activity}} of the {{Brain}}},
  doi       = {10.1007/s10548-011-0202-1},
  issn      = {0896-0267, 1573-6792},
  language  = {en},
  number    = {1},
  pages     = {1--19},
  urldate   = {2016-10-21},
  volume    = {25},
  abstract  = {Since the detection of the first biomagnetic signals in 1963 there has been continuous discussion on the properties and relative merits of bioelectric and biomagnetic measurements. In this review article it is briefly discussed the early history of this controversy. Then the theory of the independence and interdependence of bioelectric and biomagnetic signals is explained, and a clinical study on ECG and MCG that strongly supports this theory is presented. The spatial resolutions of EEG and MEG are compared in detail, and the issue of the maximum number of electrodes in EEG is also discussed. Finally, some special properties of EEG and MEG methods are described. In brief, the conclusion is that EEG and MEG are only partially independent and their spatial resolutions are about the same. Recording both of them brings some additional information on the bioelectric activity of the brain. These two methods have certain unique properties that make either of them more beneficial in certain applications.},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AIPJNIHR\\s10548-011-0202-1.html:text/html},
  journal   = {Brain Topography},
  month     = sep,
  timestamp = {2016-10-21T11:26:03Z},
  year      = {2011},
}

@Article{Schnass2008,
  author    = {Schnass, K. and Vandergheynst, P.},
  title     = {Dictionary {{Preconditioning}} for {{Greedy Algorithms}}},
  doi       = {10.1109/TSP.2007.911494},
  issn      = {1053-587X},
  number    = {5},
  pages     = {1994--2002},
  volume    = {56},
  abstract  = {This paper introduces the concept of sensing dictionaries. It presents an alteration of greedy algorithms like thresholding or (orthogonal) matching pursuit which improves their performance in finding sparse signal representations in redundant dictionaries while maintaining the same complexity. These algorithms can be split into a sensing and a reconstruction step, and the former will fail to identify correct atoms if the cumulative coherence of the dictionary is too high. We thus modify the sensing step by introducing a special sensing dictionary. The correct selection of components is then determined by the cross cumulative coherence which can be considerably lower than the cumulative coherence. We characterize the optimal sensing matrix and develop a constructive method to approximate it. Finally, we compare the performance of thresholding and OMP using the original and modified algorithms.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\R7CQTHEH\\4479507.html:text/html},
  journal   = {IEEE Transactions on Signal Processing},
  keywords  = {approximation theory,computational complexity,cross cumulative coherence,dictionary preconditioning,Greedy algorithms,matrix algebra,OMP,optimal sensing matrix,orthogonal matching pursuit,preconditioning,sensing dictionary,signal reconstruction,signal representation,signal thresholding,sparse approximation,sparse signal representation,Thresholding},
  month     = may,
  timestamp = {2016-10-04T16:17:34Z},
  year      = {2008},
}

@Misc{SPM,
  title        = {{{SPM}} - {{Statistical Parametric Mapping}}},
  howpublished = {\url{http://www.fil.ion.ucl.ac.uk/spm/}},
  urldate      = {2017-12-25},
  timestamp    = {2017-12-25T17:01:39Z},
}

@Article{Landau1967,
  author    = {Landau, H.J.},
  title     = {Necessary density conditions for sampling and interpolation of certain entire functions},
  doi       = {10.1007/BF02395039},
  issn      = {0001-5962},
  language  = {English},
  number    = {1},
  pages     = {37--52},
  volume    = {117},
  annote    = {http://dx.doi.org/10.1007/BF02395039},
  journal   = {Acta Mathematica},
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:13:59Z},
  year      = {1967},
}

@InProceedings{Donoho2005,
  author    = {Donoho, David L. and Tanner, Jared},
  booktitle = {Proceedings of The National Academy of Sciences},
  title     = {Sparse nonnegative solution of underdetermined linear equations by linear programming},
  pages     = {9446--9451},
  volume    = {102},
  owner     = {Fardin},
  timestamp = {2016-09-30T10:45:58Z},
  year      = {2005},
}

@Article{Feuer2003,
  author    = {Feuer, A. and Nemirovski, A.},
  title     = {On sparse representation in pairs of bases},
  doi       = {10.1109/TIT.2003.811926},
  issn      = {0018-9448},
  number    = {6},
  pages     = {1579--1581},
  volume    = {49},
  abstract  = {In previous work, Elad and Bruckstein (EB) have provided a sufficient
	condition for replacing an l0 optimization by linear programming
	minimization when searching for the unique sparse representation.
	We establish here that the EB condition is both sufficient and necessary.},
  annote    = {read},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\V24CV2UH\\articleDetails.html:text/html},
  journal   = {IEEE Transactions on Information Theory},
  keywords  = {Cities and towns,Cities and towns,Dictionaries,Dictionaries,Elad-Bruckstein condition,Elad-Bruckstein condition,Industrial engineering,Industrial engineering,Industrial engineering,Laboratories,Laboratories,linear programming,linear programming,linear programming minimization,linear programming minimization,matrix algebra,matrix algebra,matrix algebra,minimisation,minimisation,necessary condition,necessary condition,optimization problems,optimization problems,orthogonal matrices,orthogonal matrices,orthogonal matrices,pairs of bases,pairs of bases,Robot control,Robot control,Service robots,Service robots,signal representation,signal representation,sparse matrices,sparse matrices,Sparse matrices,sparse representation,sparse representation,sufficient condition,sufficient condition,Sufficient conditions,Sufficient conditions,Technology management,Technology management,Technology management},
  month     = jun,
  timestamp = {2016-09-30T11:03:18Z},
  year      = {2003},
}

@Article{Donoho2003,
  author               = {Donoho, D.L. and Elad, M.},
  title                = {Optimally sparse representation in general (nonorthogonal) dictionaries via $\ell^1$ minimization},
  doi                  = {10.1073/pnas.0437847100},
  issn                 = {1091-6490},
  number               = {5},
  pages                = {2197--2202},
  volume               = {100},
  abstract             = {Given a dictionary D = {dk} of vectors dk, we seek to represent a
	signal S as a linear combination S = ∑k γ(k)dk, with scalar coefficients
	γ(k). In particular, we aim for the sparsest representation possible.
	In general, this requires a combinatorial optimization process. Previous
	work considered the special case where D is an overcomplete system
	consisting of exactly two orthobases and has shown that, under a
	condition of mutual incoherence of the two bases, and assuming that
	S has a sufficiently sparse representation, this representation is
	unique and can be found by solving a convex optimization problem:
	specifically, minimizing the l1 norm of the coefficients γ̱. In
	this article, we obtain parallel results in a more general setting,
	where the dictionary D can arise from two or several bases, frames,
	or even less structured systems. We sketch three applications: separating
	linear features from planar ones in 3D data, noncooperative multiuser
	encoding, and identification of over-complete independent component
	models.},
  annote               = {read http://dx.doi.org/10.1073/pnas.0437847100 Proceedings of the National Academy of Sciences},
  citeulike-article-id = {3823938},
  citeulike-linkout-0  = {http://dx.doi.org/10.1073/pnas.0437847100},
  citeulike-linkout-1  = {http://www.pnas.org/content/100/5/2197.full.abstract},
  citeulike-linkout-2  = {http://www.pnas.org/content/100/5/2197.full.full.pdf},
  citeulike-linkout-3  = {http://www.pnas.org/cgi/content/abstract/100/5/2197},
  citeulike-linkout-4  = {http://view.ncbi.nlm.nih.gov/pubmed/16576749},
  citeulike-linkout-5  = {http://www.hubmed.org/display.cgi?uids=16576749},
  day                  = {04},
  journal              = {in Proc. Natl. Acad. Sci.},
  keywords             = {basis_pursuit,compressive_sensing,l1_minimization,Sparsity},
  month                = mar,
  owner                = {Fardin},
  pmid                 = {16576749},
  posted-at            = {2010-06-21 09:28:19},
  timestamp            = {2016-09-29T16:09:51Z},
  year                 = {2003},
}

@Article{Gemmeke2008,
  author    = {Gemmeke, J. F. and Cranen, B.},
  title     = {Noise robust digit recognition using sparse representations},
  abstract  = {Despite the use of noise robustness techniques, automatic speech recognition
	(ASR) systems make many more recognition errors than humans, especially
	in very noisy circumstances. We argue that this inferior recognition
	performance is largely due to the fact that in ASR speech is typically
	processed on a frameby-frame basis preventing the redundancy in the
	speech signal to be optimally exploited. We present a novel non-parametric
	classification method that can handle missing data while simultaneously
	exploiting the dependencies between the reliable features in an entire
	word. We compare the new method with a state-of-the-art HMM-based
	speech decoder in which missing data are imputed on a frame-by-frame
	basis. Both methods are tested on a single digit recognition task
	(based on AURORA-2 data) using an oracle and an estimated harmonicity
	mask. We show that at an SNR of-5 dB using the reliable features
	of an entire word allows an accuracy of 91 \% (using mel-log-energy
	features in combination with an oracle mask), while a conventional
	frame-based approach achieves only 61\%. Results obtained with the
	harmonicity mask suggest that this specific mask estimation technique
	is simply unable to deliver sufficient reliable features for acceptable
	recognition rates at these low SNRs.},
  journal   = {ISCA ITRW},
  timestamp = {2016-07-10T06:55:11Z},
  year      = {2008},
}

@Article{Donoho2004,
  author    = {Donoho, David L.},
  title     = {For Most Large Underdetermined Systems of Linear Equations the Minimal $\ell^1$-norm Solution is also the Sparsest Solution},
  pages     = {797--829},
  volume    = {59},
  abstract  = {We consider linear equations y =   where y is a given vector in Rn,
	  is a given n by m matrix with n < m   An, and we wish to solve
	for   2 Rm. We suppose that the columns of   are normalized to unit
	`2 norm 1 and we place uniform measure on such  . We prove the existence
	of   =  (A) so that for large n, and for all  s except a negligible
	fraction, the following property holds: For every y having a representation
	y =  0 by a coefficient vector 0 2 Rm with fewer than   n nonzeros,
	the solution  1 of the `1 minimization problem min kxk1 subject to
	  = y is unique and equal to  0. In contrast, heuristic attempts
	to sparsely solve such systems   greedy algorithms and thresholding
	  perform poorly in this challenging setting. The techniques include
	the use of random proportional embeddings and almost-spherical sections
	in Banach space theory, and deviation bounds for the eigenvalues
	of random Wishart matrices.},
  annote    = {read},
  journal   = {Comm. Pure Appl. Math},
  keywords  = {`1,Algorithms.,Almost-Euclidean,Banach,Basis,decomposition.,Eigenvalues,Greedy,in,linear,Matching,Matrices.,minimum,of,overcomplete,Pursuit.,Pursuit,Random,Representations.,Sections,Sign-Embeddings,Solution,Spaces.,Systems.,underdetermined},
  owner     = {Fardin},
  timestamp = {2016-10-07T13:26:31Z},
  year      = {2004},
}

@Article{Berg2010,
  author    = {van den Berg, E. and Friedlander, M.P.},
  title     = {Theoretical and Empirical Results for Recovery From Multiple Measurements},
  doi       = {10.1109/TIT.2010.2043876},
  issn      = {0018-9448},
  number    = {5},
  pages     = {2516--2527},
  volume    = {56},
  abstract  = {The joint-sparse recovery problem aims to recover, from sets of compressed
	measurements, unknown sparse matrices with nonzero entries restricted
	to a subset of rows. This is an extension of the single-measurement-vector
	(SMV) problem widely studied in compressed sensing. We study the
	recovery properties of two algorithms for problems with noiseless
	data and exact-sparse representation. First, we show that recovery
	using sum-of-norm minimization cannot exceed the uniform-recovery
	rate of sequential SMV using l 1 minimization, and that there are
	problems that can be solved with one approach, but not the other.
	Second, we study the performance of the ReMBo algorithm (M. Mishali
	and Y. Eldar, ¿Reduce and boost: Recovering arbitrary sets of jointly
	sparse vectors,¿ IEEE Trans. Signal Process., vol. 56, no. 10, 4692-4702,
	Oct. 2008) in combination with l 1 minimization, and show how recovery
	improves as more measurements are taken. From this analysis, it follows
	that having more measurements than the number of linearly independent
	nonzero rows does not improve the potential theoretical recovery
	rate.},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {compressed sensing,Computer science,convex optimization,Councils,Data Compression,exact-sparse representation,joint-sparse recovery problem,joint sparsity,minimisation,multiple channels,noiseless data,ReMBo algorithm,signal processing,Signal processing algorithms,single-measurement-vector,sparse matrices,sparse recovery,sum-of-norm minimization},
  month     = may,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:22:35Z},
  year      = {2010},
}

@Article{Donoho2001,
  author    = {Donoho, D.L. and Huo, X.},
  title     = {Uncertainty principles and ideal atomic decomposition},
  doi       = {10.1109/18.959265},
  issn      = {0018-9448},
  number    = {7},
  pages     = {2845--2862},
  volume    = {47},
  abstract  = {Suppose a discrete-time signal S(t), 0?t\ensuremath{<}N, is a superposition
	of atoms taken from a combined time-frequency dictionary made of
	spike sequences 1\{t=?\} and sinusoids exp\{2?iwt/N\}/?N. Can one
	recover, from knowledge of S alone, the precise collection of atoms
	going to make up S? Because every discrete-time signal can be represented
	as a superposition of spikes alone, or as a superposition of sinusoids
	alone, there is no unique way of writing S as a sum of spikes and
	sinusoids in general. We prove that if S is representable as a highly
	sparse superposition of atoms from this time-frequency dictionary,
	then there is only one such highly sparse representation of S, and
	it can be obtained by solving the convex optimization problem of
	minimizing the l1 norm of the coefficients among all decompositions.
	Here \dbendhighly sparse\dbend means that Nt+Nw\ensuremath{<}?N/2
	where Nt is the number of time atoms, Nw is the number of frequency
	atoms, and N is the length of the discrete-time signal. Underlying
	this result is a general l1 uncertainty principle which says that
	if two bases are mutually incoherent, no nonzero signal can have
	a sparse representation in both bases simultaneously. For the above
	setting, the bases are sinusoids and spikes, and mutual incoherence
	is measured in terms of the largest inner product between different
	basis elements. The uncertainty principle holds for a variety of
	interesting basis pairs, not just sinusoids and spikes. The results
	have idealized applications to band-limited approximation with gross
	errors, to error-correcting encryption, and to separation of uncoordinated
	sources. Related phenomena hold for functions of a real variable,
	with basis pairs such as sinusoids and wavelets, and for functions
	of two variables, with basis pairs such as wavelets and ridgelets.
	In these settings, if a function f is representable by a sufficiently
	sparse superposition of terms taken from both bases, then there is
	only one such sparse representation; it may be obtained by minimum
	l1 norm atomic decomposition. The condition \dbendsufficiently sparse\dbend
	becomes a multiscale condition; for example, that the number of wavelets
	at level j plus the number of sinusoids in the jth dyadic frequency
	band are together less than a constant times 2j/2},
  annote    = {read Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {band-limited approximation,convex optimization problem,cryptography,Dictionaries,discrete-time signal length,discrete-time signal representation,dyadic frequency band,error-correcting encryption,Frequency,Harmonic analysis,ideal atomic decomposition,indeterminancy,Matching pursuit algorithms,multiscale condition,mutual incoherence,NSP,null space property,Ridgelets,Signal analysis,Signal analysis,signal representation,signal representations,signal representations,sinusoids,sparse representation,sparse representation,spike sequences,time-frequency analysis,time-frequency analysis,time-frequency dictionary,time-frequency dictionary,Uncertainty,uncertainty principles,uncoordinated source separation,uncoordinated source separation,wavelet packets,wavelets,wavelet transforms,Writing},
  month     = nov,
  timestamp = {2016-09-29T14:54:05Z},
  year      = {2001},
}

@InProceedings{Xu2007a,
  author    = {Xu, W. and Hassibi, B.},
  booktitle = {IEEE Information Theory Workshop, 2007. ITW '07},
  title     = {Efficient Compressive Sensing with Deterministic Guarantees Using Expander Graphs},
  doi       = {10.1109/ITW.2007.4313110},
  pages     = {414--419},
  abstract  = {Compressive sensing is an emerging technology which can recover a sparse signal vector of dimension n via a much smaller number of measurements than n. However, the existing compressive sensing methods may still suffer from relatively high recovery complexity, such as O(n3), or can only work efficiently when the signal is super sparse, sometimes without deterministic performance guarantees. In this paper, we propose a compressive sensing scheme with deterministic performance guarantees using expander-graphs-based measurement matrices and show that the signal recovery can be achieved with complexity O(n) even if the number of nonzero elements k grows linearly with n. We also investigate compressive sensing for approximately sparse signals using this new method. Moreover, explicit constructions of the considered expander graphs exist. Simulation results are given to show the performance and complexity of the new method.},
  keywords  = {compressive sensing scheme with,compressive sensing scheme with,computational complexity,computational complexity,Digital cameras,Digital cameras,expander graphs,expander graphs,expander-graphs-based measurement matrices,expander-graphs-based measurement matrices,Graph theory,graph theory,Lakes,Lakes,Matching pursuit algorithms,Matching pursuit algorithms,Parity check codes,parity check codes,Sampling methods,Sampling methods,signal processing,signal processing,Signal processing algorithms,Signal processing algorithms,signal sampling,Signal sampling,sparse matrices,Sparse matrices,sparse signal vector complexity,sparse signal vector complexity,Testing,Testing},
  month     = sep,
  timestamp = {2016-09-30T10:49:57Z},
  year      = {2007},
}

@Article{Elad2002a,
  author    = {Elad, M. and Bruckstein, A.M.},
  title     = {A generalized uncertainty principle and sparse representation in pairs of bases},
  doi       = {10.1109/TIT.2002.801410},
  issn      = {0018-9448},
  number    = {9},
  pages     = {2558--2567},
  volume    = {48},
  abstract  = {An elementary proof of a basic uncertainty principle concerning pairs
	of representations of RN vectors in different orthonormal bases is
	provided. The result, slightly stronger than stated before, has a
	direct impact on the uniqueness property of the sparse representation
	of such vectors using pairs of orthonormal bases as overcomplete
	dictionaries. The main contribution in this paper is the improvement
	of an important result due to Donoho and Huo (2001) concerning the
	replacement of the l0 optimization problem by a linear programming
	(LP) minimization when searching for the unique sparse representation.},
  annote    = {read Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Cities and towns,Computer science,Dictionaries,Dynamic Programming,Electronic mail,generalized uncertainty principle,image coding,Image Processing,Linear algebra,linear programming,linear programming minimization,minimisation,NSP,null space property,null space property,Optimization,orthonormal bases,orthonormal bases,overcomplete dictionaries,pairs of bases,pairs of bases,signal representation,sparse representation,sparse representation,Uncertainty,uniqueness property,uniqueness property,Vectors},
  month     = sep,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:09:06Z},
  year      = {2002},
}

@Article{Fornasier2008,
  author    = {Fornasier, M. and Rauhut, H.},
  title     = {Recovery {{Algorithms}} for {{Vector}}-{{Valued Data}} with {{Joint Sparsity Constraints}}},
  doi       = {10.1137/0606668909},
  issn      = {0036-1429},
  number    = {2},
  pages     = {577--613},
  urldate   = {2016-05-30},
  volume    = {46},
  abstract  = {Vector-valued data appearing in concrete applications often possess sparse expansions with respect to a preassigned frame for each vector component individually. Additionally, different components may also exhibit common sparsity patterns. Recently, there were introduced sparsity measures that take into account such joint sparsity patterns, promoting coupling of nonvanishing components. These measures are typically constructed as weighted \$$\backslash$ell\_1\$ norms of componentwise \$$\backslash$ell\_q\$ norms of frame coefficients. We show how to compute solutions of linear inverse problems with such joint sparsity regularization constraints by fast thresholded Landweber algorithms. Next we discuss the adaptive choice of suitable weights appearing in the definition of sparsity measures. The weights are interpreted as indicators of the sparsity pattern and are iteratively updated after each new application of the thresholded Landweber algorithm. The resulting two-step algorithm is interpreted as a double-minimization scheme for a suitable target functional. We show its \$$\backslash$ell\_2\$-norm convergence. An implementable version of the algorithm is also formulated, and its norm convergence is proven. Numerical experiments in color image restoration are presented.},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\K2MA98A9\\0606668909.html:text/html},
  journal   = {SIAM Journal on Numerical Analysis},
  month     = jan,
  timestamp = {2016-09-30T11:21:47Z},
  year      = {2008},
}

@PhdThesis{Ayaz2014,
  author    = {Ayaz, Ulas},
  title     = {Sparse Recovery with Fusion Frames},
  abstract  = {Sparse signal structures have become increasingly important in signal
	processing applications as the technology progresses and a plethora
	of data needs to be handled. It has been shown in practice that various
	signals have fewer degrees of freedom compared to their actual sizes,
	i.e., can be expressed in terms of a small number of elements from
	some dictionary. Alongside an increase in applications, a recent
	theory of sparse and compressible signal recovery has been recently
	developed under the name of Compressed Sensing (CS). This approach
	states that a sparse signal can be eciently recovered from a small
	number of random linear measurements. Another powerful tool in signal
	processing is frames which provide redundant representations for
	signals. Such redundancy is desirable in many applications where
	resilience to errors and losses in data is important. The increase
	in data has also signicantly increased the demand to model applications
	requiring distributed processing which goes beyond the classical
	frames. The recent theory of Fusion Frames, which can be regarded
	as a generalization of classical frames, satises those needs by
	analyzing signals by projecting them onto multidimensional subspaces.
	Fusion frames provide a suitable mathematical framework to model
	large data systems such as sensor networks, transmission of data
	of communication networks, etc. In this thesis, we combine these
	two recent theories and consider the recovery of signals that have
	a sparse representation in a fusion frame. As in the classical CS,
	a sparse signal from a fusion frame can be sampled using very few
	random projections and eciently recovered using a convex optimization
	that minimizes the mixed `1=`2-norm. This problem has close connections
	with other similar recovery problems studied in CS literature such
	as block sparsity and joint sparsity problems. A key contribution
	in this thesis is to exploit the incoherence of the fusion frame
	subspaces in order to enhance the existing recovery results by incorporating
	this structure. In particular, we derive upper and lower bounds for
	the number of measurements required for the sparse recovery and the
	error derived by convex optimization. Aside from our results in the
	fusion frame setup, we also present results in the classical CS where
	we focus on improving constants appearing in the number of measurements
	required and prove optimal constants in the nonuniform setting with
	rather concise and simple proofs.},
  annote    = {PDF \& PPT},
  owner     = {afdidehf},
  school    = {Rheinischen Friedrich-Wilhelms-Universit{\"a}t Bonn},
  timestamp = {2017-06-23T09:52:54Z},
  year      = {2014},
}

@Article{Peotta2007,
  author    = {Peotta, L.. and Vandergheynst, P.},
  title     = {Matching Pursuit With Block Incoherent Dictionaries},
  doi       = {10.1109/TSP.2007.896022},
  issn      = {1053-587X},
  number    = {9},
  pages     = {4549--4557},
  volume    = {55},
  abstract  = {Recently, there has been an intense activity in the field of sparse
	approximations with redundant dictionaries, largely motivated by
	the practical performances of algorithms such as matching pursuit
	(MP) and basis pursuit (BP). However, most of the theoretical results
	obtained so far are valid only for the restricted class of incoherent
	dictionaries. This paper investigates a new class of overcomplete
	dictionaries, called block incoherent dictionaries, where coherence
	can be arbitrarily big. We show that a simple greedy algorithm can
	correctly identify stable subdictionaries (called blocks) and demonstrate
	how one can use the extra coherence freedom for approximation purposes.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {Approximation methods,approximation theory,basis pursuit algorithm,block incoherent dictionaries,Block-incoherent dictionaries,Block-incoherent dictionaries,block incoherent dictionaries,data processing,Dictionaries,greedy algorithm,Greedy algorithms,Harmonic analysis,Helium,matching pursuit algorithm,matching pursuit algorithm,Matching pursuit algorithms,matching pursuit (MP),Pursuit algorithms,redundant dictionaries,signal representation,signal representations,Signal synthesis,sparse signal approximation,sparse signal representation},
  month     = sep,
  owner     = {afdidehf},
  timestamp = {2016-09-30T13:51:38Z},
  year      = {2007},
}

@Article{Ayaz2016,
  author    = {Ayaz, U. and Dirksen, S. and Rauhut, H.},
  title     = {Uniform Recovery of Fusion Frame Structured Sparse Signals},
  doi       = {10.1016/j.acha.2016.03.006},
  issn      = {1063-5203},
  number    = {2},
  pages     = {341--361},
  series    = {Sparse Representations with Applications in Imaging Science, Data Analysis, and Beyond, Part IISI: ICCHAS Outgrowth, part 2},
  urldate   = {2016-10-12},
  volume    = {41},
  abstract  = {We consider the problem of recovering fusion frame sparse signals from incomplete measurements. These signals are composed of a small number of nonzero blocks taken from a family of subspaces. First, we show that, by using a-priori knowledge of a coherence parameter associated with the angles between the subspaces, one can uniformly recover fusion frame sparse signals with a significantly reduced number of vector-valued (sub-)Gaussian measurements via mixed $\mathscr{l}$ 1 / $\mathscr{l}$ 2 -minimization. We prove this by establishing an appropriate version of the restricted isometry property. Our result complements previous nonuniform recovery results in this context, and provides stronger stability guarantees for noisy measurements and approximately sparse signals. Second, we determine the minimal number of scalar-valued measurements needed to uniformly recover all fusion frame sparse signals via mixed $\mathscr{l}$ 1 / $\mathscr{l}$ 2 -minimization. This bound is achieved by scalar-valued subgaussian measurements. In particular, our result shows that the number of scalar-valued subgaussian measurements cannot be further reduced using knowledge of the coherence parameter. As a special case it implies that the best known uniform recovery result for block sparse signals using subgaussian measurements is optimal.},
  file      = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\555CV56Z\\S1063520316000294.html:text/html},
  journal   = {Applied and Computational Harmonic Analysis},
  keywords  = {block sparsity,compressed sensing,fusion frames,Mixed ℓ 1 / ℓ 2 -minimization,restricted isometry property},
  month     = sep,
  timestamp = {2016-10-12T13:01:24Z},
  year      = {2016},
}

@Article{Zhao2009,
  author    = {Zhao, Peng and Rocha, Guilherme and Yu, Bin},
  title     = {The Composite Absolute Penalties Family for Grouped and Hierarchical Variable Selection},
  number    = {6},
  pages     = {3468--3497},
  volume    = {37},
  abstract  = {Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the L1-penalized squared error minimization method Lasso has been popular in regression models and beyond. In this paper, we combine different norms including L1 to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached},
  file      = {Citeseer - Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8WKJ3VQ3\\summary.html:text/html},
  journal   = {Ann. Stat.},
  timestamp = {2017-04-19T14:40:19Z},
  year      = {2009},
}

@Article{Olshausen1997,
  author    = {Olshausen, Bruno A. and Field, David J.},
  title     = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  doi       = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
  issn      = {0042-6989},
  number    = {23},
  pages     = {3311--3325},
  volume    = {37},
  abstract  = {The spatial receptive fields of simple cells in mammalian striate
	cortex have been reasonably well described physiologically and can
	be characterized as being localized, oriented, and bandpass, comparable
	with the basis functions of wavelet transforms. Previously, we have
	shown that these receptive field properties may be accounted for
	in terms of a strategy for producing a sparse distribution of output
	activity in response to natural images. Here, in addition to describing
	this work in a more expansive fashion, we examine the neurobiological
	implications of sparse coding. Of particular interest is the case
	when the code is overcomplete—i.e., when the number of code elements
	is greater than the effective dimensionality of the input space.
	Because the basis functions are non-orthogonal and not linearly independent
	of each other, sparsifying the code will recruit only those basis
	functions necessary for representing a given input, and so the input-output
	function will deviate from being purely linear. These deviations
	from linearity provide a potential explanation for the weak forms
	of non-linearity observed in the response properties of cortical
	simple cells, and they further make predictions about the expected
	interactions among units in response to naturalistic stimuli.},
  annote    = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  journal   = {Vision Research},
  keywords  = {coding,Gabor-wavelet,Neural images,V1},
  owner     = {Fardin},
  timestamp = {2016-09-29T16:27:52Z},
  year      = {1997},
}

@Article{Tibshirani1994,
  author    = {Tibshirani, Robert},
  title     = {Regression Shrinkage and Selection Via the Lasso},
  pages     = {267--288},
  volume    = {58},
  abstract  = {We propose a new method for estimation in linear models. The "lasso"
	minimizes the residual sum of squares subject to the sum of the absolute
	value of the coefficients being less than a constant. Because of
	the nature of this constraint it tends to produce some coefficients
	that are exactly zero and hence gives interpretable models. Our simulation
	studies suggest that the lasso enjoys some of the favourable properties
	of both subset selection and ridge regression. It produces interpretable
	models like subset selection and exhibits the stability of ridge
	regression. There is also an interesting relationship with recent
	work in adaptive function estimation by Donoho and Johnstone. The
	lasso idea is quite general and can be applied in a variety of statistical
	models: extensions to generalized regression models and tree-based
	models are briefly described},
  journal   = {Journal of the Royal Statistical Society, Series B},
  keywords  = {quadratic programming,regression,shrinkage,subset selection},
  owner     = {Fardin},
  timestamp = {2016-09-30T10:50:42Z},
  year      = {1994},
}

@Article{Cai2010,
  author    = {Cai, T.T. and Wang, Lie and Xu, Guangwu},
  title     = {Shifting Inequality and Recovery of Sparse Signals},
  doi       = {10.1109/TSP.2009.2034936},
  issn      = {1053-587X},
  number    = {3},
  pages     = {1300--1308},
  volume    = {58},
  abstract  = {In this paper, we present a concise and coherent analysis of the constrained
	??1 minimization method for stable recovering of high-dimensional
	sparse signals both in the noiseless case and noisy case. The analysis
	is surprisingly simple and elementary, while leads to strong results.
	In particular, it is shown that the sparse recovery problem can be
	solved via ??1 minimization under weaker conditions than what is
	known in the literature. A key technical tool is an elementary inequality,
	called Shifting Inequality, which, for a given nonnegative decreasing
	sequence, bounds the ??2 norm of a subsequence in terms of the ??1
	norm of another subsequence by shifting the elements to the upper
	end.},
  annote    = {PDF \& PPT, read},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {$ell_{1}$ minimization,constrained minimization method,high-dimensional sparse signals,minimisation,nonnegative decreasing sequence,restricted isometry property,shifting inequality,signal processing,signal reconstruction,sparse recovery,sparse recovery problem,sparse signals recovery},
  month     = mar,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:36:16Z},
  year      = {2010},
}

@Article{Cand`es2005b,
  author    = {Cand{\`e}s, E.J. and Tao, T.},
  title     = {Decoding by linear programming},
  doi       = {10.1109/TIT.2005.858979},
  issn      = {0018-9448},
  number    = {12},
  pages     = {4203--4215},
  volume    = {51},
  abstract  = {This paper considers a natural error correcting problem with real
	valued input/output. We wish to recover an input vector f?Rn from
	corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix
	and e is an arbitrary and unknown vector of errors. Is it possible
	to recover f exactly from the data y? We prove that under suitable
	conditions on the coding matrix A, the input f is the unique solution
	to the ?1-minimization problem (||x||?1:=?i|xi|) min(g?Rn) ||y -
	Ag||?1 provided that the support of the vector of errors is not too
	large, ||e||?0:=|{i:ei ? 0}|??m for some ?>0. In short, f can
	be recovered exactly by solving a simple convex optimization problem
	(which one can recast as a linear program). In addition, numerical
	experiments suggest that this recovery procedure works unreasonably
	well; f is recovered exactly even in situations where a significant
	fraction of the output is corrupted. This work is related to the
	problem of finding sparse solutions to vastly underdetermined systems
	of linear equations. There are also significant connections with
	the problem of recovering signals from highly incomplete measurements.
	In fact, the results introduced in this paper improve on our earlier
	work. Finally, underlying the success of ?1 is a crucial property
	we call the uniform uncertainty principle that we shall describe
	in detail.},
  annote    = {read Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Basis pursuit,convex programming,Decoding,Decoding of (random) linear codes,duality in optimization,Equations,Error correction,error correction codes,Gaussian processes,Gaussian random matrices,Gaussian random matrix,indeterminancy,information theory,Linear code,linear code decoding,linear codes,linear programming,Mathematics,minimisation,minimization problem,natural error correcting problem,principal angles,random codes,restricted orthonormality,simple convex optimization problem,singular values of random matrices,sparse matrices,sparse solution,sparse solutions to underdetermined systems,uncertainty principle,Vectors},
  month     = dec,
  owner     = {Fardin},
  timestamp = {2017-06-23T13:09:07Z},
  year      = {2005},
}

@PhdThesis{Huo1999,
  author    = {Huo, Xiaoming},
  title     = {Sparse Image Representation Via Combined Transforms},
  abstract  = {We consider sparse image decomposition, in the hope that a sparser
	decomposition of an image may lead to a more efficient method of
	image coding or compression. Recently, many transforms have been
	proposed. Typically, each of them is good at processing one class
	of features in an image but not other features. For example, the
	2-D wavelet transform is good at processing point singularities and
	patches in an image but not linear singularities, while a recently
	proposed method the edgelet-like transform is good for linear
	singularities, but not for points. Intuitively, a combined scheme
	may lead to a sparser decomposition than a scheme using only a single
	transform. Combining several transforms, we get an overcomplete system
	or dictionary. For a given image, there are infinitely many ways
	to decompose it. How to find the one with the sparsest coefficients?
	We follow the idea of Basis Pursuit finding a minimum 1 norm solution.
	Some intuitive discussion and theoretical results show that this
	method is optimal in many cases. A big challenge in solving a minimum
	1 norm problem is the computational complexity. In many cases, due
	to the intrinsic nature of the high-dimensionality of images, finding
	the minimum 1 norm solution is nearly impossible. We take advantage
	of the recent advances in convex optimization and iterative methods.
	Our approach is mainly based on two facts: first, we have fast algorithms
	for each transform; second, we have efficient iterative algorithms
	to solve for the Newton direction. The numerical results (to some
	extent) verify our intuitions, in the sense that: [1] the combined
	scheme does give sparser representations than a scheme applying only
	a single transform; [2] each transform in the combined scheme captures
	the features that this transform is good at processing. (Actually,
	[2] is an extension of [1].) With improved efficiency in numerical
	algorithms, this approach has the promise of producing more compact
	image coding and compression schemes than existing ones.},
  month     = aug,
  owner     = {afdidehf},
  school    = {The Department Of Statistics Of Stanford University},
  timestamp = {2017-06-23T13:12:52Z},
  year      = {1999},
}

@Book{Golub2013,
  author    = {Golub, Gene H. and Loan, Charles F. van},
  title     = {Matrix Computations},
  edition   = {4},
  publisher = {{The Johns Hopkins Univ. Press}},
  annote    = {Johns Hopkins Series in the Mathematical Sciences},
  keywords  = {FUNDAMENTALS,Mathematics},
  owner     = {afdidehf},
  timestamp = {2017-04-20T09:22:14Z},
  year      = {2013},
}

@Article{Akalin-Acar2004,
  author    = {Akalin-Acar, Zeynep and Gen{\c c}er, Nevzat G.},
  title     = {An Advanced Boundary Element Method ({{BEM}}) Implementation for the Forward Problem of Electromagnetic Source Imaging},
  issn      = {0031-9155},
  language  = {eng},
  number    = {21},
  pages     = {5011--5028},
  volume    = {49},
  abstract  = {The forward problem of electromagnetic source imaging has two components: a numerical model to solve the related integral equations and a model of the head geometry. This study is on the boundary element method (BEM) implementation for numerical solutions and realistic head modelling. The use of second-order (quadratic) isoparametric elements and the recursive integration technique increase the accuracy in the solutions. Two new formulations are developed for the calculation of the transfer matrices to obtain the potential and magnetic field patterns using realistic head models. The formulations incorporate the use of the isolated problem approach for increased accuracy in solutions. If a personal computer is used for computations, each transfer matrix is calculated in 2.2 h. After this pre-computation period, solutions for arbitrary source configurations can be obtained in milliseconds for a realistic head model. A hybrid algorithm that uses snakes, morphological operations, region growing and thresholding is used for segmentation. The scalp, skull, grey matter, white matter and eyes are segmented from the multimodal magnetic resonance images and meshes for the corresponding surfaces are created. A mesh generation algorithm is developed for modelling the intersecting tissue compartments, such as eyes. To obtain more accurate results quadratic elements are used in the realistic meshes. The resultant BEM implementation provides more accurate forward problem solutions and more efficient calculations. Thus it can be the firm basis of the future inverse problem solutions.},
  journal   = {Physics in Medicine and Biology},
  keywords  = {algorithms,Brain,Brain Mapping,Computer Simulation,Diagnosis; Computer-Assisted,Electroencephalography,finite element analysis,Head,Humans,Magnetoencephalography,Models; Neurological,Reproducibility of Results,Sensitivity and Specificity},
  month     = nov,
  pmid      = {15584534},
  timestamp = {2017-08-22T10:00:49Z},
  year      = {2004},
}

@Article{Wyner1979,
  author    = {Wyner, A.D.},
  title     = {An analog scrambling scheme which does not expand bandwidth, Part II: Continuous time},
  doi       = {10.1109/TIT.1979.1056071},
  issn      = {0018-9448},
  number    = {4},
  pages     = {415--425},
  volume    = {25},
  abstract  = {The techniques developed in Part I[1] for discrete-time analog scrambling
	are applied to the problem of scrambling band-limited continuous-time
	signals or waveforms. The idea behind the waveform scrambler is to
	sample the waveform (which is assumed to be band-limited) at a rate
	exceeding the Nyquist rate. The resulting sequence of samples is
	band-limited in the sense of Part I. The discrete-time scrambler
	described in Part I is applied to this sequence to produce a nearly
	band-limited scrambled sequence. A scrambled waveform is formed by
	modulating the amplitudes of a chain of pulses. This scrambled waveform
	can be transmitted over a band-limited channel, and the original
	unscrambled waveform can be recovered at the receiver.},
  annote    = {Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Amplitude modulation,Bandwidth,cryptography,Fourier transforms,Frequency,Privacy,Pulse modulation,Sampling methods,Signal sampling/reconstruction},
  month     = jul,
  owner     = {afdidehf},
  timestamp = {2016-09-30T10:47:08Z},
  year      = {1979},
}

@Article{Cand`es2006,
  author    = {Cand{\`e}s, Emmanuel and Romberg, Justin and Tao, Terence},
  title     = {Stable signal recovery from incomplete and inaccurate measurements},
  doi       = {10.1002/cpa.20124},
  number    = {8},
  pages     = {1207--1223},
  volume    = {59},
  annote    = {read},
  journal   = {Communications on Pure and Applied Mathematics},
  month     = aug,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:36:39Z},
  year      = {2006},
}

@Article{Gribonval2007,
  author    = {Gribonval, R. and Nielsen, M.},
  title     = {Highly sparse representations from dictionaries are unique and independent of the sparseness measure},
  doi       = {http://dx.doi.org/10.1016/j.acha.2006.09.003},
  issn      = {1063-5203},
  number    = {3},
  pages     = {335--355},
  volume    = {22},
  abstract  = {The purpose of this paper is to study sparse representations of signals
	from a general dictionary in a Banach space. For so-called localized
	frames in Hilbert spaces, the canonical frame coefficients are shown
	to provide a near sparsest expansion for several sparseness measures.
	However, for frames which are not localized, this no longer holds
	true and sparse representations may depend strongly on the choice
	of the sparseness measure. A large class of admissible sparseness
	measures is introduced, and we give sufficient conditions for having
	a unique sparse representation of a signal from the dictionary w.r.t.
	such a sparseness measure. Moreover, we give sufficient conditions
	on a signal such that the simple solution of a linear programming
	problem simultaneously solves all the nonconvex (and generally hard
	combinatorial) problems of sparsest representation of the signal
	w.r.t. arbitrary admissible sparseness measures.},
  annote    = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  journal   = {Appl. Comput. Harmonic Anal.},
  keywords  = {incoherent dictionary,linear programming,Localized frame,Localized frame,nonconvex optimization,nonconvex optimization,redundant dictionary,Sparseness measure,sparse representation},
  owner     = {Fardin},
  timestamp = {2017-04-19T15:29:43Z},
  year      = {2007},
}

@Article{Mishali2008,
  author    = {Mishali, M. and Eldar, Y.C.},
  title     = {Reduce and Boost: Recovering Arbitrary Sets of Jointly Sparse Vectors},
  doi       = {10.1109/TSP.2008.927802},
  issn      = {1053-587X},
  number    = {10},
  pages     = {4692--4702},
  volume    = {56},
  abstract  = {The rapid developing area of compressed sensing suggests that a sparse
	vector lying in a high dimensional space can be accurately and efficiently
	recovered from only a small set of nonadaptive linear measurements,
	under appropriate conditions on the measurement matrix. The vector
	model has been extended both theoretically and practically to a finite
	set of sparse vectors sharing a common sparsity pattern. In this
	paper, we treat a broader framework in which the goal is to recover
	a possibly infinite set of jointly sparse vectors. Extending existing
	algorithms to this model is difficult due to the infinite structure
	of the sparse vector set. Instead, we prove that the entire infinite
	set of sparse vectors can be recovered by solving a single, reduced-size
	finite-dimensional problem, corresponding to recovery of a finite
	set of sparse vectors. We then show that the problem can be further
	reduced to the basic model of a single sparse vector by randomly
	combining the measurements. Our approach is exact for both countable
	and uncountable sets, as it does not rely on discretization or heuristic
	techniques. To efficiently find the single sparse vector produced
	by the last reduction step, we suggest an empirical boosting strategy
	that improves the recovery ability of any given suboptimal method
	for recovering a sparse vector. Numerical experiments on random data
	demonstrate that, when applied to infinite sets, our strategy outperforms
	discretization techniques in terms of both run time and empirical
	recovery rate. In the finite model, our boosting algorithm has fast
	run time and much higher recovery rate than known popular methods.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {arbitrary sets,Basis pursuit,boosting algorithm,compressed sensing,empirical boosting strategy,finite-dimensional problem,jointly sparse vectors,multiple measurement vectors (MMV),multiple measurement vectors (MMVs),signal representation,single sparse vector,sparse representation,suboptimal method},
  month     = oct,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:20:49Z},
  year      = {2008},
}

@Article{Mishali2009,
  author    = {Mishali, M. and Eldar, Y.C.},
  title     = {Blind Multiband Signal Reconstruction: Compressed Sensing for Analog Signals},
  doi       = {10.1109/TSP.2009.2012791},
  issn      = {1053-587X},
  number    = {3},
  pages     = {993--1009},
  volume    = {57},
  abstract  = {We address the problem of reconstructing a multiband signal from its
	sub-Nyquist pointwise samples, when the band locations are unknown.
	Our approach assumes an existing multi-coset sampling. To date, recovery
	methods for this sampling strategy ensure perfect reconstruction
	either when the band locations are known, or under strict restrictions
	on the possible spectral supports. In this paper, only the number
	of bands and their widths are assumed without any other limitations
	on the support. We describe how to choose the parameters of the multi-coset
	sampling so that a unique multiband signal matches the given samples.
	To recover the signal, the continuous reconstruction is replaced
	by a single finite-dimensional problem without the need for discretization.
	The resulting problem is studied within the framework of compressed
	sensing, and thus can be solved efficiently using known tractable
	algorithms from this emerging area. We also develop a theoretical
	lower bound on the average sampling rate required for blind signal
	reconstruction, which is twice the minimal rate of known-spectrum
	recovery. Our method ensures perfect reconstruction for a wide class
	of signals sampled at the minimal rate, and provides a first systematic
	study of compressed sensing in a truly analog setting. Numerical
	experiments are presented demonstrating blind sampling and reconstruction
	with minimal sampling rate.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {analog signals,average sampling rate,blind multiband signal reconstruction,compressed sensing,data compression,multiband,multicoset sampling,multiple measurement vectors (MMV),nonuniform periodic sampling,sampling strategy,signal reconstruction,signal sampling,single finite-dimensional problem,Sparsity,spectrum recovery,sub-Nyquist pointwise samples},
  month     = mar,
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:15:58Z},
  year      = {2009},
}

@Article{Cand`es2002,
  author    = {Cand{\`e}s, Emmanuel J. and Donoho, David L.},
  title     = {New Tight Frames of Curvelets and Optimal Representations of Objects with C2 Singularities},
  pages     = {219--266},
  volume    = {57},
  abstract  = {This paper introduces new tight frames of curvelets to address the
	problem of finding optimally sparse representations of objects with
	discontinuities along C 2 edges. Conceptually, the curvelet transform
	is a multiscale pyramid with many directions and positions at each
	length scale, and needle-shaped elements at fine scales. These elements
	have many useful geometric multiscale features that set them apart
	from classical multiscale representations such as wavelets. For instance,
	curvelets obey a parabolic scaling relation which says that at scale
	2?j , each element has an envelope which is aligned along a ridge
	of length 2?j/2 and width 2?j . We prove that curvelets provide an
	essentially optimal representation of typical objects f which are
	C 2 except for discontinuities along C 2 curves. Such representations
	are nearly as sparse as if f were not singular and turn out to be
	far more sparse than the wavelet decomposition of the object. For
	instance, the n-term partial reconstruction f C n obtained by selecting
	the n largest terms in the curvelet series obeys kf ? f C n k 2 L2
	? C  n ?2  (log n) 3 , n ? ?. This rate of convergence holds
	uniformly over a class of functions which are C 2 except for discontinuities
	along C 2 curves and is essentially optimal. In comparison, the squared
	error of n-term wavelet approximations only converges as n ?1 as
	n ? ?, which is considerably worst than the optimal behavior.},
  journal   = {Comm. Pure Appl. Math.},
  keywords  = {Curvelets,edges,nonlinear approximation,nonlinear approximation,Radon transform.,second dyadic decomposition,second dyadic decomposition,singularities,thresholding,wavelets},
  month     = nov,
  owner     = {Fardin},
  timestamp = {2017-06-23T13:05:01Z},
  year      = {2002},
}

@InProceedings{Gilbert2007,
  author     = {Gilbert, A. C. and Strauss, M. J. and Tropp, J. A. and Vershynin, R.},
  booktitle  = {Proceedings of the Thirty-ninth Annual ACM Symposium on Theory of Computing},
  title      = {One Sketch for All: Fast Algorithms for Compressed Sensing},
  doi        = {10.1145/1250790.1250824},
  isbn       = {978-1-59593-631-8},
  pages      = {237--246},
  publisher  = {{ACM}},
  series     = {STOC '07},
  urldate    = {2016-05-15},
  abstract   = {Compressed Sensing is a new paradigm for acquiring the compressible signals that arise in many applications. These signals can be approximated using an amount of information much smaller than the nominal dimension of the signal. Traditional approaches acquire the entire signal and process it to extract the information. The new approach acquires a small number of nonadaptive linear measurements of the signal and uses sophisticated algorithms to determine its information content. Emerging technologies can compute these general linear measurements of a signal at unit cost per measurement. This paper exhibits a randomized measurement ensemble and a signal reconstruction algorithm that satisfy four requirements: 1. The measurement ensemble succeeds for all signals, with high probability over the random choices in its construction. 2. The number of measurements of the signal is optimal, except for a factor polylogarithmic in the signal length. 3. The running time of the algorithm is polynomial in the amount of information in the signal and polylogarithmic in the signal length. 4. The recovery algorithm offers the strongest possible type of error guarantee. Moreover, it is a fully polynomial approximation scheme with respect to this type of error bound. Emerging applications demand this level of performance. Yet no otheralgorithm in the literature simultaneously achieves all four of these desiderata.},
  address    = {New York, NY, USA},
  keywords   = {approximation,Approximation,embedding,embedding,group testing,group testing,sketching,sketching,sparse approximation,sparse approximation,sublinear algorithms,sublinear algorithms},
  shorttitle = {One Sketch for All},
  timestamp  = {2016-09-30T10:48:36Z},
  year       = {2007},
}

@Article{Mishali2010,
  author    = {Mishali, M. and Eldar, Y.C.},
  title     = {From Theory to Practice: Sub-Nyquist Sampling of Sparse Wideband Analog Signals},
  doi       = {10.1109/JSTSP.2010.2042414},
  issn      = {1932-4553},
  number    = {2},
  pages     = {375--391},
  volume    = {4},
  abstract  = {Conventional sub-Nyquist sampling methods for analog signals exploit
	prior information about the spectral support. In this paper, we consider
	the challenging problem of blind sub-Nyquist sampling of multiband
	signals, whose unknown frequency support occupies only a small portion
	of a wide spectrum. Our primary design goals are efficient hardware
	implementation and low computational load on the supporting digital
	processing. We propose a system, named the modulated wideband converter,
	which first multiplies the analog signal by a bank of periodic waveforms.
	The product is then low-pass filtered and sampled uniformly at a
	low rate, which is orders of magnitude smaller than Nyquist. Perfect
	recovery from the proposed samples is achieved under certain necessary
	and sufficient conditions. We also develop a digital architecture,
	which allows either reconstruction of the analog input, or processing
	of any band of interest at a low rate, that is, without interpolating
	to the high Nyquist rate. Numerical simulations demonstrate many
	engineering aspects: robustness to noise and mismodeling, potential
	hardware simplifications, real-time performance for signals with
	time-varying support and stability to quantization effects. We compare
	our system with two previous approaches: periodic nonuniform sampling,
	which is bandwidth limited by existing hardware devices, and the
	random demodulator, which is restricted to discrete multitone signals
	and has a high computational load. In the broader context of Nyquist
	sampling, our scheme has the potential to break through the bandwidth
	barrier of state-of-the-art analog conversion technologies such as
	interleaved converters.},
  annote    = {Selected Topics in Signal Processing, IEEE Journal of},
  journal   = {IEEE J. Sel. Topics Signal Process.},
  keywords  = {analog input reconstruction,analog-to-digital conversion,Analog-to-digital conversion (ADC),analogue-digital conversion,blind source separation,blind sub-Nyquist sampling,compressive sampling (CS),demodulators,digital architecture,digital processing,discrete multitone signal,hardware device,infinite measurement vectors (IMV),low-pass filter,low-pass filters,modulated wideband converter,multiband sampling,multiband signal,numerical analysis,Numerical simulation,periodic nonuniform sampling,periodic waveform,random demodulator,signal reconstruction,signal sampling,sparse wideband analog signal,spectral support,spectrum-blind reconstruction,sub-Nyquist sampling,waveform analysis},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:19:09Z},
  year      = {2010},
}

@InProceedings{Elhamifar2012c,
  author     = {Elhamifar, E. and Sapiro, G. and Vidal, R.},
  booktitle  = {2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {See all by looking at a few: Sparse modeling for finding representative objects},
  doi        = {10.1109/CVPR.2012.6247852},
  pages      = {1600--1607},
  abstract   = {We consider the problem of finding a few representatives for a dataset, i.e., a subset of data points that efficiently describes the entire dataset. We assume that each data point can be expressed as a linear combination of the representatives and formulate the problem of finding the representatives as a sparse multiple measurement vector problem. In our formulation, both the dictionary and the measurements are given by the data matrix, and the unknown sparse codes select the representatives via convex optimization. In general, we do not assume that the data are low-rank or distributed around cluster centers. When the data do come from a collection of low-rank models, we show that our method automatically selects a few representatives from each low-rank model. We also analyze the geometry of the representatives and discuss their relationship to the vertices of the convex hull of the data. We show that our framework can be extended to detect and reject outliers in datasets, and to efficiently deal with new observations and large datasets. The proposed framework and theoretical foundations are illustrated with examples in video summarization and image classification using representatives.},
  keywords   = {Clustering algorithms,Clustering algorithms,codes,codes,convex hull,convex hull,convex optimization,convex optimization,convex programming,convex programming,convex programming,data matrix,data matrix,Data models,Data models,Dictionaries,Dictionaries,dictionary,Dictionary,Distributed databases,Distributed databases,Distributed databases,Geometry,Geometry,image classification,image classification,matrix algebra,matrix algebra,Optimization,Optimization,outlier detection,outlier detection,outlier detection,Outlier rejection,Outlier rejection,representative object,representative object,sparse code,sparse code,sparse matrices,sparse matrices,Sparse matrices,sparse modeling,sparse modeling,sparse multiple measurement vector problem,sparse multiple measurement vector problem,vertex,vertex,vertex functions,vertex functions,vertex functions,video signal processing,video signal processing,video summarization,video summarization},
  month      = jun,
  shorttitle = {See all by looking at a few},
  timestamp  = {2016-09-30T13:28:33Z},
  year       = {2012},
}

@Article{Elhamifar2012b,
  author    = {Elhamifar, E. and Vidal, R.},
  title     = {Block-Sparse Recovery via Convex Optimization},
  doi       = {10.1109/TSP.2012.2196694},
  number    = {8},
  pages     = {4094--4107},
  volume    = {60},
  abstract  = {Given a dictionary that consists of multiple blocks and a signal that
	lives in the range space of only a few blocks, we study the problem
	of finding a block-sparse representation of the signal, i.e., a representation
	that uses the minimum number of blocks. Motivated by signal/image
	processing and computer vision applications, such as face recognition,
	we consider the block-sparse recovery problem in the case where the
	number of atoms in each block is arbitrary, possibly much larger
	than the dimension of the underlying subspace. To find a blocksparse
	representation of a signal, we propose two classes of non-convex
	optimization programs, which aim to minimize the number of nonzero
	coefficient blocks and the number of nonzero reconstructed vectors
	from the blocks, respectively. Since both classes of problems are
	NP-hard, we propose convex relaxations and derive conditions under
	which each class of the convex programs is equivalent to the original
	non-convex formulation. Our conditions depend on the notions of mutual
	and cumulative subspace coherence of a dictionary, which are natural
	generalizations of existing notions of mutual and cumulative coherence.
	We evaluate the performance of the proposed convex programs through
	simulations as well as real experiments on face recognition. We show
	that treating the face recognition problem as a block-sparse recovery
	problem improves the state-of-the-art results by 10% with only 25%
	of the training data.},
  annote    = {read},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/tsp/ElhamifarV12},
  journal   = {IEEE Transactions on Signal Processing},
  keywords  = {block-sparse signals,convex optimization,face recognition.,principal angles,subspaces},
  owner     = {Fardin},
  timestamp = {2016-09-30T11:45:59Z},
  year      = {2012},
}

@Article{Daudet2006,
  author    = {Daudet, L.},
  title     = {Sparse and Structured Decompositions of Signals with the Molecular Matching Pursuit},
  doi       = {10.1109/TSA.2005.858540},
  issn      = {1558-7916},
  number    = {5},
  pages     = {1808--1816},
  volume    = {14},
  abstract  = {This paper describes the Molecular Matching Pursuit (MMP), an extension of the popular Matching Pursuit (MP) algorithm for the decomposition of signals. The MMP is a practical solution which introduces the notion of structures within the framework of sparse overcomplete representations; these structures are based on the local dependency of significant time-frequency or time-scale atoms. We show that this algorithm is well adapted to the representation of real signals such as percussive audio signals. This is at the cost of a slight sub-optimality in terms of the rate of convergence for the approximation error, but the benefits are numerous, most notably a significant reduction in the computational cost, which facilitates the processing of long signals. Results show that this algorithm is very promising for high-quality adaptive coding of audio signals},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TFG8J5IG\\articleDetails.html:text/html},
  journal   = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords  = {approximation error,audio coding,audio signal adaptive coding,convergence,cost reduction,Costs,Dictionaries,Discrete wavelet transforms,Iterative algorithms,Matching pursuit,Matching pursuit algorithms,molecular matching pursuit,overcomplete representations,parametric audio coding,percussive audio signals,Pursuit algorithms,signal decomposition,signal processing,signal representation,sparse overcomplete representations,time-frequency analysis,Time frequency analysis,time-frequency atoms,time-frequency transforms,time-scale atoms},
  month     = sep,
  timestamp = {2016-09-30T13:31:35Z},
  year      = {2006},
}

@InProceedings{Keriven2013,
  author    = {Keriven, N. and O'Hanlon, K. and Plumbley, M. D.},
  booktitle = {2013 {{IEEE International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  title     = {Structured Sparsity Using Backwards Elimination for {{Automatic Music Transcription}}},
  doi       = {10.1109/MLSP.2013.6661917},
  pages     = {1--6},
  abstract  = {Musical signals can be thought of as being sparse and structured, with few elements active at a given instant and temporal continuity of active elements observed. Greedy algorithms such as Orthogonal Matching Pursuit (OMP), and structured variants, have previously been proposed for Automatic Music Transcription (AMT), however some problems have been noted. Hence, we propose the use of a backwards elimination strategy in order to perform sparse decompositions for AMT, in particular with a proposed alternative sparse cost function. However, the main advantage of this approach is the ease with which structure can be incorporated. The use of group sparsity is shown to give increased AMT performance, while a molecular method incorporating onset information is seen to provide further improvements with little computational effort.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CUSIFG52\\articleDetails.html:text/html},
  keywords  = {alternative sparse cost function,AMT performance,automatic music transcription,backwards elimination,backwards elimination strategy,compressed sensing,Cost function,Dictionaries,Greedy algorithms,group sparsity,Matching pursuit algorithms,molecular method,music,musical signals,music transcription,onset information,sparse decompositions,Sparse matrices,spectrogram,structured sparsity,structured variants,Transforms,Vectors},
  month     = sep,
  timestamp = {2016-09-30T13:31:57Z},
  year      = {2013},
}

@Article{Model2006,
  author    = {Model, Dmitri and Zibulevsky, Michael},
  title     = {Signal Reconstruction in Sensor Arrays Using Sparse Representations},
  doi       = {10.1016/j.sigpro.2005.05.033},
  issn      = {0165-1684},
  number    = {3},
  pages     = {624--638},
  series    = {Sparse Approximations in Signal and Image ProcessingSparse Approximations in Signal and Image Processing},
  urldate   = {2016-05-30},
  volume    = {86},
  abstract  = {We propose a technique of multisensor signal reconstruction based on the assumption, that source signals are spatially sparse, as well as have sparse representation in a chosen dictionary in time domain. This leads to a large scale convex optimization problem, which involves combined l 1 - l 2 norm minimization. The optimization is carried by the truncated Newton method, using preconditioned conjugate gradients in inner iterations. The byproduct of reconstruction is the estimation of source locations.},
  file      = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5N2XNZPR\\S0165168405002252.html:text/html},
  journal   = {Signal Processing},
  keywords  = {Multipath,Source localization,Source reconstruction,Wideband},
  month     = mar,
  timestamp = {2016-09-29T16:14:05Z},
  year      = {2006},
}

@InProceedings{Eksioglu2011,
  author    = {Eksioglu, E.M.},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  title     = {A clustering based framework for dictionary block structure identification},
  doi       = {10.1109/ICASSP.2011.5947240},
  pages     = {4044-4047},
  abstract  = {Sparse representations over redundant dictionaries offer an efficient
	paradigm for signal representation. Recently block-sparsity has been
	put forward as a prior condition for some sparse representation applications,
	where the coefficients of the sparse representation occur in blocks
	rather than being distributed randomly over the sparse vector. Block-sparse
	representation algorithms, which are extensions of the regular sparse
	representation algorithms have been developed. However, these algorithms
	work under the assumption that both the dictionary and its corresponding
	block structure are known. In this paper, we consider the problem
	of recovering the optimally block sparsifying block structure for
	a given data set and dictionary pair. We propose a block structure
	identification framework employing a clustering step which can be
	realized using the standard clustering schemes from the literature.
	The block structure identification algorithm works efficiently, and
	for synthetically generated block-sparse data the underlying block
	structure is retrieved even for comparably short data records.},
  annote    = {read},
  keywords  = {block sparse representations,Block-sparsity,block structure identification,clustering,Clustering algorithms,clustering schemes,Couplings,Dictionaries,dictionary block structure,Matching pursuit algorithms,measurement,pattern clustering,redundant dictionaries,Signal processing algorithms,signal representation,signal representations,sparse vector},
  month     = may,
  owner     = {afdidehf},
  timestamp = {2016-07-08T10:08:11Z},
  year      = {2011},
}

@Article{Gilbert2006,
  author    = {Gilbert, A. C. and Strauss, M. J. and Tropp, J. A. and Vershynin, R.},
  title     = {Algorithmic linear dimension reduction in the $\ell_1$ norm for sparse vectors},
  abstract  = {Using a number of different algorithms, we can recover approximately
	a sparse signal with limited noise, i.e, a vector of length d with
	at least d-m zeros or near-zeros, using little more than mlog(d)
	nonadaptive linear measurements rather than the d measurements needed
	to recover an arbitrary signal of length d. We focus on two important
	properties of such algorithms. \textbullet{} Uniformity. A single
	measurement matrix should work simultaneously for all signals. \textbullet{}
	Computational Efficiency. The time to recover such an msparse signal
	should be close to the obvious lower bound, mlog(d/m). This paper
	develops a new method for recovering msparse signals that is simultaneously
	uniform and quick. We present a reconstruction algorithm whose run
	time, O(mlog2(m) log2(d)), is sublinear in the length d of the signal.
	The reconstruction error is within a logarithmic factor (in m) of
	the optimal m-term approximation error in `1. In particular, the
	algorithm recovers m-sparse signals perfectly and noisy signals are
	recovered with polylogarithmic distortion. Our algorithm makes O(mlog2(d))
	measurements, which is within a logarithmic factor of optimal. We
	also present a smallspace implementation of the algorithm. These
	sketching techniques and the corresponding reconstruction algorithms
	provide an algorithmic dimension reduction in the `1 norm. In particular,
	vectors of support m in dimension d can be linearly embedded into
	O(mlog2 d) dimensions with polylogarithmic distortion. We can reconstruct
	a vector from its low-dimensional sketch in time O(mlog2(m) log2(d)).
	Furthermore, this reconstruction is stable and robust under small
	perturbations.},
  journal   = {in Proc. 44th Annu. Allerton Conf. Communication, Control, Computing},
  timestamp = {2016-09-29T15:36:31Z},
  year      = {2006},
}

@Article{Gribonval2003,
  author    = {Gribonval, R. and Nielsen, M.},
  title     = {Sparse decompositions in "incoherent" dictionaries},
  doi       = {10.1109/ICIP.2003.1246891},
  pages     = {I-33--I-36},
  volume    = {1},
  abstract  = {The purpose of this paper is to generalize a result by Donoho, Huo,
	Elad and Bruckstein on sparse representations of signals/images in
	a union of two orthonormal bases. We consider general (redundant)
	dictionaries in finite dimension, and derive sufficient conditions
	on a signal/image for having a unique sparse representation in such
	a dictionary. In particular, it is proved that the result of Donoho
	and Huo, concerning the replacement of a combinatorial optimization
	problem with a linear programming problem when searching for sparse
	representations, has an analog for dictionaries that may be highly
	redundant. The special case where the dictionary is given by a union
	of several orthonormal bases is studied in more detail and some examples
	are given.},
  annote    = {read},
  journal   = {in Proc. IEEE Int. Conf. Image Process.},
  keywords  = {combinatorial mathematics,combinatorial optimization problem,Dictionaries,Hilbert space,image coding,Image Processing,image representation,incoherent dictionary,linear programming,multidimensional signal processing,Noise reduction,NSP,null space property,signal processing,source separation,sparse decomposition,Sufficient conditions},
  month     = sep,
  owner     = {Fardin},
  timestamp = {2017-04-19T15:13:12Z},
  year      = {2003},
}

@Article{Stojnic2008,
  author    = {Stojnic, M. and Xu, W. and Hassibi, B.},
  title     = {Compressed sensing - probabilistic analysis of a null-space characterization},
  doi       = {10.1109/ICASSP.2008.4518375},
  pages     = {3377--3380},
  abstract  = {It is well known that compressed sensing problems reduce to solving large under-determined systems of equations. To assure that the problem is well defined, i.e., that the solution is unique the vector of unknowns is of course assumed to be sparse. Nonetheless, even when the solution is unique, finding it in general may be computationally difficult. However, starting with the seminal work of Candes and Tao [2005], it has been shown that linear programming techniques, obtained from an l1-norm relaxation of the original non-convex problem, can provably find the unknown vector in certain instances. In particular, using a certain restricted isometry property, Candes and Tao [2005] shows that for measurement matrices chosen from a random Gaussian ensemble, l1 optimization can find the correct solution with overwhelming probability even when the number of non-zero entries of the unknown vector is proportional to the number of measurements (and the total number of unknowns). The subsequent paper [Donoho and Tanner, 2005] uses results on neighborly polytopes from [Vershik and Sporyshev, 1992] to give a "sharp" bound on what this proportionality should be in the Gaussian case. In the current paper, we observe that what matters is not so much the distribution from which the entries of the measurement matrix A are drawn, but rather the statistics of the null-space of A. Using this observation, we provide an alternative proof of the main result of Candes and Tao [2005] by analyzing matrices whose null-space is isotropic (of which i.i.d. Gaussian ensembles are a special case).},
  journal   = {IEEE Int. Conf. Acoust., Speech, Signal Process.},
  keywords  = {compressed sensing,compressed sensing,Current measurement,Current measurement,data compression,data compression,Equations,Equations,Gaussian processes,Gaussian processes,l1-norm relaxation,l1-norm relaxation,l1-optimization,l1-optimization,linear programming,linear programming,linear programming techniques,linear programming techniques,measurement matrix,measurement matrix,neighborly polytopes,neighborly polytopes,Noise generators,Noise generators,nonconvex problem,nonconvex problem,null-space characterization,null-space characterization,optimisation,optimisation,Optimization,Optimization,Particle measurements,Particle measurements,probabilistic analysis,probabilistic analysis,probability,probability,random Gaussian ensemble,random Gaussian ensemble,Statistical analysis,statistical analysis,Statistical distributions,Statistical distributions,Sufficient conditions,Sufficient conditions,Vectors,Vectors},
  month     = mar,
  timestamp = {2017-04-20T09:23:52Z},
  year      = {2008},
}

@TechReport{Zhang2005a,
  author      = {Zhang, Y.},
  institution = {Depart. Comput. Appl. Math., Rice Univ., Houston},
  title       = {A Simple Proof for Recoverability of $\ell_1$-Minimization: Go Over or Under?},
  annote      = {Depart. Comput. Appl. Math., Rice Univ., Houston, TX, Tech. Rep.     TR05-09},
  timestamp   = {2017-06-23T11:46:55Z},
  year        = {2005},
}

@PhdThesis{Tropp2004b,
  author    = {Tropp, Joel Aaron},
  title     = {Topics in sparse approximation},
  language  = {eng},
  type      = {Computational and Applied Mathematics},
  urldate   = {2016-04-13},
  abstract  = {Sparse approximation problems request a good approximation of an input
	signal as a linear combination of elementary signals, yet they stipulate
	that the approximation may involve only a few of the elementary signals.
	This class of problems arises throughout applied mathematics, statistics,
	and electrical engineering, but small theoretical progress has been
	made over the last fifty years. This dissertation offers four main
	contributions to the theory of sparse approximation. The first two
	contributions concern the analysis of two types of numerical algorithms
	for sparse approximation: greedy methods and convex relaxation methods.
	Greedy methods make a sequence of locally optimal choices in an effort
	to obtain a globally optimal solution. Convex relaxation methods
	replace the combinatorial sparse approximation problem with a related
	convex optimization in hope that their solutions will coincide. This
	work delineates conditions under which greedy methods and convex
	relaxation methods actually succeed in solving a well-defined sparse
	approximation problem in part or in full. The conditions for both
	classes of algorithms are remarkably similar, in spite of the fact
	that the two analyses differ significantly. The study of these algorithms
	yields geometric conditions on the collection of elementary signals
	which ensure that sparse approximation problems are computationally
	tractable. One may interpret these conditions as a requirement that
	the elementary signals should form a good packing of points in projective
	space. The third contribution of this work is an alternating projection
	algorithm that can produce good packings of points in projective
	space. The output of this algorithm frequently matches the best recorded
	solutions of projective packing problems. It can also address many
	related packing problems that have never been studied numerically.
	Finally, the dissertation develops a novel connection between sparse
	approximation problems and clustering problems. This perspective
	shows that many clustering problems from the literature can be viewed
	as sparse approximation problems where the collection of elementary
	signals must be learned along with the optimal sparse approximation.
	This treatment also yields many novel clustering problems, and it
	leads to a numerical method for solving them.},
  month     = aug,
  school    = {Univ. Texas at Austin},
  timestamp = {2017-04-19T14:54:01Z},
  year      = {2004},
}

@Article{Bruckstein2009,
  author    = {Bruckstein, A.M. and Donoho, D.L. and Elad, M.},
  title     = {From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images},
  number    = {1},
  pages     = {34--81},
  volume    = {51},
  abstract  = {A full-rank matrix A ? Rn m with n < m generates an underdetermined
	system of linear equations Ax = b having infinitely many solutions.
	Suppose we seek the sparsest solution, i.e., the one with the fewest
	nonzero entries. Can it ever be unique? If so, when? As optimization
	of sparsity is combinatorial in nature, are there efficient methods
	for finding the sparsest solution? These questions have been answered
	positively and constructively in recent years, exposing a wide variety
	of surprising phenomena, in particular the existence of easily verifiable
	conditions under which optimally sparse solutions can be found by
	concrete, effective computational methods. Such theoretical results
	inspire a bold perspective on some important practical problems in
	signal and image processing. Several well-known signal and image
	processing problems can be cast as demanding solutions of undetermined
	systems of equations. Such problems have previously seemed, to many,
	intractable, but there is considerable evidence that these problems
	often have sparse solutions. Hence, advances in finding sparse solutions
	to underdetermined systems have energized research on such signal
	and image processing problems to striking effect. In this paper
	we review the theoretical results on sparse solutions of linear systems,
	empirical results on sparse modeling of signals and images, and recent
	applications in inverse problems and compression in image processing.
	This work lies at the intersection of signal processing and applied
	mathematics, and arose initially from the wavelets and harmonic analysis
	research communities. The aim of this paper is to introduce a few
	key notions and applications connected to sparsity, targeting newcomers
	interested in either the mathematical aspects of this area or its
	applications.},
  journal   = {SIAM Rev.},
  keywords  = {Basis pursuit,compression,denoising,dictionary learning,inverse problems,linear system of equations,linear system of equations,matching pursuit,mutual coherence,overcomplete,redundant,Sparse Coding,Sparse coding,Sparse-Land,sparse representation,underdetermined},
  owner     = {Fardin},
  timestamp = {2017-04-19T14:55:21Z},
  year      = {2009},
}

@Article{DeBrunner1997,
  author    = {DeBrunner, V.E. and Chen, Lixiang and Li, Hong-Jian},
  title     = {Lapped multiple bases algorithms for still image compression without blocking effect},
  doi       = {10.1109/83.623194},
  issn      = {1057-7149},
  number    = {9},
  pages     = {1316--1321},
  volume    = {6},
  abstract  = {We describe a system for still image compression that uses several
	transform sets in a multiple bases realization algorithm. Our algorithms
	reduce the number of encoded transform coefficients 20% beyond DCT-only
	compression. We extend these algorithms to use several newly developed
	lapped orthogonal transform (LOT) bases, resulting in useful algorithms
	for low bit rate (high compression) operation without blocking effect},
  journal   = {Image Processing, IEEE Transactions on},
  keywords  = {Bit rate,block transform coding,data compression,discrete cosine transforms,Discrete transforms,image coding,Image reconstruction,ISO standards,lapped multiple bases algorithms,lapped orthogonal transform,low bit rate,Pixel,signal representations,Standards development,still image compression,Transform coding,transform coding,transform coefficients,transforms},
  month     = sep,
  owner     = {Fardin},
  timestamp = {2016-09-29T16:28:27Z},
  year      = {1997},
}

@Article{Lv2011,
  author    = {Lv, Xiaolei and Bi, Guoan and Wan, C.},
  title     = {The Group Lasso for Stable Recovery of Block-Sparse Signal Representations},
  doi       = {10.1109/TSP.2011.2105478},
  issn      = {1053-587X},
  number    = {4},
  pages     = {1371--1382},
  volume    = {59},
  abstract  = {Group Lasso is a mixed l1/l2-regularization method for a block-wise
	sparse model that has attracted a lot of interests in statistics,
	machine learning, and data mining. This paper establishes the possibility
	of stably recovering original signals from the noisy data using the
	adaptive group Lasso with a combination of sufficient block-sparsity
	and favorable block structure of the overcomplete dictionary. The
	corresponding theoretical results about the solution uniqueness,
	support recovery and representation error bound are derived based
	on the properties of block-coherence and subcoherence. Compared with
	the theoretical results on the parametrized quadratic program of
	conventional sparse representation, our stability results are more
	general. A comparison with block-based orthogonal greedy algorithm
	is also presented. Numerical experiments demonstrate the validity
	and correctness of theoretical derivation and also show that in case
	of noisy situation, the adaptive group Lasso has a better reconstruction
	performance than the quadratic program approach if the observed sparse
	signals have a natural block structure.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {block-coherence,block-sparse signal representation,Greedy algorithms,group lasso,group Lasso,group theory,l1/l2-regularization method,orthogonal greedy algorithm,orthogonal greedy algorithm,parametrized quadratic program,quadratic programming,signal representation,sparse representation,stable recovery,subcoherence},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2016-09-30T13:55:17Z},
  year      = {2011},
}

@Article{Stojnic2009a,
  author    = {Stojnic, M. and Parvaresh, F. and Hassibi, B.},
  title     = {On the Reconstruction of Block-Sparse Signals With an Optimal Number of Measurements},
  doi       = {10.1109/TSP.2009.2020754},
  issn      = {1053-587X},
  number    = {8},
  pages     = {3075--3085},
  volume    = {57},
  abstract  = {Let A be an M by N matrix (M < N) which is an instance of a real random
	Gaussian ensemble. In compressed sensing we are interested in finding
	the sparsest solution to the system of equations A x = y for a given
	y. In general, whenever the sparsity of x is smaller than half the
	dimension of y then with overwhelming probability over A the sparsest
	solution is unique and can be found by an exhaustive search over
	x with an exponential time complexity for any y. The recent work
	of Candes, Donoho, and Tao shows that minimization of the lscr 1
	norm of x subject to Ax = y results in the sparsest solution provided
	the sparsity of x, say K, is smaller than a certain threshold for
	a given number of measurements. Specifically, if the dimension of
	y approaches the dimension of x , the sparsity of x should be K <
	0.239 N. Here, we consider the case where x is block sparse, i.e.,
	x consists of n = N /d blocks where each block is of length d and
	is either a zero vector or a nonzero vector (under nonzero vector
	we consider a vector that can have both, zero and nonzero components).
	Instead of lscr1 -norm relaxation, we consider the following relaxation:
	times min ||X 1||2 + ||X 2||2 + ldrldrldr + ||X n ||2, subject to
	A x = y (*) where X i = (x ( i-1)d+1, x ( i-1)d+2, ldrldrldr , x
	i d)T for i = 1, 2, ldrldrldr , N. Our main result is that as n rarr
	infin, (*) finds the sparsest solution to A=x = y, with overwhelming
	probability in A, for any x whose sparsity is k/n < (1/2) - O (isi-
	- n), provided m /n > 1 - 1/d, and d = Omega(log(1/isin)/isin3) .
	The relaxation given in (*) can be solved in polynomial time using
	semi-definite programming.},
  annote    = {read},
  journal   = {IEEE Trans. Signal Process.},
  keywords  = {block-sparse signals,block-sparse signals reconstruction,compressed sensing,mathematical programming,Matrix,nonzero vector,overwhelming probability,polynomial matrices,polynomial time,probability,real random Gaussian ensemble,relaxation,relaxation theory,semi-definite programming,signal reconstruction},
  month     = aug,
  owner     = {afdidehf},
  timestamp = {2017-04-19T15:31:08Z},
  year      = {2009},
}

@Article{Cohen2003,
  author    = {Cohen, D. and Halgren, E.},
  title     = {Magnetoencephalography ({{Neuromagnetism}})},
  journal   = {Encyclopedia of Neuroscience},
  timestamp = {2017-08-21T12:35:48Z},
  year      = {2003},
}

@Article{Daubechies2008,
  author    = {Daubechies, Ingrid and Defrise, Michel and Mol, Christine De},
  title     = {An Iterative Thresholding Algorithm For Linear Inverse Problems With A Sparsity Constraint},
  abstract  = {We consider linear inverse problems where the solution is assumed
	to have a sparse expansion on an arbitrary pre-assigned orthonormal
	basis. We prove that replacing the usual quadratic regularizing penalties
	by weighted l^p-penalties on the coefficients of such expansions,
	with 1 < or = p < or =2, still regularizes the problem. If p < 2,
	regularized solutions of such l^p-penalized problems will have sparser
	expansions, with respect to the basis under consideration. To compute
	the corresponding regularized solutions we propose an iterative algorithm
	that amounts to a Landweber iteration with thresholding (or nonlinear
	shrinkage) applied at each iteration step. We prove that this algorithm
	converges in norm. We also review some potential applications of
	this method.},
  journal   = {ArXiv Mathematics e-prints},
  month     = feb,
  owner     = {Fardin},
  timestamp = {2016-07-08T11:24:37Z},
  year      = {2008},
}

@Article{Malioutov2005,
  author    = {Malioutov, D. and Cetin, M. and Willsky, A.S.},
  title     = {A sparse signal reconstruction perspective for source localization with sensor arrays},
  doi       = {10.1109/TSP.2005.850882},
  issn      = {1053-587X},
  number    = {8},
  pages     = {3010--3022},
  volume    = {53},
  abstract  = {We present a source localization method based on a sparse representation
	of sensor measurements with an overcomplete basis composed of samples
	from the array manifold. We enforce sparsity by imposing penalties
	based on the ?1-norm. A number of recent theoretical results on sparsifying
	properties of ?1 penalties justify this choice. Explicitly enforcing
	the sparsity of the representation is motivated by a desire to obtain
	a sharp estimate of the spatial spectrum that exhibits super-resolution.
	We propose to use the singular value decomposition (SVD) of the data
	matrix to summarize multiple time or frequency samples. Our formulation
	leads to an optimization problem, which we solve efficiently in a
	second-order cone (SOC) programming framework by an interior point
	implementation. We propose a grid refinement method to mitigate the
	effects of limiting estimates to a grid of spatial locations and
	introduce an automatic selection criterion for the regularization
	parameter involved in our approach. We demonstrate the effectiveness
	of the method on simulated data by plots of spatial spectra and by
	comparing the estimator variance to the Cramer-Rao bound (CRB).
	We observe that our approach has a number of advantages over other
	source localization techniques, including increased resolution, improved
	robustness to noise, limitations in data quantity, and correlation
	of the sources, as well as not requiring an accurate initialization.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {Acoustic sensors,array signal processing,automatic selection criterion,data matrix,direction-of-arrival estimation,grid refinement method,grid refinement method,Maximum likelihood estimation,maximum likelihood estimation,Multiple signal classification,Optimization,overcomplete representation,regularization parameter,second-order cone programming framework,sensor array processing,Sensor arrays,Sensor arrays,signal reconstruction,signal representation,signal representation,signal representations,signal resolution,Signal resolution,signal sampling,signal superresolution,signal superresolution,Singular Value Decomposition,singular value decomposition,source localization,source localization method,source localization method,sparse representation,sparse representation,sparse signal reconstruction perspective,sparse signal reconstruction perspective,sparse signal representation,sparse signal representation,Spatial resolution,spatial spectrum estimation,spatial spectrum estimation,superresolution,time-frequency analysis,time-frequency analysis},
  month     = aug,
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:14:12Z},
  year      = {2005},
}

@InProceedings{Peng2015,
  author    = {Peng, Y. and Lu, B. L.},
  booktitle = {2015 IEEE 28th Canadian Conference on Electrical and Computer Engineering (CCECE)},
  title     = {Robust group sparse representation via half-quadratic optimization for face recognition},
  doi       = {10.1109/CCECE.2015.7129176},
  pages     = {146--151},
  abstract  = {Sparse representation-based classifier (SRC), which represents a test sample with a linear combination of training samples, has shown promise in pattern classification. However, there are two shortcomings in SRC: (1) the ℓ1-norm used to measure the reconstruction fidelity is noise-sensitive and (2) the ℓ2-norm induced sparsity did not consider the correlation among the training samples. Furthermore, in real applications, face images with similar variations, such as illumination or expression, often have higher correlation than those from the same subject. Therefore, we propose to improve the performance of SRC from two aspects: (1) replace the noise-sensitive ℓ2-norm with an M-estimator to enhance its robustness and (2) emphasize the sparsity of the number of classes instead of the number of training samples, which leads to the group sparsity. The proposed robust group sparse representation (RGSR) can be efficiently optimized via alternating minimization under the Half-Quadratic (HQ) framework. Extensive experiments on representative face data sets show that RGSR can achieve competitive performance in face recognition and outperforms several state-of-the-art methods in dealing with various types of noise such as corruption, occlusion and disguise.},
  keywords  = {Computers,Computers,Conferences,Conferences,Decision support systems,Decision support systems,face images,face images,face recognition,face recognition,group sparsity,group sparsity,group sparsity,half-quadratic optimization,half-quadratic optimization,HQ framework,HQ framework,image classification,image classification,Image reconstruction,image reconstruction,Image reconstruction,image representation,image representation,ℓ1-norm,l1-norm,ℓ2-norm induced sparsity,l2-norm induced sparsity,M-estimator,M-estimator,Minimization,minimization,noise-sensitive reconstruction fidelity,noise-sensitive reconstruction fidelity,noise-sensitive reconstruction fidelity,pattern classification,pattern classification,quadratic programming,quadratic programming,RGSR,RGSR,robust group sparse representation,robust group sparse representation,robust group sparse representation,sparse representation-based classifier,sparse representation-based classifier,SRC,SRC},
  month     = may,
  timestamp = {2016-09-30T11:46:28Z},
  year      = {2015},
}

@Article{Molins2008,
  author    = {Molins, A. and Stufflebeam, S. M. and Brown, E. N. and H{\"a}m{\"a}l{\"a}inen, M. S.},
  title     = {Quantification of the benefit from integrating MEG and EEG data in minimum l2-norm estimation},
  doi       = {http://dx.doi.org/10.1016/j.neuroimage.2008.05.064},
  issn      = {1053-8119},
  number    = {3},
  pages     = {1069--1077},
  volume    = {42},
  abstract  = {Source current estimation from electromagnetic (MEG and EEG) signals
	is an ill-posed problem that often produces blurry or inaccurately
	positioned estimates. The two modalities have distinct factors limiting
	the resolution, e.g., \{MEG\} cannot detect radially oriented sources,
	while \{EEG\} is sensitive to accuracy of the head model. This makes
	combined \{EEG\} + \{MEG\} estimation techniques desirable, but different
	acquisition noise statistics, complexity of the head models, and
	lack of pertinent metrics all complicate the assessment of the resulting
	improvements. We investigated analytically the effect of including
	\{EEG\} recordings in \{MEG\} studies versus the addition of new
	\{MEG\} channels when computing noise-normalized minimum l2-norm
	estimates. Three-compartment boundary-element forward models were
	constructed using structural \{MRI\} scans for four subjects. Singular
	value analysis of the resulting forward models predicted better performance
	of the \{EEG\} + \{MEG\} case in the form of higher matrix rank.
	\{MNE\} inverse operators for EEG, \{MEG\} and \{EEG\} + \{MEG\}
	were constructed using the sensor noise covariance estimated from
	data. Metrics derived from the resolution matrices predicted higher
	spatial resolution in \{EEG\} + \{MEG\} as compared to \{MEG\} due
	to decreased spread (lower spatial dispersion, higher resolution
	index) with no reduction in dipole localization error. The effect
	was apparent in all source locations, with increased magnitude for
	deep areas such as the cingulate cortex. We were also able to corroborate
	the results for the somatosensory cortex using median nerve responses.},
  journal   = {NeuroImage},
  keywords  = {EEG},
  owner     = {afdidehf},
  timestamp = {2016-10-21T13:40:44Z},
  year      = {2008},
}

@InProceedings{Maleki2009,
  author    = {Maleki, A.},
  booktitle = {47th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}, 2009. {{Allerton}} 2009},
  title     = {Coherence Analysis of Iterative Thresholding Algorithms},
  doi       = {10.1109/ALLERTON.2009.5394802},
  pages     = {236--243},
  abstract  = {There is a recent surge of interest in developing algorithms for finding sparse solutions of underdetermined systems of linear equations y = {\^A}\textquestiondown{}x. In many applications, extremely large problem sizes are envisioned, with at least tens of thousands of equations and hundreds of thousands of unknowns. For such problem sizes, low computational complexity is paramount. The best studied l1 minimization algorithm is not fast enough to fulfill this need. Iterative thresholding algorithms have been proposed to address this problem. In this paper we want to analyze three of these algorithms theoretically, and give sufficient conditions under which they recover the sparsest solution.},
  annote    = {read},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8WU4MK95\\5394802.html:text/html},
  keywords  = {Algorithm design and analysis,coherence analysis,computational complexity,Equations,Greedy algorithms,Iterative algorithms,iterative methods,iterative thresholding algorithms,Large-scale systems,linear equations underdetermined systems,Matching pursuit algorithms,Minimization methods,Polynomials,signal processing,Signal processing algorithms,Sparse matrices,sparse solutions},
  month     = sep,
  timestamp = {2016-10-04T15:31:15Z},
  year      = {2009},
}

@Article{Cohen2009,
  author    = {Cohen, A. and Dahmen, W. and Devore, R.},
  title     = {Compressed sensing and best k -term approximation},
  doi       = {10.1090/S0894-0347-08-00610-3},
  number    = {1},
  pages     = {211--231},
  volume    = {22},
  abstract  = {Compressed sensing is a new concept in signal processing where one
	seeks to minimize the number of measurements to be taken from signals
	while still retaining the information necessary to approximate them
	well. The ideas have their origins in certain abstract results from
	functional analysis and approximation theory by Kashin [23] but were
	recently brought into the forefront by the work of Cand`es, Romberg
	and Tao [7, 5, 6] and Donoho [9] who constructed concrete algorithms
	and showed their promise in application. There remain several fundamental
	questions on both the theoretical and practical side of compressed
	sensing. This paper is primarily concerned about one of these theoretical
	issues revolving around just how well compressed sensing can approximate
	a given signal from a given budget of fixed linear measurements,
	as compared to adaptive linear measurements. More precisely, we consider
	discrete signals x 2 IRN, allocate n < N linear measurements of x,
	and we describe the range of k for which these measurements encode
	enough information to recover x in the sense of `p to the accuracy
	of best k-term approximation. We also consider the problem of having
	such accuracy only with high probability.},
  journal   = {Journal of The American Mathematical Society},
  keywords  = {best k-term approximation,compressed sensing,instance,instance optimality,mixed,norm estimates.,null space property,optimality in probability,restricted isometry property},
  masid     = {4476754},
  month     = jul,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:06:58Z},
  year      = {2009},
}

@Article{Zelinski2008a,
  author    = {Zelinski, A. C. and Wald, L. L. and Setsompop, K. and Goyal, V. K. and Adalsteinsson, E.},
  title     = {Sparsity-{{Enforced Slice}}-{{Selective MRI RF Excitation Pulse Design}}},
  doi       = {10.1109/TMI.2008.920605},
  issn      = {0278-0062},
  number    = {9},
  pages     = {1213--1229},
  volume    = {27},
  abstract  = {We introduce a novel algorithm for the design of fast slice-selective spatially-tailored magnetic resonance imaging (MRI) excitation pulses. This method, based on sparse approximation theory, uses a second-order cone optimization to place and modulate a small number of slice-selective sine-like radio-frequency (RF) pulse segments ("spokes") in excitation fc-space, enforcing sparsity on the number of spokes allowed while si multaneously encouraging those that remain to be placed and modulated in a way that best forms a user-defined in-plane target magnetization. Pulses are designed to mitigate B1 inhomogeneity in a water phantom at 7 T and to produce highly-structured excitations in an oil phantom on an eight-channel parallel excitation system at 3 T. In each experiment, pulses generated by the spar- sity-enfoldquoced method outperform those created via conventional Fourier-based techniques, e.g., when attempting to produce a uniform magnetization in the presence of severe B1 inhomogeneity, a 5.7-ms 15-spoke pulse generated by the sparsity-enforced method produces an excitation with 1.28 times lower root mean square error than conventionally-designed 15-spoke pulses. To achieve this same level of uniformity, the conventional methods need to use 29-spoke pulses that are 7.8 ms long.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7M8I8DTA\\articleDetails.html:text/html},
  journal   = {IEEE Transactions on Medical Imaging},
  keywords  = {<sub>1</sub> inhomogeneity mitigation,3-D RF excitation,algorithm,Algorithm design and analysis,algorithms,Approximation methods,B1,biomedical MRI,Brain,Fourier-based techniques,high field strength,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Imaging phantoms,Imaging; Three-Dimensional,inhomogeneity mitigation,Magnetic modulators,magnetic resonance imaging,magnetic resonance imaging (MRI) radio-frequency (RF) pulse sequence design,magnetization,MRI RF pulse sequence design,Optimization methods,parallel transmission,phantom,phantoms,Pulse generation,Pulse modulation,Radio frequency,RF excitation pulse design,root mean square error,Signal Processing; Computer-Assisted,sparse approximation,sparsity-enforced slice-selective MRI,three-dimensional (3-D) RF excitation},
  month     = sep,
  timestamp = {2016-09-30T13:30:33Z},
  year      = {2008},
}

@Article{Tillmann2013,
  author    = {Tillmann, Andreas M. and Pfetsch, Marc E.},
  title     = {The Computational Complexity of Spark, RIP, and NSP},
  annote    = {PDF \& PPT},
  journal   = {Signal Processing with Adaptive Sparse Structured Representations (SPARS'13 )},
  owner     = {afdidehf},
  timestamp = {2016-07-11T17:00:17Z},
  year      = {2013},
}

@Article{Okada1997,
  author    = {Okada, Yoshio C. and Wu, Jie and Kyuhou, Shinichi},
  title     = {Genesis of {{MEG}} Signals in a Mammalian {{CNS}} Structure},
  doi       = {10.1016/S0013-4694(97)00043-6},
  issn      = {0013-4694},
  number    = {4},
  pages     = {474--485},
  volume    = {103},
  abstract  = {Neuromagnetic signals of guinea pig hippocampal slices were characterized and compared with the extracellular field potential to elucidate the genesis of magnetoencephalographic signals in a mammalian CNS structure. The spatial distribution of magnetic evoked field (MEF) directed normal to bath surface was similar for transverse CA1, longitudinal CA1 and longitudinal CA3 slices in the presence of 0.1 mM picrotoxin (PTX) which blocks inhibitory synaptic transmission. Their MEFs were produced by currents along the longitudinal axis of the pyramidal cells. Comparisons of the MEF with the laminar potential profile revealed that the MEF was generated by intracellular longitudinal currents. The dipolar component of the intracellular currents was the dominant factor generating the MEF even at a distance of 2 mm from the slice. The MEF from a slice in Ringer's solution without PTX became similar in temporal waveform with time to the MEF in the presence of PTX, indicating the predominance of excitatory connections in generating the MEF and the existence of highly synchronous population activities across the slice even in PTX-free Ringer's solution. The presence of such highly synchronous population activities underlying the MEFs was verified directly with field potentials recorded across the slice. A systematic variation of the stimulation site revealed a characteristic waveform for each site. The variation of the waveform with stimulation site suggested the contribution of many factors, both synaptic and voltage-sensitive conductances, to the overall waveform of the MEF.},
  file      = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NW5FR332\\S0013469497000436.html:text/html},
  journal   = {Electroencephalography and Clinical Neurophysiology},
  keywords  = {biomagnetism,CA1,CA3,electroencephalography (EEG),Hippocampus,magnetoencephalography (MEG)},
  month     = oct,
  timestamp = {2017-08-21T12:59:12Z},
  year      = {1997},
}

@InProceedings{Pati1993,
  author    = {Pati, Y.C. and Rezaiifar, R. and Krishnaprasad, P.S.},
  booktitle = {Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on},
  title     = {Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition},
  doi       = {10.1109/ACSSC.1993.342465},
  pages     = {40-44 vol.1},
  abstract  = {We describe a recursive algorithm to compute representations of functions
	with respect to nonorthogonal and possibly overcomplete dictionaries
	of elementary building blocks e.g. affine (wavelet) frames. We propose
	a modification to the matching pursuit algorithm of Mallat and Zhang
	(1992) that maintains full backward orthogonality of the residual
	(error) at every step and thereby leads to improved convergence.
	We refer to this modified algorithm as orthogonal matching pursuit
	(OMP). It is shown that all additional computation required for the
	OMP algorithm may be performed recursively},
  keywords  = {affine wavelet frames,approximation theory,backward orthogonality,Convergence,convergence of numerical methods,Dictionaries,Educational institutions,Function approximation,Information systems,Iterative algorithms,Laboratories,matching pursuit algorithm,Matching pursuit algorithms,orthogonal matching pursuit,overcomplete dictionaries,Pursuit algorithms,recursive algorithm,recursive estimation,recursive function approximation,signal representation,signal representation,wavelet decomposition,wavelet transforms,Zinc},
  month     = nov,
  owner     = {Fardin},
  timestamp = {2016-07-10T07:20:36Z},
  year      = {1993},
}

@Article{Tillmann2014,
  author    = {Tillmann, A.M. and Pfetsch, M.E.},
  title     = {The Computational Complexity of the Restricted Isometry Property, the Nullspace Property, and Related Concepts in Compressed Sensing},
  doi       = {10.1109/TIT.2013.2290112},
  issn      = {0018-9448},
  number    = {2},
  pages     = {1248--1259},
  volume    = {60},
  abstract  = {This paper deals with the computational complexity of conditions which
	guarantee that the NP-hard problem of finding the sparsest solution
	to an underdetermined linear system can be solved by efficient algorithms.
	In the literature, several such conditions have been introduced.
	The most well-known ones are the mutual coherence, the restricted
	isometry property (RIP), and the nullspace property (NSP). While
	evaluating the mutual coherence of a given matrix is easy, it has
	been suspected for some time that evaluating RIP and NSP is computationally
	intractable in general. We confirm these conjectures by showing that
	for a given matrix A and positive integer k, computing the best constants
	for which the RIP or NSP hold is, in general, NP-hard. These results
	are based on the fact that determining the spark of a matrix is NP-hard,
	which is also established in this paper. Furthermore, we also give
	several complexity statements about problems related to the above
	concepts.},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {compressed sensing,computational complexity,information theory,Information theory,mutual coherence,mutual coherence,NP hard problem,nullspace property,Polynomials,restricted isometry property,Sparks,sparse matrices,sparse recovery conditions,underdetermined linear system,Vectors},
  month     = feb,
  owner     = {afdidehf},
  timestamp = {2016-09-30T13:53:37Z},
  year      = {2014},
}

@Article{Maleki2010,
  author    = {Maleki, A. and Donoho, D. L.},
  title     = {Optimally {{Tuned Iterative Reconstruction Algorithms}} for {{Compressed Sensing}}},
  doi       = {10.1109/JSTSP.2009.2039176},
  issn      = {1932-4553},
  number    = {2},
  pages     = {330--341},
  volume    = {4},
  abstract  = {We conducted an extensive computational experiment, lasting multiple CPU-years, to optimally select parameters for two important classes of algorithms for finding sparse solutions of underdetermined systems of linear equations. We make the optimally tuned implementations available at sparselab.stanford.edu; they run ??out of the box?? with no user tuning: it is not necessary to select thresholds or know the likely degree of sparsity. Our class of algorithms includes iterative hard and soft thresholding with or without relaxation, as well as CoSaMP, subspace pursuit and some natural extensions. As a result, our optimally tuned algorithms dominate such proposals. Our notion of optimality is defined in terms of phase transitions, i.e., we maximize the number of nonzeros at which the algorithm can successfully operate. We show that the phase transition is a well-defined quantity with our suite of random underdetermined linear systems. Our tuning gives the highest transition possible within each class of algorithms. We verify by extensive computation the robustness of our recommendations to the amplitude distribution of the nonzero coefficients as well as the matrix ensemble defining the underdetermined system. Our findings include the following. 1) For all algorithms, the worst amplitude distribution for nonzeros is generally the constant-amplitude random-sign distribution, where all nonzeros are the same amplitude. 2) Various random matrix ensembles give the same phase transitions; random partial isometries may give different transitions and require different tuning. 3) Optimally tuned subspace pursuit dominates optimally tuned CoSaMP, particularly so when the system is almost square.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\W2H54EUJ\\5419070.html:text/html},
  journal   = {IEEE Journal of Selected Topics in Signal Processing},
  keywords  = {compressed sensing,degree of sparsity,Iterative algorithms,iterative methods,iterative reconstruction algorithms,linear equations,matrix ensembles,optimal tuning,phase transition,Random Matrices,random partial isometries,signal reconstruction,Sparse matrices,sparse solutions,Thresholding,tuning,underdetermined linear systems},
  month     = apr,
  timestamp = {2016-10-07T10:17:32Z},
  year      = {2010},
}

@Article{Ahlfors2010,
  author    = {Ahlfors, Seppo P. and Han, Jooman and Belliveau, John W. and H{\"a}m{\"a}l{\"a}inen, Matti S.},
  title     = {Sensitivity of {{MEG}} and {{EEG}} to {{Source Orientation}}},
  doi       = {10.1007/s10548-010-0154-x},
  issn      = {0896-0267, 1573-6792},
  language  = {en},
  number    = {3},
  pages     = {227--232},
  urldate   = {2016-10-21},
  volume    = {23},
  abstract  = {An important difference between magnetoencephalography (MEG) and electroencephalography (EEG) is that MEG is insensitive to radially oriented sources. We quantified computationally the dependency of MEG and EEG on the source orientation using a forward model with realistic tissue boundaries. Similar to the simpler case of a spherical head model, in which MEG cannot see radial sources at all, for most cortical locations there was a source orientation to which MEG was insensitive. The median value for the ratio of the signal magnitude for the source orientation of the lowest and the highest sensitivity was 0.06 for MEG and 0.63 for EEG. The difference in the sensitivity to the source orientation is expected to contribute to systematic differences in the signal-to-noise ratio between MEG and EEG.},
  file      = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AF22KJK5\\s10548-010-0154-x.html:text/html},
  journal   = {Brain Topography},
  month     = jul,
  timestamp = {2016-10-21T11:16:51Z},
  year      = {2010},
}

@TechReport{Davies2008,
  author    = {Davies, Mike and Gribonval, Remi},
  title     = {Restricted Isometry Constants where $p$ sparse recovery can fail for $0 < p \leq 1$},
  annote    = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  keywords  = {compressed sensing,convex optimisation,inverse problem,iterative reweighted optimisation.,iterative reweighted optimisation.,nonconvex optimisation,overcomplete dictionary,overcomplete dictionary,restricted isometry property,restricted isometry property,sparse representation,underdetermined linear system},
  month     = jul,
  owner     = {Fardin},
  timestamp = {2016-07-10T08:02:45Z},
  year      = {2008},
}

@InProceedings{Krstulovic2005,
  author    = {Krstulovic, S. and Gribonval, R. and Leveau, P. and Daudet, L.},
  booktitle = {{{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}}, 2005.},
  title     = {A Comparison of Two Extensions of the Matching Pursuit Algorithm for the Harmonic Decomposition of Sounds},
  doi       = {10.1109/ASPAA.2005.1540219},
  pages     = {259--262},
  abstract  = {In the framework of audio signal analysis, it is desired to obtain sparse representations that are able to reflect the harmonic structures, e.g., issued from musical instruments. In this paper, we compare two approaches which introduce some explicit models of harmonic features into the matching pursuit analysis framework. The first approach is the harmonic matching pursuit (HMP), where the harmonic structures are modeled by sets of harmonically related Gabor atoms which are directly optimized in the analysis loop. The second approach, called meta-molecular matching pursuit (M3P), is based on the a posteriori agglomeration of elementary features coming from a short time Fourier transform. We discuss the pros and cons of each method through experiments involving different audio signals, and conclude on possible approaches for combining the two methods.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RCHMB27I\\articleDetails.html:text/html},
  keywords  = {Acoustics,acoustic signal processing,Algorithm design and analysis,a posteriori agglomeration,audio signal analysis,audio signal processing,Dictionaries,Fourier transforms,Gabor atoms,Harmonic analysis,harmonic decomposition,harmonics,harmonic structures,Instruments,iterative methods,matching pursuit algorithm,Matching pursuit algorithms,metamolecular matching pursuit,musical instruments,Pursuit algorithms,short time Fourier transform,Signal analysis,Signal processing algorithms,sparse representations,time-frequency analysis},
  month     = oct,
  timestamp = {2016-09-30T13:31:14Z},
  year      = {2005},
}

@Article{Davies2009,
  author    = {Davies, Mike and Gribonval, Remi},
  title     = {The Restricted Isometry Property and $\ell^p$ sparse recovery failure},
  abstract  = {This paper considers conditions based on the restricted isometry constant
	(RIC) under which the solution of an underdetermined linear system
	with minimal ? p norm, 0 \ensuremath{<} p ? 1, is guaranteed to be
	also the sparsest one. Specifically matrices are identified that
	have RIC, ?2m, arbitrarily close to 1/ ? 2 ? 0.707 where sparse recovery
	with p = 1 fails for at least one m-sparse vector. This indicates
	that there is limited room for improvement over the best known positive
	results of Foucart and Lai, which guarantee that ? 1 -minimisation
	recovers all m-sparse vectors for any matrix with ?2m \ensuremath{<}
	2(3 ? ? 2)/7 ? 0.4531. We also present results that show, compared
	to ? 1 minimisation, ? p minimisation recovery failure is only slightly
	delayed in terms of the RIC values. Furthermore when ? p optimisation
	is attempted using an iterative reweighted ? 1 scheme, failure can
	still occur for ?2m arbitrarily close to 1/ ? 2.},
  annote    = {read https://hal.inria.fr/inria-00370402 - Signal Processing with Adaptive Sparse Structured Representations},
  journal   = {SPARS09},
  keywords  = {NSP,null space property},
  month     = apr,
  owner     = {Fardin},
  timestamp = {2016-07-11T17:03:48Z},
  year      = {2009},
}

@Article{Fuchs2004a,
  author    = {Fuchs, J.-J.},
  title     = {On sparse representations in arbitrary redundant bases},
  doi       = {10.1109/TIT.2004.828141},
  issn      = {0018-9448},
  number    = {6},
  pages     = {1341--1344},
  volume    = {50},
  abstract  = {The purpose of this contribution is to generalize some recent results
	on sparse representations of signals in redundant bases. The question
	that is considered is the following: given a matrix A of dimension
	(n,m) with m>n and a vector b=Ax, find a sufficient condition for
	b to have a unique sparsest representation x as a linear combination
	of columns of A. Answers to this question are known when A is the
	concatenation of two unitary matrices and either an extensive combinatorial
	search is performed or a linear program is solved. We consider arbitrary
	A matrices and give a sufficient condition for the unique sparsest
	solution to be the unique solution to both a linear program or a
	parametrized quadratic program. The proof is elementary and the possibility
	of using a quadratic program opens perspectives to the case where
	b=Ax+e with e a vector of noise or modeling errors.},
  journal   = {Information Theory, IEEE Transactions on},
  keywords  = {Basis pursuit,combinatorial mathematics,combinatorial search,Dictionaries,global matched filter,global matched filter,linear program,linear programming,matched filters,matrix algebra,modeling error,Noise,parameter estimation,parametrized quadratic program,quadratic program,quadratic programming,redundant dictionaries,redundant dictionaries,signal representation,sparse matrices,sparse representations,sparse signal representation,Sufficient conditions,System testing,unitary matrix,Vectors},
  month     = jun,
  timestamp = {2016-09-29T16:31:06Z},
  year      = {2004},
}

@Article{Lai2011,
  author    = {Lai, Ming-Jun and Liu, Yang},
  title     = {The null space property for sparse recovery from multiple measurement vectors},
  doi       = {http://dx.doi.org/10.1016/j.acha.2010.11.002},
  issn      = {1063-5203},
  number    = {3},
  pages     = {402--406},
  volume    = {30},
  abstract  = {We prove a null space property for the uniqueness of the sparse solution
	vectors recovered from a minimization in l p quasi-norm subject to
	multiple systems of linear equations, where p ∈ ( 0 , 1 ] . Furthermore,
	we show that the null space property is equivalent to the null space
	property for the standard l p minimization subject to a single linear
	system. This answers the questions raised in Foucart and Gribonval
	(2010) [17].},
  journal   = {Applied and Computational Harmonic Analysis},
  keywords  = {Optimization,sparse recovery},
  owner     = {Fardin},
  timestamp = {2016-09-30T11:23:41Z},
  year      = {2011},
}

@Article{Okada1993,
  author    = {Okada, Y.},
  title     = {Empirical Bases for Constraints in Current-Imaging Algorithms},
  issn      = {0896-0267},
  language  = {eng},
  number    = {4},
  pages     = {373--377},
  volume    = {5},
  journal   = {Brain Topography},
  keywords  = {algorithms,Brain,Evoked Potentials,Humans,Magnetoencephalography,Models; Neurological},
  pmid      = {8357710},
  timestamp = {2017-08-21T16:12:56Z},
  year      = {1993},
}

@Article{Eldar2009c,
  author    = {Eldar, Y.C. and Mishali, M.},
  title     = {Robust Recovery of Signals From a Structured Union of Subspaces},
  doi       = {10.1109/TIT.2009.2030471},
  issn      = {0018-9448},
  number    = {11},
  pages     = {5302--5316},
  volume    = {55},
  abstract  = {Traditional sampling theories consider the problem of reconstructing
	an unknown signal x from a series of samples. A prevalent assumption
	which often guarantees recovery from the given measurements is that
	x lies in a known subspace. Recently, there has been growing interest
	in nonlinear but structured signal models, in which x lies in a union
	of subspaces. In this paper, we develop a general framework for robust
	and efficient recovery of such signals from a given set of samples.
	More specifically, we treat the case in which x lies in a sum of
	k subspaces, chosen from a larger set of m possibilities. The samples
	are modeled as inner products with an arbitrary set of sampling functions.
	To derive an efficient and robust recovery algorithm, we show that
	our problem can be formulated as that of recovering a block-sparse
	vector whose nonzero elements appear in fixed blocks. We then propose
	a mixed lscr2/lscr1 program for block sparse recovery. Our main result
	is an equivalence condition under which the proposed convex algorithm
	is guaranteed to recover the original signal. This result relies
	on the notion of block restricted isometry property (RIP), which
	is a generalization of the standard RIP used extensively in the context
	of compressed sensing. Based on RIP, we also prove stability of our
	approach in the presence of noise and modeling errors. A special
	case of our framework is that of recovering multiple measurement
	vectors (MMV) that share a joint sparsity pattern. Adapting our results
	to this context leads to new MMV recovery methods as well as equivalence
	conditions under which the entire set can be determined efficiently.},
  annote    = {Information Theory, IEEE Transactions on},
  journal   = {IEEE Trans. Inf. Theory},
  keywords  = {Block restricted isometry property,block sparse vector recovery,block sparsity,compressed sensing,Frequency,History,joint sparsity pattern,joint sparsity pattern,mixed-norm recovery,mixed-norm recovery,multiple measurement vector recovery,multiple measurement vector recovery,multiple measurement vectors (MMV),multiple measurement vectors (MMV),nonlinear signal model,restricted isometry property,restricted isometry property,Robustness,sampling functions,sampling functions,Sampling methods,sampling theory,sampling theory,signal processing,Signal processing algorithms,Signal processing algorithms,signal reconstruction,signal recovery,signal recovery,signal sampling,signal sampling,Stability,subspace structured union,union of linear subspaces,union of linear subspaces,Vectors},
  month     = nov,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:22:16Z},
  year      = {2009},
}

@InProceedings{Eldar2009b,
  author    = {Eldar, Y.C. and Bolcskei, H.},
  booktitle = {Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
  title     = {Block-sparsity: Coherence and efficient recovery},
  doi       = {10.1109/ICASSP.2009.4960226},
  pages     = {2885--2888},
  abstract  = {We consider compressed sensing of block-sparse signals, i.e., sparse
	signals that have nonzero coefficients occurring in clusters. Based
	on an uncertainty relation for block-sparse signals, we define a
	block-coherence measure and show that a block-version of the orthogonal
	matching pursuit algorithm recovers block k-sparse signals in no
	more than k steps if the block-coherence is sufficiently small. The
	same condition on block-sparsity is shown to guarantee successful
	recovery through a mixed lscr2/lscr1 optimization approach. The significance
	of the results lies in the fact that making explicit use of block-sparsity
	can yield better reconstruction properties than treating the signal
	as being sparse in the conventional sense, thereby ignoring the additional
	structure in the problem.},
  keywords  = {block-coherence measure,block k-sparse signals,Block-sparsity,Clustering algorithms,Coherence,compressed sensing,Context,Matching pursuit algorithms,orthogonal matching pursuit algorithm,Pursuit algorithms,Robustness,signal processing,Sufficient conditions,Uncertainty,uncertainty relations,Wireless communication},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2016-09-30T13:32:16Z},
  year      = {2009},
}

@Article{Eldar2009a,
  author    = {Eldar, Y.C.},
  title     = {Compressed Sensing of Analog Signals in Shift-Invariant Spaces},
  doi       = {10.1109/TSP.2009.2020750},
  issn      = {1053-587X},
  number    = {8},
  pages     = {2986--2997},
  volume    = {57},
  abstract  = {A traditional assumption underlying most data converters is that the
	signal should be sampled at a rate exceeding twice the highest frequency.
	This statement is based on a worst-case scenario in which the signal
	occupies the entire available bandwidth. In practice, many signals
	are sparse so that only part of the bandwidth is used. In this paper,
	we develop methods for low-rate sampling of continuous-time sparse
	signals in shift-invariant (SI) spaces, generated by m kernels with
	period T . We model sparsity by treating the case in which only k
	out of the m generators are active, however, we do not know which
	k are chosen. We show how to sample such signals at a rate much lower
	than m/T, which is the minimal sampling rate without exploiting sparsity.
	Our approach combines ideas from analog sampling in a subspace with
	a recently developed block diagram that converts an infinite set
	of sparse equations to a finite counterpart. Using these two components
	we formulate our problem within the framework of finite compressed
	sensing (CS) and then rely on algorithms developed in that context.
	The distinguishing feature of our results is that in contrast to
	standard CS, which treats finite-length vectors, we consider sampling
	of analog signals for which no underlying finite-dimensional model
	exists. The proposed framework allows to extend much of the recent
	literature on CS to the analog domain.},
  journal   = {Signal Processing, IEEE Transactions on},
  keywords  = {analog compressed sensing,analog compressed sensing signal,Bandwidth,Data Compression,data converter,finite compressed sensing,finite-dimensional model,finite-length vector,low-rate sampling,matrix algebra,shift-invariant spaces,signal sampling,sparse equation,Sparsity,subNyquist sampling,sub-Nyquist sampling},
  month     = aug,
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:27:11Z},
  year      = {2009},
}

@Book{Elad2010,
  author    = {Elad, Michael},
  title     = {Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing},
  editor    = {{{Springer}}},
  publisher = {{\{Springer\}}},
  annote    = {read},
  owner     = {Fardin},
  timestamp = {2016-07-10T08:24:13Z},
  year      = {2010},
}

@Book{Bertsekas2003,
  author    = {Bertsekas, Dimitri and Nedic, Angelia},
  title     = {Convex {{Analysis}} and {{Optimization}}},
  isbn      = {978-1-886529-45-8},
  language  = {English},
  publisher = {{Athena Scientific}},
  abstract  = {A uniquely pedagogical, insightful, and rigorous treatment of the analytical/geometrical foundations of optimization. Among its special features, the book: 1) Develops rigorously and comprehensively the theory of convex sets and functions, in the classical tradition of Fenchel and Rockafellar 2) Provides a geometric, highly visual treatment of convex and nonconvex optimization problems, including existence of solutions, optimality conditions, Lagrange multipliers, and duality 3) Includes an insightful and comprehensive presentation of minimax theory and zero sum games, and its connection with duality 4) Describes dual optimization, the associated computational methods, including the novel incremental subgradient methods, and applications in linear, quadratic, and integer programming 5) Contains many examples, illustrations, and exercises with complete solutions (about 200 pages) posted on the internet. From the preface:This book focuses on the theory of convex sets and functions, and its connections with a number of topics that span a broad range from continuous to discrete optimization. These topics include Lagrange multiplier theory, Lagrangian and conjugate/Fenchel duality, minimax theory, and nondifferentiable optimization.The book evolved from a set of lecture notes for a graduate course at M.I.T. It is widely recognized that, aside from being an eminently useful subject in engineering, operations research, and economics, convexity is an excellent vehicle for assimilating some of the basic concepts of real analysis within an intuitive geometrical setting. Unfortunately, the subject's coverage in academic curricula is scant and incidental. We believe that at least part of the reason is the shortage of textbooks that are suitable for classroom instruction, particularly for nonmathematics majors. We have therefore tried to make convex analysis accessible to a broader audience by emphasizing its geometrical character, while maintaining mathematical rigor. We have included as many insightful illustrations as possible, and we have used geometric visualization as a principal tool for maintaining the students' interest in mathematical proofs.Our treatment of convexity theory is quite comprehensive, with all major aspects of the subject receiving substantial treatment. The mathematical prerequisites are a course in linear algebra and a course in real analysis in finite dimensional spaces (which is the exclusive setting of the book). A summary of this material, without proofs, is provided in Section 1.1. The coverage of the theory has been significantly extended in the exercises, which represent a major component of the book. Detailed solutions of all the exercises (nearly 200 pages) are internet-posted in the book's www pageSome of the exercises may be attempted by the reader without looking at the solutions, while others are challenging but may be solved by the advanced reader with the assistance of hints. Still other exercises represent substantial theoretical results, and in some cases include new and unpublished research. Readers and instructors should decide for themselves how to make best use of the internet-posted solutions.An important part of our approach has been to maintain a close link between the theoretical treatment of convexity and its application to optimization.},
  address   = {Belmont, Mass},
  month     = apr,
  timestamp = {2017-03-31T12:04:19Z},
  year      = {2003},
}

@InProceedings{Ganesh2009,
  author    = {Ganesh, A. and Zhou, Z. and Ma, Y.},
  booktitle = {Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
  title     = {Separation of a subspace-sparse signal: Algorithms and conditions},
  doi       = {10.1109/ICASSP.2009.4960290},
  pages     = {3141--3144},
  abstract  = {In this paper, we show how two classical sparse recovery algorithms,
	orthogonal matching pursuit and basis pursuit, can be naturally extended
	to recover block-sparse solutions for subspace-sparse signals. A
	subspace-sparse signal is sparse with respect to a set of subspaces,
	instead of atoms. By generalizing the notion of mutual incoherence
	to the set of subspaces, we show that all classical sufficient conditions
	remain exactly the same for these algorithms to work for subspace-sparse
	signals, in both noiseless and noisy cases. The sufficient conditions
	provided are easy to verify for large systems. We conduct simulations
	to compare the performance of the proposed algorithms.},
  annote    = {read},
  keywords  = {Dictionaries,iterative methods,Linear systems,Matching pursuit algorithms,Optimization methods,orthogonal basis pursuit,orthogonal matching pursuit,Pursuit algorithms,signal processing,signal representations,Sparks,sparse matrices,sparse recovery algorithm,subspace base pursuit,subspace incoherence,subspace matching pursuit,subspace sparse,subspace-sparse signal,Sufficient conditions,time-frequency analysis,Vectors},
  month     = apr,
  owner     = {afdidehf},
  timestamp = {2016-09-30T13:47:52Z},
  year      = {2009},
}

@Article{Boufounos2011,
  author    = {Boufounos, P. and Kutyniok, G. and Rauhut, H.},
  title     = {Sparse {{Recovery From Combined Fusion Frame Measurements}}},
  doi       = {10.1109/TIT.2011.2143890},
  issn      = {0018-9448},
  number    = {6},
  pages     = {3864--3876},
  volume    = {57},
  abstract  = {Sparse representations have emerged as a powerful tool in signal and information processing, culminated by the success of new acquisition and processing techniques such as compressed sensing (CS). Fusion frames are very rich new signal representation methods that use collections of subspaces instead of vectors to represent signals. This work combines these exciting fields to introduce a new sparsity model for fusion frames. Signals that are sparse under the new model can be compressively sampled and uniquely reconstructed in ways similar to sparse signals using standard CS. The combination provides a promising new set of mathematical tools and signal models useful in a variety of applications. With the new model, a sparse signal has energy in very few of the subspaces of the fusion frame, although it does not need to be sparse within each of the subspaces it occupies. This sparsity model is captured using a mixed l1/l2 norm for fusion frames. A signal sparse in a fusion frame can be sampled using very few random projections and exactly reconstructed using a convex optimization that minimizes this mixed l1/l2 norm. The provided sampling conditions generalize coherence and RIP conditions used in standard CS theory. It is demonstrated that they are sufficient to guarantee sparse recovery of any signal sparse in our model. More over, a probabilistic analysis is provided using a stochastic model on the sparse signal that shows that under very mild conditions the probability of recovery failure decays exponentially with in creasing dimension of the subspaces.},
  file      = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\A59FCNMH\\articleDetails.html:text/html},
  journal   = {IEEE Transactions on Information Theory},
  keywords  = {<sub>1;2</sub>-minimization,$ell_1$-minimization,acquisition technique,Analytical models,Coherence,combined fusion frame measurement,compressed sensing,Compressed sensing (CS),convex optimization,convex programming,fusion frames,information processing,mathematical tools,minimization,mutual coherence,Null space,probabilistic analysis,Probabilistic logic,probability,Random Matrices,sampling condition,sensor fusion,sensors,signal detection,signal model,signal processing,signal representation,signal representation method,Signal sampling,Sparse matrices,sparse recovery,sparse representation,sparse signal,stochastic model,stochastic processes},
  month     = jun,
  timestamp = {2016-09-30T13:50:04Z},
  year      = {2011},
}

@Article{Rao2012a,
  author    = {Rao, Nikhil and Recht, Benjamin and Nowak, Robert},
  title     = {Universal {{Measurement Bounds}} for {{Structured Sparse Signal Recovery}}},
  pages     = {942--950},
  journal   = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  timestamp = {2016-10-10T11:45:19Z},
  year      = {2012},
}

@Comment{jabref-meta: databaseType:biblatex;}
