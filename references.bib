% Encoding: UTF-8

@article{Benedek1961,
  title = {The space $l^{p}$, with mixed norm},
  volume = {28},
  issn = {0012-7094, 1547-7398},
  doi = {10.1215/S0012-7094-61-02828-9},
  abstract = {Project Euclid - mathematics and statistics online},
  timestamp = {2016-07-11T17:05:21Z},
  number = {3},
  urldate = {2016-05-07},
  journal = {Duke Mathematical Journal},
  author = {Benedek, A. and Panzone, R.},
  year = {1961},
  pages = {301-324},
  mrnumber = {MR0126155},
  zmnumber = {0107.08902}
}

@article{Samarah2005,
  title = {A Schur test for weighted mixed-norm spaces},
  volume = {31},
  issn = {0133-3852, 1588-273X},
  doi = {10.1007/s10476-005-0022-1},
  abstract = {Summary We prove a Schur test for mixed-norm spaces Lp,q, 1 < p,q < ∞. Also we prove another version of the Schur test for discrete weighted mixed-norm spaces lp,q w, 1 < p,q < ∞, and wis a weight. We show that if w 1, and w 2are two weight functions on the index sets Jx Iand K x Lrespectively, and A =(a ji, kl ) j∈J, i∈I, k∈K, l∈L is an infinite matrix, then under certain conditions, Ais a bounded operator from lp,q w1, 1 < p,q < ∞ to lp,q w2. This will be a key result in proving boundedness of important operators in our work in time-frequency analysis.</o:p>},
  language = {en},
  timestamp = {2016-07-08T11:28:17Z},
  number = {4},
  urldate = {2016-05-07},
  journal = {Analysis Mathematica},
  author = {Samarah, Salti and Obeidat, S. and Salman, R.},
  month = nov,
  year = {2005},
  keywords = {analysis,Analysis},
  pages = {277-289}
}

@inproceedings{Elhamifar2011,
  title = {Robust classification using structured sparse representation},
  doi = {10.1109/CVPR.2011.5995664},
  abstract = {In many problems in computer vision, data in multiple classes lie in multiple low-dimensional subspaces of a high-dimensional ambient space. However, most of the existing classification methods do not explicitly take this structure into account. In this paper, we consider the problem of classification in the multi-sub space setting using sparse representation techniques. We exploit the fact that the dictionary of all the training data has a block structure where the training data in each class form few blocks of the dictionary. We cast the classification as a structured sparse recovery problem where our goal is to find a representation of a test example that uses the minimum number of blocks from the dictionary. We formulate this problem using two different classes of non-convex optimization programs. We propose convex relaxations for these two non-convex programs and study conditions under which the relaxations are equivalent to the original problems. In addition, we show that the proposed optimization programs can be modified properly to also deal with corrupted data. To evaluate the proposed algorithms, we consider the problem of automatic face recognition. We show that casting the face recognition problem as a structured sparse recovery problem can improve the results of the state-of-the-art face recognition algorithms, especially when we have relatively small number of training data for each class. In particular, we show that the new class of convex programs can improve the state-of-the-art face recognition results by 10% with only 25% of the training data. In addition, we show that the algorithms are robust to occlusion, corruption, and disguise.},
  timestamp = {2016-09-30T11:45:25Z},
  booktitle = {2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {Elhamifar, E. and Vidal, R.},
  month = jun,
  year = {2011},
  keywords = {automatic face recognition,automatic face recognition,computer vision,computer vision,concave programming,concave programming,convex relaxations,convex relaxations,convex
	relaxations,Dictionaries,Dictionaries,Face,Face,face recognition,face recognition,image classification,image classification,image representation,image representation,image
	representation,multi-sub space setting,multi-sub space setting,nonconvex optimization programs,nonconvex optimization programs,Optimization,Optimization,robust classification,robust
	classification,robust classification,structured sparse representation,structured sparse representation,Training data,Training data,Vectors,Vectors},
  pages = {1873--1879}
}

@article{Wright2009a,
  title = {Robust Face Recognition via Sparse Representation},
  volume = {31},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2008.79},
  abstract = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
  timestamp = {2016-09-30T11:28:15Z},
  number = {2},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Wright, J. and Yang, A. Y. and Ganesh, A. and Sastry, S. S. and Ma, Y.},
  month = feb,
  year = {2009},
  keywords = {Algorithms,algorithms,Artificial Intelligence,Artificial Intelligence,Automated,Biometry,Biometry,Classification algorithms,Classification
	algorithms,Classification algorithms,Classifier design and evaluation,Classifier design and evaluation,Cluster Analysis,cluster analysis,compressed sensing,compressed sensing,compressed
	sensing,Computer-Assisted,downsampled images,downsampled images,eigenfaces,eigenfaces,ell^1–minimization,ell^1–minimization,Face,Face,Face and gesture recognition,Face and gesture recognition,Face
	and gesture recognition,face recognition,face recognition,Feature evaluation and selection,Feature evaluation and selection,feature extraction,feature extraction,Feature
	Extraction,Humans,Humans,illumination,illumination,image-based object recognition,image-based object recognition,Image Enhancement,Image Enhancement,Image
	Enhancement,Image Interpretation,Image Interpretation; Computer-Assisted,Image recognition,Image recognition,Laplacianfaces,Laplacianfaces,Lighting,lighting,lightning,lightning,Linear regression,Linear regression,Linear
	regression,multiple linear regression model,multiple linear regression model,object recognition,Object Recognition,occlusion,occlusion,occlusion and corruption,occlusion and corruption,occlusion
	and corruption,Outlier rejection,Outlier rejection,Pattern Recognition,Pattern Recognition; Automated,random processes,random processes,random projections,random projections,random
	projections,regression analysis,regression analysis,Reproducibility of Results,Reproducibility of Results,robust face recognition,robust face recognition,robust
	face recognition,Robustness,Robustness,Sensitivity and Specificity,Sensitivity and Specificity,signal representation,signal representation,signal representations,signal
	representations,Signal representations,Spare representation,Spare representation,sparse representation,sparse representation,sparse signal representation,sparse signal representation,sparse
	signal representation,Subtraction Technique,Subtraction Technique,validation and outlier rejection.,validation and outlier rejection.,validation and outlier
	rejection.},
  pages = {210--227}
}

@inproceedings{Peng2015,
  title = {Robust group sparse representation via half-quadratic optimization for face recognition},
  doi = {10.1109/CCECE.2015.7129176},
  abstract = {Sparse representation-based classifier (SRC), which represents a test sample with a linear combination of training samples, has shown promise in pattern classification. However, there are two shortcomings in SRC: (1) the ℓ1-norm used to measure the reconstruction fidelity is noise-sensitive and (2) the ℓ2-norm induced sparsity did not consider the correlation among the training samples. Furthermore, in real applications, face images with similar variations, such as illumination or expression, often have higher correlation than those from the same subject. Therefore, we propose to improve the performance of SRC from two aspects: (1) replace the noise-sensitive ℓ2-norm with an M-estimator to enhance its robustness and (2) emphasize the sparsity of the number of classes instead of the number of training samples, which leads to the group sparsity. The proposed robust group sparse representation (RGSR) can be efficiently optimized via alternating minimization under the Half-Quadratic (HQ) framework. Extensive experiments on representative face data sets show that RGSR can achieve competitive performance in face recognition and outperforms several state-of-the-art methods in dealing with various types of noise such as corruption, occlusion and disguise.},
  timestamp = {2016-09-30T11:46:28Z},
  booktitle = {2015 IEEE 28th Canadian Conference on Electrical and Computer Engineering (CCECE)},
  author = {Peng, Y. and Lu, B. L.},
  month = may,
  year = {2015},
  keywords = {Computers,Computers,Conferences,Conferences,Decision support systems,Decision support systems,face images,face images,face recognition,face recognition,group sparsity,group sparsity,group
	sparsity,half-quadratic optimization,half-quadratic optimization,HQ framework,HQ framework,image classification,image classification,Image reconstruction,image reconstruction,Image
	reconstruction,image representation,image representation,ℓ1-norm,l1-norm,ℓ2-norm induced sparsity,l2-norm induced sparsity,M-estimator,M-estimator,Minimization,minimization,noise-sensitive reconstruction fidelity,noise-sensitive reconstruction fidelity,noise-sensitive
	reconstruction fidelity,pattern classification,pattern classification,quadratic programming,quadratic programming,RGSR,RGSR,robust group sparse representation,robust group sparse representation,robust
	group sparse representation,sparse representation-based classifier,sparse representation-based classifier,SRC,SRC},
  pages = {146--151}
}

@inproceedings{Elhamifar2010,
  title = {Clustering disjoint subspaces via sparse representation},
  doi = {10.1109/ICASSP.2010.5495317},
  abstract = {Given a set of data points drawn from multiple low-dimensional linear subspaces of a high-dimensional space, we consider the problem of clustering these points according to the subspaces they belong to. Our approach exploits the fact that each data point can be written as a sparse linear combination of all the other points. When the subspaces are independent, the sparse coefficients can be found by solving a linear program. However, when the subspaces are disjoint, but not independent, the problem becomes more challenging. In this paper, we derive theoretical bounds relating the principal angles between the subspaces and the distribution of the data points across all the subspaces under which the coefficients are guaranteed to be sparse. The clustering of the data is then easily obtained from the sparse coefficients. We illustrate the validity of our results through simulation experiments.},
  timestamp = {2016-09-30T13:27:54Z},
  booktitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  author = {Elhamifar, E. and Vidal, R.},
  month = mar,
  year = {2010},
  keywords = {Application software,Application software,Clustering methods,Clustering methods,computer vision,computer vision,data clustering,data clustering,disjoint subspace clustering,disjoint subspace clustering,disjoint
	subspace clustering,disjoint subspaces,disjoint subspaces,Image Processing,Image processing,Image segmentation,Image segmentation,pattern clustering,pattern clustering,pattern
	clustering,signal processing,signal processing,sparse coefficients,sparse coefficients,sparse linear combination,sparse linear combination,sparse matrices,sparse
	matrices,Sparse matrices,sparse representation,sparse representation,Sparsity,sparsity,Statistical analysis,statistical analysis,subspace angles,subspace angles,subspace
	angles,Subspace clustering,subspace clustering,Video sequences,video sequences},
  pages = {1926--1929}
}

@article{Elhamifar2013,
  title = {Sparse Subspace Clustering: Algorithm, Theory, and Applications},
  volume = {35},
  issn = {0162-8828},
  shorttitle = {Sparse Subspace Clustering},
  doi = {10.1109/TPAMI.2013.57},
  abstract = {Many real-world problems deal with collections of high-dimensional data, such as images, videos, text, and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efficient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.},
  timestamp = {2016-09-30T13:27:00Z},
  number = {11},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Elhamifar, E. and Vidal, R.},
  month = nov,
  year = {2013},
  keywords = {$(ell_1)$-minimization,$(ell_1)$-minimization,Algorithms,algorithms,Artificial Intelligence,Artificial Intelligence,Automated,Biometry,Biometry,clustering,clustering,Clustering algorithms,Clustering algorithms,Clustering
	algorithms,computational complexity,computational complexity,Computer-Assisted,computer vision,computer vision,convex programming,convex programming,convex
	programming,convex relaxation,convex relaxation,data point clustering,data point clustering,data point representation,data point representation,data structures,data structures,data
	structures,Face,Face,face clustering,face clustering,general NP-hard problem,general NP-hard problem,High-dimensional data,high-dimensional
	data,high-dimensional data,high-dimensional data collection,high-dimensional data collection,Humans,Humans,Image Interpretation,Image Interpretation; Computer-Assisted,intrinsic low-dimensionality,intrinsic low-dimensionality,intrinsic
	low-dimensionality,minimisation,minimisation,minimization program,minimization program,motion segmentation,motion segmentation,Noise,Noise,Optimization,Optimization,pattern clustering,pattern clustering,pattern
	clustering,Pattern Recognition,Pattern Recognition; Automated,principal angles,principal angles,Sample Size,Sample Size,sparse matrices,sparse
	matrices,Sparse matrices,sparse optimization program,sparse optimization program,sparse representation,sparse representation,sparse subspace clustering algorithm,sparse subspace clustering algorithm,sparse
	subspace clustering algorithm,spectral clustering,spectral clustering,spectral clustering framework,spectral clustering framework,spectral clustering
	framework,subspaces,subspaces,synthetic data,synthetic data,Vectors,Vectors},
  pages = {2765--2781}
}

@inproceedings{Elhamifar2012c,
  title = {See all by looking at a few: Sparse modeling for finding representative objects},
  shorttitle = {See all by looking at a few},
  doi = {10.1109/CVPR.2012.6247852},
  abstract = {We consider the problem of finding a few representatives for a dataset, i.e., a subset of data points that efficiently describes the entire dataset. We assume that each data point can be expressed as a linear combination of the representatives and formulate the problem of finding the representatives as a sparse multiple measurement vector problem. In our formulation, both the dictionary and the measurements are given by the data matrix, and the unknown sparse codes select the representatives via convex optimization. In general, we do not assume that the data are low-rank or distributed around cluster centers. When the data do come from a collection of low-rank models, we show that our method automatically selects a few representatives from each low-rank model. We also analyze the geometry of the representatives and discuss their relationship to the vertices of the convex hull of the data. We show that our framework can be extended to detect and reject outliers in datasets, and to efficiently deal with new observations and large datasets. The proposed framework and theoretical foundations are illustrated with examples in video summarization and image classification using representatives.},
  timestamp = {2016-09-30T13:28:33Z},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {Elhamifar, E. and Sapiro, G. and Vidal, R.},
  month = jun,
  year = {2012},
  keywords = {Clustering algorithms,Clustering algorithms,codes,codes,convex hull,convex hull,convex optimization,convex optimization,convex programming,convex programming,convex
	programming,data matrix,data matrix,Data models,Data models,Dictionaries,Dictionaries,dictionary,Dictionary,Distributed databases,Distributed databases,Distributed
	databases,Geometry,Geometry,image classification,image classification,matrix algebra,matrix algebra,Optimization,Optimization,outlier detection,outlier detection,outlier
	detection,Outlier rejection,Outlier rejection,representative object,representative object,sparse code,sparse code,sparse matrices,sparse
	matrices,Sparse matrices,sparse modeling,sparse modeling,sparse multiple measurement vector problem,sparse multiple measurement vector problem,vertex,vertex,vertex functions,vertex functions,vertex
	functions,video signal processing,video signal processing,video summarization,video summarization},
  pages = {1600--1607}
}

@techreport{Abbasi2012,
  title = {EEG Inverse Problem},
  abstract = {This project aims to understand the EEG source localization medical
	imaging problem. One main approach taken was to understand a solution
	to the inverse problem called eLORETA and see if any improvements
	can be made to this algorithm. An alternate approach to the problem
	using variational methods was also explored.},
  timestamp = {2016-07-08T12:17:20Z},
  institution = {Fields MITACS Undergraduate Summer Research Program},
  author = {Abbasi, Bilal and Bragnalo, Carrie and Chi, Feng and Han, Jiho and Sivak, Iryna},
  year = {2012},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@article{2008,
  title = {Tractable Upper Bounds on the Restricted Isometry Constant},
  timestamp = {2016-07-11T17:09:02Z},
  journal = {SIAM annual meeting},
  author = {d'Aspremont, Alex and Bach, Francis and Ghaoui, Laurent El},
  year = {2008},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@techreport{Asbury2011,
  title = {Brain Imaging Technologies and Their Applications in Neuroscience},
  timestamp = {2016-07-08T11:49:16Z},
  institution = {The DANA Foundation},
  author = {Asbury, Carolyn},
  month = nov,
  year = {2011},
  note = {read},
  owner = {Fardin}
}

@techreport{Belk2011,
  title = {Convexity, Inequalities, and Norms},
  timestamp = {2016-07-08T12:03:54Z},
  institution = {Mathematics Program, Bard College},
  author = {Belk, James M.},
  year = {2011},
  owner = {afdidehf}
}

@techreport{BJORKSTROM2001,
  title = {Ridge regression and inverse problems},
  abstract = {Why is ridge regression (RR) often a useful method even in cases where
	multiple linear regression (MLR) is dubious or inadequate as a model?
	We suggest that some light can be shed on this question if one notes
	that RR is an application of Tikhonov regularization (TR), a method
	that has been explored in the approximation theory literature for
	about as long as RR has been used in statistics. TR has proven useful
	for many inverse problems, but it has often been applied without
	stating a statistical model at all. In order to indicate how alternatives
	to MLR might be defined, we give a subjective overview of some inverse
	problems from the geophysical sciences. We conclude that estimation
	is often at least as important as prediction.},
  timestamp = {2016-07-10T08:04:10Z},
  institution = {Stockholm University, Sweden},
  author = {BJORKSTROM, ANDERS},
  month = jan,
  year = {2001},
  keywords = {inverse problems,partial least squares,principal components regression,regularized
	estimators,ridge regression,Tikhonov regularization},
  owner = {Fardin}
}

@article{Cand`es2001,
  title = {Curvelets},
  timestamp = {2016-09-29T16:23:17Z},
  journal = {Dept. Statistics, Stanford Univ., Stanford, CA},
  author = {Cand{\`e}s, E. J. and Donoho, D. L.},
  year = {2001},
  owner = {afdidehf}
}

@techreport{Cho2013,
  title = {Precisely Verifying the Null Space Conditions in Compressed Sensing: 	A Sandwiching Algorithm},
  abstract = {The null space condition of sensing matrices plays an important role
	in guaranteeing the success of compressed sensing. In this paper,
	we propose new efficient algorithms to verify the null space condition
	in compressed sensing (CS). Given an (n ? m) � n (m > 0) CS matrix
	A and a positive k, we are interested in computing ?k = max {z:Az=0,z?=0}
	max {K:|K|?k} ?zK?1 ?z?1 , where K represents subsets of {1, 2, ...,n},
	and |K| is the cardinality of K. In particular, we are interested
	in finding the maximum k such that ?k < 1 2. However, computing ?k
	is known to be extremely challenging. In this paper, we first propose
	a series of new polynomial-time algorithms to compute upper bounds
	on ?k. Based on these new polynomial-time algorithms, we further
	design a new sandwiching algorithm, to compute the exact ?k with
	greatly reduced complexity. When needed, this new sandwiching algorithm
	also achieves a smooth tradeoff between computational complexity
	and result accuracy. Empirical results show the performance improvements
	of our algorithm over existing known methods; and our algorithm outputs
	precise values of ?k, with much lower complexity than exhaustive
	search.},
  timestamp = {2016-07-10T07:33:01Z},
  institution = {Department of Electrical and Computer Engineering, University of 	Iowa, Iowa City},
  author = {Cho, Myung and Xu, Weiyu},
  year = {2013},
  keywords = {?1 minimization,compressed sensing,the null space condition,the null space
	condition,verifying the null space condition,verifying
	the null space condition},
  owner = {Fardin}
}

@techreport{Comon2011,
  title = {Sparse Representations And Low-rank Tensor Approximation},
  timestamp = {2016-07-11T16:52:30Z},
  institution = {UNSA-CNRS},
  author = {Comon, Pierre and Lim, Lek-Heng},
  year = {2011},
  owner = {Fardin}
}

@techreport{Deng2011,
  title = {Group Sparse Optimization By Alternating Direction Method},
  abstract = {This paper proposes ecient algorithms for group sparse optimization
	with mixed `2;1-regularization, which arises from the reconstruction
	of group sparse signals in compressive sensing, and the group Lasso
	problem in statistics and machine learning. It is known that encoding
	the group information in addition to sparsity will lead to better
	signal recovery/feature selection. The `2;1-regularization promotes
	group sparsity, but the resulting problem, due to the mixed-norm
	structure and possible grouping irregularity, is considered more
	dicult to solve than the conventional `1-regularized problem. Our
	approach is based on a variable splitting strategy and the classic
	alternating direction method (ADM). Two algorithms are presented,
	one derived from the primal and the other from the dual of the `2;1-regularized
	problem. The convergence of the proposed algorithms is guaranteed
	by the existing ADM theory. General group congurations such as overlapping
	groups and incomplete covers can be easily handled by our approach.
	Computational results show that on random problems the proposed ADM
	algorithms exhibit good eciency, and strong stability and robustness.},
  timestamp = {2016-07-08T12:40:22Z},
  institution = {Department of Computational and Applied Mathematics, Rice University},
  author = {Deng, Wei and Yin, Wotao and Zhang, Yin},
  year = {2011},
  keywords = {alternating direction method,augmented Lagrangian,compressive sensing,group
	Lasso,group sparsity,joint sparsity,mixed norm},
  owner = {Fardin}
}

@techreport{Donoho2004a,
  title = {For most large underdetermined systems of equations, the minimal $\ell^1$-norm near-solution approximates the sparsest near-solution},
  abstract = {We consider inexact linear equations y  where y is a given vector
	in Rn,  is a given n by m matrix, and we wish to find an  which
	is sparse and gives an approximate solution, obeying ky ?  In general this requires combinatorial optimization and so is
	considered intractable. On the other hand, the `1 minimization problem
	min k1 subject to ky ?  is convex, and is considered tractable.
	We show that for most  the solution ) of this
	problem is quite generally a good approximation for . We suppose
	that the columns of  are normalized to unit `2 norm 1 and we place
	uniform measure on such . We study the underdetermined case where
	m  An, A > 1 and prove the existence of (A) and C > 0 so that
	for large n, and for all s except a negligible fraction, the
	following approximate sparse solution property of holds: For every
	y having an approximation ky ?  by a coefficient vector0
	2 Rm with fewer than n nonzeros, we have k
	C. This has two implications. First: for most , whenever the
	combinatorial optimization result  would be very sparse, 
	is a good approximation to. Second: suppose we are given noisy
	data obeying y = 0 + z where the unknown 0 is known to be sparse
	and the noise kzk2. For most , noise-tolerant `1-minimization
	will stably recover 0 from y in the presence of noise z. We study
	also the barely-determined case m = n and reach parallel conclusions
	by slightly different arguments. The techniques include the use of
	almost-spherical sections in Banach space theory and concentration
	of measure for eigenvalues of random matrices.},
  timestamp = {2017-06-23T09:46:01Z},
  institution = {Comm. Pure Appl. Math},
  author = {Donoho, David L.},
  month = aug,
  year = {2004},
  keywords = {Almost-Spherical,Approximate,Banach,Eigenvalues,equations.,linear,Matrices.,of,Random,Sections,Solution,Spaces.,Sparse,Systems.,underdetermined},
  owner = {Fardin}
}

@techreport{Donoho2004c,
  title = {Stable Recovery of Sparse Overcomplete Representations in the Presence 	of Noise},
  timestamp = {2016-07-11T16:56:30Z},
  institution = {Dep. Statistics, Stanford Univ.,},
  author = {Donoho, David L. and Elad, Michael and Temlyakov, Vladimir N.},
  year = {2004},
  owner = {Fardin}
}

@techreport{Eldar2015a,
  title = {Compressed Sensing},
  timestamp = {2016-07-08T11:55:32Z},
  institution = {Electrical Engineering Department, Technion-Israel Institute of Technology, 	Haifa, Israel, 32000},
  author = {Eldar, Yonina C.},
  year = {2015},
  owner = {afdidehf}
}

@article{2010,
  title = {Weak Recovery Conditions using Graph Partitioning Bounds},
  timestamp = {2016-07-11T17:11:42Z},
  author = {d'Aspremont, Alexandre and Karoui, Noureddine El},
  month = oct,
  year = {2010},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Princeton University},
  owner = {afdidehf}
}

@techreport{Foucart2009,
  title = {Notes on Compressed Sensing},
  timestamp = {2016-07-10T07:01:27Z},
  institution = {Vanderbilt University, Department of Mathematics},
  author = {Foucart, Simon},
  year = {2009},
  owner = {afdidehf}
}

@techreport{Hamalainen2013,
  title = {Overcoming Challenges of MEGEEG Data Analysis- Insights from Biophysics, 	Anatomy, and Physiology},
  abstract = {By noninvasively measuring electromagnetic signals ensuing from neurons,
	magnetoencephalography (MEG) and electroencephalography (EEG) are
	the only noninvasive human brain imaging tools that provide submillisecond
	temporal accuracy. In this way, they help to unravel precise dynamics
	of brain function. Functional magnetic resonance imaging (fMRI) provides
	a spatial resolution in the millimeter scale, but its temporal resolution
	is limited because it measures neuronal activity indirectly by imaging
	the sluggish hemodynamic response. In contrast, MEG and EEG measure
	the magnetic and electric fields that are directly related to the
	underlying electrophysiological processes and can thus attain their
	high temporal resolution. Processing MEG or EEG data to obtain accurate
	localization of active neural sources is a complicated task. It involves
	numerous steps: signal denoising; segmenting various structures from
	anatomical MRIs; numerical solution of the electromagnetic forward
	problem; a solution to the ill-posed electromagnetic inverse problem;
	and appropriate control of multiple statistical comparisons spanning
	space, time, and frequency across experimental conditions and groups
	of subjects. This complexity not only constitutes a challenge to
	MEG investigators but also offers a great deal of flexibility in
	data analysis. However, thanks to the direct relationship between
	the MEG and EEG signals and the underlying neural currents, much
	insight into these methods can be gained by understanding the associated
	biophysics in the context of neurophysiology and anatomy. This chapter
	discusses the relationship of the macroscopic MEG and EEG signals
	and their physiological sources, thus providing the foundation for
	understanding the analysis methods applied to estimate the time courses
	of brain activity. It also gives an overview of available source
	estimation methods to help beginners understand their underlying
	assumptions and their applicability to particular experimental data.},
  timestamp = {2016-07-10T07:21:02Z},
  institution = {Athinoula A. Martinos Center for Biomedical Imaging Massachusetts 	General Hospital Charlestown, Massachusetts},
  author = {H{\"a}m{\"a}l{\"a}inen, Matti S.},
  year = {2013},
  owner = {Fardin}
}

@techreport{Kettenring2003,
  title = {Statistics: Challenges and Opportunities for the Twenty First Century},
  timestamp = {2016-07-11T16:57:07Z},
  institution = {NSF report},
  author = {Kettenring, Jon and Lindsay, Bruce and Siegmund, David},
  month = apr,
  year = {2003},
  owner = {afdidehf}
}

@techreport{Li2015,
  title = {Norms and norm inequalities},
  timestamp = {2016-07-10T07:00:50Z},
  institution = {Department of Mathematics, College of William and Mary},
  author = {Li, Chi-Kwong},
  year = {2015},
  owner = {afdidehf}
}

@techreport{Lillo1991,
  title = {Neural networks for constrained optimization problems},
  abstract = {This paper is concerned with utilizing neural networks and analog
	circuits to solve constrained optimization problems. A novel neural
	network architecture is proposed for solving a class of nonlinear
	programming problems. The proposed neural network, or more precisely
	a physically realizable approximation, is then used to solve minimum
	norm problems subject to linear constraints. Minimum norm problems
	have many applications in various areas, but we focus on their applications
	to the control of discrete dynamic processes. The applicability of
	the proposed neural network is demonstrated on numerical examples.},
  timestamp = {2016-07-10T06:49:39Z},
  institution = {Purdue University Purdue e-Pubs, Electrical and Computer Engineering},
  author = {Lillo, Walter E. and Hui, Stefen and Zak, Stanislaw H.},
  year = {1991},
  keywords = {Analog circuits,Constrained optimization,Minimum norm problems},
  owner = {Fardin}
}

@techreport{Mansour2015,
  title = {On the Value of Support Estimation: An Analysis of the Null Space 	Property for Weighted $\ell_1$-Minimization},
  abstract = {We study the problem of recovering sparse signals from underdetermined
	linear measurements when a potentially erroneous support estimate
	is available. Our results are twofold. First, we derive necessary
	and sufficient conditions for signal recovery from compressively
	sampled measurements using weighted `1-norm minimization. These conditions,
	which depend on the size and accuracy of the support estimate, are
	on the null space of the measurement matrix and can guarantee recovery
	even when standard `1 minimization fails. Second, we derive bounds
	on the number of Gaussian measurements for these conditions to be
	satisfied, i.e., for weighted `1 minimization to successfully recover
	all sparse signals whose support has been estimated sufficiently
	accurately. Our bounds show that weighted `1 minimization requires
	significantly fewer measurements than standard `1 minimization to
	guarantee exact recovery when the support estimate is relatively
	accurate.},
  timestamp = {2016-07-10T07:18:02Z},
  institution = {Department of Mathematics, University of California, San Diego (UCSD)},
  author = {Mansour, Hassan and Saab, Rayan},
  year = {2015},
  keywords = {adaptive recovery,compressed sensing,Gaussian widths.,null space property,weighted
	`1 minimization},
  owner = {afdidehf}
}

@techreport{Nie2010,
  title = {Review of Eigenvalues},
  timestamp = {2016-07-10T08:03:15Z},
  institution = {UCSD, Mathematics Department},
  author = {Nie, Jiawang},
  year = {2010},
  owner = {afdidehf}
}

@techreport{Sacchi2013,
  title = {Notes on linear inverse theory, minimum norm solutions, and quadratic 	regularization with MATLAB examples},
  timestamp = {2016-07-10T07:01:54Z},
  institution = {Department of Physics, Geophysics, University of Alberta},
  author = {Sacchi, M. D.},
  year = {2013},
  owner = {afdidehf}
}

@techreport{Saunders2013,
  title = {PDCO-Primal-Dual Interior Methods},
  timestamp = {2016-07-10T07:31:57Z},
  institution = {Stanford University, Management Science \& Engineering},
  author = {Saunders, Michael},
  year = {2013},
  owner = {afdidehf}
}

@book{Abbeel2012,
  edition = {1},
  series = {Lecture Notes in Computer Science 7523},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, 	Part I},
  isbn = {978-3-642-33459-7 3-642-33459-8 978-3-642-33460-3 3-642-33460-1},
  timestamp = {2016-10-24T16:41:08Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Abbeel, Pieter},
  editor = {Flach, Peter A. and Bie, Tijl De and Cristianini, Nello},
  year = {2012},
  owner = {Fardin}
}

@inproceedings{Abhari2012,
  title = {Medical image denoising using low pass filtering in sparse domain},
  doi = {10.1109/EMBC.2012.6345884},
  abstract = {In this work, we introduce a new approach for medical image denoising.
	An innovative method is proposed to extend the concept of low-pass
	filtering to the sparse representation framework. A weight matrix
	is applied to the definition of the sparse coding optimization problem
	intended to reduce coefficients corresponding to atoms with higher
	frequency contents, which dominantly represent the image noise. In
	parallel, a new overcomplete Discrete Cosine Transform (DCT) dictionary
	is constructed to include both frequency and phase information, aiming
	to remove blocking artifacts without considering patch-overlap. The
	proposed denoising approach was applied on low-dose Computed Tomography
	(CT) phantoms. The resultant observations demonstrate qualitative
	and quantitative improvements, in terms of peak signal to noise ratio
	(PSNR), in comparison to some previous approaches.},
  timestamp = {2016-07-09T20:10:02Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 2012 Annual International 	Conference of the IEEE},
  author = {Abhari, K. and Marsousi, M. and Babyn, P. and Alirezaie, J.},
  month = aug,
  year = {2012},
  keywords = {blocking artifacts removal,Computed tomography,computerised tomography,CT,DCT,diagnostic
	radiography,Dictionaries,discrete cosine transform,discrete cosine
	transforms,Humans,image denoising,Image reconstruction,low-dose computed
	tomography phantoms,low pass filtering,low-pass filters,medical image
	processing,Models,Noise reduction,peak signal to noise ratio,phantoms,PSNR,Radiographic
	Image Enhancement,Signal-To-Noise Ratio,sparse coding optimization
	problem,sparse matrices,sparse representation,Theoretical,Tomography,weight
	matrix,X-Ray Computed},
  pages = {114-117},
  owner = {afdidehf}
}

@inproceedings{Abhishek2015,
  title = {A trick to improve PRD during compressed sensing ECG reconstruction},
  doi = {10.1109/SPIN.2015.7095319},
  abstract = {In the problem of compressed sensing (CS) successful reconstruction
	can be achieved by maintaining a low mutual coherence between the
	columns in the vector space. In this work, a way to increase the
	mutual incoherence is introduced. This is achieved by replacing certain
	matrix domain of the sparse random matrix, which is used as the measurement
	matrix with null space bases. For convenience, this can be replaced
	even by identity matrices. The result shows that there is a substantial
	improvement in Peak Root mean Square deviation (PRD). Many different
	alternatives have been tried out and relative PRD were plotted. Thresholding
	is generally adapted in CS in order to reduce the PRD values. It
	was found that without using thresholding technique, it is possible
	to obtain reduction in PRD values. The time algorithmic performance
	was also analyzed and found to be better.},
  timestamp = {2016-07-08T11:33:53Z},
  booktitle = {Signal Processing and Integrated Networks (SPIN), 2015 2nd International 	Conference on},
  author = {Abhishek, S. and Veni, S. and Narayanankutty, K.A.},
  month = feb,
  year = {2015},
  keywords = {Blocked identity matrix,compressed sensing,Compressed Sensing (CS),compressed
	sensing ECG reconstruction,computational complexity,electrocardiography,electrocardiography
	(ECG),Gaussian distribution,identity matrices,matrix algebra,matrix
	domain,medical signal processing,mutual incoherence,Null space,null
	space bases,peak root mean square deviation,PRD,PRD values,Random
	variables,Sensors,signal reconstruction,sparse matrices,time algorithmic
	performance},
  pages = {174-179},
  owner = {afdidehf}
}

@techreport{Tropp2004a,
  title = {Just relax: Convex programming methods for subset selection and sparse 	approximation},
  timestamp = {2016-07-09T19:51:09Z},
  institution = {ICES Report 0404, The University of Texas at Austin},
  author = {Tropp, Joel A.},
  month = feb,
  year = {2004},
  keywords = {approximation algorithm,Basis pursuit,combinatorial optimization,convex programming,convex
	programming,covering,Highly nonlinear approximation,Lasso,orthogonal
	matching pursuit,overcomplete representation,packing,projective spaces,relaxation,sparse approximation,sparse
	approximation,subset selection},
  owner = {Fardin}
}

@techreport{Tropp2005,
  title = {Signal Recovery From Partial Information via Orthogonal Matching 	Pursuit},
  abstract = {This article demonstrates theoretically and empirically that a greedy
	algorithm called Orthogonal Matching Pursuit (OMP) can reliably recover
	a signal with m nonzero entries in dimension d given O(mln d) random
	linear measurements of that signal. This is a massive improvement
	over previous results for OMP, which require O(m2) measurements.
	The new results for OMP are comparable with recent results for another
	algorithm called Basis Pursuit (BP). The OMP algorithm is much faster
	and much easier to implement, which makes it an attractive alternative
	to BP for signal recovery problems.},
  timestamp = {2016-07-10T08:17:30Z},
  institution = {Department of Mathematics, The University of Michigan},
  author = {Tropp, Joel A. and Gilbert, Anna C.},
  month = apr,
  year = {2005},
  keywords = {Algorithms,Approximation,Basis pursuit,group testing,orthogonal matching
	pursuit,orthogonal
	matching pursuit,signal recovery,sparse approximation.},
  owner = {Fardin}
}

@techreport{Zhang2008,
  title = {Theory of compressive sensing via $\ell_1$- minimization: A Non-RIP 	analysis and extensions},
  abstract = {Compressive sensing (CS) is an emerging methodology in computational
	signal processing that has recently attracted intensive research
	activities. At present, the basic CS theory includes recoverability
	and stability: the former quanties the central fact that a sparse
	signal of length n can be exactly recovered from far fewer than n
	measurements via `1-minimization or other recovery techniques, while
	the latter species the stability of a recovery technique in the
	presence of measurement errors and inexact sparsity. So far, most
	analyses in CS rely heavily on the Restricted Isometry Property (RIP)
	for matrices. In this paper, we present an alternative, non-RIP analysis
	for CS via `1-minimization. Our purpose is three-fold: (a) to introduce
	an elementary and RIP-free treatment of the basic CS theory; (b)
	to extend the current recoverability and stability results so that
	prior knowledge can be utilized to enhance recovery via `1-minimization;
	and (c) to substantiate a property called uniform recoverability
	of `1-minimization; that is, for almost all random measurement matrices
	recoverability is asymptotically identical. With the aid of two classic
	results, the non-RIP approach enables us to quickly derive from scratch
	all basic results for the extended theory.},
  timestamp = {2016-07-11T17:02:33Z},
  institution = {Technical Report, Rice Univ.},
  author = {Zhang, Yin},
  year = {2008},
  keywords = {`1-minimization,compressive sensing,non-RIP analysis,prior information,recoverability
	and stability,uniform recoverability.},
  owner = {afdidehf}
}

@techreport{Zhang2005,
  title = {Solution-recovery in $\ell_1$-norm for non-square linear systems-deterministic 	conditions and open questions},
  abstract = {Consider an over-determined linear system AT x  b and an under-determined
	linear system By = c. Given b = AT �x+h, under what conditions
	�x will minimize the residual AT x ? b in `1-norm (i.e., khk1 =
	minx kAT x ? bk1)? On the other hand, given c = Bh, under what conditions
	h will be the minimum `1-norm solution of By = c? These two �solution-recovery�
	problems have been the focus of a number of recent works. Moreover,
	these two problems are equivalent under appropriate conditions on
	the data sets (A, b) and (B, c). In this paper, we give deterministic
	conditions for these solution- recoverary problems and raise a few
	open questuions. Some of the results in this paper are already known
	or partially known, but our derivations are different and thus may
	provide different perspectives.},
  timestamp = {2016-07-10T08:19:44Z},
  institution = {Department of Computational and Applied Mathematics, Rice University, 	Houston, Texas, 77005, U.S.A.},
  author = {Zhang, Yin},
  year = {2005},
  owner = {Fardin}
}

@article{Zdunek2016a,
  title = {Numerical Numerical Methods-Ill-posed Least Squares Problems},
  timestamp = {2016-09-30T11:04:39Z},
  urldate = {2016-04-05},
  author = {Zdunek, Rafa\l{}},
  year = {2016},
  keywords = {Ill-posed,least,problems,squares},
  annote = {PPT},
  annote = {PPT}
}

@book{Bach2011,
  title = {Convex Optimization with Sparsity-Inducing Norms},
  timestamp = {2016-07-08T12:05:08Z},
  author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
  year = {2011},
  file = {Citeseer - Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2HUBQUAM\\summary.html:text/html;Citeseer - Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P5J8JRU9\\summary.html:text/html}
}

@article{Bach2012a,
  title = {Optimization with Sparsity-Inducing Penalties},
  volume = {4},
  issn = {1935-8237},
  doi = {10.1561/2200000015},
  abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. They were first dedicated to linear variable selection but numerous extensions have now emerged such as structured sparsity or kernel selection. It turns out that many of the related estimation problems can be cast as convex optimization problems by regularizing the empirical risk with appropriate nonsmooth norms. The goal of this monograph is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties. We cover proximal methods, block-coordinate descent, reweighted l2-penalized techniques, working-set and homotopy methods, as well as non-convex formulations and extensions, and provide an extensive set of experiments to compare various algorithms from a computational point of view.},
  timestamp = {2016-07-10T07:20:27Z},
  number = {1},
  urldate = {2016-05-11},
  journal = {Found. Trends Mach. Learn.},
  author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
  month = jan,
  year = {2012},
  pages = {1–106}
}

@techreport{Skogestad2016,
  title = {Matrix theory and norms},
  timestamp = {2016-07-09T20:09:19Z},
  author = {Skogestad, Sigurd},
  year = {2016}
}

@techreport{Stensby2016,
  title = {Matrix Norms},
  timestamp = {2016-07-09T20:09:14Z},
  author = {Stensby, John L.},
  year = {2016}
}

@book{Petersen2012,
  title = {The Matrix Cookbook},
  timestamp = {2016-07-11T17:01:39Z},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year = {2012 Nov.}
}

@techreport{Vandenberghe2016,
  title = {Orthogonal matrices},
  timestamp = {2016-07-10T07:20:40Z},
  author = {Vandenberghe, L.},
  year = {2016}
}

@techreport{Simovici2016,
  title = {Norms for matrices},
  timestamp = {2016-07-10T07:00:55Z},
  author = {Simovici, Dan A.},
  year = {2016}
}

@article{Tropp2006b,
  series = {Sparse Approximations in Signal and Image ProcessingSparse Approximations in Signal and Image Processing},
  title = {Algorithms for simultaneous sparse approximation. Part II: Convex relaxation},
  volume = {86},
  issn = {0165-1684},
  shorttitle = {Algorithms for simultaneous sparse approximation. Part II},
  doi = {10.1016/j.sigpro.2005.05.031},
  abstract = {A simultaneous sparse approximation problem requests a good approximation of several input signals at once using different linear combinations of the same elementary signals. At the same time, the problem balances the error in approximation against the total number of elementary signals that participate. These elementary signals typically model coherent structures in the input signals, and they are chosen from a large, linearly dependent collection.

The first part of this paper presents theoretical and numerical results for a greedy pursuit algorithm, called simultaneous orthogonal matching pursuit.

The second part of the paper develops another algorithmic approach called convex relaxation. This method replaces the combinatorial simultaneous sparse approximation problem with a closely related convex program that can be solved efficiently with standard mathematical programming software. The paper develops conditions under which convex relaxation computes good solutions to simultaneous sparse approximation problems.},
  timestamp = {2016-07-08T10:24:23Z},
  number = {3},
  urldate = {2016-05-16},
  journal = {Signal Processing},
  author = {Tropp, Joel A.},
  month = mar,
  year = {2006},
  keywords = {Combinatorial optimization,combinatorial optimization,convex relaxation,convex relaxation,Multiple measurement vectors,multiple measurement
	vectors,Simultaneous sparse approximation,Simultaneous sparse approximation},
  pages = {589-602},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VMVR5TIF\\S0165168405002239.html:text/html}
}

@article{Milenkovic2007,
  title = {Compressed Sensing Meets Bionformatics: {{A}} New {{DNA}} Microarray Architecture},
  timestamp = {2016-07-08T11:56:05Z},
  journal = {presented at the Information Theory and Applications Workshop, San Diego, CA},
  author = {Milenkovic, O. and Baraniuk, R. and Simunic-Rosing, T.},
  year = {2007}
}

@inproceedings{Malioutov2003,
  title = {Source localization by enforcing sparsity through a Laplacian prior: an SVD-based approach},
  shorttitle = {Source localization by enforcing sparsity through a Laplacian prior},
  doi = {10.1109/SSP.2003.1289535},
  abstract = {We present a source localization method based upon a sparse representation of sensor measurements with an overcomplete basis composed of samples from the array manifold. We enforce sparsity by imposing an ℓ1-norm penalty; this can also be viewed as an estimation problem with a Laplacian prior. Explicitly enforcing the sparsity of the representation is motivated by a desire to obtain a sharp estimate of the spatial spectrum which exhibits superresolution. To summarize multiple time samples we use the singular value decomposition (SVD) of the data matrix. Our formulation leads to an optimization problem, which we solve efficiently in a second-order cone (SOC) programming framework by an interior point implementation. We demonstrate the effectiveness of the method on simulated data by plots of spatial spectra and by comparing the estimator variance to the Cramer-Rao bound (CRB). We observe that our approach has advantages over other source localization techniques including increased resolution; improved robustness to noise, limitations in data quantity, and correlation of the sources; as well as not requiring an accurate initialization.},
  timestamp = {2016-07-10T08:23:18Z},
  booktitle = {2003 IEEE Workshop on Statistical Signal Processing},
  author = {Malioutov, D. M. and Cetin, M. and Willsky, A. S.},
  month = sep,
  year = {2003},
  keywords = {Cramer-Rao bound,Cramer-Rao bound,data matrix,data matrix,Laboratories,Laboratories,Laplace equations,Laplace equations,Laplacian prior,Laplacian
	prior,mathematical programming,mathematical programming,matrix decomposition,Matrix decomposition,Multiple signal classification,Multiple signal
	classification,Noise robustness,Noise robustness,Position measurement,Position measurement,second-order cone programming framework,second-order
	cone programming framework,Sensor arrays,Sensor arrays,sensor measurements,sensor measurements,Sensors,sensors,signal
	representation,signal representation,signal resolution,Signal resolution,signal sampling,Signal sampling,Singular Value Decomposition,singular value decomposition,Spatial resolution,Spatial
	resolution,spatial spectrum,spatial spectrum},
  pages = {573-576}
}

@inproceedings{Gilbert2007,
  address = {New York, NY, USA},
  series = {STOC '07},
  title = {One Sketch for All: Fast Algorithms for Compressed Sensing},
  isbn = {978-1-59593-631-8},
  shorttitle = {One Sketch for All},
  doi = {10.1145/1250790.1250824},
  abstract = {Compressed Sensing is a new paradigm for acquiring the compressible signals that arise in many applications. These signals can be approximated using an amount of information much smaller than the nominal dimension of the signal. Traditional approaches acquire the entire signal and process it to extract the information. The new approach acquires a small number of nonadaptive linear measurements of the signal and uses sophisticated algorithms to determine its information content. Emerging technologies can compute these general linear measurements of a signal at unit cost per measurement. This paper exhibits a randomized measurement ensemble and a signal reconstruction algorithm that satisfy four requirements: 1. The measurement ensemble succeeds for all signals, with high probability over the random choices in its construction. 2. The number of measurements of the signal is optimal, except for a factor polylogarithmic in the signal length. 3. The running time of the algorithm is polynomial in the amount of information in the signal and polylogarithmic in the signal length. 4. The recovery algorithm offers the strongest possible type of error guarantee. Moreover, it is a fully polynomial approximation scheme with respect to this type of error bound. Emerging applications demand this level of performance. Yet no otheralgorithm in the literature simultaneously achieves all four of these desiderata.},
  timestamp = {2016-09-30T10:48:36Z},
  urldate = {2016-05-15},
  booktitle = {Proceedings of the Thirty-ninth Annual ACM Symposium on Theory of Computing},
  publisher = {{ACM}},
  author = {Gilbert, A. C. and Strauss, M. J. and Tropp, J. A. and Vershynin, R.},
  year = {2007},
  keywords = {approximation,Approximation,embedding,embedding,group testing,group testing,sketching,sketching,sparse approximation,sparse approximation,sublinear algorithms,sublinear
	algorithms},
  pages = {237--246}
}

@article{Gorodnitsky1995,
  title = {Neuromagnetic Source Imaging with {{FOCUSS}}: A Recursive Weighted Minimum Norm Algorithm},
  volume = {95},
  issn = {0013-4694},
  shorttitle = {Neuromagnetic Source Imaging with {{FOCUSS}}},
  abstract = {The paper describes a new algorithm for tomographic source reconstruction in neural electromagnetic inverse problems. Termed FOCUSS (FOCal Underdetermined System Solution), this algorithm combines the desired features of the two major approaches to electromagnetic inverse procedures. Like multiple current dipole modeling methods, FOCUSS produces high resolution solutions appropriate for the highly localized sources often encountered in electromagnetic imaging. Like linear estimation methods, FOCUSS allows current sources to assume arbitrary shapes and it preserves the generality and ease of application characteristic of this group of methods. It stands apart from standard signal processing techniques because, as an initialization-dependent algorithm, it accommodates the non-unique set of feasible solutions that arise from the neuroelectric source constraints. FOCUSS is based on recursive, weighted norm minimization. The consequence of the repeated weighting procedure is, in effect, to concentrate the solution in the minimal active regions that are essential for accurately reproducing the measurements. The FOCUSS algorithm is introduced and its properties are illustrated in the context of a number of simulations, first using exact measurements in 2- and 3-D problems, and then in the presence of noise and modeling errors. The results suggest that FOCUSS is a powerful algorithm with considerable utility for tomographic current estimation.},
  language = {eng},
  timestamp = {2017-04-19T14:31:47Z},
  number = {4},
  journal = {Electroencephalogr. Clin. Neurophysiol.},
  author = {Gorodnitsky, I. F. and George, J. S. and Rao, B. D.},
  month = oct,
  year = {1995},
  keywords = {algorithms,Brain Mapping,Humans,Image Processing; Computer-Assisted,Magnetoencephalography,Models; Neurological},
  pages = {231--251},
  pmid = {8529554}
}

@Article{Stojnic2008,
  author    = {Stojnic, M. and Xu, W. and Hassibi, B.},
  title     = {Compressed sensing - probabilistic analysis of a null-space characterization},
  journal   = {IEEE Int. Conf. Acoust., Speech, Signal Process.},
  year      = {2008},
  pages     = {3377--3380},
  month     = mar,
  abstract  = {It is well known that compressed sensing problems reduce to solving large under-determined systems of equations. To assure that the problem is well defined, i.e., that the solution is unique the vector of unknowns is of course assumed to be sparse. Nonetheless, even when the solution is unique, finding it in general may be computationally difficult. However, starting with the seminal work of Candes and Tao [2005], it has been shown that linear programming techniques, obtained from an l1-norm relaxation of the original non-convex problem, can provably find the unknown vector in certain instances. In particular, using a certain restricted isometry property, Candes and Tao [2005] shows that for measurement matrices chosen from a random Gaussian ensemble, l1 optimization can find the correct solution with overwhelming probability even when the number of non-zero entries of the unknown vector is proportional to the number of measurements (and the total number of unknowns). The subsequent paper [Donoho and Tanner, 2005] uses results on neighborly polytopes from [Vershik and Sporyshev, 1992] to give a "sharp" bound on what this proportionality should be in the Gaussian case. In the current paper, we observe that what matters is not so much the distribution from which the entries of the measurement matrix A are drawn, but rather the statistics of the null-space of A. Using this observation, we provide an alternative proof of the main result of Candes and Tao [2005] by analyzing matrices whose null-space is isotropic (of which i.i.d. Gaussian ensembles are a special case).},
  doi       = {10.1109/ICASSP.2008.4518375},
  keywords  = {compressed sensing,compressed sensing,Current measurement,Current measurement,data compression,data compression,Equations,Equations,Gaussian processes,Gaussian processes,l1-norm relaxation,l1-norm relaxation,l1-optimization,l1-optimization,linear programming,linear programming,linear programming techniques,linear programming techniques,measurement matrix,measurement matrix,neighborly polytopes,neighborly polytopes,Noise generators,Noise generators,nonconvex problem,nonconvex problem,null-space characterization,null-space characterization,optimisation,optimisation,Optimization,Optimization,Particle measurements,Particle measurements,probabilistic analysis,probabilistic analysis,probability,probability,random Gaussian ensemble,random Gaussian ensemble,Statistical analysis,statistical analysis,Statistical distributions,Statistical distributions,Sufficient conditions,Sufficient conditions,Vectors,Vectors},
  timestamp = {2017-04-20T09:23:52Z},
}

@article{Tropp2006a,
  series = {Sparse Approximations in Signal and Image ProcessingSparse Approximations in Signal and Image Processing},
  title = {Algorithms for simultaneous sparse approximation. Part I: Greedy pursuit},
  volume = {86},
  issn = {0165-1684},
  shorttitle = {Algorithms for simultaneous sparse approximation. Part I},
  doi = {10.1016/j.sigpro.2005.05.030},
  abstract = {A simultaneous sparse approximation problem requests a good approximation of several input signals at once using different linear combinations of the same elementary signals. At the same time, the problem balances the error in approximation against the total number of elementary signals that participate. These elementary signals typically model coherent structures in the input signals, and they are chosen from a large, linearly dependent collection.

The first part of this paper proposes a greedy pursuit algorithm, called simultaneous orthogonal matching pursuit (S-OMP), for simultaneous sparse approximation. Then it presents some numerical experiments that demonstrate how a sparse model for the input signals can be identified more reliably given several input signals. Afterward, the paper proves that the S-OMP algorithm can compute provably good solutions to several simultaneous sparse approximation problems.

The second part of the paper develops another algorithmic approach called convex relaxation, and it provides theoretical results on the performance of convex relaxation for simultaneous sparse approximation.},
  timestamp = {2016-07-08T10:22:03Z},
  number = {3},
  urldate = {2016-05-16},
  journal = {Signal Processing},
  author = {Tropp, Joel A. and Gilbert, Anna C. and Strauss, Martin J.},
  month = mar,
  year = {2006},
  keywords = {Greedy algorithms,Greedy algorithms,multiple measurement vectors,Multiple measurement vectors,orthogonal matching
	pursuit,orthogonal matching pursuit,Simultaneous sparse approximation,Simultaneous sparse approximation,subset selection,subset selection},
  pages = {572-588},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IFSIRBSD\\S0165168405002227.html:text/html}
}

@article{Erickson2005,
  title = {Empirical Bayes Estimation of a Sparse Vector of Gene Expression Changes},
  volume = {4},
  issn = {1544-6115},
  doi = {10.2202/1544-6115.1132},
  abstract = {Gene microarray technology is often used to compare the expression of thousand of genes in two different cell lines. Typically, one does not expect measurable changes in transcription amounts for a large number of genes; furthermore, the noise level of array experiments is rather high in relation to the available number of replicates. For the purpose of statistical analysis, inference on the "population'' difference in expression for genes across the two cell lines is often cast in the framework of hypothesis testing, with the null hypothesis being no change in expression. Given that thousands of genes are investigated at the same time, this requires some multiple comparison correction procedure to be in place. We argue that hypothesis testing, with its emphasis on type I error and family analogues, may not address the exploratory nature of most microarray experiments. We instead propose viewing the problem as one of estimation of a vector known to have a large number of zero components. In a Bayesian framework, we describe the prior knowledge on expression changes using mixture priors that incorporate a mass at zero, and we choose a loss function that favors the selection of sparse solutions. We consider two different models applicable to the microarray problem, depending on the nature of replicates available, and show how to explore the posterior distributions of the parameters using MCMC. Simulations show an interesting connection between this Bayesian estimation framework and false discovery rate (FDR) control. Finally, two empirical examples illustrate the practical advantages of this Bayesian estimation paradigm.},
  language = {eng},
  timestamp = {2016-09-29T14:51:01Z},
  number = {1},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  author = {Erickson, Stephen and Sabatti, Chiara},
  year = {2005},
  pages = {22},
  pmid = {16646840}
}

@article{Tropp2005b,
  title = {Simultaneous sparsity},
  timestamp = {2016-07-10T08:18:43Z},
  author = {Tropp, Joel A. and Gilbert, Anna C. and Strauss, Martin J.},
  year = {2005},
  annote = {PPT},
  annote = {PPT}
}

@article{Donoho2005b,
  title = {High-Dimensional Centrally Symmetric Polytopes with Neighborliness Proportional to Dimension},
  volume = {35},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-005-1220-0},
  language = {en},
  timestamp = {2016-10-07T14:42:32Z},
  number = {4},
  urldate = {2016-05-15},
  journal = {Discrete and Computational Geometry},
  author = {Donoho, David L.},
  month = dec,
  year = {2005},
  keywords = {Combinatorics,Combinatorics,Computational Mathematics and Numerical Analysis,Computational Mathematics and Numerical Analysis},
  pages = {617--652}
}

@inproceedings{Donoho2006d,
  title = {Thresholds for the Recovery of Sparse Solutions via L1 Minimization},
  doi = {10.1109/CISS.2006.286462},
  abstract = {The ubiquitous least squares method for systems of linear equations returns solutions which typically have all non-zero entries. However, solutions with the least number of non-zeros allow for greater insight. An exhaustive search for the sparsest solution is intractable, NP-hard. Recently, a great deal of research showed that linear programming can find the sparsest solution for certain 'typical' systems of equations, provided the solution is sufficiently sparse. In this note we report recent progress determining conditions under which the sparsest solution to large systems is available by linear programming. Our work shows that there are sharp thresholds on sparsity below which these methods will succeed and above which they fail; it evaluates those thresholds precisely and hints at several interesting applications.},
  timestamp = {2016-09-30T10:46:41Z},
  booktitle = {2006 40th Annual Conference on Information Sciences and Systems},
  author = {Donoho, D. L. and Tanner, J.},
  month = mar,
  year = {2006},
  keywords = {Cities and towns,Cities and towns,compressed sensing,compressed sensing,Equations,Equations,L1 minimization,l1 minimization,least squares approximations,Least
	squares approximations,Least squares methods,Least squares methods,linear programming,linear programming,Mathematics,Mathematics,Minimization
	methods,Minimization methods,NP-hard,NP-hard,Sampling methods,Sampling methods,sparse matrices,Sparse matrices,sparse solution,sparse solution,Statistics,Statistics,ubiquitous least squares method,ubiquitous
	least squares method},
  pages = {202--206}
}

@inproceedings{Cormode2006,
  title = {Combinatorial Algorithms for Compressed Sensing},
  doi = {10.1109/CISS.2006.286461},
  abstract = {In sparse approximation theory, the fundamental problem is to reconstruct a signal AisinRn from linear measurements (A,psii) with respect to a dictionary of psii's. Recently, there is focus on the novel direction of Compressed Sensing where the reconstruction can be done with very few-O(klogn)-linear measurements over a modified dictionary if the signal is compressible, that is, its information is concentrated in k coefficients with the original dictionary. In particular, the results prove that there exists a single O(klogn)timesn measurement matrix such that any such signal can be reconstructed from these measurements, with error at most O(1) times the worst case error for the class of such signals. Compressed sensing has generated tremendous excitement both because of the sophisticated underlying mathematics and because of its potential applications. In this paper, we address outstanding open problems in Compressed Sensing. Our main result is an explicit construction of a non-adaptive measurement matrix and the corresponding reconstruction algorithm so that with a number of measurements polynomial in k, logn, 1/epsiv, we can reconstruct compressible signals. This is the first known polynomial time explicit construction of any such measurement matrix. In addition, our result improves the error guarantee from O(1) to 1+epsiv and improves the reconstruction time from poly(n) to poly (klogn). Our second result is a randomized construction of O(kpolylog(n)) measurements that work for each signal with high probability and gives per-instance approximation guarantees rather than over the class of all signals. Previous work on compressed sensing does not provide such per-instance approximation guarantees; our result improves the best known number of measurements known from prior work in other areas including learning theory, streaming algorithms and complexity theory for this case. Our approach is combinatorial. In particular, we use two p- arallel sets of group tests, one to filter and the other to certify and estimate; the resulting algorithms are quite simple to implement.},
  timestamp = {2016-09-30T10:47:49Z},
  booktitle = {2006 40th Annual Conference on Information Sciences and Systems},
  author = {Cormode, G. and Muthukrishnan, S.},
  month = mar,
  year = {2006},
  keywords = {Approximation methods,Approximation methods,approximation theory,approximation theory,Area measurement,Area measurement,combinatorial algorithm,combinatorial
	algorithm,compressed sensing,compressed sensing,computational complexity,computational complexity,Dictionaries,Dictionaries,Mathematics,Mathematics,non-adaptive measurement matrix,non-adaptive
	measurement matrix,Particle measurements,Particle measurements,Polynomials,Polynomials,Pressure measurement,Pressure measurement,Reconstruction algorithms,Reconstruction
	algorithms,signal reconstruction,signal reconstruction,sparse approximation theory,sparse approximation theory,sparse
	matrices,Sparse matrices,Time measurement,Time measurement},
  pages = {198--201}
}

@inproceedings{Xu2007a,
  title = {Efficient Compressive Sensing with Deterministic Guarantees Using Expander Graphs},
  doi = {10.1109/ITW.2007.4313110},
  abstract = {Compressive sensing is an emerging technology which can recover a sparse signal vector of dimension n via a much smaller number of measurements than n. However, the existing compressive sensing methods may still suffer from relatively high recovery complexity, such as O(n3), or can only work efficiently when the signal is super sparse, sometimes without deterministic performance guarantees. In this paper, we propose a compressive sensing scheme with deterministic performance guarantees using expander-graphs-based measurement matrices and show that the signal recovery can be achieved with complexity O(n) even if the number of nonzero elements k grows linearly with n. We also investigate compressive sensing for approximately sparse signals using this new method. Moreover, explicit constructions of the considered expander graphs exist. Simulation results are given to show the performance and complexity of the new method.},
  timestamp = {2016-09-30T10:49:57Z},
  booktitle = {IEEE Information Theory Workshop, 2007. ITW '07},
  author = {Xu, W. and Hassibi, B.},
  month = sep,
  year = {2007},
  keywords = {compressive sensing scheme with,compressive sensing scheme with,computational complexity,computational complexity,Digital cameras,Digital cameras,expander graphs,expander
	graphs,expander-graphs-based measurement matrices,expander-graphs-based measurement matrices,Graph theory,graph theory,Lakes,Lakes,Matching
	pursuit algorithms,Matching pursuit algorithms,Parity check codes,parity check codes,Sampling methods,Sampling methods,signal processing,signal processing,Signal processing algorithms,Signal
	processing algorithms,signal sampling,Signal sampling,sparse matrices,Sparse matrices,sparse signal vector complexity,sparse signal
	vector complexity,Testing,Testing},
  pages = {414--419}
}

@Article{Donoho2003a,
  author    = {Donoho, D.L. and Elad, M.},
  title     = {Maximal Sparsity Representation via $l_1$ Minimization},
  journal   = {Proceedings of the National Academy of Sciences of the United States of America},
  year      = {2003},
  volume    = {100},
  number    = {50},
  pages     = {2197--2202},
  annote    = {read},
  timestamp = {2016-10-05T11:41:30Z},
}

@inproceedings{Adalbjornsson2014,
  title = {High resolution sparse estimation of exponentially decaying two-dimensional 	signals},
  abstract = {In this work, we consider the problem of high-resolution estimation
	of the parameters detailing a two-dimensional (2-D) signal consisting
	of an unknown number of exponentially decaying sinusoidal components.
	Interpreting the estimation problem as a block (or group) sparse
	representation problem allows the decoupling of the 2-D data structure
	into a sum of outer-products of 1-D damped sinusoidal signals with
	unknown damping and frequency. The resulting non-zero blocks will
	represent each of the 1-D damped sinusoids, which may then be used
	as non-parametric estimates of the corresponding 1-D signals; this
	implies that the sought 2-D modes may be estimated using a sequence
	of 1-D optimization problems. The resulting sparse representation
	problem is solved using an iterative ADMM-based algorithm, after
	which the damping and frequency parameter can be estimated by a sequence
	of simple 1-D optimization problems.},
  timestamp = {2016-07-08T12:44:01Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd 	European},
  author = {Adalbj{\"o}rnsson, S.I. and Sward, J. and Jakobsson, A.},
  month = sep,
  year = {2014},
  keywords = {1D damped sinusoidal signals,1D
	damped sinusoidal signals,1D optimization problems,2D data structure
	decoupling,2D signal,ADMM,block sparse representation problem,Damping,Dictionaries,Estimation,exponentially
	decaying two-dimensional signals,frequency estimation,Frequency
	estimation,frequency parameter,high resolution
	sparse estimation,high
	resolution sparse estimation,iterative ADMM-based algorithm,Minimization,nonparametric estimates,nonparametric
	estimates,nonzero blocks,nonzero
	blocks,Nuclear magnetic resonance,optimisation,outer-products,parameter estimation,parameter
	estimation,signal representation,signal resolution,Signal to noise
	ratio,Sparse reconstruction,Sparse signal modeling,Sparse signal
	modeling,Spectral analysis,unknown damping,unknown
	damping},
  pages = {491-495},
  owner = {afdidehf}
}

@article{Adamo2015,
  title = {ECG compression retaining the best natural basis k-coefficients via 	sparse decomposition},
  volume = {15},
  issn = {1746-8094},
  doi = {http://dx.doi.org/10.1016/j.bspc.2014.09.002},
  abstract = {Abstract A novel and efficient signal compression algorithm aimed
	at finding the sparsest representation of electrocardiogram (ECG)
	signals is presented and analyzed. The idea behind the method relies
	on basis elements drawn from the initial transitory of a signal itself,
	and the sparsity promotion process applied to its subsequent blocks
	grabbed by a sliding window. The saved coefficients rescaled in a
	convenient range, quantized and compressed by a lossless entropy-based
	algorithm. Experiments on signals extracted from the MIT-BIH Arrhythmia
	database show that the method achieves in most of the cases very
	high performance.},
  timestamp = {2016-07-08T12:16:30Z},
  journal = {Biomedical Signal Processing and Control},
  author = {Adamo, Alessandro and Grossi, Giuliano and Lanzarotti, Raffaella and Lin, Jianyi},
  year = {2015},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(ECG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(ECG\\\\\\\\\\\\\\\\,\\\\\\\\(ECG\\\\\\\\,\\\\(ECG\\\\,\\(ECG\\,compression,Fixed-point,iteration,Orthogonal,projections,recovery,scheme,Sparsity},
  pages = {11 - 17},
  owner = {afdidehf}
}

@article{Alberaxxxx,
  title = {S{\'e}paration de sources en ing{\'e}nierie biom{\'e}dicale},
  timestamp = {2016-07-10T08:10:52Z},
  author = {Albera, Laurent},
  year = {xxxx},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@article{Alberaxxxxa,
  title = {Biom{\'e}dical : localisation intrac{\'e}r{\'e}brale},
  timestamp = {2016-07-08T11:38:44Z},
  author = {Albera, Laurent},
  year = {xxxx},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@article{Alberti2015,
  title = {Disjoint sparsity for signal separation and applications to hybrid 	inverse problems in medical imaging},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2015.08.013},
  abstract = {Abstract The main focus of this work is the reconstruction of the
	signals f and g i , i = 1 , ... , N , from the knowledge of their
	sums h i = f + g i , under the assumption that f and the g i 's can
	be sparsely represented with respect to two different dictionaries
	A f and A g . This generalizes the well-known “morphological component
	analysis�? to a multi-measurement setting. The main result of the
	paper states that f and the g i 's can be uniquely and stably reconstructed
	by finding sparse representations of h i for every i with respect
	to the concatenated dictionary [ A f , A g ] , provided that enough
	incoherent measurements g i are available. The incoherence is measured
	in terms of their mutual disjoint sparsity. This method finds applications
	in the reconstruction procedures of several hybrid imaging inverse
	problems, where internal data are measured. These measurements usually
	consist of the main unknown multiplied by other unknown quantities,
	and so the disjoint sparsity approach can be directly applied. As
	an example, we show how to apply the method to the reconstruction
	in quantitative photoacoustic tomography, also in the case when the
	Grüneisen parameter, the optical absorption and the diffusion coefficient
	are all unknown.},
  timestamp = {2016-07-08T12:13:39Z},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Alberti, Giovanni S. and Ammari, Habib},
  year = {2015},
  keywords = {analysis,component,Disjoint,Hybrid,imaging,Inverse,Morphological,photoacoustic,problems,Quantitative,representations,separation,Signal,Sparse,Sparsity,Tomography},
  pages = {-},
  owner = {afdidehf}
}

@article{Asaei2015,
  title = {Computational methods for underdetermined convolutive speech localization 	and separation via model-based sparse component analysis},
  issn = {0167-6393},
  doi = {http://dx.doi.org/10.1016/j.specom.2015.07.002},
  abstract = {Abstract In this paper, the problem of speech source localization
	and separation from recordings of convolutive underdetermined mixtures
	is addressed. This problem is cast as recovering the spatio-spectral
	speech information embedded in a microphone array compressed measurements
	of the acoustic field. A model-based sparse component analysis framework
	is formulated for sparse reconstruction of the speech spectra in
	a reverberant acoustic resulting in joint localization and separation
	of the individual sources. We compare and contrast the algorithmic
	approaches to model-based sparse recovery exploiting spatial sparsity
	as well as spectral structures underlying spectrographic representation
	of speech signals. In this context, we explore identification of
	the sparsity structures at the auditory and acoustic representation
	spaces. The audiory structures are formulated upon the principles
	of structural grouping based on proximity, autoregressive correlation
	and harmonicity of the spectral coefficients and they are incoporated
	for sparse reconstruction. The acoustic structures are formulated
	upon the image model of multipath propagation and they are exploited
	to characterize the compressive measurement matrix associated with
	microphone array recordings. Three approaches to sparse recovery
	relying on combinatorial optimization, convex relaxation and sparse
	Bayesian learning are studied and evaluated on thorough experiments.
	The sparse Bayesian learning method is shown to yield better perception
	quality while the interference suppression is also achieved using
	the combinatorial approach with the advantage of offering the most
	efficient computational cost. Furthermore, it is demonstrated that
	an average autoregressive model can be learned for speech localization
	while exploiting the proximity structure in the form of block sparse
	coefficients enables accurate localization and high quality speech
	separation. Throughout the extensive empirical evaluation, we confirm
	that a large and random placement of the microphones enables significant
	improvement in source localization and separation performance.},
  timestamp = {2016-07-08T12:01:42Z},
  journal = {Speech Communication},
  author = {Asaei, Afsaneh and Bourlard, Hervé and Taghizadeh, Mohammad J. and Cevher, Volkan},
  year = {2015},
  keywords = {Computational auditory scene analysis,Computational
	auditory scene analysis,Reverberation,Source localization and separation,Source localization
	and separation,Sparse component analysis,Structured sparse representation Model-based sparse recovery,Structured sparse representation
	Model-based sparse recovery},
  pages = {-},
  owner = {afdidehf}
}

@article{Badier2010,
  title = {ElectroEnc{\'e}phalographie et MagnetoEncephaloGraphie},
  timestamp = {2016-07-08T12:20:21Z},
  author = {Badier, Jean-Michel},
  year = {2010},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Bagshaw2006,
  title = {Correspondence between EEG-fMRI and EEG dipole localisation of interictal 	discharges in focal epilepsy},
  volume = {30},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.09.033},
  abstract = {EEG-fMRI and \{EEG\} dipole source localisation are two non-invasive
	imaging methods that can be applied to the study of the haemodynamic
	and electrical consequences of epileptic discharges. Using them in
	combination has the potential to allow imaging with the spatial resolution
	of fMRI and the temporal resolution of EEG. However, although considerable
	data are available concerning their concordance in studies involving
	event-related potentials (ERPs), less is known about how well they
	agree in epilepsy. To this end, 17 patients were selected from a
	database of 57 who had undergone an EEG-fMRI scanning session followed
	by a separate \{EEG\} session outside of the scanner. Spatiotemporal
	dipole modelling was compared with the peak and closest EEG-fMRI
	activations and deactivations. On average, the dipoles were 58.5
	mm from the voxel with the highest positive t value and 32.5 mm from
	the nearest activated voxel. For deactivations, the corresponding
	values were 60.8 and 34.0 mm. These values are considerably higher
	than is generally observed with ERPs, probably as a result of the
	relatively widespread field, which can lead to artificially deep
	dipoles, and the occurrence of EEG-fMRI responses remote from the
	presumed focus of the epileptic activity. The results suggest that
	\{EEG\} and \{MEG\} inverse solutions for equivalent current dipole
	approaches should not be strongly constrained by EEG-fMRI results
	in epilepsy, and that the use of distributed source modelling will
	be a more appropriate way of combining EEG-fMRI results with source
	localisation techniques.},
  timestamp = {2016-07-08T12:05:52Z},
  number = {2},
  journal = {NeuroImage},
  author = {Bagshaw, Andrew P. and Kobayashi, Eliane and Dubeau, François and Pike, G. Bruce and Gotman, Jean},
  year = {2006},
  pages = {417 - 425},
  owner = {afdidehf}
}

@article{Bai2007,
  title = {Evaluation of cortical current density imaging methods using intracranial 	electrocorticograms and functional MRI},
  volume = {35},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2006.12.026},
  abstract = {Objective: \{EEG\} source imaging provides important information regarding
	the underlying neural activity from noninvasive electrophysiological
	measurements. The aim of the present study was to evaluate source
	reconstruction techniques by means of the intracranial electrocorticograms
	(ECoGs) and functional MRI. Methods: Five source imaging algorithms,
	including the minimum norm least square (MNLS), \{LORETA\} with Lp-norm
	(p equal to 1, 1.5 and 2), sLORETA, the minimum Lp-norm (p equal
	to 1 and 1.5; when p = 2, the \{MNLS\} method is mathematically equivalent
	to the minimum Lp-norm) and L1-norm (the linear programming) methods,
	were evaluated in a group of 10 human subjects, in a paradigm with
	somatosensory stimulation. Cortical current density (CCD) distributions
	were estimated from the scalp somatosensory evoked potentials (SEPs),
	at approximately 30 ms following electrical stimulation of median
	nerve at the wrist. Realistic geometry boundary element head models
	were constructed from the \{MRIs\} of each subject and used in the
	\{CCD\} analysis. Functional \{MRI\} results obtained from a motor
	task and sensory stimulation in all subjects were used to identify
	the central sulcus, motor and sensory areas. In three patients undergoing
	neurosurgical evaluation, \{ECoGs\} were recorded in response to
	the somatosensory stimulation, and were used to help determine the
	central sulcus and the sensory cortex. Results: The \{CCD\} distributions
	estimated by the Lp-norm and LORETA-Lp methods were smoother when
	the p values were high. The \{LORETA\} based on the L1-norm performed
	better than the LORETA-L2 method for imaging well localized sources
	such as the \{P30\} component of the SEP. The mean and standard deviation
	of the distance between the location of maximum \{CCD\} value and
	the central sulcus, estimated by the minimum Lp-norm (with p equal
	to 1), L1-norm (the Linear programming) and LORETA-Lp (with p equal
	to 1) methods, were 4, 7, 7 mm and 3, 4, 2 mm, respectively (after
	converting into Talairach coordinates). The mean and standard deviation
	of the aforementioned distance, estimated by the MNLS, \{LORETA\}
	with Lp-norm (p equal to 1.5 and 2.0), sLORETA and the minimum Lp-norm
	(p equal to 1.5) methods, were over 11 mm and 6 mm, respectively.
	Conclusions: The present experimental study suggests that L1-norm-based
	algorithms provide better performance than \{L2\} and L1.5-norm-based
	algorithms, in the context of \{CCD\} imaging of well localized sources
	induced by somatosensory electrical stimulation of median nerve at
	the wrist.},
  timestamp = {2016-07-08T12:26:11Z},
  number = {2},
  journal = {NeuroImage},
  author = {Bai, Xiaoxiao and Towle, Vernon L. and He, Eric J. and He, Bin},
  year = {2007},
  keywords = {Brain,imaging},
  pages = {598 - 608},
  owner = {afdidehf}
}

@article{Bajway2014,
  title = {Average Case Analysis of High-Dimensional Block-Sparse Recovery and 	Regression for Arbitrary Designs},
  abstract = {This paper studies conditions for highdimensional inference when the
	set of observations is given by a linear combination of a small number
	of groups of columns of a design matrix, termed the \block-sparse"
	case. In this regard, it rst species conditions on the design matrix
	under which most of its block submatrices are well conditioned. It
	then leverages this result for average-case analysis of high-dimensional
	block-sparse recovery and regression. In contrast to earlier works:
	(i) this paper provides conditions on arbitrary designs that can
	be explicitly computed in polynomial time, (ii) the provided conditions
	translate into near-optimal scaling of the number of observations
	with the number of active blocks of the design matrix, and (iii)
	the conditions suggest that the spectral norm, rather than the column/block
	coherences, of the design matrix fundamentally limits the performance
	of computational methods in high-dimensional settings.},
  timestamp = {2016-09-30T11:23:06Z},
  journal = {Proceedings of the 17th International Con- ference on Artificial Intelligence 	and Statistics (AISTATS)},
  author = {Bajway, Waheed U. and Duarte, Marco F. and Calderbank, Robert},
  year = {2014},
  owner = {Fardin}
}

@article{Jr2008,
  title = {Lexical influences on speech perception: A Granger causality analysis 	of MEG and EEG source estimates},
  volume = {43},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2008.07.027},
  abstract = {Behavioral and functional imaging studies have demonstrated that lexical
	knowledge influences the categorization of perceptually ambiguous
	speech sounds. However, methodological and inferential constraints
	have so far been unable to resolve the question of whether this interaction
	takes the form of direct top–down influences on perceptual processing,
	or feedforward convergence during a decision process. We examined
	top–down lexical influences on the categorization of segments in
	a /s/-/∫/ continuum presented in different lexical contexts to
	produce a robust Ganong effect. Using integrated MEG/EEG and \{MRI\}
	data we found that, within a network identified by 40 Hz gamma phase
	locking, activation in the supramarginal gyrus associated with wordform
	representation influences phonetic processing in the posterior superior
	temporal gyrus during a period of time associated with lexical processing.
	This result provides direct evidence that lexical processes influence
	lower level phonetic perception, and demonstrates the potential value
	of combining Granger causality analyses and high spatiotemporal resolution
	multimodal imaging data to explore the functional architecture of
	cognition.},
  timestamp = {2016-07-09T19:57:11Z},
  number = {3},
  journal = {NeuroImage},
  author = {Jr, David W. Gow and Segawa, Jennifer A. and Ahlfors, Seppo P. and Lin, Fa-Hsuan},
  year = {2008},
  pages = {614 - 623},
  owner = {afdidehf}
}

@article{Bechler2012,
  title = {Existence of the best n-term approximants for structured dictionaries},
  volume = {99},
  issn = {0003-889X, 1420-8938},
  doi = {10.1007/s00013-012-0406-y},
  abstract = {This paper investigates the existence of elements of the best n-term
	approximation in infinite dimensional Hilbert spaces. The notion
	of uniform linear independence (ULI) for a dictionary is introduced.
	It is shown that if the dictionary used for approximation satisfies
	the Bessel inequality and has the ULI property, then for every element
	of the Hilbert space there exists an element of the best n-term approximation.
	It is also shown that if a dictionary does not satisfy the ULI property,
	then there exists an arbitrarily small compact perturbation of this
	dictionary for which the elements of the best n-term approximation
	need not exist. The obtained results are applied to frames.},
  language = {en},
  timestamp = {2016-07-08T12:26:58Z},
  number = {1},
  urldate = {2016-04-05},
  journal = {Archiv der Mathematik},
  author = {Bechler, Pawe\l{}},
  month = jul,
  year = {2012},
  keywords = {41A65,41A65,Bessel inequality,Bessel inequality,Best approximations,Best approximations,frames,Frames,general,Mathematics,Mathematics; general,nonlinear approximation,nonlinear
	approximation,Nonlinear approximation,Primary 41A46,Primary 41A46,Secondary 41A50,Secondary 41A50},
  pages = {61-70},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\D8TRRTS2\\s00013-012-0406-y.html:text/html}
}

@inproceedings{Becker2014,
  title = {Fast, variation-based methods for the analysis of extended brain 	sources},
  abstract = {Identifying the location and spatial extent of several highly correlated
	and simultaneously active brain sources from electroencephalographic
	(EEG) recordings and extracting the corresponding brain signals is
	a challenging problem. In a recent comparison of source imaging techniques,
	the VB-SCCD algorithm, which exploits the sparsity of the variational
	map of the sources, proved to be a promising approach. In this paper,
	we propose several ways to improve this method. In order to adjust
	the size of the estimated sources, we add a regularization term that
	imposes sparsity in the original source domain. Furthermore, we demonstrate
	the application of ADMM, which permits to efficiently solve the optimization
	problem. Finally, we also consider the exploitation of the temporal
	structure of the data by employing L1;2-norm regularization. The
	performance of the resulting algorithm, called L1;2-SVB-SCCD, is
	evaluated based on realistic simulations in comparison to VB-SCCD
	and several state-of-the-art techniques for extended source localization.},
  timestamp = {2016-07-08T12:29:56Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd 	European},
  author = {Becker, H. and Albera, L. and Comon, P. and Gribonval, R. and Merlet, I.},
  month = sep,
  year = {2014},
  keywords = {2-norm regularization,2-SVB-SCCD,ADMM,Brain modeling,brain signals,brain
	signals,correlated active brain sources,correlated
	active brain sources,Correlation,EEG,EEG recordings,EEG
	recordings,electroencephalographic recordings,electroencephalographic
	recordings,electroencephalography,extended brain source analysis,extended
	brain source analysis,extended
	source localization,Image reconstruction,Image
	reconstruction,imaging,L1,location ldentification,medical image processing,medical
	image processing,optimisation,Optimization,optimization problem,Robustness,simultaneously
	active brain sources,source domain,source imaging,Sparsity,spatial extent,spatial
	extent,temporal structure,temporal
	structure,variation-based methods,VB-SCCD algorithm},
  pages = {41-45},
  owner = {Fardin}
}

@article{Belilovsky2015,
  title = {Predictive sparse modeling of fMRI data for improved classification, 	regression, and visualization using the k-support norm},
  issn = {0895-6111},
  doi = {http://dx.doi.org/10.1016/j.compmedimag.2015.03.007},
  abstract = {Abstract We explore various sparse regularization techniques for analyzing
	fMRI data, such as the l1 norm (often called \{LASSO\} in the context
	of a squared loss function), elastic net, and the recently introduced
	k-support norm. Employing sparsity regularization allows us to handle
	the curse of dimensionality, a problem commonly found in fMRI analysis.
	In this work we consider sparse regularization in both the regression
	and classification settings. We perform experiments on fMRI scans
	from cocaine-addicted as well as healthy control subjects. We show
	that in many cases, use of the k-support norm leads to better predictive
	performance, solution stability, and interpretability as compared
	to other standard approaches. We additionally analyze the advantages
	of using the absolute loss function versus the standard squared loss
	which leads to significantly better predictive performance for the
	regularization methods tested in almost all cases. Our results support
	the use of the k-support norm for fMRI analysis and on the clinical
	side, the generalizability of the I-RISA model of cocaine addiction.},
  timestamp = {2016-07-10T07:38:04Z},
  journal = {Computerized Medical Imaging and Graphics},
  author = {Belilovsky, Eugene and Gkirtzou, Katerina and Misyrlis, Michail and Konova, Anna B. and Honorio, Jean and Alia-Klein, Nelly and Z. Goldstein, Rita and Samaras, Dimitris and Blaschko, Matthew B.},
  year = {2015},
  keywords = {addiction,Cocaine,fMRI,k-Support,norm,regularization,Sparsity},
  pages = {-},
  owner = {afdidehf}
}

@inproceedings{Berg1999,
  title = {A survey of mixed transform techniques for speech and image coding},
  volume = {4},
  doi = {10.1109/ISCAS.1999.779953},
  abstract = {The goal of transform based coding is to build a representation of
	a signal using the smallest number of weighted basis functions possible,
	while maintaining the ability to reconstruct the signal with adequate
	fidelity. Mixed transform techniques, which employ subsets of non-orthogonal
	basis functions chosen from two or more transform domains, have been
	shown to consistently yield more efficient signal representations
	than those based on one transform. This paper provides a survey of
	mixed transform techniques, also known as multitransforms or mixed
	basis representations, which have been developed for speech and image
	coding},
  timestamp = {2016-09-29T16:28:53Z},
  booktitle = {Circuits and Systems, 1999. ISCAS '99. Proceedings of the 1999 IEEE 	International Symposium on},
  author = {Berg, A.P. and Mikhael, W.B.},
  month = jul,
  year = {1999},
  keywords = {Baseband,Compaction,data compression,Dictionaries,discrete cosine
	transforms,Discrete transforms,Fourier transforms,image coding,Image
	reconstruction,mixed basis representations,mixed transform techniques,multitransforms,nonorthogonal
	basis functions,Prototypes,signal representation,signal representations,speech coding,Speech
	coding,transform based coding,Transform coding,transforms,weighted
	basis functions},
  pages = {106--109},
  owner = {Fardin}
}

@article{Birot2011,
  title = {Localization of extended brain sources from EEG/MEG: The ExSo-MUSIC 	approach},
  volume = {56},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2011.01.054},
  abstract = {We propose a new MUSIC-like method, called 2q-ExSo-MUSIC (q&#xa0;≥&#xa0;1).
	This method is an extension of the 2q-MUSIC (q&#xa0;≥&#xa0;1) approach
	for solving the EEG/MEG inverse problem, when spatially-extended
	neocortical sources (“ExSo�?) are considered. It introduces a
	novel ExSo-MUSIC principle. The novelty is two-fold: i) the parameterization
	of the spatial source distribution that leads to an appropriate metric
	in the context of distributed brain sources and ii) the introduction
	of an original, efficient and low-cost way of optimizing this metric.
	In 2q-ExSo-MUSIC, the possible use of higher order statistics (q&#xa0;≥&#xa0;2)
	offers a better robustness with respect to Gaussian noise of unknown
	spatial coherence and modeling errors. As a result we reduced the
	penalizing effects of both the background cerebral activity that
	can be seen as a Gaussian and spatially correlated noise, and the
	modeling errors induced by the non-exact resolution of the forward
	problem. Computer results on simulated \{EEG\} signals obtained with
	physiologically-relevant models of both the sources and the volume
	conductor show a highly increased performance of our 2q-ExSo-MUSIC
	method as compared to the classical 2q-MUSIC algorithms.},
  timestamp = {2016-07-09T19:57:53Z},
  number = {1},
  journal = {NeuroImage},
  author = {Birot, Gwenael and Albera, Laurent and Wendling, Fabrice and Merlet, Isabelle},
  year = {2011},
  keywords = {localization,Source},
  pages = {102 - 113},
  owner = {Fardin}
}

@article{Bonnefoy2014,
  title = {Dynamic Screening: Accelerating First-Order Algorithms for the Lasso 	and Group-Lasso},
  abstract = {Recent computational strategies based on screening tests have been
	proposed to accelerate algorithms addressing penalized sparse regression
	problems such as the Lasso. Such approaches build upon the idea that
	it is worth dedicating some small computational effort to locate
	inactive atoms and remove them from the dictionary in a preprocessing
	stage so that the regression algorithm working with a smaller dictionary
	will then converge faster to the solution of the initial problem.
	We believe that there is an even more efficient way to screen the
	dictionary and obtain a greater acceleration: inside each iteration
	of the regression algorithm, one may take advantage of the algorithm
	computations to obtain a new screening test for free with increasing
	screening effects along the iterations. The dictionary is henceforth
	dynamically screened instead of being screened statically, once and
	for all, before the first iteration. We formalize this dynamic screening
	principle in a general algorithmic scheme and apply it by embedding
	inside a number of first-order algorithms adapted existing screening
	tests to solve the Lasso or new screening tests to solve the Group-Lasso.
	Computational gains are assessed in a large set of experiments on
	synthetic data as well as real-world sounds and images. They show
	both the screening efficiency and the gain in terms running times.},
  timestamp = {2016-07-08T12:16:16Z},
  journal = {hal-01084986},
  author = {Bonnefoy, Antoine and Emiya, Valentin and Ralaivola, Liva and Gribonval, R�mi},
  month = nov,
  year = {2014},
  keywords = {Dynamic screening,Group-Lasso,Iterative Soft Thresholding,Iterative Soft
	Thresholding,Lasso,Screening test,Screening
	test,Sparsity},
  pages = {1-19},
  owner = {Fardin}
}

@Article{Bruckstein2009,
  author    = {Bruckstein, A.M. and Donoho, D.L. and Elad, M.},
  title     = {From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images},
  journal   = {SIAM Rev.},
  year      = {2009},
  volume    = {51},
  number    = {1},
  pages     = {34--81},
  abstract  = {A full-rank matrix A ? Rn m with n < m generates an underdetermined
	system of linear equations Ax = b having infinitely many solutions.
	Suppose we seek the sparsest solution, i.e., the one with the fewest
	nonzero entries. Can it ever be unique? If so, when? As optimization
	of sparsity is combinatorial in nature, are there efficient methods
	for finding the sparsest solution? These questions have been answered
	positively and constructively in recent years, exposing a wide variety
	of surprising phenomena, in particular the existence of easily verifiable
	conditions under which optimally sparse solutions can be found by
	concrete, effective computational methods. Such theoretical results
	inspire a bold perspective on some important practical problems in
	signal and image processing. Several well-known signal and image
	processing problems can be cast as demanding solutions of undetermined
	systems of equations. Such problems have previously seemed, to many,
	intractable, but there is considerable evidence that these problems
	often have sparse solutions. Hence, advances in finding sparse solutions
	to underdetermined systems have energized research on such signal
	and image processing problems to striking effect. In this paper
	we review the theoretical results on sparse solutions of linear systems,
	empirical results on sparse modeling of signals and images, and recent
	applications in inverse problems and compression in image processing.
	This work lies at the intersection of signal processing and applied
	mathematics, and arose initially from the wavelets and harmonic analysis
	research communities. The aim of this paper is to introduce a few
	key notions and applications connected to sparsity, targeting newcomers
	interested in either the mathematical aspects of this area or its
	applications.},
  keywords  = {Basis pursuit,compression,denoising,dictionary learning,inverse problems,linear system of equations,linear system of equations,matching pursuit,mutual coherence,overcomplete,redundant,Sparse Coding,Sparse coding,Sparse-Land,sparse representation,underdetermined},
  owner     = {Fardin},
  timestamp = {2017-04-19T14:55:21Z},
}

@article{Benar2006,
  title = {EEG-fMRI of epileptic spikes: Concordance with EEG source localization 	and intracranial EEG},
  volume = {30},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.11.008},
  abstract = {Simultaneous \{EEG\} and fMRI recordings permit the non-invasive investigation
	of the generators of spontaneous brain activity such as epileptic
	spikes. Despite a growing interest in this technique, the precise
	relationship between its results and the actual regions of activated
	cortex is not clear. In this study, we have quantified for the first
	time the concordance between EEG–fMRI results and stereotaxic \{EEG\}
	(SEEG) recordings in 5 patients with partial epilepsy. We also compared
	fMRI and \{SEEG\} with other non-invasive maps based on scalp \{EEG\}
	alone. We found that \{SEEG\} measures largely validated the results
	of \{EEG\} and fMRI. Indeed, when there is an intracranial electrode
	in the vicinity of an \{EEG\} or fMRI peak (in the range 20–40
	mm), then it usually includes one active contact. This was the case
	for both increases (‘activations’) and decreases (‘deactivations’)
	of the fMRI signal: in our patients, fMRI signal decrease could be
	as important in understanding the complete picture of activity as
	increase of fMRI signal. The concordance between \{EEG\} and fMRI
	was not as good as the concordance between either of these non-invasive
	techniques and SEEG. This shows that the two techniques can show
	different regions of activity: they are complementary for the localization
	of the areas involved in the generation of epileptic spikes. Moreover,
	we found that the sign of the fMRI response correlated with the low
	frequency content of the \{SEEG\} epileptic transients, this latter
	being a reflection of the slow waves. Thus, we observed a higher
	proportion of energy in the low frequencies for the \{SEEG\} recorded
	in regions with fMRI signal increase compared to the regions with
	fMRI signal decrease. This could reflect an increase of metabolism
	linked to the presence of slow waves, which suggests that fMRI is
	a new source of information on the mechanisms of spike generation.},
  timestamp = {2016-07-08T12:17:03Z},
  number = {4},
  journal = {NeuroImage},
  author = {B{'e}nar, Christian-G. and Grova, Christophe and Kobayashi, Eliane and Bagshaw, Andrew P. and Aghakhani, Yahya and Dubeau, Fran\c{c}ois and Gotman, Jean},
  year = {2006},
  pages = {1161 - 1170},
  owner = {afdidehf}
}

@article{Calderon2015,
  title = {Reconstruction of channelized geological facies based on RIPless 	compressed sensing},
  volume = {77},
  issn = {0098-3004},
  doi = {http://dx.doi.org/10.1016/j.cageo.2015.01.006},
  abstract = {Abstract This work proposes a new approach for multichannel facies
	image reconstruction based on compressed sensing where the image
	is recovered from pixel-based measurements without the use of prior
	information from a training image. An l 1 - minimization reconstruction
	algorithm is proposed, and a performance guaranteed result is adopted
	to evaluate its reconstruction. From this analysis, we formulate
	the problem of basis selection, where it is shown that for unstructured
	pixel-based measurements the Discrete Cosine Transform is the best
	choice for the problem. In the experimental side, signal-to-noise
	ratios and similarity perceptual indicators are used to evaluate
	the quality of the reconstructions, and promising reconstruction
	results are obtained. The potential of this new approach is demonstrated
	in under-sampled scenario of 2–4% of direct data, which is known
	to be very challenging in the absence of prior knowledge from a training
	image.},
  timestamp = {2016-07-10T07:41:21Z},
  journal = {Computers \& Geosciences},
  author = {Calder{\'o}n, Hern{\'a}n and Silva, Jorge F. and Ortiz, Juli{\'a}n
	M. and Ega{\~n}a, Alvaro},
  year = {2015},
  keywords = {Basis,Compressed,cosine,Discrete,facies,images,Multichannel,selection,sensing,transform},
  pages = {54 - 65},
  owner = {afdidehf}
}

@inproceedings{Cand`es2014,
  title = {Mathematics of sparsity (and a few other things)},
  abstract = {In the last decade, there has been considerable interest in understanding
	when it is possible to nd structured solutions to underdetermined
	systems of linear equations. This paper surveys some of the mathematical
	theories, known as compressive sensing and matrix completion, that
	have been developed to nd sparse and low-rank solutions via convex
	programming techniques. Our exposition emphasizes the important role
	of the concept of incoherence.},
  timestamp = {2016-07-09T20:07:54Z},
  booktitle = {Proceedings of the International Congress of Mathematicians},
  author = {Cand{\`e}s, Emmanuel},
  year = {2014},
  keywords = {`1 norm,compressive sensing,convex programing,Gaussian widths.,low-rank-matrices,matrix completion,matrix
	completion,nuclear norm,nuclear
	norm,Sparsity,Underdetermined systems of linear equations,Underdetermined systems of linear
	equations},
  owner = {afdidehf}
}

@book{Cand`es2005a,
  title = {$\ell_1$-magic : Recovery of Sparse Signals via Convex Programming},
  timestamp = {2016-07-08T09:34:33Z},
  author = {Cand{\`e}s, Emmanuel and Romberg, Justin and {{{{{Caltech}}}}}},
  month = oct,
  year = {2005},
  owner = {Fardin}
}

@article{Cand`es2007b,
  title = {Rejoinder: the dantzig selector: statistical estimation when p is 	much larger than n},
  volume = {35},
  timestamp = {2016-09-29T15:34:37Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Cand{\`e}s, Emmanuel and Tao, Terence},
  year = {2007},
  pages = {2392--2404},
  owner = {afdidehf}
}

@article{Cand`es2008a,
  title = {The restricted isometry property and its implications for compressed 	sensing},
  volume = {346},
  issn = {1631-073X},
  doi = {http://dx.doi.org/10.1016/j.crma.2008.03.014},
  abstract = {It is now well-known that one can reconstruct sparse or compressible
	signals accurately from a very limited number of measurements, possibly
	contaminated with noise. This technique known as “compressed sensing?
	or “compressive sampling? relies on properties of the sensing
	matrix such as the restricted isometry property. In this Note, we
	establish new results about the accuracy of the reconstruction from
	undersampled measurements which improve on earlier estimates, and
	have the advantage of being more elegant. To cite this article: E.J.
	Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008).},
  timestamp = {2017-06-23T13:04:18Z},
  number = {9-10},
  journal = {Comptes Rendus Mathematique},
  author = {Cand{\`e}s, Emmanuel J.},
  year = {2008},
  pages = {589--592},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1631073X08000964},
  owner = {Fardin}
}

@article{Cand`es2002,
  title = {New Tight Frames of Curvelets and Optimal Representations of Objects 	with C2 Singularities},
  volume = {57},
  abstract = {This paper introduces new tight frames of curvelets to address the
	problem of finding optimally sparse representations of objects with
	discontinuities along C 2 edges. Conceptually, the curvelet transform
	is a multiscale pyramid with many directions and positions at each
	length scale, and needle-shaped elements at fine scales. These elements
	have many useful geometric multiscale features that set them apart
	from classical multiscale representations such as wavelets. For instance,
	curvelets obey a parabolic scaling relation which says that at scale
	2?j , each element has an envelope which is aligned along a ridge
	of length 2?j/2 and width 2?j . We prove that curvelets provide an
	essentially optimal representation of typical objects f which are
	C 2 except for discontinuities along C 2 curves. Such representations
	are nearly as sparse as if f were not singular and turn out to be
	far more sparse than the wavelet decomposition of the object. For
	instance, the n-term partial reconstruction f C n obtained by selecting
	the n largest terms in the curvelet series obeys kf ? f C n k 2 L2
	? C  n ?2  (log n) 3 , n ? ?. This rate of convergence holds
	uniformly over a class of functions which are C 2 except for discontinuities
	along C 2 curves and is essentially optimal. In comparison, the squared
	error of n-term wavelet approximations only converges as n ?1 as
	n ? ?, which is considerably worst than the optimal behavior.},
  timestamp = {2017-06-23T13:05:01Z},
  journal = {Comm. Pure Appl. Math.},
  author = {Cand{\`e}s, Emmanuel J. and Donoho, David L.},
  month = nov,
  year = {2002},
  keywords = {Curvelets,edges,nonlinear approximation,nonlinear
	approximation,Radon transform.,second dyadic decomposition,second dyadic
	decomposition,singularities,thresholding,wavelets},
  pages = {219--266},
  owner = {Fardin}
}

@article{Cand`es2000,
  title = {Recovering Edges in Ill-posed Inverse Problems: Optimality of Curvelet 	Frames},
  volume = {30},
  timestamp = {2016-07-10T07:41:53Z},
  number = {3},
  journal = {Annals of Statistics},
  author = {Cand{\`e}s, E. J. and Donoho, D. L.},
  year = {2000},
  keywords = {Curvelets,Deconvolution,Edge,Edge-Preserving Regularization,Ill-Posed Inverse Problems,Ill-Posed
	Inverse Problems,Minimax Estimation,Optimal Rates of Convergence,Radon Transform,Radon
	Transform,regularization,Ridgelets,Singular Value Decomposition,wavelets,Wavelet Shrinkage,Wavelet
	Shrinkage,Wavelet-Vaguelette-Decomposition,White Noise Model.},
  pages = {784-842},
  owner = {afdidehf}
}

@article{Casazza2004,
  title = {Frames of subspaces},
  volume = {345},
  abstract = {One approach to ease the construction of frames is to first construct
	local components and then build a global frame from these. In this
	paper we will show that the study of the relation between a frame
	and its local components leads to the definition of a frame of subspaces.
	We introduce this new notion and prove that it provides us with the
	link we need. It will also turn out that frames of subspaces behave
	as a generalization of frames. In particular, we can define an analysis,
	a synthesis and a frame operator for a frame of subspaces, which
	even yield a reconstruction formula. Also concepts such as completeness,
	minimality, and exactness are introduced and investigated. We further
	study several constructions of frames of subspaces, and also of frames
	and Riesz frames using the theory of frames of subspaces. An important
	special case are harmonic frames of subspaces which generalize harmonic
	frames. We show that wavelet subspaces coming from multiresolution
	analysis belong to this class.},
  timestamp = {2016-09-30T11:24:26Z},
  journal = {Wavelets, Frames, and Operator Theory, ser. Contemp. Math. . Providence, 	RI: Amer. Math. Soc.},
  author = {Casazza, P. G. and Kutyniok, G.},
  year = {2004},
  pages = {87--113},
  owner = {afdidehf}
}

@article{Caune2014,
  title = {Evaluating dipolar source localization feasibility from intracerebral 	SEEG recordings},
  volume = {98},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.04.058},
  abstract = {Abstract Stereo-electroencephalography (SEEG) is considered as the
	golden standard for exploring targeted structures during pre-surgical
	evaluation in drug-resistant partial epilepsy. The depth electrodes,
	inserted in the brain, consist of several collinear measuring contacts
	(sensors). Clinical routine analysis of \{SEEG\} signals is performed
	on bipolar montage, providing a focal view of the explored structures,
	thus eliminating activities of distant sources that propagate through
	the brain volume. We propose in this paper to exploit the common
	reference \{SEEG\} signals. In this case, the volume propagation
	information is preserved and electrical source localization (ESL)
	approaches can be proposed. Current \{ESL\} approaches used to localize
	and estimate the activity of the neural generators are mainly based
	on surface EEG/MEG signals, but very few studies exist on real \{SEEG\}
	recordings, and the case of equivalent current dipole source localization
	has not been explored yet in this context. In this study, we investigate
	the influence of volume conduction model, spatial configuration of
	\{SEEG\} sensors and level of noise on the \{ESL\} accuracy, using
	a realistic simulation setup. Localizations on real \{SEEG\} signals
	recorded during intracerebral electrical stimulations (ICS, known
	sources) as well as on epileptic interictal spikes are carried out.
	Our results show that, under certain conditions, a straightforward
	approach based on an equivalent current dipole model for the source
	and on simple analytical volume conduction models yields sufficiently
	precise solutions (below 10 mm) of the localization problem. Thus,
	electrical source imaging using \{SEEG\} signals is a promising tool
	for distant brain source investigation and might be used as a complement
	to routine visual interpretations.},
  timestamp = {2016-07-08T12:25:39Z},
  journal = {NeuroImage},
  author = {Caune, V. and Ranta, R. and Cam, S. Le and Hofmanis, J. and Maillard, L. and Koessler, L. and Louis-Dorr, V.},
  year = {2014},
  keywords = {Electrical,imaging,Source},
  pages = {118 - 133},
  owner = {afdidehf}
}

@techreport{Cevher2015,
  title = {A Tutorial on Sparse Signal Acquisition and Recovery with Graphical 	Models},
  timestamp = {2016-07-08T11:34:21Z},
  author = {Cevher, Volkan and Indyk, Piotr and Carin, Lawrence and Baraniuk, Richard G.},
  year = {2015},
  owner = {afdidehf}
}

@article{Chakareski2014,
  title = {Know thy neighbor: Community-aware recovery of content selection 	preferences},
  volume = {101},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.01.028},
  abstract = {Abstract We design two strategies for recovering unknown content selection
	preferences in online communities. The techniques take advantage
	of the community graph and the peers’ affinities, expressed through
	its edge weights, to optimize the computation of the missing data.
	The first strategy is distributed and comprises a local computation
	step and a message passing step that are iteratively applied at each
	vertex until convergence. We carry out a random walk based analysis
	of its operation and verify the analytical findings via numerical
	experiments. The second strategy is centralized and involves a sparsifying
	transform of the content preferences represented as a function over
	the community graph. We solve the related optimization problem of
	recovering the unknown preferences via an iterative algorithm based
	on variable splitting and alternating direction of multipliers. We
	take into account the data specifics by incorporating multiple regularization
	terms into the optimization. We investigate the underpinnings of
	the sparse reconstruction technique via simulations that reveal its
	characteristics and how they affect its performance. We also carry
	out experiments using Twitter data on which we further study the
	performance of our strategies and verify the modeling assumption
	made in the context of the decentralized one. Our experiments include
	a comparison to common reference methods. We show that our message
	passing technique outperforms the reference methods by a considerable
	margin. We also show that though our multi-regularized sparse reconstruction
	technique improves over conventional sparse recovery, it still suffers
	from the graph-signal smoothness assumption it implicitly considers.},
  timestamp = {2016-07-09T19:51:57Z},
  journal = {Signal Processing},
  author = {Chakareski, Jacob},
  year = {2014},
  keywords = {communities,Content,Graph-based,Message,Multi-regularization,Online,passing,preferences,recovery,selection,Sparse},
  pages = {151 - 161},
  owner = {afdidehf}
}

@article{Chen2014b,
  title = {Sparse representation for face recognition by discriminative low-rank 	matrix recovery},
  volume = {25},
  issn = {1047-3203},
  doi = {http://dx.doi.org/10.1016/j.jvcir.2014.01.015},
  abstract = {Abstract This paper proposes a discriminative low-rank representation
	(DLRR) method for face recognition in which both the training and
	test samples are corrupted owing to variations in occlusion and disguise.
	The proposed method extends the sparse representation-based classification
	algorithm by incorporating the low-rank structure of data representation.
	The \{DLRR\} algorithm recovers a clean dictionary with enhanced
	discrimination ability from the corrupted training samples for sparse
	representation. Simultaneously, it learns a low-rank projection matrix
	to correct corrupted test samples by projecting them onto their corresponding
	underlying subspaces. The dictionary elements from different classes
	are encouraged to be as independent as possible by regularizing the
	structural incoherence of the original training samples. This leads
	to a compact representation of a corrected test sample by a linear
	combination of more dictionary elements from the corrected class.
	The experimental results on benchmark databases show the effectiveness
	and robustness of our face recognition technique.},
  timestamp = {2016-07-11T16:51:42Z},
  number = {5},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Chen, Jie and Yi, Zhang},
  year = {2014},
  keywords = {A,Dictionary,Eigenface,Face,learning,Low-rank,Matrix,projection,recognition,recovery,representation,Sparse,Subspace},
  pages = {763 - 773},
  owner = {afdidehf}
}

@inproceedings{Chen1994,
  title = {Basis pursuit},
  volume = {1},
  doi = {10.1109/ACSSC.1994.471413},
  abstract = {The time-frequency and time-scale communities have recently developed
	an enormous number of over-complete signal dictionaries, wavelets,
	wavelet packets, cosine packets, Wilson bases, chirplets, warped
	bases, and hyperbolic cross bases being a few examples. Basis pursuit
	is a technique for decomposing a signal into an optimal superposition
	of dictionary elements. The optimization criterion is the l1 norm
	of coefficients. The method has several advantages over matching
	pursuit and best ortho basis, including super-resolution and stability},
  timestamp = {2017-06-23T13:05:30Z},
  booktitle = {Signals, Systems and Computers, 1994. 1994 Conference Record of the 	Twenty-Eighth Asilomar Conference on},
  author = {Chen, Shaobing and Donoho, D.},
  month = oct,
  year = {1994},
  keywords = {adaptive representations,adaptive signal processing,Basis pursuit,Chirp,chirplets,coefficients,cosine
	packets,Dictionaries,dictionary elements,Explosions,hyperbolic cross
	bases,Matching pursuit algorithms,optimal superposition,over-complete
	signal dictionaries,signal decompositon,signal representation,signal
	representations,signal resolution,Stability,Statistics,super-resolution,time-frequency analysis,Time frequency analysis,time-frequency
	analysis,Time
	frequency analysis,time-scale analysis,warped bases,warped
	bases,wavelet packets,Wavelet
	packets,wavelets,Wilson bases},
  pages = {41--44},
  owner = {afdidehf}
}

@article{Chowdhury2013,
  title = {MEG Source Localization of Spatially Extended Generators of Epileptic 	Activity: Comparing Entropic and Hierarchical Bayesian Approaches},
  volume = {8},
  abstract = {Localizing the generators of epileptic activity in the brain using
	Electro-EncephaloGraphy (EEG) or Magneto- EncephaloGraphy (MEG) signals
	is of particular interest during the pre-surgical investigation of
	epilepsy. Epileptic discharges can be detectable from background
	brain activity, provided they are associated with spatially extended
	generators. Using realistic simulations of epileptic activity, this
	study evaluates the ability of distributed source localization methods
	to accurately estimate the location of the generators and their sensitivity
	to the spatial extent of such generators when using MEG data. Source
	localization methods based on two types of realistic models have
	been investigated: (i) brain activity may be modeled using cortical
	parcels and (ii) brain activity is assumed to be locally smooth within
	each parcel. A Data Driven Parcellization (DDP) method was used to
	segment the cortical surface into non-overlapping parcels and diffusion-based
	spatial priors were used to model local spatial smoothness within
	parcels. These models were implemented within the Maximum Entropy
	on the Mean (MEM) and the Hierarchical Bayesian (HB) source localization
	frameworks. We proposed new methods in this context and compared
	them with other standard ones using Monte Carlo simulations of realistic
	MEG data involving sources of several spatial extents and depths.
	Detection accuracy of each method was quantified using Receiver Operating
	Characteristic (ROC) analysis and localization error metrics. Our
	results showed that methods implemented within the MEM framework
	were sensitive to all spatial extents of the sources ranging from
	3 cm2 to 30 cm2, whatever were the number and size of the parcels
	defining the model. To reach a similar level of accuracy within the
	HB framework, a model using parcels larger than the size of the sources
	should be considered.},
  timestamp = {2016-07-09T20:11:30Z},
  number = {2},
  journal = {PLoS ONE},
  author = {Chowdhury, Rasheda Arman and Lina, Jean Marc and Kobayashi, Eliane and Grova, Christophe},
  month = feb,
  year = {2013},
  owner = {afdidehf}
}

@article{Chavez-Dominguez2015,
  title = {Stability of low-rank matrix recovery and its connections to Banach 	space geometry},
  volume = {427},
  issn = {0022-247X},
  doi = {http://dx.doi.org/10.1016/j.jmaa.2015.02.041},
  abstract = {Abstract There are well-known relationships between compressed sensing
	and the geometry of the finite-dimensional l p spaces. A result of
	Kashin and Temlyakov [20] can be described as a characterization
	of the stability of the recovery of sparse vectors via l 1 -minimization
	in terms of the Gelfand widths of certain identity mappings between
	finite-dimensional l 1 and l 2 spaces, whereas a more recent result
	of Foucart, Pajor, Rauhut and Ullrich [16] proves an analogous relationship
	even for l p spaces with p &lt; 1 . In this paper we prove what we
	call matrix or noncommutative versions of these results: we characterize
	the stability of low-rank matrix recovery via Schatten p-(quasi-)norm
	minimization in terms of the Gelfand widths of certain identity mappings
	between finite-dimensional Schatten p-spaces.},
  timestamp = {2016-07-11T16:56:06Z},
  number = {1},
  journal = {Journal of Mathematical Analysis and Applications},
  author = {Ch{\'a}vez-Dom{\'i}nguez, Javier Alejandro and Kutzarova, Denka},
  year = {2015},
  keywords = {Compressed,Gelfand,Low-rank,Matrix,p-minimization,recovery,Schatten,sensing,widths},
  pages = {320 - 335},
  owner = {afdidehf}
}

@inproceedings{Coifman1992,
  address = {Washington, D.C., USA},
  series = {ICIAM 91},
  title = {Adapted Wave Form Analysis, Wavelet-packets and Applications},
  isbn = {0-89871-302-1},
  timestamp = {2016-09-29T16:06:41Z},
  booktitle = {Proceedings of the Second International Conference on Industrial 	and Applied Mathematics},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Coifman, R. and Meyer, Y. and Wickerhauser, V.},
  year = {1992},
  pages = {41--50},
  acmid = {166345},
  numpages = {10},
  owner = {afdidehf}
}

@techreport{Conrad2016,
  title = {$L^p$-SPACES FOR $0 < p < 1$},
  timestamp = {2016-07-08T09:37:16Z},
  author = {Conrad, Keith},
  year = {2016},
  owner = {afdidehf}
}

@article{Cotter2002,
  title = {Sparse channel estimation via matching pursuit with application to 	equalization},
  volume = {50},
  issn = {0090-6778},
  doi = {10.1109/26.990897},
  abstract = {Channels with a sparse impulse response arise in a number of communication
	applications. Exploiting the sparsity of the channel, we show how
	an estimate of the channel may be obtained using a matching pursuit
	(MP) algorithm. This estimate is compared to thresholded variants
	of the least squares (LS) channel estimate. Among these sparse channel
	estimates, the MP estimate is computationally much simpler to implement
	and a shorter training sequence is required to form an accurate channel
	estimate leading to greater information throughput},
  timestamp = {2016-09-29T16:29:32Z},
  number = {3},
  journal = {Communications, IEEE Transactions on},
  author = {Cotter, S.F. and Rao, B.D.},
  month = mar,
  year = {2002},
  keywords = {Broadband communication,Broadband communication,channel estimation,Channel estimation,decision feedback equalisers,decision feedback equalisers,decision feedback equalizer,decision
	feedback equalizer,Delay estimation,Delay estimation,DFE,DFE,HDTV,HDTV,information throughput,information throughput,intersymbol interference,intersymbol interference,intersymbol
	interference,ISI,ISI,Least squares approximation,Least squares approximation,least squares approximations,Least
	squares approximations,least squares channel estimate,least squares channel estimate,matching pursuit algorithm,matching pursuit
	algorithm,Matching pursuit algorithms,Matching pursuit algorithms,parameter estimation,parameter estimation,Pursuit
	algorithms,Pursuit algorithms,sparse channel estimation,sparse channel estimation,sparse impulse response,sparse impulse response,telecommunication channels,telecommunication
	channels,Throughput,Throughput,training sequence,training sequence,transient response,transient response,TV,TV,White noise,White
	noise},
  pages = {374--377},
  owner = {Fardin}
}

@article{Dalal2011,
  title = {MEG/EEG Source Reconstruction, Statistical Evaluation, and Visualization 	with NUTMEG},
  volume = {2011},
  doi = {doi:10.1155/2011/758973},
  abstract = {NUTMEG is a source analysis toolbox geared towards cognitive neuroscience
	researchers using MEG and EEG, including intracranial recordings.
	Evoked and unaveraged data can be imported to the toolbox for source
	analysis in either the time or time-frequency domains. NUTMEG offers
	several variants of adaptive beamformers, probabilistic reconstruction
	algorithms, as well as minimum-norm techniques to generate functional
	maps of spatiotemporal neural source activity. Lead fields can be
	calculated from single and overlapping sphere head models or imported
	from other software. Group averages and statistics can be calculated
	as well. In addition to data analysis tools, NUTMEG provides a unique
	and intuitive graphical interface for visualization of results. Source
	analyses can be superimposed onto a structural MRI or headshape to
	provide a convenient visual correspondence to anatomy. These results
	can also be navigated interactively, with the spatial maps and source
	time series or spectrogram linked accordingly. Animations can be
	generated to view the evolution of neural activity over time. NUTMEG
	can also display brain renderings and perform spatial normalization
	of functional maps using SPM�s engine. As a MATLAB package, the
	end user may easily link with other toolboxes or add customized functions.},
  timestamp = {2016-07-09T20:11:17Z},
  journal = {Computational Intelligence and Neuroscience},
  author = {Dalal, Sarang S. and Zumer, Johanna M. and Guggisberg, Adrian G. and Trumpis, Michael and E.Wong, Daniel D. and Sekihara, Kensuke and Nagarajan, and S, Srikantan},
  year = {2011},
  pages = {1-17},
  owner = {afdidehf}
}

@article{Daubechies2008,
  title = {An Iterative Thresholding Algorithm For Linear Inverse Problems With 	A Sparsity Constraint},
  abstract = {We consider linear inverse problems where the solution is assumed
	to have a sparse expansion on an arbitrary pre-assigned orthonormal
	basis. We prove that replacing the usual quadratic regularizing penalties
	by weighted l^p-penalties on the coefficients of such expansions,
	with 1 < or = p < or =2, still regularizes the problem. If p < 2,
	regularized solutions of such l^p-penalized problems will have sparser
	expansions, with respect to the basis under consideration. To compute
	the corresponding regularized solutions we propose an iterative algorithm
	that amounts to a Landweber iteration with thresholding (or nonlinear
	shrinkage) applied at each iteration step. We prove that this algorithm
	converges in norm. We also review some potential applications of
	this method.},
  timestamp = {2016-07-08T11:24:37Z},
  journal = {ArXiv Mathematics e-prints},
  author = {Daubechies, Ingrid and Defrise, Michel and Mol, Christine De},
  month = feb,
  year = {2008},
  owner = {Fardin}
}

@techreport{Davies2008,
  title = {Restricted Isometry Constants where $p$ sparse recovery can fail 	for $0 < p \leq 1$},
  timestamp = {2016-07-10T08:02:45Z},
  author = {Davies, Mike and Gribonval, Remi},
  month = jul,
  year = {2008},
  keywords = {compressed sensing,convex optimisation,inverse problem,iterative
	reweighted optimisation.,iterative reweighted
	optimisation.,nonconvex optimisation,overcomplete dictionary,overcomplete
	dictionary,restricted isometry
	property,restricted
	isometry property,sparse representation,underdetermined linear system},
  annote = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  annote = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  annote = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  annote = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  annote = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  annote = {read https://hal.inria.fr/inria-00294735v3 [Research Report]},
  owner = {Fardin}
}

@article{Davies2009a,
  title = {lp minimization and sparse approximation failure for compressible signals},
  timestamp = {2016-09-30T11:07:47Z},
  journal = {SAMPTA},
  author = {Davies, M. E. and Gribonval, R.},
  year = {2009},
  owner = {afdidehf}
}

@article{Day1940,
  title = {The Spaces $L^p$ With $0<p<1$},
  timestamp = {2016-07-11T17:05:52Z},
  journal = {Institute For Advanced Study},
  author = {Day, Mahlon M.},
  month = feb,
  year = {1940},
  owner = {afdidehf}
}

@article{Deng2015,
  title = {A fast image recovery algorithm based on splitting deblurring and 	denoising},
  volume = {287},
  issn = {0377-0427},
  doi = {http://dx.doi.org/10.1016/j.cam.2015.03.035},
  abstract = {Abstract In this paper, we employ a popular splitting strategy to
	design a fast iterative algorithm for image restoration. We divide
	the algorithm into two steps, i.e., deblurring step and denoising
	step. In the deblurring step, Fourier transform is employed for image
	deblurring under the periodic boundary condition. In the denoising
	step, we use a simple and fast method, called fast iterative shrinkage/thresholding
	algorithm (FISTA), to reduce image noise. In addition, we also give
	the convergence analysis for the proposed method. Visual and quantitative
	results demonstrate the proposed algorithm, applied to l 1 regularization
	model and total-variation (TV) regularization model, is a faster
	algorithm and keeps image details well.},
  timestamp = {2016-07-08T10:15:52Z},
  journal = {Journal of Computational and Applied Mathematics},
  author = {Deng, Liang-Jian and Guo, Huiqing and Huang, Ting-Zhu},
  year = {2015},
  keywords = {deblurring,denoising,Image,L1,regularization,restoration,shrinkage,Total-variation},
  pages = {88 - 97},
  owner = {afdidehf}
}

@article{Dighe2015,
  title = {Sparse modeling of neural network posterior probabilities for exemplar-based 	speech recognition},
  issn = {0167-6393},
  doi = {http://dx.doi.org/10.1016/j.specom.2015.06.002},
  abstract = {Abstract In this paper, a compressive sensing (CS) perspective to
	exemplar-based speech processing is proposed. Relying on an analytical
	relationship between \{CS\} formulation and statistical speech recognition
	(Hidden Markov Models – HMM), the automatic speech recognition
	(ASR) problem is cast as recovery of high-dimensional sparse word
	representation from the observed low-dimensional acoustic features.
	The acoustic features are exemplars obtained from (deep) neural network
	sub-word conditional posterior probabilities. Low-dimensional word
	manifolds are learned using these sub-word posterior exemplars and
	exploited to construct a linguistic dictionary for sparse representation
	of word posteriors. Dictionary learning has been found to be a principled
	way to alleviate the need of having huge collection of exemplars
	as required in conventional exemplar-based approaches, while still
	improving the performance. Context appending and collaborative hierarchical
	sparsity are used to exploit the sequential and group structure underlying
	word sparse representation. This formulation leads to a posterior-based
	sparse modeling approach to speech recognition. The potential of
	the proposed approach is demonstrated on isolated word (Phonebook
	corpus) and continuous speech (Numbers corpus) recognition tasks.},
  timestamp = {2016-07-11T16:49:17Z},
  journal = {Speech Communication},
  author = {Dighe, Pranay and Asaei, Afsaneh and Bourlard, Hervé},
  year = {2015},
  keywords = {Automatic speech recognition Deep neural network posterior features,compressive sensing,Compressive
	sensing,dictionary learning,dictionary
	learning,sparse modeling,Sparse word posterior probabilities,Sparse word posterior
	probabilities},
  pages = {-},
  owner = {afdidehf}
}

@article{Ding2013,
  title = {Simultaneous EEG and MEG Source Reconstruction in Sparse Electromagnetic 	Source Imaging},
  volume = {34},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) have
	different sensitivities to differently configured brain activations,
	making them complimentary in providing independent information for
	better detection and inverse reconstruction of brain sources. In
	the present study, we developed an integrative approach, which integrates
	a novel sparse electromagnetic source imaging method, i.e., variation-based
	cortical current density (VB-SCCD), together with the combined use
	of EEG and MEG data in reconstructing complex brain activity. To
	perform simultaneous analysis of multimodal data, we proposed to
	normalize EEG and MEG signals according to their individual noise
	levels to create unit-free measures. Our Monte Carlo simulations
	demonstrated that this integrative approach is capable of reconstructing
	complex cortical brain activations (up to 10 simultaneously activated
	and randomly located sources). Results from experimental data showed
	that complex brain activations evoked in a face recognition task
	were successfully reconstructed using the integrative approach, which
	were consistent with other research findings and validated by independent
	data from functional magnetic resonance imaging using the same stimulus
	protocol. Reconstructed cortical brain activations from both simulations
	and experimental data provided precise source localizations as well
	as accurate spatial extents of localized sources. In comparison with
	studies using EEG or MEG alone, the performance of cortical source
	reconstructions using combined EEG and MEG was significantly improved.
	We demonstrated that this new sparse ESI methodology with integrated
	analysis of EEG and MEG data could accurately probe spatiotemporal
	processes of complex human brain activations. This is promising for
	noninvasively studying large-scale brain networks of high clinical
	and scientific significance.},
  timestamp = {2016-10-21T13:54:29Z},
  journal = {Human Brain Mapping},
  author = {Ding, Lei and Yuan, Han},
  year = {2013},
  keywords = {EEG,MEG,multi-modal data analysis,multi-modal
	data analysis,SNR transformation,sparse electromagnetic source imaging,sparse electromagnetic
	source imaging,VB-SCCD},
  pages = {775--795},
  owner = {afdidehf}
}

@article{Donoho2006a,
  title = {Stable recovery of sparse overcomplete representations in the presence 	of noise},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.860430},
  abstract = {Overcomplete representations are attracting interest in signal processing
	theory, particularly due to their potential to generate sparse representations
	of signals. However, in general, the problem of finding sparse representations
	must be unstable in the presence of noise. This paper establishes
	the possibility of stable recovery under a combination of sufficient
	sparsity and favorable structure of the overcomplete system. Considering
	an ideal underlying signal that has a sufficiently sparse representation,
	it is assumed that only a noisy version of it can be observed. Assuming
	further that the overcomplete system is incoherent, it is shown that
	the optimally sparse approximation to the noisy data differs from
	the optimally sparse decomposition of the ideal noiseless signal
	by at most a constant multiple of the noise level. As this optimal-sparsity
	method requires heavy (combinatorial) computational effort, approximation
	algorithms are considered. It is shown that similar stability is
	also available using the basis and the matching pursuit algorithms.
	Furthermore, it is shown that these methods result in sparse approximation
	of the noisy data that contains only terms also appearing in the
	unique sparsest representation of the ideal noiseless sparse signal.},
  timestamp = {2016-09-29T16:17:03Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Donoho, D.L. and Elad, M. and Temlyakov, V.N.},
  month = jan,
  year = {2006},
  keywords = {approximation theory,Basis pursuit,Dictionaries,greedy approximation,greedy
	approximation algorithm,incoherent dictionary,iterative methods,Kruskal
	rank,Linear algebra,matching pursuit,Matching pursuit algorithms,Noise
	generators,Noise level,noisy data,optimal sparse decomposition,overcomplete
	representation,signal denoising,signal processing,Signal processing
	algorithms,signal processing theory,signal representation,signal
	representations,sparse overcomplete representation,sparse representation,Stability,stable
	recovery,stepwise regression,superresolution,superresolution signal,time-frequency
	analysis,Vectors},
  pages = {6--18},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Donoho2001,
  title = {Uncertainty principles and ideal atomic decomposition},
  volume = {47},
  issn = {0018-9448},
  doi = {10.1109/18.959265},
  abstract = {Suppose a discrete-time signal S(t), 0?t\ensuremath{<}N, is a superposition
	of atoms taken from a combined time-frequency dictionary made of
	spike sequences 1\{t=?\} and sinusoids exp\{2?iwt/N\}/?N. Can one
	recover, from knowledge of S alone, the precise collection of atoms
	going to make up S? Because every discrete-time signal can be represented
	as a superposition of spikes alone, or as a superposition of sinusoids
	alone, there is no unique way of writing S as a sum of spikes and
	sinusoids in general. We prove that if S is representable as a highly
	sparse superposition of atoms from this time-frequency dictionary,
	then there is only one such highly sparse representation of S, and
	it can be obtained by solving the convex optimization problem of
	minimizing the l1 norm of the coefficients among all decompositions.
	Here \dbendhighly sparse\dbend means that Nt+Nw\ensuremath{<}?N/2
	where Nt is the number of time atoms, Nw is the number of frequency
	atoms, and N is the length of the discrete-time signal. Underlying
	this result is a general l1 uncertainty principle which says that
	if two bases are mutually incoherent, no nonzero signal can have
	a sparse representation in both bases simultaneously. For the above
	setting, the bases are sinusoids and spikes, and mutual incoherence
	is measured in terms of the largest inner product between different
	basis elements. The uncertainty principle holds for a variety of
	interesting basis pairs, not just sinusoids and spikes. The results
	have idealized applications to band-limited approximation with gross
	errors, to error-correcting encryption, and to separation of uncoordinated
	sources. Related phenomena hold for functions of a real variable,
	with basis pairs such as sinusoids and wavelets, and for functions
	of two variables, with basis pairs such as wavelets and ridgelets.
	In these settings, if a function f is representable by a sufficiently
	sparse superposition of terms taken from both bases, then there is
	only one such sparse representation; it may be obtained by minimum
	l1 norm atomic decomposition. The condition \dbendsufficiently sparse\dbend
	becomes a multiscale condition; for example, that the number of wavelets
	at level j plus the number of sinusoids in the jth dyadic frequency
	band are together less than a constant times 2j/2},
  timestamp = {2016-09-29T14:54:05Z},
  number = {7},
  journal = {IEEE Trans. Inf. Theory},
  author = {Donoho, D.L. and Huo, X.},
  month = nov,
  year = {2001},
  keywords = {band-limited approximation,convex optimization problem,cryptography,Dictionaries,discrete-time
	signal length,discrete-time signal representation,dyadic frequency
	band,error-correcting encryption,Frequency,Harmonic analysis,ideal
	atomic decomposition,indeterminancy,Matching pursuit algorithms,multiscale
	condition,mutual incoherence,NSP,null space property,Ridgelets,Signal analysis,Signal
	analysis,signal representation,signal representations,signal
	representations,sinusoids,sparse representation,sparse
	representation,spike sequences,time-frequency analysis,time-frequency
	analysis,time-frequency dictionary,time-frequency
	dictionary,Uncertainty,uncertainty principles,uncoordinated
	source separation,uncoordinated source
	separation,wavelet packets,wavelets,wavelet transforms,Writing},
  pages = {2845--2862},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on}
}

@techreport{Donoho2006b,
  title = {Sparse Solution of Underdetermined Linear Equations by Stagewise 	Orthogonal Matching Pursuit},
  abstract = {Finding the sparsest solution to underdetermined systems of linear
	equations y = ?x is NP-hard in general. We show here that for systems
	with �typical�/�random� ?, a good approximation to the sparsest
	solution is obtained by applying a fixed number of standard operations
	from linear algebra. Our proposal, Stagewise Orthogonal Matching
	Pursuit (StOMP), successively transforms the signal into a negligible
	residual. Starting with initial residual r0 = y, at the s -th stage
	it forms the �matched filter� ?Trs-1, identifies all coordinates
	with amplitudes exceeding a specially chosen threshold, solves a
	least-squares problem using the selected coordinates, and subtracts
	the least-squares fit, producing a new residual. After a fixed number
	of stages (e.g., 10), it stops. In contrast to Orthogonal Matching
	Pursuit (OMP), many coefficients can enter the model at each stage
	in StOMP while only one enters per stage in OMP; and StOMP takes
	a fixed number of stages (e.g., 10), while OMP can take many (e.g.,
	n). We give both theoretical and empirical support for the large-system
	effectiveness of StOMP. We give numerical examples showing that StOMP
	rapidly and reliably finds sparse solutions in compressed sensing,
	decoding of error-correcting codes, and overcomplete representation.},
  timestamp = {2016-07-11T16:54:00Z},
  author = {Donoho, D.L. and Tsaig, Y. and Drori, I. and Starck, J.-L.},
  month = mar,
  year = {2006},
  keywords = {compressed sensing,decoding error-correcting codes,false alarm rate. MIMO channel,false alarm rate.
	MIMO channel,false discovery rate,large-system limit. random matrix
	theory. Gaussian approximation. `1minimization. stepwise regression.
	thresholding,mutual access interference,sparse overcomplete
	representation. phase transition,sparse overcomplete representation.
	phase transition,successive interference cancellation.
	iterative decoding,successive interference cancellation. iterative
	decoding},
  pages = {1094-1121},
  owner = {Fardin}
}

@article{Donoho2004,
  title = {For Most Large Underdetermined Systems of Linear Equations the Minimal $\ell^1$-norm Solution is also the Sparsest Solution},
  volume = {59},
  abstract = {We consider linear equations y =   where y is a given vector in Rn,
	  is a given n by m matrix with n < m   An, and we wish to solve
	for   2 Rm. We suppose that the columns of   are normalized to unit
	`2 norm 1 and we place uniform measure on such  . We prove the existence
	of   =  (A) so that for large n, and for all  s except a negligible
	fraction, the following property holds: For every y having a representation
	y =  0 by a coefficient vector 0 2 Rm with fewer than   n nonzeros,
	the solution  1 of the `1 minimization problem min kxk1 subject to
	  = y is unique and equal to  0. In contrast, heuristic attempts
	to sparsely solve such systems   greedy algorithms and thresholding
	  perform poorly in this challenging setting. The techniques include
	the use of random proportional embeddings and almost-spherical sections
	in Banach space theory, and deviation bounds for the eigenvalues
	of random Wishart matrices.},
  timestamp = {2016-10-07T13:26:31Z},
  journal = {Comm. Pure Appl. Math},
  author = {Donoho, David L.},
  year = {2004},
  keywords = {`1,Algorithms.,Almost-Euclidean,Banach,Basis,decomposition.,Eigenvalues,Greedy,in,linear,Matching,Matrices.,minimum,of,overcomplete,Pursuit.,Pursuit,Random,Representations.,Sections,Sign-Embeddings,Solution,Spaces.,Systems.,underdetermined},
  pages = {797--829},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@techreport{Donoho2004b,
  title = {Neighborly Polytopes and Sparse Solution of Underdetermined Linear 	Equations},
  abstract = {Consider a d � n matrix A, with d < n. The problem of solving for
	x in y = Ax is underdetermined, and has many possible solutions (if
	there are any). In several fields it is of interest to find the sparsest
	solution � the one with fewest nonzeros � but in general this
	involves combinatorial optimization. Let ai denote the i-th column
	of A, 1  i  n. Associate to A the quotient polytope P formed by
	taking the convex hull of the 2n points (�ai) in Rd. P is centrosymmetric
	and is called (centrally) k-neighborly if every subset of k + 1 elements
	(�ilail )k+1 l=1 are the vertices of a face of P. We show that
	if P is k-neighborly, then if a system y = Ax has a solution with
	at most k nonzeros, that solution is also the unique solution of
	the convex optimization problem min kxk1 subject to y = Ax; the converse
	holds as well. This complete equivalence between the study of sparse
	solution by `1 minimization and neighborliness of convex polytopes
	immediately gives new results in each field. On the one hand, we
	get new families of neighborly centrosymmetric polytopes, by exploiting
	known results about sparsity of `1 minimization; on the other, we
	get new limits on the ability of `1 minimization to find sparse solutions,
	by exploiting known limits on neighborliness of centrally symmetric
	polytopes. Weaker notions of equivalence between `1 and sparse optimization
	have also been studied recently. These are equivalent to other interesting
	properties of the quotient polytope. Thus, suppose the columns of
	A are in general position. Consider the vectors having k < d/2 nonzeros
	that are simultaneously the sparsest solution of y = Ax and the minimal
	`1 solution. These make up a fraction 1 ?  of all vectors with k
	nonzeros if and only if the quotient polytope P has at least 1?
	as many k-dimensional faces as the regular cross polytope Cn. Combining
	this with recent work on face numbers of randomly-projected cross-polytopes,
	we learn that for large d, the overwhelming majority of systems of
	linear equations with d equations and 4d/3 unknowns have the following
	property: if there is a solution with fewer than .49d nonzeros, it
	is the unique minimum `1 solution. A stylized application in digital
	communication is sketched; for large n, it is possible to transmit
	n/4 pieces of information using a codeword of length n with immunity
	to .49n gross errors in the received codeword, if the signs and sites
	of the gross errors are random, and with immunity to .11n gross errors
	chosen by a malicious opponent. The receiver uses `1 minimization.},
  timestamp = {2016-07-10T06:49:07Z},
  author = {Donoho, David L.},
  month = dec,
  year = {2004},
  keywords = {`1,Breakdown,Centrally-Neighborly,Centrosymetric,Combinatorial,equations.,Errors.,Gross,linear,of,optimization.,Point.,Polytopes.,recovery,Signal,systems,underdetermined,with},
  owner = {Fardin}
}

@article{Donoho1995,
  title = {De-noising by soft-thresholding},
  volume = {41},
  abstract = {Donoho and Johnstone (1994) proposed a method for reconstructing an
	unknown function f on {[}0,1] from noisy data di=f(ti )+\&sigma;zi,
	i=0, \&hellip;, n-1,ti=i/n, where the zi are independent and identically
	distributed standard Gaussian random variables. The reconstruction
	f\&circ;*n is defined in the wavelet domain by translating all the
	empirical wavelet coefficients of d toward 0 by an amount \&sigma;\&middot;\&radic;(2log
	(n)/n). The authors prove two results about this type of estimator.
	{[}Smooth]: with high probability f\&circ;*n is at least as smooth
	as f, in any of a wide variety of smoothness measures. {[}Adapt]:
	the estimator comes nearly as close in mean square to f as any measurable
	estimator can come, uniformly over balls in each of two broad scales
	of smoothness classes. These two properties are unprecedented in
	several ways. The present proof of these results develops new facts
	about abstract statistical inference and its connection with an optimal
	recovery model},
  timestamp = {2016-09-29T16:12:33Z},
  number = {3},
  journal = {IEEE Transactions on Information Theory},
  author = {Donoho, D. L.},
  month = may,
  year = {1995},
  pages = {613--627}
}

@inproceedings{Donoho2005,
  title = {Sparse nonnegative solution of underdetermined linear equations by 	linear programming},
  volume = {102},
  timestamp = {2016-09-30T10:45:58Z},
  booktitle = {Proceedings of The National Academy of Sciences},
  author = {Donoho, David L. and Tanner, Jared},
  year = {2005},
  pages = {9446--9451},
  owner = {Fardin}
}

@article{Dornaika2015,
  title = {Decremental Sparse Modeling Representative Selection for prototype 	selection},
  volume = {48},
  issn = {0031-3203},
  doi = {http://dx.doi.org/10.1016/j.patcog.2015.05.018},
  abstract = {Abstract Selecting few representatives or examples that can efficiently
	and reliably describe a set of data has always been a challenging
	task in computer vision and pattern recognition. Recently, Sparse
	Modeling Representative Selection (SMRS) was proposed as a powerful
	filter method for selecting the most relevant examples/instances
	in subspaces. The selection is achieved by ranking the examples using
	the \{L2\} norm of the associated row in a coding matrix. This coding
	matrix is computed using data self-representativeness (the dictionary
	is given by the examples themselves) adopting block sparsity regularization.
	In this paper, we propose a decremental Sparse Modeling Representative
	Selection (D-SMRS) in which the selection of the representatives
	is broken down into several nested processes. The key contribution
	is a new scheme of sparse modeling that proceeds by progressive coding
	and pruning. It proceeds by eliminating outlier and noisy samples
	in the first stages so that the final stage (coding and selection)
	is performed on clean data. Thus, the final instance selection will
	not be heavily affected by the presence of outliers and aberrant
	samples. The proposed method was qualitatively and quantitatively
	evaluated. The qualitative evaluation concerned image selection and
	video summarization. The quantitative evaluation was performed on
	six benchmark image datasets using several state-of-the art selection
	methods with four different classifiers: 1-Nearest Neighbor (NN),
	Nearest Subspace (NS), Sparse Representation Classifier (SRC), and
	Support Vector Machines (SVM). The outlier rejection ability of the
	proposed method is also studied on real images. In all cases the
	selection computed by our algorithm achieved or outperformed existing
	state-of-the-art results.},
  timestamp = {2016-07-08T12:10:21Z},
  number = {11},
  journal = {Pattern Recognition},
  author = {Dornaika, F. and Aldine, I. Kamal},
  year = {2015},
  keywords = {Block,classification,Data,Image,Object,Prototype,recognition,regularization,representation,selection,self-representativeness,Sparsity,Subspace},
  pages = {3714 - 3727},
  owner = {afdidehf}
}

@article{Dossal2012,
  title = {A necessary and sufficient condition for exact sparse recovery by 	minimization},
  volume = {350},
  issn = {1631-073X},
  doi = {http://dx.doi.org/10.1016/j.crma.2011.12.014},
  abstract = {In this Note, a new sharp sufficient condition for exact sparse recovery
	by l 1 -penalized minimization from linear measurements is proposed.
	The main contribution of this paper is to show that, for most matrices,
	this condition is also necessary. Moreover, when the l 1 minimizer
	is unique, we investigate its sensitivity to the measurements and
	we establish that the application associating the measurements to
	this minimizer is Lipschitz-continuous.},
  timestamp = {2016-07-08T10:29:48Z},
  number = {1–2},
  journal = {Comptes Rendus Mathematique},
  author = {Dossal, Charles},
  year = {2012},
  pages = {117 - 120},
  owner = {Fardin}
}

@article{Eftekhari2015,
  title = {New analysis of manifold embeddings and signal recovery from compressive 	measurements},
  volume = {39},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2014.08.005},
  abstract = {Abstract Compressive Sensing (CS) exploits the surprising fact that
	the information contained in a sparse signal can be preserved in
	a small number of compressive, often random linear measurements of
	that signal. Strong theoretical guarantees have been established
	concerning the embedding of a sparse signal family under a random
	measurement operator and on the accuracy to which sparse signals
	can be recovered from noisy compressive measurements. In this paper,
	we address similar questions in the context of a different modeling
	framework. Instead of sparse models, we focus on the broad class
	of manifold models, which can arise in both parametric and non-parametric
	signal families. Using tools from the theory of empirical processes,
	we improve upon previous results concerning the embedding of low-dimensional
	manifolds under random measurement operators. We also establish both
	deterministic and probabilistic instance-optimal bounds in l 2 for
	manifold-based signal recovery and parameter estimation from noisy
	compressive measurements. In line with analogous results for sparsity-based
	CS, we conclude that much stronger bounds are possible in the probabilistic
	setting. Our work supports the growing evidence that manifold-based
	models can be used with high accuracy in compressive signal processing.},
  timestamp = {2016-07-10T06:53:11Z},
  number = {1},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Eftekhari, Armin and Wakin, Michael B.},
  year = {2015},
  keywords = {Compressive,Dimensionality,embeddings,Estimation,Manifold,Manifolds,Parameter,projections,Random,recovery,reduction,sensing,Signal},
  pages = {67 - 109},
  owner = {afdidehf}
}

@article{Engemann2015,
  title = {Automated model selection in covariance estimation and spatial whitening 	of MEG and EEG signals},
  volume = {108},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.12.040},
  abstract = {Abstract Magnetoencephalography and electroencephalography (M/EEG)
	measure non-invasively the weak electromagnetic fields induced by
	post-synaptic neural currents. The estimation of the spatial covariance
	of the signals recorded on M/EEG sensors is a building block of modern
	data analysis pipelines. Such covariance estimates are used in brain–computer
	interfaces (BCI) systems, in nearly all source localization methods
	for spatial whitening as well as for data covariance estimation in
	beamformers. The rationale for such models is that the signals can
	be modeled by a zero mean Gaussian distribution. While maximizing
	the Gaussian likelihood seems natural, it leads to a covariance estimate
	known as empirical covariance (EC). It turns out that the \{EC\}
	is a poor estimate of the true covariance when the number of samples
	is small. To address this issue the estimation needs to be regularized.
	The most common approach downweights off-diagonal coefficients, while
	more advanced regularization methods are based on shrinkage techniques
	or generative models with low rank assumptions: probabilistic \{PCA\}
	(PPCA) and factor analysis (FA). Using cross-validation all of these
	models can be tuned and compared based on Gaussian likelihood computed
	on unseen data. We investigated these models on simulations, one
	electroencephalography (EEG) dataset as well as magnetoencephalography
	(MEG) datasets from the most common \{MEG\} systems. First, our results
	demonstrate that different models can be the best, depending on the
	number of samples, heterogeneity of sensor types and noise properties.
	Second, we show that the models tuned by cross-validation are superior
	to models with hand-selected regularization. Hence, we propose an
	automated solution to the often overlooked problem of covariance
	estimation of M/EEG signals. The relevance of the procedure is demonstrated
	here for spatial whitening and source localization of \{MEG\} signals.},
  timestamp = {2016-07-08T11:35:38Z},
  journal = {NeuroImage},
  author = {Engemann, Denis A. and Gramfort, Alexandre},
  year = {2015},
  keywords = {(EEG),electroencephalography},
  pages = {328 - 342},
  owner = {afdidehf}
}

@article{Esser2013,
  title = {A Method for Finding Structured Sparse Solutions to Non-negative 	Least Squares Problems with Applications},
  abstract = {Unmixing problems in many areas such as hyperspectral imaging and
	differential optical absorption spectroscopy (DOAS) often require
	finding sparse nonnegative linear combinations of dictionary elements
	that match observed data. We show how aspects of these problems,
	such as misalignment of DOAS references and uncertainty in hyperspectral
	endmembers, can be modeled by expanding the dictionary with grouped
	elements and imposing a structured sparsity assumption that the combinations
	within each group should be sparse or even 1-sparse. If the dictionary
	is highly coherent, it is difficult to obtain good solutions using
	convex or greedy methods, such as nonnegative least squares (NNLS)
	or orthogonal matching pursuit. We use penalties related to the Hoyer
	measure, which is the ratio of the l1 and l2 norms, as sparsity penalties
	to be added to the objective in NNLS-type models. For solving the
	resulting nonconvex models, we propose a scaled gradient projection
	algorithm that requires solving a sequence of strongly convex quadratic
	programs. We discuss its close connections to convex splitting methods
	and difference of convex programming. We also present promising numerical
	results for example DOAS analysis and hyperspectral unmixing problems.},
  timestamp = {2016-07-08T10:28:11Z},
  number = {4},
  author = {Esser, Ernie and Lou, Yifei and Xin, Jack},
  month = jan,
  year = {2013},
  keywords = {Basis pursuit,difference of convex programming,differential optical absorption spectroscopy,differential optical
	absorption spectroscopy,Hyperspectral imaging,Hyperspectral
	imaging,non-negative least squares,non-negative least
	squares,scaled gradient projection,Structured Sparsity,unmixing},
  pages = {2010-2046},
  owner = {Fardin}
}

@phdthesis{Eydelzon2007,
  title = {A Study on Conditions for Sparse Solution Recovery in Compressive 	Sensing},
  abstract = {It is well-known by now that under suitable conditions `1 minimization
	can recover sparse solutions to under-determined linear systems of
	equations. More precisely, by solving the convex optimization problem
	minfkxk1 : Ax = bg, where A is an m  n measurement matrix with m
	< n, one can obtain the sparsest solution x to Ax = b provided that
	the measurement matrix A has certain properties and the sparsity
	level k of x is suciently small. This fact has led to active research
	in the area of compressive sensing and other applications. The central
	question for this problem is the following. Given a type of measurements,
	a signal's length n and sparsity level k, what is the minimum measurement
	size m that ensures recovery? Or equivalently, given a type of measurements,
	a signal length n and a measurement size m, what is the maximum recoverable
	sparsity level k? The above fundamental question has been answered,
	with varying degrees of precision, by a number of researchers for
	a number of dierent random or semi-random measurement matrices.
	However, all the existing results still involve unknown constants
	of some kind and thus are unable to provide precise answers to specic
	situations. For example, let A be an m  n partial DCT matrix with
	n = 107 and iii m = 5  105 (n=m = 20). Can we provide a reasonably
	good estimate on the maximum recoverable sparsity k? In this research,
	we attempt to provide a more precise answer to the central question
	raised above. By studying new sucient conditions for exact recovery
	of sparse solutions, we propose a new technique to estimate recoverable
	sparsity for dierent kinds of deterministic, random and semi-random
	matrices. We will present empirical evidence to show the practical
	success of our approach, though further research is still needed
	to formally establish its eectiveness.},
  timestamp = {2016-07-08T11:32:06Z},
  author = {Eydelzon, Anatoly},
  month = aug,
  year = {2007},
  owner = {Fardin}
}

@inproceedings{Fan2006,
  title = {Statistical challenges with high dimensionality: feature selection 	in knowledge discovery},
  abstract = {Technological innovations have revolutionized the process of scientific
	research and knowledge discovery. The availability of massive data
	and challenges from frontiers of research and development have reshaped
	statistical thinking, data analysis and theoretical studies. The
	challenges of high-dimensionality arise in diverse fields of sciences
	and the humanities, ranging from computational biology and health
	studies to financial engineering and risk management. In all of these
	fields, variable selection and feature extraction are crucial for
	knowledge discovery. We first give a comprehensive overviewof statistical
	challenges with high dimensionality in these diverse disciplines.
	We then approach the problem of variable selection and feature extraction
	using a unified framework: penalized likelihood methods. Issues relevant
	to the choice of penalty functions are addressed. We demonstrate
	that for a host of statistical problems, as long as the dimensionality
	is not excessively large, we can estimate the model parameters as
	well as if the best model is known in advance. The persistence property
	in risk minimization is also addressed. The applicability of such
	a theory and method to diverse statistical problems is demonstrated.
	Other related problems with high-dimensionality are also discussed.},
  timestamp = {2016-07-11T16:56:54Z},
  booktitle = {Proceedings of the International Congress of Mathematicians},
  author = {Fan, Jianqing and Li, Runze},
  year = {2006},
  pages = {595-622},
  owner = {afdidehf}
}

@article{Fan2001,
  title = {Variable Selection via Nonconcave Penalized Likelihood and its Oracle 	Properties},
  volume = {96},
  abstract = {Variable selection is fundamental to high-dimensiona l statistical
	modeling, including nonparametri c regression. Many approaches in
	use are stepwise selection procedures, which can be computationally
	expensive and ignore stochastic errors in the variable selection
	process. In this article, penalized likelihood approaches are proposed
	to handle these kinds of problems. The proposed methods select variables
	and estimate coef? cients simultaneously. Hence they enable us to
	construct con? dence intervals for estimated parameters. The proposed
	approaches are distinguished from others in that the penalty functions
	are symmetric, nonconcav e on 401�5, and have singularities at
	the origin to produce sparse solutions. Furthermore, the penalty
	functions should be bounded by a constant to reduce bias and satisfy
	certain conditions to yield continuous solutions. A new algorithm
	is proposed for optimizing penalized likelihood functions. The proposed
	ideas are widely applicable. They are readily applied to a variety
	of parametric models such as generalized linear models and robust
	regression models. They can also be applied easily to nonparametri
	c modeling by using wavelets and splines. Rates of convergenc e of
	the proposed penalized likelihood estimators are established. Furthermore,
	with proper choice of regularization parameters, we show that the
	proposed estimators perform as well as the oracle procedure in variable
	selection; namely, they work as well as if the correct submodel were
	known. Our simulation shows that the newly proposed methods compare
	favorably with other variable selection techniques. Furthermore,
	the standard error formulas are tested to be accurate enough for
	practical applications.},
  timestamp = {2016-07-11T17:10:29Z},
  number = {456},
  journal = {Journal of the American Statistical Association},
  author = {Fan, Jianqing and Li, Runze},
  month = dec,
  year = {2001},
  keywords = {Hard thresholding,Lasso,Nonnegative garrote,oracle estimator,Penalized likelihood,Penalized
	likelihood,SCAD,Soft thresholding.},
  pages = {1348�1360},
  owner = {afdidehf}
}

@article{Fang2011,
  title = {Recovery of Block-Sparse Representations from Noisy Observations 	via Orthogonal Matching Pursuit},
  abstract = {We study the problem of recovering the sparsity pattern of block-sparse
	signals from noise-corrupted measurements. A simple, efficient recovery
	method, namely, a block-version of the orthogonal matching pursuit
	(OMP) method, is considered in this paper and its behavior for recovering
	the block-sparsity pattern is analyzed. We provide sufficient conditions
	under which the block-version of the OMP can successfully recover
	the block-sparse representations in the presence of noise. Our analysis
	reveals that exploiting block-sparsity can improve the recovery ability
	and lead to a guaranteed recovery for a higher sparsity level. Numerical
	results are presented to corroborate our theoretical claim.},
  timestamp = {2016-07-10T07:42:18Z},
  journal = {ArXiv e-prints},
  author = {Fang, J. and Li, H.},
  month = sep,
  year = {2011},
  keywords = {Block-sparsity,compressed sensing.,Computer Science - Information Theory,Computer Science - Information
	Theory,orthogonal matching pursuit,orthogonal
	matching pursuit},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1109.5430F},
  archiveprefix = {arXiv},
  owner = {afdidehf},
  primaryclass = {cs.IT}
}

@article{Feuer2003,
  title = {On sparse representation in pairs of bases},
  volume = {49},
  issn = {0018-9448},
  doi = {10.1109/TIT.2003.811926},
  abstract = {In previous work, Elad and Bruckstein (EB) have provided a sufficient
	condition for replacing an l0 optimization by linear programming
	minimization when searching for the unique sparse representation.
	We establish here that the EB condition is both sufficient and necessary.},
  timestamp = {2016-09-30T11:03:18Z},
  number = {6},
  journal = {IEEE Transactions on Information Theory},
  author = {Feuer, A. and Nemirovski, A.},
  month = jun,
  year = {2003},
  keywords = {Cities and towns,Cities and towns,Dictionaries,Dictionaries,Elad-Bruckstein condition,Elad-Bruckstein condition,Industrial engineering,Industrial
	engineering,Industrial engineering,Laboratories,Laboratories,linear programming,linear programming,linear programming minimization,linear programming minimization,matrix algebra,matrix
	algebra,matrix algebra,minimisation,minimisation,necessary condition,necessary condition,optimization problems,optimization problems,orthogonal matrices,orthogonal
	matrices,orthogonal matrices,pairs of bases,pairs of bases,Robot control,Robot control,Service robots,Service robots,signal representation,signal representation,sparse matrices,sparse
	matrices,Sparse matrices,sparse representation,sparse representation,sufficient condition,sufficient condition,Sufficient conditions,Sufficient conditions,Technology management,Technology
	management,Technology management},
  pages = {1579--1581},
  annote = {read},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\V24CV2UH\\articleDetails.html:text/html}
}

@article{Fornasier2007,
  title = {Domain decomposition methods for linear inverse problems with sparsity 	constraints},
  volume = {23},
  abstract = {Quantities of interest appearing in concrete applications often possess
	sparse expansions with respect to a preassigned frame. Recently,
	there were introduced sparsity measures which are typically constructed
	on the basis of weighted l 1 norms of frame coefficients. One can
	model the reconstruction of a sparse vector from noisy linear measurements
	as the minimization of the functional defined by the sum of the discrepancy
	with respect to the data and the weighted l 1 -norm of suitable frame
	coefficients. Thresholded Landweber iterations were proposed for
	the solution of the variational problem. Despite its simplicity which
	makes it very attractive to users, this algorithm converges slowly.
	In this paper, we investigate methods to accelerate significantly
	the convergence. We introduce and analyze sequential and parallel
	iterative algorithms based on alternating subspace corrections for
	the solution of the linear inverse problem with sparsity constraints.
	We prove their norm convergence to minimizers of the functional.
	We compare the computational cost and the behavior of these new algorithms
	with respect to the thresholded Landweber iterations.},
  timestamp = {2016-07-08T12:15:55Z},
  number = {6},
  journal = {Inverse Problems},
  author = {Fornasier, Massimo},
  year = {2007},
  keywords = {domain decomposition methods,Linear inverse problems,Sparsity,thresholded Landweber iterations,thresholded
	Landweber iterations},
  pages = {2505},
  owner = {Fardin}
}

@article{Foucart2010,
  title = {A note on guaranteed sparse recovery via $\ell_1$-minimization},
  volume = {29},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2009.10.004},
  abstract = {It is proved that every s-sparse vector x ∈ C N can be recovered
	from the measurement vector y = A x ∈ C m via l 1 -minimization
	as soon as the 2s-th restricted isometry constant of the matrix A
	is smaller than 3 / ( 4 + 6 ) ≈ 0.4652 , or smaller than 4 / (
	6 + 6 ) ≈ 0.4734 for large values of s.},
  timestamp = {2016-09-29T16:36:00Z},
  number = {1},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Foucart, Simon},
  year = {2010},
  keywords = {compressive sensing,l1-minimization,Restricted isometry constants},
  pages = {97--103},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520309001158},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520309001158},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520309001158},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520309001158},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520309001158},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520309001158},
  owner = {Fardin}
}

@article{Foucart2009a,
  title = {Sparsest solutions of underdetermined linear systems via $\ell_q 	$-minimization for $0 < q \leq 1$},
  volume = {26},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2008.09.001},
  abstract = {We present a condition on the matrix of an underdetermined linear
	system which guarantees that the solution of the system with minimal
	l q -quasinorm is also the sparsest one. This generalizes, and slightly
	improves, a similar result for the l 1 -norm. We then introduce a
	simple numerical scheme to compute solutions with minimal l q -quasinorm,
	and we study its convergence. Finally, we display the results of
	some experiments which indicate that the l q -method performs better
	than other available methods.},
  timestamp = {2016-09-29T16:34:37Z},
  number = {3},
  journal = {App. Comput. Harmonic Anal.},
  author = {Foucart, Simon and Lai, Ming-Jun},
  year = {2009},
  pages = {395--407},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882
	Applied and Computational Harmonic Analysis},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882
	Applied and Computational Harmonic Analysis},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882
	Applied and Computational Harmonic Analysis},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882
	Applied and Computational Harmonic Analysis},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882Applied
	and Computational Harmonic Analysis},
  annote = {read http://www.sciencedirect.com/science/article/pii/S1063520308000882	Applied and Computational Harmonic Analysis},
  owner = {Fardin}
}

@article{Friedlander2007,
  title = {Discussion: The Dantzig Selector: Statistical Estimation When p is 	Much Larger Than n},
  volume = {35},
  timestamp = {2016-07-08T12:13:23Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Friedlander, Michael P. and Saunders, Michael A.},
  year = {2007},
  pages = {2385�2391},
  owner = {afdidehf}
}

@article{Fuchs2005,
  title = {Recovery of exact sparse representations in the presence of bounded 	noise},
  volume = {51},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.855614},
  abstract = {The purpose of this contribution is to extend some recent results
	on sparse representations of signals in redundant bases developed
	in the noise-free case to the case of noisy observations. The type
	of question addressed so far is as follows: given an (n,m)-matrix
	A with m>n and a vector b=Axo, i.e., admitting a sparse representation
	xo, find a sufficient condition for b to have a unique sparsest representation.
	The answer is a bound on the number of nonzero entries in xo. We
	consider the case b=Axo+e where xo satisfies the sparsity conditions
	requested in the noise-free case and e is a vector of additive noise
	or modeling errors, and seek conditions under which xo can be recovered
	from b in a sense to be defined. The conditions we obtain relate
	the noise energy to the signal level as well as to a parameter of
	the quadratic program we use to recover the unknown sparsest representation.
	When the signal-to-noise ratio is large enough, all the components
	of the signal are still present when the noise is deleted; otherwise,
	the smallest components of the signal are themselves erased in a
	quite rational and predictable way},
  timestamp = {2016-09-30T11:09:28Z},
  number = {10},
  journal = {Information Theory, IEEE Transactions on},
  author = {Fuchs, J.J.},
  month = oct,
  year = {2005},
  keywords = {Acoustic materials,Additive noise,Approximation error,Basis pursuit,Basis
	Pursuit,Dictionaries,global matched filter,global
	matched filter,matched filters,matched
	filters,minimisation,mixed,Noise level,nonsmooth optimization,nonsmooth
	optimization,norm minimization,norm
	minimization,quadratic program,quadratic programming,redundant dictionaries,redundant
	dictionaries,signal representation,signal
	representation,Signal to noise ratio,smoothing methods,smoothing
	methods,sparse matrices,sparse matrix,sparse
	matrix,sparse representations,Speech processing,Speech
	processing,Sufficient conditions,time-frequency analysis,time-frequency
	analysis,Vectors},
  pages = {3601--3608},
  owner = {Fardin}
}

@article{Fuchs1998,
  title = {Improving source reconstructions by combining bioelectric and biomagnetic 	data},
  volume = {107},
  issn = {0013-4694},
  doi = {http://dx.doi.org/10.1016/S0013-4694(98)00046-7},
  abstract = {Objectives: A framework for combining bioelectric and biomagnetic
	data is presented. The data are transformed to signal-to-noise ratios
	and reconstruction algorithms utilizing a new regularization approach
	are introduced. Methods: Extensive simulations are carried out for
	19 different \{EEG\} and \{MEG\} montages with radial and tangential
	test dipoles at different eccentricities and noise levels. The methods
	are verified by real SEP/SEF measurements. A common realistic volume
	conductor is used and the less-well-known in-vivo conductivities
	are matched by calibration to the magnetic data. Single equivalent
	dipole fits as well as spatiotemporal source models are presented
	for single and combined modality evaluations and overlaid to anatomic
	\{MR\} images. Results: Normalized sensitivity and dipole resolution
	profiles of these acquisition systems are derived from these synthetic
	data. The methods are verified by simultaneously measured somasensory
	data. Conclusions: Superior spatial resolution of the combined data
	studies is revealed, which is due to the complementary nature of
	both modalities and the increased number of sensors. a better understanding
	of the underlying neironal processes can be acheived, since an improved
	differentiation between quasi-tangential and quasi-radial sources
	is possible.},
  timestamp = {2016-10-21T13:29:31Z},
  number = {2},
  journal = {Electroencephalography and Clinical Neurophysiology},
  author = {Fuchs, Manfred and Wagner, Michael and Wischmann, Hans-Aloys and Koöhler, Thomas and Theißen, Annette and Drenckhahn, Ralf and Buchner, Helmut},
  year = {1998},
  keywords = {Electroencephalogram},
  pages = {93--111},
  owner = {afdidehf}
}

@article{Galmarino1965,
  title = {Lp-spaces with mixed norm, for P a sequence},
  volume = {10},
  issn = {0022-247X},
  doi = {10.1016/0022-247X(65)90110-1},
  timestamp = {2016-09-30T11:02:44Z},
  number = {3},
  urldate = {2016-05-09},
  journal = {Journal of Mathematical Analysis and Applications},
  author = {Galmarino, Alberto Ra{\'u}l and Panzone, Rafael},
  month = jun,
  year = {1965},
  pages = {494-518},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HTFUPNEI\\0022247X65901101.html:text/html}
}

@article{Garg2011,
  title = {Block-sparse Solutions using Kernel Block RIP and its Application 	to Group Lasso},
  volume = {15},
  abstract = {We propose kernel block restricted isome- try property (KB-RIP) as
	a generalization of the well-studied RIP and prove a vari- ety of
	results. First, we present a �sum- of-norms�-minimization based
	formulation of the sparse recovery problem and prove that under suitable
	conditions on KB-RIP, it re- covers the optimal sparse solution exactly.
	The Group Lasso formulation, widely used as a good heuristic, arises
	naturally from the La- grangian relaxation of our formulation. We
	present an efficient combinatorial algorithm for provable sparse
	recovery under similar as- sumptions on KB-RIP. This result improves
	the previously known assumptions on RIP under which a combinatorial
	algorithm was known. Finally, we provide numerical evi- dence to
	illustrate that not only are our sum- of-norms-minimization formulation
	and com- binatorial algorithm significantly faster than Lasso, they
	also outperforms Lasso in terms of recovery.},
  timestamp = {2016-07-08T11:48:01Z},
  journal = {Proceedings of the 14th International Conference on Artificial Intelligence 	and Statistics (AISTATS)},
  author = {Garg, Rahul and Khandekar, Rohit},
  year = {2011},
  owner = {Fardin}
}

@techreport{Garrett2008,
  title = {$\ell^p$ with $0 < p < 1$ is not locally convex},
  timestamp = {2016-07-08T09:32:44Z},
  author = {Garrett, Paul},
  year = {2008},
  annote = {http://www.math.umn.edu/�garrett/},
  annote = {http://www.math.umn.edu/�garrett/},
  annote = {http://www.math.umn.edu/�garrett/},
  annote = {http://www.math.umn.edu/�garrett/},
  annote = {http://www.math.umn.edu/�garrett/},
  annote = {http://www.math.umn.edu/�garrett/},
  owner = {afdidehf}
}

@article{Ge2015,
  title = {A two-step super-Gaussian independent component analysis approach 	for fMRI data},
  volume = {118},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2015.05.088},
  abstract = {Abstract Independent component analysis (ICA) has been widely applied
	to functional magnetic resonance imaging (fMRI) data analysis. Although
	\{ICA\} assumes that the sources underlying data are statistically
	independent, it usually ignores sources’ additional properties,
	such as sparsity. In this study, we propose a two-step super-GaussianICA
	(2SGICA) method that incorporates the sparse prior of the sources
	into the \{ICA\} model. 2SGICA uses the super-Gaussian \{ICA\} (SGICA)
	algorithm that is based on a simplified Lewicki-Sejnowski’s model
	to obtain the initial source estimate in the first step. Using a
	kernel estimator technique, the source density is acquired and fitted
	to the Laplacian function based on the initial source estimates.
	The fitted Laplacian prior is used for each source at the second
	\{SGICA\} step. Moreover, the automatic target generation process
	for initial value generation is used in 2SGICA to guarantee the stability
	of the algorithm. An adaptive step size selection criterion is also
	implemented in the proposed algorithm. We performed experimental
	tests on both simulated data and real fMRI data to investigate the
	feasibility and robustness of 2SGICA and made a performance comparison
	between InfomaxICA, FastICA, mean field \{ICA\} (MFICA) with Laplacian
	prior, sparse online dictionary learning (ODL), \{SGICA\} and 2SGICA.
	Both simulated and real fMRI experiments showed that the 2SGICA was
	most robust to noises, and had the best spatial detection power and
	the time course estimation among the six methods.},
  timestamp = {2016-07-08T11:34:45Z},
  journal = {NeuroImage},
  author = {Ge, Ruiyang and Yao, Li and Zhang, Hang and Long, Zhiying},
  year = {2015},
  keywords = {ATGP,fMRI,ICA,Sparsity,Super-Gaussian},
  pages = {344 - 358},
  owner = {afdidehf}
}

@article{Geer2004,
  title = {High-dimensional data: $p \gg n$ in mathematical statistics and bio-medical 	applications},
  volume = {10},
  timestamp = {2016-09-29T14:53:23Z},
  number = {6},
  journal = {Bernoulli},
  author = {Geer, Sara A. Van De and Houwelingen, Hans C. Van},
  year = {2004},
  pages = {939--942},
  owner = {afdidehf}
}

@article{Gehre2014,
  title = {Expectation propagation for nonlinear inverse problems – with an 	application to electrical impedance tomography},
  volume = {259},
  issn = {0021-9991},
  doi = {http://dx.doi.org/10.1016/j.jcp.2013.12.010},
  abstract = {Abstract In this paper, we study a fast approximate inference method
	based on expectation propagation for exploring the posterior probability
	distribution arising from the Bayesian formulation of nonlinear inverse
	problems. It is capable of efficiently delivering reliable estimates
	of the posterior mean and covariance, thereby providing an inverse
	solution together with quantified uncertainties. Some theoretical
	properties of the iterative algorithm are discussed, and the efficient
	implementation for an important class of problems of projection type
	is described. The method is illustrated with one typical nonlinear
	inverse problem, electrical impedance tomography with complete electrode
	model, under sparsity constraints. Numerical results for real experimental
	data are presented, and compared with that by Markov chain Monte
	Carlo. The results indicate that the method is accurate and computationally
	very efficient.},
  timestamp = {2016-07-08T12:27:11Z},
  journal = {Journal of Computational Physics},
  author = {Gehre, Matthias and Jin, Bangti},
  year = {2014},
  keywords = {constraints,Electrical,Expectation,impedance,Inverse,Nonlinear,problem,propagation,quantification,Sparsity,Tomography,Uncertainty},
  pages = {513 - 535},
  owner = {afdidehf}
}

@article{Gemmeke2008,
  title = {Noise robust digit recognition using sparse representations},
  abstract = {Despite the use of noise robustness techniques, automatic speech recognition
	(ASR) systems make many more recognition errors than humans, especially
	in very noisy circumstances. We argue that this inferior recognition
	performance is largely due to the fact that in ASR speech is typically
	processed on a frameby-frame basis preventing the redundancy in the
	speech signal to be optimally exploited. We present a novel non-parametric
	classification method that can handle missing data while simultaneously
	exploiting the dependencies between the reliable features in an entire
	word. We compare the new method with a state-of-the-art HMM-based
	speech decoder in which missing data are imputed on a frame-by-frame
	basis. Both methods are tested on a single digit recognition task
	(based on AURORA-2 data) using an oracle and an estimated harmonicity
	mask. We show that at an SNR of-5 dB using the reliable features
	of an entire word allows an accuracy of 91 \% (using mel-log-energy
	features in combination with an oracle mask), while a conventional
	frame-based approach achieves only 61\%. Results obtained with the
	harmonicity mask suggest that this specific mask estimation technique
	is simply unable to deliver sufficient reliable features for acceptable
	recognition rates at these low SNRs.},
  timestamp = {2016-07-10T06:55:11Z},
  journal = {ISCA ITRW},
  author = {Gemmeke, J. F. and Cranen, B.},
  year = {2008}
}

@inproceedings{Gemmeke2008a,
  title = {Using sparse representations for missing data imputation in noise 	robust speech recognition},
  abstract = {Noise robustness of automatic speech recognition benefits from using
	missing data imputation: Prior to recognition the parts of the spectrogram
	dominated by noise are replaced by clean speech estimates. Especially
	at low SNRs each frame contains at best only a few uncorrupted coefficients.
	This makes frame-by-frame restoration of corrupted feature vectors
	error-prone, and recognition accuracy will mostly be sub-optimal.
	In this paper we present a novel imputation technique working on
	entire words. A word is sparsely represented in an overcomplete basis
	of exemplar (clean) speech signals using only the uncorrupted time-frequency
	elements of the word. The corrupted elements are replaced by estimates
	obtained by projecting the sparse representation in the basis. We
	achieve recognition accuracies of 92\% at SNR -5 dB using oracle
	masks on AURORA-2 as compared to 61\% using a conventional frame-based
	approach. The performance obtained with estimated masks can be directly
	related to the proportion of correctly identified uncorrupted coefficients.},
  timestamp = {2016-09-30T11:47:19Z},
  booktitle = {Signal Processing Conference, 2008 16th European},
  author = {Gemmeke, J. F. and Cranen, B.},
  month = aug,
  year = {2008},
  keywords = {Accuracy,AURORA-2,clean speech estimation,corrupted feature vector
	error-prone,exemplar speech signals,frame-by-frame restoration,missing
	data imputation,noise robust automatic speech recognition accuracy,oracle
	masks,Reliability,signal restoration,Signal to noise ratio,SNR,sparse
	representations,spectrogram,Speech,speech recognition,time-frequency
	analysis,uncorrupted time-frequency elements,Vectors},
  pages = {1--5}
}

@article{Giryes2015a,
  title = {Near oracle performance and block analysis of signal space greedy 	methods},
  volume = {194},
  issn = {0021-9045},
  doi = {http://dx.doi.org/10.1016/j.jat.2015.02.007},
  abstract = {Abstract Compressive sampling (CoSa) is a new methodology which demonstrates
	that sparse signals can be recovered from a small number of linear
	measurements. Greedy algorithms like CoSaMP have been designed for
	this recovery, and variants of these methods have been adapted to
	the case where sparsity is with respect to some arbitrary dictionary
	rather than an orthonormal basis. In this work we present an analysis
	of the so-called Signal Space CoSaMP method when the measurements
	are corrupted with mean-zero white Gaussian noise. We establish near-oracle
	performance for recovery of signals sparse in some arbitrary dictionary.
	In addition, we analyze the block variant of the method for signals
	whose supports obey a block structure, extending the method into
	the model-based compressed sensing framework. Numerical experiments
	confirm that the block method significantly outperforms the standard
	method in these settings.},
  timestamp = {2016-07-10T06:48:32Z},
  journal = {Journal of Approximation Theory},
  author = {Giryes, Raja and Needell, Deanna},
  year = {2015},
  keywords = {block sparsity,Coherent dictionaries,compressed sensing,restricted isometry property,restricted
	isometry property,Signal space methods,sparse approximation},
  pages = {157 - 174},
  owner = {afdidehf}
}

@inproceedings{Golato2014,
  title = {Multimodal sparse reconstruction in Lamb wave-based structural health 	monitoring},
  volume = {9109},
  abstract = {Lamb waves are utilized extensively for structural health monitoring
	of thin structures, such as plates and shells. Normal practice involves
	fixing a network of piezoelectric transducers to the structural plate
	member for generating and receiving Lamb waves. Using the transducers
	in pitch-catch pairs, the scattered signals from defects in the plate
	can be recorded. In this paper, we propose an l1-norm minimization
	approach for localizing defects in thin plates, which inverts a multimodal
	Lamb wave based model through exploitation of the sparseness of the
	defects. We consider both symmetric and anti-symmetric fundamental
	propagating Lamb modes. We construct model-based dictionaries for
	each mode, taking into account the associated dispersion and attenuation
	through the medium. Reconstruction of the area being interrogated
	is then performed jointly across the two modes using the group sparsity
	constraint. Performance validation of the proposed defect localization
	scheme is provided using simulated data for an aluminum plate.},
  timestamp = {2016-07-09T20:16:23Z},
  booktitle = {Proceedings of SPIE},
  author = {Golato, A. and Santhanam, S. and Ahmad, F. and Amin, M. G.},
  month = may,
  year = {2014},
  keywords = {compressed sensing,Lamb waves,multimodal,Sparse reconstruction,structural health monitoring,structural
	health monitoring},
  owner = {Fardin}
}

@inproceedings{Gong2011,
  title = {Structured sparsity preserving projections for radio transmitter 	recognition},
  abstract = {Bispectrum is an effective measurement to the intrinsic stray feature
	of radio transmitters, however, it is not feasible to directly exploit
	bispectra for radio transmitter recognition because of the high dimensionality.
	In this paper, a novel dimensionality reduction method named structured
	sparsity preserving projections (SSPP) is proposed for feature extraction.
	SSPP captures the structured sparse reconstructive relationship among
	data samples based on a structured regularization and then projects
	the data samples to a low-dimensional subspace with the relationship
	best preserved. SSPP naturally uses the class label information and
	sub-block information of the samples to improve the generalization
	capability. Experimental results on 10 MSK modulation radios, including
	closed-set test (target identification) and open set test (target
	verification), show that SSPP outperforms integral bispectra+PCA,
	LPP, SPP for radio transmitter feature extraction.},
  timestamp = {2016-07-11T16:58:34Z},
  booktitle = {Mobile IT Convergence (ICMIC), 2011 International Conference on},
  author = {Gong, Yong and Hu, Guyu and Pan, Zhisong},
  month = sep,
  year = {2011},
  keywords = {bispectrum,closed-set test,dimensionality reduction method,dimensionality
	reduction method,feature extraction,Feature
	Extraction,integral bispectra,Lasso,low-dimensional
	subspace,minimum shift keying,MSK modulation radios,open set test,PCA,Principal component analysis,Principal
	component analysis,radio transmitter recognition,radio transmitters,radio
	transmitters,structured sparsity
	preserving projections,structured
	sparsity preserving projections,Transmitters},
  pages = {68-73},
  owner = {afdidehf}
}

@article{Goswami2015,
  title = {Group sparse representation based classification for multi-feature 	multimodal biometrics},
  issn = {1566-2535},
  doi = {http://dx.doi.org/10.1016/j.inffus.2015.06.007},
  abstract = {Abstract Multimodal biometrics technology consolidates information
	obtained from multiple sources at sensor level, feature level, match
	score level, and decision level. It is used to increase robustness
	and provide broader population coverage for inclusion. Due to the
	inherent challenges involved with feature-level fusion, combining
	multiple evidences is attempted at score, rank, or decision level
	where only a minimal amount of information is preserved. In this
	paper, we propose the Group Sparse Representation based Classifier
	(GSRC) which removes the requirement for a separate feature-level
	fusion mechanism and integrates multi-feature representation seamlessly
	into classification. The performance of the proposed algorithm is
	evaluated on two multimodal biometric datasets. Experimental results
	indicate that the proposed classifier succeeds in efficiently utilizing
	a multi-feature representation of input data to perform accurate
	biometric recognition.},
  timestamp = {2016-07-08T12:40:33Z},
  journal = {Information Fusion},
  author = {Goswami, Gaurav and Mittal, Paritosh and Majumdar, Angshul and Vatsa, Mayank and Singh, Richa},
  year = {2015},
  keywords = {Biometrics,Feature-level,Fusion,multimodal,representation,Sparse},
  pages = {-},
  owner = {afdidehf}
}

@article{Gramfort2012,
  title = {Mixed-norm estimates for the M/EEG inverse problem using accelerated 	gradient methods},
  volume = {57},
  abstract = {Magneto- and electroencephalography (M/EEG) measure the electromagnetic
	fields produced by the neural electrical currents. Given a conductor
	model for the head, and the distribution of source currents in the
	brain, Maxwell�s equations allow one to compute the ensuing M/EEG
	signals. Given the actual M/EEG measurements and the solution of
	this forward problem, one can localize, in space and in time, the
	brain regions than have produced the recorded data. However, due
	to the physics of the problem, the limited number of sensors compared
	to the number of possible source locations, and measurement noise,
	this inverse problem is ill-posed. Consequently, additional constraints
	are needed. Classical inverse solvers, often called Minimum Norm
	Estimates (MNE), promote source estimates with a small ?2 norm. Here,
	we consider a more general class of priors based on mixed-norms.
	Such norms have the ability to structure the prior in order to incorporate
	some additional assumptions about the sources. We refer to such solvers
	as Mixed-Norm Estimates (MxNE). In the context of M/EEG, MxNE can
	promote spatially focal sources with smooth temporal estimates with
	a two-level ?1/?2 mixed-norm, while a three-level mixed-norm can
	be used to promote spatially non-overlapping sources between different
	experimental conditions. In order to efficiently solve the optimization
	problems of MxNE, we introduce fast first-order iterative schemes
	that for the ?1/?2 norm give solutions in a few seconds making such
	a prior as convenient as the simple MNE. Furhermore, thanks to the
	convexity of the optimization problem, we can provide optimality
	conditions that guarantee global convergence. The utility of the
	methods is demonstrated both with simulations and experimental MEG
	data.},
  timestamp = {2016-07-09T20:12:13Z},
  number = {7},
  journal = {Phys. Med. Biol.},
  author = {Gramfort, A. and Kowalski, M. and H�m�l�inen, M.},
  month = apr,
  year = {2012},
  note = {read},
  keywords = {convex optimization,convex
	optimization,electroencephalography,functional brain imaging,functional brain
	imaging,inverse problem,inverse
	problem,magnetoencephalography,mixed-norms,Structured Sparsity},
  pages = {1937-1961},
  owner = {Fardin}
}

@article{Gramfort2011,
  title = {Tracking cortical activity from M/EEG using graph cuts with spatiotemporal 	constraints},
  volume = {54},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.09.087},
  abstract = {This work proposes to use magnetoencephalography (MEG) and electroencephalography
	(EEG) source imaging to provide cinematic representations of the
	temporal dynamics of cortical activations. Cortical activation maps,
	seen as images of the active brain, are scalar maps defined at the
	vertices of a triangulated cortical surface. They can be computed
	from M/EEG data using a linear inverse solver every millisecond.
	Taking as input these activation maps and exploiting both the graph
	structure of the cortical mesh and the high sampling rate of M/EEG
	recordings, neural activations are tracked over time using an efficient
	graph cut based algorithm. The method estimates the spatiotemporal
	support of the active brain regions. It consists in computing a minimum
	cut on a particularly designed weighted graph imposing spatiotemporal
	regularity constraints on the activation patterns. Each node of the
	graph is assigned a label (active or non active). The method works
	globally on the full time-period of interest, can cope with spatially
	extended active regions and allows the active domain to exhibit topology
	changes over time. The algorithm is illustrated and validated on
	synthetic data. Results of the method are provided on two \{MEG\}
	cognitive experiments in the visual and somatosensory cortices, demonstrating
	the ability of the algorithm to handle various types of data.},
  timestamp = {2016-07-11T17:09:00Z},
  number = {3},
  journal = {NeuroImage},
  author = {Gramfort, Alexandre and Papadopoulo, Theodore and Baillet, Sylvain and Clerc, Maureen},
  year = {2011},
  keywords = {Electroencephalography (EEG),functional brain imaging,Graph cut optimization,Magnetoencephalography
	(MEG),Tracking},
  pages = {1930 - 1941},
  owner = {Fardin}
}

@incollection{Gramfort2011c,
  series = {Lecture Notes in Computer Science},
  title = {Functional Brain Imaging with M/EEG Using Structured Sparsity in Time-Frequency Dictionaries},
  copyright = {\textcopyright{}2011 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-22091-3 978-3-642-22092-0},
  abstract = {Magnetoencephalography (MEG) and electroencephalography (EEG) allow
	functional brain imaging with high temporal resolution. While time-frequency
	analysis is often used in the field, it is not commonly employed
	in the context of the ill-posed inverse problem that maps the MEG
	and EEG measurements to the source space in the brain. In this work,
	we detail how convex structured sparsity can be exploited to achieve
	a principled and more accurate functional imaging approach. Importantly,
	time-frequency dictionaries can capture the non-stationary nature
	of brain signals and state-of-the-art convex optimization procedures
	based on proximal operators allow the derivation of a fast estimation
	algorithm. We compare the accuracy of our new method to recently
	proposed inverse solvers with help of simulations and analysis of
	real MEG data.},
  language = {en},
  timestamp = {2016-07-08T12:37:54Z},
  number = {6801},
  urldate = {2016-04-05},
  booktitle = {Information {{Processing}} in {{Medical Imaging}}},
  publisher = {{\{$\backslash$\{Springer Berlin Heidelberg$\backslash$\}\}}},
  author = {Gramfort, Alexandre and Strohmeier, Daniel and Haueisen, Jens and Hamalainen, Matti and Kowalski, Matthieu},
  editor = {Sz{\'e}kely, G{\'a}bor and Hahn, Horst K.},
  month = jul,
  year = {2011},
  keywords = {Artificial Intelligence (incl. Robotics),Artificial Intelligence (incl. Robotics),computer graphics,Computer Graphics,Health Informatics,Health
	Informatics,Health Informatics,Image Processing and Computer Vision,Image Processing and Computer Vision,Imaging / Radiology,Imaging / Radiology,Pattern Recognition,Pattern
	Recognition,Pattern Recognition},
  pages = {600-611},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JTB3F5Q9\\978-3-642-22092-0_49.html:text/html}
}

@article{Gramfort2013a,
  title = {Time-frequency mixed-norm estimates: Sparse M/EEG imaging with non-stationary 	source activations},
  volume = {70},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2012.12.051},
  abstract = {Abstract Magnetoencephalography (MEG) and electroencephalography (EEG)
	allow functional brain imaging with high temporal resolution. While
	solving the inverse problem independently at every time point can
	give an image of the active brain at every millisecond, such a procedure
	does not capitalize on the temporal dynamics of the signal. Linear
	inverse methods (minimum-norm, dSPM, sLORETA, beamformers) typically
	assume that the signal is stationary: regularization parameter and
	data covariance are independent of time and the time varying signal-to-noise
	ratio (SNR). Other recently proposed non-linear inverse solvers promoting
	focal activations estimate the sources in both space and time while
	also assuming stationary sources during a time interval. However
	such a hypothesis holds only for short time intervals. To overcome
	this limitation, we propose time-frequency mixed-norm estimates (TF-MxNE),
	which use time-frequency analysis to regularize the ill-posed inverse
	problem. This method makes use of structured sparse priors defined
	in the time-frequency domain, offering more accurate estimates by
	capturing the non-stationary and transient nature of brain signals.
	State-of-the-art convex optimization procedures based on proximal
	operators are employed, allowing the derivation of a fast estimation
	algorithm. The accuracy of the TF-MxNE is compared with recently
	proposed inverse solvers with help of simulations and by analyzing
	publicly available \{MEG\} datasets.},
  timestamp = {2016-07-11T17:06:55Z},
  number = {0},
  journal = {NeuroImage},
  author = {Gramfort, A. and Strohmeier, D. and Haueisen, J. and H�m�l�inen, M. S. and Kowalski, M.},
  month = apr,
  year = {2013},
  note = {read},
  keywords = {Inverse,problem},
  pages = {410 - 422},
  owner = {Fardin}
}

@article{Gribonval2007,
  title = {Highly sparse representations from dictionaries are unique and independent 	of the sparseness measure},
  volume = {22},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2006.09.003},
  abstract = {The purpose of this paper is to study sparse representations of signals
	from a general dictionary in a Banach space. For so-called localized
	frames in Hilbert spaces, the canonical frame coefficients are shown
	to provide a near sparsest expansion for several sparseness measures.
	However, for frames which are not localized, this no longer holds
	true and sparse representations may depend strongly on the choice
	of the sparseness measure. A large class of admissible sparseness
	measures is introduced, and we give sufficient conditions for having
	a unique sparse representation of a signal from the dictionary w.r.t.
	such a sparseness measure. Moreover, we give sufficient conditions
	on a signal such that the simple solution of a linear programming
	problem simultaneously solves all the nonconvex (and generally hard
	combinatorial) problems of sparsest representation of the signal
	w.r.t. arbitrary admissible sparseness measures.},
  timestamp = {2017-04-19T15:29:43Z},
  number = {3},
  journal = {Appl. Comput. Harmonic Anal.},
  author = {Gribonval, R. and Nielsen, M.},
  year = {2007},
  keywords = {incoherent dictionary,linear programming,Localized frame,Localized
	frame,nonconvex optimization,nonconvex
	optimization,redundant dictionary,Sparseness measure,sparse representation},
  pages = {335--355},
  annote = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  annote = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  annote = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  annote = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  annote = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  annote = {http://www.sciencedirect.com/science/article/pii/S1063520306001175},
  annote = {read},
  owner = {Fardin}
}

@techreport{Gribonval2005a,
  title = {Beyond sparsity recovering structured representations by l1 minimization 	and greedy algorithms. - Application to the Analysis of Sparse underdetermined 	ICA-},
  timestamp = {2016-07-08T11:37:39Z},
  author = {Gribonval, R. and Nielsen, M.},
  year = {2005},
  owner = {Fardin}
}

@article{Grova2006,
  title = {Evaluation of EEG localization methods using realistic simulations 	of interictal spikes},
  volume = {29},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.08.053},
  abstract = {Performing an accurate localization of sources of interictal spikes
	from \{EEG\} scalp measurements is of particular interest during
	the presurgical investigation of epilepsy. The purpose of this paper
	is to study the ability of six distributed source localization methods
	to recover extended sources of activated cortex. Due to the frequent
	lack of a gold standard to evaluate source localization methods,
	our evaluation was performed in a controlled environment using realistic
	simulations of \{EEG\} interictal spikes, involving several anatomical
	locations with several spatial extents. Simulated data were corrupted
	by physiological \{EEG\} noise. Simulations involving pairs of sources
	with the same amplitude were also studied. In addition to standard
	validation criteria (e.g., geodesic distance or mean square error),
	we proposed an original criterion dedicated to assess detection accuracy,
	based on receiver operating characteristic (ROC) analysis. Six source
	localization methods were evaluated: the minimum norm, the minimum
	norm weighted by multivariate source prelocalization (MSP), cortical
	\{LORETA\} with or without additional minimum norm regularization,
	and two derivations of the maximum entropy on the mean (MEM) approach.
	Results showed that LORETA-based and MEM-based methods were able
	to accurately recover sources of different spatial extents, with
	the exception of sources in temporo-mesial and fronto-mesial regions.
	Several spurious sources were generated by those methods, however,
	whereas methods using the \{MSP\} always located very accurately
	the maximum of activity but not its spatial extent. These findings
	suggest that one should always take into account the results from
	different localization methods when analyzing real interictal spikes.},
  timestamp = {2016-07-08T12:26:27Z},
  number = {3},
  journal = {NeuroImage},
  author = {Grova, C. and Daunizeau, J. and Lina, J.-M. and Bénar, C. G. and Benali, H. and Gotman, J.},
  year = {2006},
  keywords = {EEG},
  pages = {734 - 753},
  owner = {afdidehf}
}

@article{Hadamard1902,
  title = {Sur les probl{\`e}mes aux d{\'e}riv{\'e}s partielles et leur signification 	physique},
  volume = {13},
  timestamp = {2017-04-19T14:24:06Z},
  journal = {Princeton Univ.},
  author = {Hadamard, Jacques},
  year = {1902},
  keywords = {Ill-posed,problem},
  pages = {49--52}
}

@article{Han2015,
  title = {MR image reconstruction with block sparsity and iterative support 	detection},
  volume = {33},
  issn = {0730-725X},
  doi = {http://dx.doi.org/10.1016/j.mri.2015.01.011},
  abstract = {Abstract This work aims to develop a novel magnetic resonance (MR)
	image reconstruction approach motivated by the recently proposed
	sampling framework with union-of-subspaces model (SUoS). Based on
	SUoS, we propose a mathematical formalism that effectively integrates
	a block sparsity constraint and support information which is estimated
	in an iterative fashion. The resulting optimization problem consists
	of a data fidelity term and a support detection based block sparsity
	(SDBS) promoting term penalizing entries within the complement of
	the estimated support. We provide optional strategies for block assignment,
	and we also derive unique and robust recovery conditions in terms
	of the structured restricted isometric property (RIP), namely the
	block-RIP. The block-RIP constant we derive is lower than that of
	the previous structured sparse method, which leads to a reduction
	of the measurements. Simulation results for reconstructing individual
	and multiple T1/T2-weighted images demonstrate the consistency with
	our theoretical claims, and show considerable improvement in comparison
	with methods using only block sparsity or support information.},
  timestamp = {2016-07-09T20:15:20Z},
  number = {5},
  journal = {Magnetic Resonance Imaging},
  author = {Han, Yu and Du, Huiqian and Mei, Wenbo and Fang, Liping},
  year = {2015},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(MR\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(MR\\\\\\\\\\\\\\\\,\\\\\\\\(MR\\\\\\\\,\\\\(MR\\\\,\\(MR\\,Block,detection,Image,Iterative,reconstruction,Sparsity,support,Union-of-subspaces},
  pages = {624 - 634},
  owner = {afdidehf}
}

@book{Hara2011,
  edition = {1},
  series = {Lecture Notes in Computer Science 6912 Lecture notes in artifical 	intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, 	Part II},
  isbn = {978-3-642-23782-9 3-642-23782-7 978-3-642-23779-9 3-642-23779-7 978-3-642-23807-9 3-642-23807-6},
  timestamp = {2016-10-24T16:40:57Z},
  publisher = {{Springer}},
  author = {Hara, Satoshi and Washio, Takashi},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  owner = {Fardin}
}

@article{Harley2013,
  title = {Sparse recovery of the multimodal and dispersive characteristics 	of Lamb waves},
  volume = {133},
  abstract = {Guided waves in plates, known as Lamb waves, are characterized by
	complex, multimodal, and frequency dispersive wave propagation, which
	distort signals and make their analysis difficult. Estimating these
	multimodal and dispersive characteristics from experimental data
	becomes a difficult, underdetermined inverse problem. To accurately
	and robustly recover these multimodal and dispersive properties,
	this paper presents a methodology referred to as sparse wavenumber
	analysis based on sparse recovery methods. By utilizing a general
	model for Lamb waves, waves propagating in a plate structure, and
	robust �1 optimization strategies, sparse wavenumber analysis accurately
	recovers the Lamb wave�s frequency-wavenumber representation with
	a limited number of surface mounted transducers. This is demonstrated
	with both simulated and experimental data in the presence of multipath
	reflections. With accurate frequency-wavenumber representations,
	sparse wavenumber synthesis is then used to accurately remove multipath
	interference in each measurement and predict the responses between
	arbitrary points on a plate.},
  timestamp = {2016-07-11T16:51:17Z},
  number = {5},
  journal = {Journal of Acoustical Society of America},
  author = {Harley, Joel B. and Moura, Jose M. F.},
  month = may,
  year = {2013},
  pages = {2732 - 2745},
  owner = {Fardin}
}

@article{Hauk2011,
  title = {Comparison of noise-normalized minimum norm estimates for MEG analysis 	using multiple resolution metrics},
  volume = {54},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.09.053},
  abstract = {Noise-normalization has been shown to partly compensate for the localization
	bias towards superficial sources in minimum norm estimation. However,
	it has been argued that in order to make inferences for the case
	of multiple sources, localization properties alone are insufficient.
	Instead, multiple measures of resolution should be applied to both
	point-spread and cross-talk functions (PSFs and CTFs). Here, we demonstrate
	that noise-normalization affects the shapes of PSFs, but not of CTFs.
	We evaluated \{PSFs\} and \{CTFs\} for the MNE, dSPM and sLORETA
	inverse operators, on the metrics dipole localization error (DLE),
	spatial dispersion (SD) and overall amplitude (OA). We used 306-channel
	\{MEG\} configurations obtained from 17 subjects in a real experiment,
	including individual noise covariance matrices and head geometries.
	We confirmed that for \{PSFs\} \{DLE\} improved after noise normalization,
	and is zero for sLORETA. However, \{SD\} was generally lower for
	the unnormalized MNE. \{OA\} distributions were similar for all three
	methods, indicating that all three methods may greatly underestimate
	some sources relative to others. The reliability of differences between
	methods across subjects was demonstrated using distributions of standard
	deviations and p-values from paired t-tests. As predicted, the shapes
	of \{CTFs\} were the same for all methods, reflecting the general
	resolution limits of the inverse problem. This means that noise-normalization
	is of no consequence where linear estimation procedures are used
	as “spatial filters.�? While low \{DLE\} is advantageous for
	the localization of a single source, or possibly a few spatially
	distinct sources, the benefit for the case of complex source distributions
	is not obvious. We suggest that software packages for source estimation
	should include comprehensive tools for evaluating the performance
	of different methods.},
  timestamp = {2016-07-08T11:54:16Z},
  number = {3},
  journal = {NeuroImage},
  author = {Hauk, Olaf and Wakeman, Daniel G. and Henson, Richard},
  year = {2011},
  keywords = {Amplitude,inverse problem,Localization error,source analysis,Spatial dispersion,Spatial
	dispersion},
  pages = {1966 - 1974},
  owner = {afdidehf}
}

@article{Henson2009,
  title = {MEG and EEG data fusion: Simultaneous localisation of face-evoked 	responses},
  volume = {47},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2009.04.063},
  abstract = {We present an empirical Bayesian scheme for distributed multimodal
	inversion of electromagnetic forward models of \{EEG\} and \{MEG\}
	signals. We used a generative model with common source activity and
	separate error components for each modality. Under this scheme, the
	weightings of error for each modality, relative to source components,
	are estimated automatically from the data, by optimising the model-evidence.
	This obviates the need for arbitrary user-defined weightings. To
	evaluate the scheme, we acquired three types of data simultaneously
	from twelve participants: total magnetic flux (as recorded by 102
	magnetometers), orthogonal in-plane gradients of the magnetic field
	(as recorded by 204 planar gradiometers) and voltage differences
	in the electrical field (recorded by 70 electrodes). We assessed
	the relative precision of each sensor-type in terms of signal-to-noise
	ratio (SNR); using empirical sample variances and optimised estimators
	from the generative model. We then compared the localisation of face-evoked
	responses, using each modality separately, with that obtained by
	their “fusion�? under the common generative model. Finally, we
	quantified the conditional precisions of the source estimates using
	their posterior covariance, confirming that \{EEG\} can improve MEG-based
	source reconstructions.},
  timestamp = {2016-10-21T13:43:29Z},
  number = {2},
  journal = {NeuroImage},
  author = {Henson, Richard N. and Mouchlianitis, Elias and Friston, Karl J.},
  year = {2009},
  keywords = {Inverse,problem},
  pages = {581--589},
  owner = {Fardin}
}

@article{Hsiao2013,
  title = {Neural correlates of somatosensory paired-pulse suppression: A MEG 	study using distributed source modeling and dynamic spectral power 	analysis},
  volume = {72},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2013.01.041},
  abstract = {Abstract Paired-pulse stimulation has been used previously to evaluate
	cortical excitability and sensory gating. To help elucidate the neural
	network involved in paired-pulse suppression of somatosensory cortical
	processing, magnetoencephalographic (MEG) responses to paired-pulse
	electrical stimulation of the left median nerve of the wrists of
	13 healthy males were recorded using an intra-pair interstimulus
	interval (ISI) of 500 ms and an inter-pair \{ISI\} of 8 s. Minimum
	norm estimates showed the presence of cortical activation in the
	bilateral primary somatosensory cortex, the post-central sulcus and
	the supplementary motor areas. Compared with the responses to the
	first stimulation, the responses to the second stimulation were attenuated
	in these areas with gating ratios (the amplitude ratios of the second
	response to the first response) of 0.54–0.69. By spectral power
	dynamic analysis, beta frequency oscillations were found to be associated
	with an early-latency (30–36 ms) gating process in the contralateral
	primary somatosensory cortex and post-central sulcus, whereas theta
	and alpha oscillations were correlated with paired-pulse suppression
	of activations at 98–136 ms in the ipsilateral primary somatosensory
	cortex, the bilateral post-central sulcus and the supplementary motor
	areas. In summary, it can be concluded that differential oscillatory
	activities are involved in the pair-pulse suppression in various
	somatosensory regions in response to repetitive external stimulations.},
  timestamp = {2016-07-10T06:49:29Z},
  journal = {NeuroImage},
  author = {Hsiao, Fu-Jung and Cheng, Chia-Hsiung and Chen, Wei-Ta and Lin, Yung-Yang},
  year = {2013},
  keywords = {magnetoencephalography,Minimum norm estimates,Paired-pulse suppression,Somatosensory,Spectral
	power},
  pages = {133 - 142},
  owner = {afdidehf}
}

@article{Huang2014,
  title = {Comparison of sparse recovery algorithms for channel estimation in 	underwater acoustic OFDM with data-driven sparsity learning},
  volume = {13, Part C},
  issn = {1874-4907},
  doi = {http://dx.doi.org/10.1016/j.phycom.2014.08.001},
  abstract = {Abstract Through exploiting the sparse nature of underwater acoustic
	(UWA) channels, compressed sensing (CS) based sparse channel estimation
	has demonstrated superior performance compared to the conventional
	least-squares (LS) method. However, a priori information of channel
	sparsity is often required to set a regularization constraint. In
	this work, we propose a data-driven sparsity learning approach based
	on a linear minimum mean square error (LMMSE) equalizer to tune the
	regularization parameter for the orthogonal frequency division multiplexing
	(OFDM) transmissions. A golden section search is used to accelerate
	the sparsity learning process. In the context of the intercarrier
	interference (ICI)-ignorant and ICI-aware \{UWA\} \{OFDM\} systems,
	the block error rates (BLERs) using different sparse recovery algorithms
	for channel estimation under the L 0 , L 1 / 2 , L 1 , and L 2 constraints
	are compared. Simulation and experimental results show that the data-driven
	sparsity learning approach is effective, overcoming the drawback
	of using a fixed regularization parameter in different channel conditions.
	When the sparsity parameter for each approach is optimized based
	on the data-driven approach, the L 1 / 2 recovery algorithm and the
	considered four L 1 recovery algorithms: SpaRSA, FISTA, Nesterov,
	and TwIST, have nearly the same \{BLER\} performance, outperforming
	L 0 and L 2 algorithms.},
  timestamp = {2016-07-08T11:54:38Z},
  journal = {Physical Communication},
  author = {Huang, Yi and Wan, Lei and Zhou, Shengli and Wang, Zhaohui and Huang, Jianzhong},
  year = {2014},
  keywords = {acoustic,channel,communication,Compressed,Data-driven,Estimation,learning,OFDM,Parameter,regularization,sensing,Sparse,Sparsity,Underwater},
  pages = {156 - 167},
  owner = {afdidehf}
}

@techreport{XXX2016,
  title = {The $L^p$ spaces ($1 \leq p < \infty$)},
  timestamp = {2016-07-11T16:59:33Z},
  author = {{XXX}},
  year = {2016},
  owner = {afdidehf}
}

@book{Hamalainen2009,
  timestamp = {2016-05-26T09:45:41Z},
  author = {H{\"a}m{\"a}l{\"a}inen, Matti},
  year = {2009},
  owner = {Fardin}
}

@article{Hamalainen2007,
  title = {Introduction to Source Estimation: Sources of MEG and EEG Signals 	and Dipole Models},
  timestamp = {2016-07-08T12:50:50Z},
  author = {H{\"a}m{\"a}l{\"a}inen, Matti},
  year = {2007},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {MGH/MIT/HMS Athinoula A. Martinos Center for Biomedical Imaging},
  owner = {Fardin}
}

@phdthesis{Jin2011,
  title = {Algorithm development for sparse signal recovery and performance 	limits using multiple-user information theory},
  timestamp = {2016-07-08T10:19:00Z},
  school = {UC San Diego},
  author = {Jin, Yuzhe},
  year = {2011},
  owner = {Fardin}
}

@techreport{Johnson2002,
  title = {MATLAB Programming Style Guidelines},
  timestamp = {2016-07-09T20:08:10Z},
  author = {Johnson, Richard},
  month = oct,
  year = {2002},
  owner = {afdidehf}
}

@article{Juditsky2014,
  title = {On a unified view of nullspace-type conditions for recoveries associated 	with general sparsity structures},
  volume = {441},
  issn = {0024-3795},
  doi = {http://dx.doi.org/10.1016/j.laa.2013.07.025},
  abstract = {Abstract We discuss a general notion of “sparsity structure�?
	and associated recoveries of a sparse signal from its linear image
	of reduced dimension possibly corrupted with noise. Our approach
	allows for unified treatment of (a) the “usual sparsity�? and
	“usual l 1 recovery�?, (b) block-sparsity with possibly overlapping
	blocks and associated block- l 1 recovery, and (c) low-rank-oriented
	recovery by nuclear norm minimization. The proposed recovery routines
	are natural extensions of the usual l 1 minimization used in Compressed
	Sensing. Specifically, within this framework, we present nullspace-type
	sufficient conditions for the recovery to be precise on sparse signals
	in the noiseless case. Then we derive error bounds for imperfect
	(nearly sparse signal, presence of observation noise, etc.) recovery
	under these conditions. In all of these cases, we present efficiently
	verifiable sufficient conditions for the validity of the associated
	nullspace properties.},
  timestamp = {2016-07-10T07:11:37Z},
  journal = {Linear Algebra and its Applications},
  author = {Juditsky, Anatoli and Karzan, Fatma K?l?n� and Nemirovski, Arkadi},
  year = {2014},
  note = {Special Issue on Sparse Approximate Solution of Linear Systems},
  keywords = {Block-sparse,Low-rank,Matrix,Minimization,norm,Nuclear,Nullspace,property,recovery,Sparserecovery},
  pages = {124 - 151},
  owner = {afdidehf}
}

@techreport{Juditsky2010,
  title = {Verifiable conditions of $\ell_1$-recovery of sparse signals with 	sign restrictions},
  timestamp = {2016-07-11T17:11:31Z},
  author = {Juditsky, Anatoli and Nemirovski, Fatma Kilinc Karzan Arkadi},
  month = sep,
  year = {2010},
  owner = {afdidehf}
}

@article{Jun2008,
  title = {Bayesian brain source imaging based on combined MEG/EEG and fMRI 	using MCMC},
  volume = {40},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2007.12.029},
  abstract = {A number of brain imaging techniques have been developed in order
	to investigate brain function and to develop diagnostic tools for
	various brain disorders. Each modality has strengths as well as weaknesses
	compared to the others. Recent work has explored how multiple modalities
	can be integrated effectively so that they complement one another
	while maintaining their individual strengths. Bayesian inference
	employing Markov Chain Monte Carlo (MCMC) techniques provides a straightforward
	way to combine disparate forms of information while dealing with
	the uncertainty in each. In this paper we introduce methods of Bayesian
	inference as a way to integrate different forms of brain imaging
	data in a probabilistic framework. We formulate Bayesian integration
	of magnetoencephalography (MEG) data and functional magnetic resonance
	imaging (fMRI) data by incorporating fMRI data into a spatial prior.
	The usefulness and feasibility of the method are verified through
	testing with both simulated and empirical data.},
  timestamp = {2016-07-08T11:37:08Z},
  number = {4},
  journal = {NeuroImage},
  author = {Jun, Sung C. and George, John S. and Kim, Woohan and Pare-Blagoev, Juliana and Plis, Sergey and Ranken, Doug M. and Schmidt, David M.},
  year = {2008},
  pages = {1581-1594},
  owner = {Fardin}
}

@article{Kandel2015,
  title = {Eigenanatomy: Sparse dimensionality reduction for multi-modal medical 	image analysis},
  volume = {73},
  issn = {1046-2023},
  doi = {http://dx.doi.org/10.1016/j.ymeth.2014.10.016},
  abstract = {Abstract Rigorous statistical analysis of multimodal imaging datasets
	is challenging. Mass-univariate methods for extracting correlations
	between image voxels and outcome measurements are not ideal for multimodal
	datasets, as they do not account for interactions between the different
	modalities. The extremely high dimensionality of medical images necessitates
	dimensionality reduction, such as principal component analysis (PCA)
	or independent component analysis (ICA). These dimensionality reduction
	techniques, however, consist of contributions from every region in
	the brain and are therefore difficult to interpret. Recent advances
	in sparse dimensionality reduction have enabled construction of a
	set of image regions that explain the variance of the images while
	still maintaining anatomical interpretability. The projections of
	the original data on the sparse eigenvectors, however, are highly
	collinear and therefore difficult to incorporate into multi-modal
	image analysis pipelines. We propose here a method for clustering
	sparse eigenvectors and selecting a subset of the eigenvectors to
	make interpretable predictions from a multi-modal dataset. Evaluation
	on a publicly available dataset shows that the proposed method outperforms
	\{PCA\} and ICA-based regressions while still maintaining anatomical
	meaning. To facilitate reproducibility, the complete dataset used
	and all source code is publicly available.},
  timestamp = {2016-07-08T12:20:08Z},
  journal = {Methods},
  author = {Kandel, Benjamin M. and Wang, Danny J. J. and Gee, James C. and Avants, Brian B.},
  year = {2015},
  note = {Spatial mapping of multi-modal data in neuroscience},
  keywords = {Multi-modal},
  pages = {43 - 53},
  owner = {Fardin}
}

@article{Karahanoglu2016,
  title = {Improving A OMP: Theoretical and empirical analyses with a novel 	dynamic cost model},
  volume = {118},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2015.06.011},
  abstract = {Abstract Best-first search has been recently utilized for compressed
	sensing (CS) by the A ⋆ orthogonal matching pursuit ( A ⋆ \{OMP\}
	) algorithm. In this work, we concentrate on theoretical and empirical
	analyses of A ⋆ \{OMP\} . We present a restricted isometry property
	(RIP) based general condition for exact recovery of sparse signals
	via A ⋆ \{OMP\} . In addition, we develop online guarantees which
	promise improved recovery performance with the residue-based termination
	instead of the sparsity-based one. We demonstrate the recovery capabilities
	of A ⋆ \{OMP\} with extensive recovery simulations using the adaptive-multiplicative
	(AMul) cost model, which effectively compensates for the path length
	differences in the search tree. The presented results, involving
	phase transitions for different nonzero element distributions as
	well as recovery rates and average error, reveal not only the superior
	recovery accuracy of A ⋆ \{OMP\} , but also the improvements with
	the residue-based termination and the \{AMul\} cost model. Comparison
	of the run times indicates the speed up by the \{AMul\} cost model.
	We also demonstrate a hybrid of \{OMP\} and A ⋆ \{OMP\} to accelerate
	the search further. Finally, we run A ⋆ \{OMP\} on sparse images
	to illustrate its recovery performance for more realistic coefficient
	distributions.},
  timestamp = {2016-07-08T12:48:52Z},
  journal = {Signal Processing},
  author = {Karahanoglu, Nazim Burak and Erdogan, Hakan},
  year = {2016},
  keywords = {A?,Adaptive-multiplicativecostmodel,Compressed,matchingpursuit,Orthogonal,Restrictedisometryproperty,sensing},
  pages = {62 - 74},
  owner = {afdidehf}
}

@article{Khanna2015,
  title = {Consensus based Decentralized Sparse Bayesian Learning for Joint 	Sparse Signal Recovery},
  abstract = {This work proposes a decentralized, iterative, Bayesian algorithm
	called CB-DSBL for in-network estimation of multiple jointly sparse
	vectors by a network of nodes, using noisy and underdetermined linear
	measurements. The proposed algorithm exploits the network wide joint
	sparsity of the unknown sparse vectors to recover them from significantly
	fewer number of local measurements compared to standalone sparse
	signal recovery schemes. To reduce the amount of inter-node communication
	and the associated overheads, the nodes exchange messages with only
	a small subset of their single hop neighbors. Under this communication
	scheme, we separately analyze the convergence of the underlying Alternating
	Directions Method of Multipliers (ADMM) iterations used in our proposed
	algorithm and establish its linear convergence rate for a generic
	consensus based optimization. The findings from the convergence analysis
	of ADMM are used to optimize the convergence rate of the proposed
	CB-DSBL algorithm. Using Monte Carlo simulations, we demonstrate
	the superior signal reconstruction as well as support recovery performance
	of our proposed algorithm compared to existing decentralized algorithms:
	DRL-1, DCOMP and DCSP.},
  timestamp = {2016-07-08T12:03:02Z},
  journal = {xxxx},
  author = {Khanna, Saurabh and and, Chandra R. Murthy},
  month = jul,
  year = {2015},
  keywords = {Decentralized Estimation,Distributed Compressive Sensing,joint sparsity,Sensor Networks.,Sensor
	Networks.,sparse Bayesian learning},
  owner = {Fardin}
}

@mastersthesis{Kim2008,
  title = {Comparison of data-driven analysis methods for identification of 	functional connectivity in fMRI},
  abstract = {Data-driven analysis methods, such as independent component analysis
	(ICA) and clustering, have found a fruitful application in the analysis
	of functional magnetic resonance imaging (fMRI) data for identifying
	functionally connected brain networks. Unlike the traditional regression-based
	hypothesis-driven analysis methods, the principal advantage of data-driven
	methods is their applicability to experimental paradigms in the absence
	of a priori model of brain activity. Although ICA and clustering
	rely on very different assumptions on the underlying distributions,
	they produce surprisingly similar results for signals with large
	variation. The main goal of this thesis is to understand the factors
	that contribute to the differences in the identification of functional
	connectivity based on ICA and a more general version of clustering,
	Gaussian mixture model (GMM), and their relations. We provide a detailed
	empirical comparison of ICA and clustering based on GMM. We introduce
	a component-wise matching and comparison scheme of resulting ICA
	and GMM components based on their correlations. We apply this scheme
	to the synthetic fMRI data and investigate the influence of noise
	and length of time course on the performance of ICA and GMM, comparing
	with ground truth and with each other. For the real fMRI data, we
	propose a method of choosing a threshold to determine which of resulting
	components are meaningful to compare using the cumulative distribution
	function of their empirical correlations. In addition, we present
	an alternate method to model selection for selecting the optimal
	total number of components for ICA and GMM using the task-related
	and contrast functions. For extracting task-related components, we
	find that GMM outperforms ICA when the total number of components
	are less then ten and the performance between ICA and GMM is almost
	identical for larger numbers of the total components. Furthermore,
	we observe that about a third of the components of each model are
	meaningful to be compared to the components of the other.},
  timestamp = {2016-07-08T11:54:06Z},
  school = {Department of Electrical Engineering and Computer Science},
  author = {Kim, Yongwook Bryce},
  month = feb,
  year = {2008},
  owner = {Fardin}
}

@inproceedings{Kronvall2014,
  title = {Joint DOA and multi-pitch estimation via block sparse dictionary 	learning},
  abstract = {In this paper, we introduce a novel sparse method for joint estimation
	of the direction of arrivals (DOAs) and pitches of a set of multi-pitch
	signals impinging on a sensor array. Extending on earlier approaches,
	we formulate a novel dictionary learning framework from which an
	estimate is formed without making assumptions on the model orders.
	The proposed method alternatively uses a block sparse approach to
	estimate the pitches, using an alternating direction method of multipliers
	framework, and alternatively a nonlinear least squares approach to
	estimate the DOAs. The preferable performance of the proposed algorithm,
	as compared to earlier methods, is shown using numerical examples.},
  timestamp = {2016-07-09T19:49:18Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd 	European},
  author = {Kronvall, T. and Adalbjornsson, S.I. and Jakobsson, A.},
  month = sep,
  year = {2014},
  keywords = {Acoustics,ADMM,alternating direction method,Arrays,array signal processing,block sparse approach,block
	sparse approach,block sparse dictionary learning,block
	sparse dictionary learning,block sparsity,Dictionaries,dictionary
	learning,dictionary learning framework,direction-of-arrival,direction-of-arrival estimation,Direction-of-arrival
	estimation,Estimation,group sparsity,Harmonic analysis,Harmonic
	analysis,joint DOA-multipitch estimation,Joints,least squares approximations,least squares
	approximations,model orders,model
	orders,multi-pitch estimation,multipitch signals,multipitch
	signals,nonlinear least square approach,nonlinear least
	square approach,pitch estimation,sensor array},
  pages = {1053-1057},
  owner = {afdidehf}
}

@article{Kruskal1977,
  title = {Three-way arrays: rank and uniqueness of trilinear decompositions, 	with application to arithmetic complexity and statistics},
  volume = {18},
  issn = {0024-3795},
  doi = {http://dx.doi.org/10.1016/0024-3795(77)90069-6},
  abstract = {A three-way array X (or three-dimensional matrix) is an array of numbers
	xijk subscripted by three indices. A triad is a multiplicative array,
	xijk = aibjck. Analogous to the rank and the row rank of a matrix,
	we define rank (X) to be the minimum number of triads whose sum is
	X, and dim1(X) to be the dimensionality of the space of matrices
	generated by the 1-slabs of X. (Rank and dim1 may not be equal.)
	We prove several lower bounds on rank. For example, a special case
	of Theorem 1 is that rank(X)⩾dim1(UX) + rank(XW) − dim1(UXW),
	where U and W are matrices; this generalizes a matrix theorem of
	Frobenius. We define the triple product [A, B, C] of three matrices
	to be the three-way array whose (i, j, k) element is given by ⩞rairbjrckr;
	in other words, the triple product is the sum of triads formed from
	the columns of A, B, and C. We prove several sufficient conditions
	for the factors of a triple product to be essentially unique. For
	example (see Theorem 4a), suppose [A, B, C] = [Ā, B̄, C̄], and
	each of the matrices has R columns. Suppose every set of rank (A)
	columns of A are independent, and similar conditions hold for B and
	C. Suppose rank (A) + rank (B) + rank (C) ⩾ 2R + 2. Then there
	exist diagonal matrices Λ, M, N and a permutation matrix P such
	that Ā = APΛ, B̄ = BPM, C̄ = CPN. Our results have applications
	to arithmetic complexity theory and to statistical models used in
	three-way multidimensional scaling.},
  timestamp = {2016-07-11T17:06:14Z},
  number = {2},
  journal = {Linear Algebra and its Applications},
  author = {Kruskal, Joseph B.},
  year = {1977},
  pages = {95 - 138},
  owner = {Fardin}
}

@mastersthesis{Kupper2012,
  title = {Combined EEG and MEG for improving source analysis in patients with 	focal epilepsy},
  abstract = {Brain activity can be recorded by electroencephalography (EEG) and
	magnetoencephalography (MEG). The acquired data can be used to localize
	the underlying sources of the currents in the brain tissue, a method
	called source localization. In epilepsy patients some of the recorded
	patterns can lead to the generators of seizures. Usually medical
	staff marks the specific patterns and this data often is used without
	any additional verification by non-clinical staff for further processing
	of the source localisation. Up to now in many groups the hand marking
	of the patterns has been taken as gold standard. This thesis aims
	to find, evaluate and improve possible unexpected or unrecognized
	factors influencing the signal quality at the intersection between
	medical and methodological staff involved. The main factors influencing
	the signal quality are the precision of the marking of epileptiform
	discharges as well as the clustering of the spikes. This thesis shows
	that rather minor imprecisions as incorrect peak markings and mixed
	clustering cause a significant change in the dipole localisation
	of up to 3 cm shift. Also the influence of semi-automatic template
	search algorithms on the source localisation precision is evaluated.
	In addition we offer a Matlab tool to correct the data easily without
	profound clinical knowledge. Now it is possible to create reliable
	and precise data as a basis for the source reconstruction. This data
	can also be used to reasonably evaluate upcoming new and highly sophisticated
	head models.},
  timestamp = {2016-10-21T13:50:33Z},
  school = {Beuth Hochschule für Technik Berlin, University of Applied Sciences},
  author = {K{\"u}pper, Philipp},
  year = {2012},
  owner = {afdidehf}
}

@article{L2015,
  title = {RBF-network based sparse signal recovery algorithm for compressed 	sensing reconstruction},
  volume = {63},
  issn = {0893-6080},
  doi = {http://dx.doi.org/10.1016/j.neunet.2014.10.010},
  abstract = {Abstract The approach of applying a cascaded network consisting of
	radial basis function nodes and least square error minimization block
	to Compressed Sensing for recovery of sparse signals is analyzed
	in this paper to improve the computation time and convergence of
	an existing \{ANN\} based recovery algorithm. The proposed radial
	basis function—least square error projection cascade network for
	sparse signal Recovery (RASR) utilizes the smoothed L 0 norm optimization,
	L 2 least square error projection and feedback network model to improve
	the signal recovery performance over the existing \{CSIANN\} algorithm.
	The use of \{ANN\} architecture in the recovery algorithm gives a
	marginal reduction in computational time compared to an existing
	L 0 relaxation based algorithm SL0. The simulation results and experimental
	evaluation of the algorithm performance are presented here.},
  timestamp = {2016-07-10T07:40:59Z},
  journal = {Neural Networks},
  author = {L, Vidya and V, Vivekanand and U, Shyamkumar and Mishra, Deepak},
  year = {2015},
  keywords = {Basis,Compressed,Convergence,function,Matrix,measurement,Radial,rate,sensing},
  pages = {66 - 78},
  owner = {afdidehf}
}

@inproceedings{Lahat2014a,
  title = {Joint blind source separation of multidimensional components: Model 	and algorithm},
  abstract = {This paper deals with joint blind source separation (JBSS) of multidimensional
	components. JBSS extends classical BSS to simultaneously resolve
	several BSS problems by assuming statistical dependence between latent
	sources across mixtures. JBSS offers some significant advantages
	over BSS, such as identifying more than one Gaussian white stationary
	source within a mixture. Multidimensional BSS extends classical BSS
	to deal with a more general and more flexible model within each mixture:
	the sources can be partitioned into groups exhibiting dependence
	within a given group but independence between two different groups.
	Motivated by various applications, we present a model that is inspired
	by both extensions. We derive an algorithm that achieves asymptotically
	the minimal mean square error (MMSE) in the estimation of Gaussian
	multidimensional components. We demonstrate the superior performance
	of this model over a two-step approach, in which JBSS, which ignores
	the multidimensional structure, is followed by a clustering step.},
  timestamp = {2016-07-09T19:48:09Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd 	European},
  author = {Lahat, D. and Jutten, C.},
  month = sep,
  year = {2014},
  keywords = {blind source separation,Clustering algorithms,Convergence,Data models,Data
	models,Gaussian
	multidimensional components,Gaussian processes,independent subspace analysis,independent subspace
	analysis,independent vector analysis,independent
	vector analysis,JBSS,joint blind source separation,Joint BSS,Joint
	BSS,Joints,latent sources,latent
	sources,least mean squares methods,minimal mean square error,minimal mean
	square error,MMSE,multidimensional ICA,statistical dependence,two-step
	approach,Vectors},
  pages = {1417-1421},
  owner = {Fardin}
}

@article{Lai2011,
  title = {The null space property for sparse recovery from multiple measurement 	vectors},
  volume = {30},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2010.11.002},
  abstract = {We prove a null space property for the uniqueness of the sparse solution
	vectors recovered from a minimization in l p quasi-norm subject to
	multiple systems of linear equations, where p ∈ ( 0 , 1 ] . Furthermore,
	we show that the null space property is equivalent to the null space
	property for the standard l p minimization subject to a single linear
	system. This answers the questions raised in Foucart and Gribonval
	(2010) [17].},
  timestamp = {2016-09-30T11:23:41Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Lai, Ming-Jun and Liu, Yang},
  year = {2011},
  keywords = {Optimization,sparse recovery},
  pages = {402--406},
  owner = {Fardin}
}

@book{Landini2005,
  title = {Advanced Image Processing in Magnetic Resonance Imaging},
  abstract = {Magnetic Resonance (MR) imaging produces images of the human tissues
	in a noninvasive manner, revealing the structure, metabolism, and
	function of tissues and organs. The impact of this image technique
	in diagnostic radiology is impressive, due to its versatility and
	flexibility in joining high-quality anatomical images with functional
	information. Signal and image processing play a decisive role in
	the exploitation of MR imaging features, allowing for the extraction
	of diagnostic and metabolic information from images.This book attempts
	to cover all updated aspects of MR image processing, ranging from
	new acquisition techniques to state-of-art imaging techniques. Because
	the textbook provides the tools necessary to understand the physical
	and chemical principles, and the basic signal and image processing
	concepts and applications, it is a valuable reference book for scientists,
	and an essential source for upper-level undergraduate and graduate
	students in these disciplines. The book�s 18 chapters are divided
	into five sections. The first section focuses on MR signal and image
	generation and reconstruction, the basics of MR imaging, advanced
	reconstruction algorithms, and the parallel MRI field. In the second
	section, the state-of-art techniques for MR images filtering are
	described. In particular, the second section covers the signal and
	noise estimation, the inhomogeneities correction and the more advanced
	image filtering techniques, taking into account the peculiar features
	of the noise in MR images. Quantitative analysis is a key issue in
	MR diagnostic imaging. The third section�s topics range from image
	registration to integration of EEG and MEG techniques with MR imaging.
	Two chapters cover the cardiac image quantitative analysis issue.
	In the fourth section, MR spectroscopy is described, from both the
	signal generation and the data analysis point of view. Diffusion
	tensor MR imaging and MR elastography are also examined. Finally,
	in the last section, the functional MR image processing is described
	in detail. Fundamentals and advanced data analysis (as exploratory
	approach), bayesian inference, and nonlinear analysis are also depicted.
	Experts from all fields of MRI have cooperated in preparing this
	book. We would like to thank all authors for their excellent and
	up-to-date chapters. Some overlap was unavoidable, due to the multiple
	connections in MR image processing fields. The outline of this book
	was born at the MR Laboratory at the CNR Institute of Clinical Physiology
	in Pisa, Italy. The MR Laboratory is a multidisciplinary arena for
	biomedical engineers, physicists, computer scientists, radiologists,
	cardiologists, and neuroscientists, and it shares experts from CNR
	and university departments. For this textbook, we are sincerely grateful
	to professor Luigi Donato, the chief of the CNR Institute of Clinical
	Physiology, and the pioneer of the multidisciplinary approach to
	biomedicine.},
  timestamp = {2016-07-08T10:14:09Z},
  publisher = {{\{$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}vphantom$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}CRC 	Press, Taylor $\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}ensuremath$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}backslash$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}ensuremath$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}ensuremath$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}backslash$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}backslash$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}ensuremath$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}backslash$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\& 	Francis Group$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}vphantom$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}ensuremath$\backslash$\{$\backslash$ensuremath\{$\backslash$backslash\}backslash$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}vphantom$\backslash$\{$\backslash$vphantom\{$\backslash$\}\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\{$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}$\backslash$ensuremath\{$\backslash$backslash\}$\backslash$\}$\backslash$vphantom\{$\backslash$\{\}$\backslash$\}\}}},
  author = {Landini, Luigi and Positano, Vincenzo and Santarelli, Maria Filomena},
  year = {2005},
  owner = {Fardin}
}

@inproceedings{Abtahi2015,
  title = {Energy allocation for parameter estimation in block CS-based distributed 	MIMO systems},
  doi = {10.1109/SAMPTA.2015.7148946},
  abstract = {Exploiting Compressive Sensing (CS) in MIMO radars, we can remove
	the need of the high rate A/D converters and send much less samples
	to the fusion center. In distributed MIMO radars, the received signal
	can be modeled as a block sparse signal in a basis. Thus, block CS
	methods can be used instead of classical CS ones to achieve more
	accurate target parameter estimation. In this paper a new method
	of energy allocation to the transmitters is proposed to improve the
	performance of the block CS-based distributed MIMO radars. This method
	is based on the minimization of an upper bound of the sensing matrix
	block-coherence. Simulation results show a significant increase in
	the accuracy of multiple targets parameter estimation.},
  timestamp = {2016-07-08T12:20:58Z},
  booktitle = {Sampling Theory and Applications (SampTA), 2015 International Conference 	on},
  author = {Abtahi, A. and Modarres-Hashemi, M. and Marvasti, F. and Tabataba, F.S.},
  month = may,
  year = {2015},
  keywords = {A-D converter,analog-digital converter,analogue-digital conversion,block
	CS method,block sparse signal,compressed sensing,compressive sensing,distributed
	MIMO system,energy allocation,Estimation,fusion center,Matching pursuit
	algorithms,matrix algebra,MIMO radar,multiple-input multiple-output
	radar,parameter estimation,Receivers,Resource management,Resource
	management,sensing matrix block-coherence,sensing
	matrix block-coherence,Sensors,Transmitters},
  pages = {523-527},
  owner = {afdidehf}
}

@inproceedings{Acar2003,
  title = {Sensitivity of EEG and MEG to conductivity perturbations},
  volume = {3},
  doi = {10.1109/IEMBS.2003.1280508},
  abstract = {Solution of the electro-magnetic source imaging (EMSI) problem requires
	an accurate representation of the head using a numerical model. Some
	of the errors in source estimation are due to the differences between
	this model and the actual head. This study investigates the effects
	of conductivity perturbations, that is, changing the conductivity
	of a small region by a small amount, on the EEG and MEG measurements.
	By computing the change in measurements for perturbations throughout
	the volume, it is possible to obtain a spatial distribution of sensitivity.
	Using this information, it is possible, for a given source configuration,
	to identify the regions to which a measurement is most sensitive.
	In this work, two mathematical expressions for efficient computation
	of the sensitivity distribution are presented. The formulation is
	implemented for a numerical head model using the finite element method
	(FEM). 3D sensitivity distributions are computed and analyzed for
	selected dipoles and sensors. It was observed that the voltage measurements
	are sensitive to the skull, the regions near the dipole and the electrodes.
	The magnetic field measurements are mostly sensitive to regions near
	the dipole. It could also be possible to use the computed sensitivity
	matrices to estimate or update the conductivity of the tissues from
	EEG and MEG measurements.},
  timestamp = {2016-07-10T08:10:37Z},
  booktitle = {Engineering in Medicine and Biology Society, 2003. Proceedings of 	the 25th Annual International Conference of the IEEE},
  author = {Acar, C.E. and Gencer, N.G.},
  month = sep,
  year = {2003},
  keywords = {bioelectric phenomena,biological tissues,biomedical electrodes,biosensors,Brain
	modeling,Conductivity measurement,conductivity perturbations,dipoles,Distributed
	computing,EEG,electroencephalography,electro-magnetic source imaging,Estimation
	error,finite element analysis,finite element method,Finite element
	methods,magnetic field measurement,magnetic field measurements,Magnetic
	heads,magnetoencephalography,medical image processing,MEG,numerical
	head model,Numerical models,physiological models,sensitivity distribution,Sensors,source
	estimation,voltage measurement,voltage measurements,Volume measurement},
  pages = {2834-2837 Vol.3},
  owner = {afdidehf}
}

@article{Acar2012,
  title = {Forward and inverse problem of EEG},
  timestamp = {2016-07-08T12:32:23Z},
  author = {Acar, Zeynep Akalin},
  month = nov,
  year = {2012},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Swartz Center for Computational Neuroscience},
  owner = {Fardin}
}

@inproceedings{Adalbjornsson2013,
  title = {Estimating multiple pitches using block sparsity},
  doi = {10.1109/ICASSP.2013.6638861},
  abstract = {We study the problem of estimating the fundamental frequencies of
	a signal containing multiple harmonically related sinusoidal signals
	using a novel block sparsity representation of the signal model.
	An efficient algorithm for solving the resulting optimization is
	devised exploiting an alternating directions method of multipliers
	(ADMM) formulation of the problem. The superiority of the proposed
	method, as compared to earlier methods, is demonstrated using both
	simulated and measured audio signals.},
  timestamp = {2016-07-08T12:24:03Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Adalbj{\"o}rnsson, S.I. and Jakobsson, A. and Christensen, M.G.},
  month = may,
  year = {2013},
  keywords = {audio signal processing,audio signals,block sparsity,block sparsity
	representation,Convex functions,Estimation,frequency estimation,Frequency
	estimation,Harmonic analysis,Minimization,multiple pitches estimation,Multiple
	pitch signals,multipliers alternating directions method,Optimization,order
	estimation,pitch estimation,Signal to noise ratio,sinusoidal signals},
  pages = {6220-6224},
  owner = {afdidehf}
}

@article{Adalbjornsson2015,
  title = {Multi-pitch estimation exploiting block sparsity},
  volume = {109},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.10.014},
  abstract = {Abstract We study the problem of estimating the fundamental frequencies
	of a signal containing multiple harmonically related sinusoidal components
	using a novel block sparse signal representation. An efficient algorithm
	for solving the resulting optimization problem is devised exploiting
	a novel variable step-size alternating direction method of multipliers
	(ADMM). The resulting algorithm has guaranteed convergence and shows
	notable robustness to the f0 vs f 0 / 2 ambiguity problem. The superiority
	of the proposed method, as compared to earlier presented estimation
	techniques, is demonstrated using both simulated and measured audio
	signals, clearly indicating the preferable performance of the proposed
	technique.},
  timestamp = {2016-07-09T20:16:48Z},
  number = {0},
  journal = {Signal Processing},
  author = {Adalbj{\"o}rnsson, Stefan I. and Jakobsson, Andreas and Christensen, Mads G.},
  year = {2015},
  keywords = {block sparsity,order estimation,pitch estimation,Spectral smoothness,Totalvariation},
  pages = {236 - 247},
  owner = {Fardin}
}

@article{Adde2005,
  title = {Imaging Methods for MEG/EEG Inverse Problem},
  volume = {7},
  abstract = {Recovering electrical activity of the brain from MEG/EEG measurements
	is known as the MEEG inverse problem. It is an ill-posed problem
	in several senses. One is that there is further less data observed
	than data to recover. One way to address this issue is to search
	for regular solutions. We present here a framework for applying image
	processing filtering techniques to the MEEG inverse problem. Exprimentations
	are presented on synthetic dara and validation is carried out on
	one real MEG data set.},
  timestamp = {2016-07-08T12:47:59Z},
  number = {2},
  journal = {IJBEM},
  author = {Adde, Geoffray and Clerc, Maureen and Keriven, Renaud},
  year = {2005},
  pages = {111-114},
  owner = {Fardin}
}

@article{Ahani2015,
  title = {Colour image steganography method based on sparse representation},
  volume = {9},
  issn = {1751-9659},
  doi = {10.1049/iet-ipr.2014.0351},
  abstract = {The authors address the use of sparse representation to securely hide
	a message within non-overlapping blocks of a given colour image in
	the wavelet domain. All four sub-images of the two-dimensional wavelet
	transform of two colour bands are used for data embedding without
	affecting the image perceptibility. Bit error rate of hidden data
	extraction is reduced to zero by introducing a novel refinement procedure
	in the proposed algorithm. The refinement procedure introduced solves
	the hidden bit extraction errors caused by the rounding process,
	the overflows and the nature of approximation in sparse decomposition.
	Capacity of the proposed method is calculated using necessary conditions
	for uniqueness of the sparse representations and validated by means
	of experimental results. The authors experimental results show that
	the embedded data are invisible perceptually. The average peak signal-to-noise
	ratio (PSNR) value of the introduced method is about 40 and 11 dB
	higher than the average PSNR values of the MPSteg-colour method and
	the wavelet domain 2-LSB embedding method, respectively. The security
	of the proposed method is also evaluated comparatively, using five
	well-known steganalysers, which confirms superior performance of
	the new method comparing with the MPSteg-colour method and the wavelet
	domain 2-LSB embedding method.},
  timestamp = {2016-07-08T11:52:03Z},
  number = {6},
  journal = {Image Processing, IET},
  author = {Ahani, S. and Ghaemmaghami, S.},
  year = {2015},
  keywords = {2D wavelet transform,bit error rate,colour image steganography,cryptography,data
	embedding,error statistics,hidden bit extraction errors,hidden data
	extraction,image colour analysis,image perceptibility,image representation,MPSteg-colour
	method,sparse representation,wavelet domain 2-LSB embedding method,wavelet
	transforms},
  pages = {496-505},
  owner = {afdidehf}
}

@inproceedings{Ahani2010,
  title = {Image steganography based on sparse decomposition in wavelet space},
  doi = {10.1109/ICITIS.2010.5689508},
  abstract = {Sparse decomposition of wavelet coefficients of cover image blocks
	for data hiding is addressed in this paper. By using the proposed
	algorithm, the embedded secret message can be reliably extracted
	without resorting to the original image. We use all four sub-images
	(LL, LH, HL and HH) of the 2D wavelet transform for data embedding
	without losing the image imperceptibility. An over-complete dictionary
	matrix is estimated by using the KSVD dictionary learning algorithm,
	and then the secret message bits are inserted in the sparse representation
	of the wavelet coefficients over the estimated dictionary. This is
	believed to be one of the first approaches to the image data hiding
	that uses the sparse decomposition. Our experimental results show
	that the proposed method is robust against cropping and noise addition
	attacks. It is also robust against the lower than 0.2 degree rotation
	attacks. The results also show it possesses resistance to high order
	statistics analysis.},
  timestamp = {2016-07-08T12:47:20Z},
  booktitle = {Information Theory and Information Security (ICITIS), 2010 IEEE International 	Conference on},
  author = {Ahani, S. and Ghaemmaghami, S.},
  month = dec,
  year = {2010},
  keywords = {2D wavelet transform,bit error rate,component,data embedding,data
	hiding,Dictionaries,dictionary learning,discrete wavelet transform,Discrete
	wavelet transforms,embedded secret message,image block,image coding,image
	imperceptibility,image steganography,KSVD dictionary learning algorithm,Matrix
	decomposition,Noise,over-complete dictionary matrix,Robustness,secret
	message bit,sparse decomposition,sparse matrices,sparse representation,sparse
	representation,steganography,wavelet coefficient,wavelet
	coefficient,Wavelet coefficients,wavelet space,wavelet
	space,wavelet transforms},
  pages = {632-637},
  owner = {afdidehf}
}

@inproceedings{Ahmed2011,
  title = {Compressive sampling of correlated signals},
  doi = {10.1109/ACSSC.2011.6190203},
  abstract = {The recently developed theory of Compressive sensing (CS) has shown
	that sparse signals can be reconstructed from a much smaller number
	of measurements than their bandwidth suggests. In this paper we present
	a sampling scheme to acquire ensembles of correlated signals at a
	sub-Nyquist rate. The sampling architecture uses simple analog building
	blocks including analog vector matrix multiplier (AVMM) and linear
	time invariant (LTI) random filters to analog preprocess the signals
	before sampling them with non-uniform Analog-to-digital converters
	(ADCs). The sampling strategy takes advantage of the (a priori unknown)
	correlation structure in the ensemble to sample at a sub-Nyquist
	rate and stably recover the information using convex optimization.
	We close the discussion with some applications.},
  timestamp = {2016-07-08T12:00:27Z},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2011 Conference Record 	of the Forty Fifth Asilomar Conference on},
  author = {Ahmed, A. and Romberg, J.},
  month = nov,
  year = {2011},
  keywords = {analog building blocks,analog vector matrix multiplier,Antenna arrays,Arrays,AVMM,Coherence,compressive
	sampling,compressive sensing,Convex functions,convex optimization,convex
	programming,correlated signals,Correlation,Electrodes,filtering theory,linear
	time invariant random filters,LTI random filters,nonuniform ADC,nonuniform
	analog-to-digital converters,signal reconstruction,signal sampling,sparse
	signals,sub-Nyquist rate,Vectors},
  pages = {1188-1192},
  owner = {afdidehf}
}

@article{Akaike1974,
  title = {A new look at the statistical model identification},
  volume = {19},
  issn = {0018-9286},
  doi = {10.1109/TAC.1974.1100705},
  abstract = {The history of the development of statistical hypothesis testing in
	time series analysis is reviewed briefly and it is pointed out that
	the hypothesis testing procedure is not adequately defined as the
	procedure for statistical model identification. The classical maximum
	likelihood estimation procedure is reviewed and a new estimate minimum
	information theoretical criterion (AIC) estimate (MAICE) which is
	designed for the purpose of statistical identification is introduced.
	When there are several competing models the MAICE is defined by the
	model and the maximum likelihood estimates of the parameters which
	give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood)
	+ 2(number of independently adjusted parameters within the model).
	MAICE provides a versatile procedure for statistical model identification
	which is free from the ambiguities inherent in the application of
	conventional hypothesis testing procedure. The practical utility
	of MAICE in time series analysis is demonstrated with some numerical
	examples.},
  timestamp = {2016-09-29T15:38:43Z},
  number = {6},
  journal = {Automatic Control, IEEE Transactions on},
  author = {Akaike, H.},
  month = dec,
  year = {1974},
  keywords = {Art,Estimation theory,History,Linear systems,Maximum likelihood estimation,maximum-likelihood
	(ML) estimation,Parameter identification,Roundoff errors,Sampling
	methods,Stochastic processes,Testing,Time series,Time series analysis},
  pages = {716--723},
  owner = {afdidehf}
}

@article{Akcakaya2008,
  title = {A Frame Construction and a Universal Distortion Bound for Sparse 	Representations},
  volume = {56},
  issn = {1053-587X},
  doi = {10.1109/TSP.2007.914344},
  abstract = {We consider approximations of signals by the elements of a frame in
	a complex vector space of dimension N and formulate both the noiseless
	and the noisy sparse representation problems. The noiseless representation
	problem is to find sparse representations of a signal r given that
	such representations exist. In this case, we explicitly construct
	a frame, referred to as the Vandermonde frame, for which the noiseless
	sparse representation problem can be solved uniquely using O(N2)
	operations, as long as the number of non-zero coefficients in the
	sparse representation of r is isinN for some 0 les isin les 0.5.
	It is known that isin les 0.5 cannot be relaxed without violating
	uniqueness. The noisy sparse representation problem is to find sparse
	representations of a signal r satisfying a distortion criterion.
	In this case, we establish a lower bound on the tradeoff between
	the sparsity of the representation, the underlying distortion and
	the redundancy of any given frame.},
  timestamp = {2016-09-29T15:35:05Z},
  number = {6},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Akcakaya, M. and Tarokh, Vahid},
  month = jun,
  year = {2008},
  keywords = {Distortion,frame construction,frames,noisy sparse representation problems,redundancy,signal
	reconstruction,signals approximations,sparse representations,Sparsity,universal
	distortion bound},
  pages = {2443--2450},
  owner = {afdidehf}
}

@inproceedings{Akhtar2015,
  title = {Compressed Sensing for Multistatic Radar Systems with Arbitrary Block 	Codes},
  doi = {10.1109/RADAR.2015.7130969},
  abstract = {Multistatic radar systems with multiple elements at transmitting and/or
	receiving side permit multiple target observations from different
	viewpoints providing various advantageous. In an environment with
	multiple transmitters the emitters may transmit independent waveform
	or use a coding structure to emit pulses across antennas over a predetermined
	coherent time-interval. In this work we extend the principles of
	employing orthogonal block codes for this purpose and emphasize on
	arbitrary or randomly designed block codes with a compressed sensing
	based detection at receiver. By employing a compressed sensing approach
	only a subset of incoming data needs to be stored and processed thus
	significantly reducing the sampling and integration requirement at
	the receivers. We show that the use of more general type of block
	codes allows for greater flexibility and better performance in target
	detection and are hence more suitable for compressed sensing methods
	than orthogonal, or quasi-orthogonal, block codes. Through simulations
	it is validated that such a system can operate well and compressed
	sensing techniques allow for data reduction and improved detection
	with a shorter dwell period.},
  timestamp = {2016-07-08T11:55:56Z},
  booktitle = {Radar Conference (RadarCon), 2015 IEEE},
  author = {Akhtar, J.},
  month = may,
  year = {2015},
  keywords = {arbitrary block codes,block codes,coding structure,compressed sensing,compressed
	sensing methods,compressed sensing techniques,emitters,independent
	waveform,multiple transmitters,multistatic radar systems,orthogonal
	block codes,quasi-orthogonal,radar target recognition,radar transmitters,Receiving
	antennas,sparse matrices,target detection,target observations,transmitting
	antennas},
  pages = {0051-0055},
  owner = {afdidehf}
}

@inproceedings{Al-Naffouri2013,
  title = {Distribution agnostic structured sparsity recovery algorithms},
  doi = {10.1109/WoSSPA.2013.6602377},
  abstract = {We present an algorithm and its variants for sparse signal recovery
	from a small number of its measurements in a distribution agnostic
	manner. The proposed algorithm finds Bayesian estimate of a sparse
	signal to be recovered and at the same time is indifferent to the
	actual distribution of its non-zero elements. Termed Support Agnostic
	Bayesian Matching Pursuit (SABMP), the algorithm also has the capability
	of refining the estimates of signal and required parameters in the
	absence of the exact parameter values. The inherent feature of the
	algorithm of being agnostic to the distribution of the data grants
	it the flexibility to adapt itself to several related problems. Specifically,
	we present two important extensions to this algorithm. One extension
	handles the problem of recovering sparse signals having block structures
	while the other handles multiple measurement vectors to jointly estimate
	the related unknown signals. We conduct extensive experiments to
	show that SABMP and its variants have superior performance to most
	of the state-of-the-art algorithms and that too at low-computational
	expense.},
  timestamp = {2016-07-08T12:15:15Z},
  booktitle = {Systems, Signal Processing and their Applications (WoSSPA), 2013 	8th International Workshop on},
  author = {Al-Naffouri, T.Y. and Masood, M.},
  month = may,
  year = {2013},
  keywords = {Bayesian estimate,Bayes methods,block structures,distribution agnostic
	structured sparsity recovery algorithm,Greedy algorithms,iterative
	methods,Matching pursuit algorithms,measurement vector,nonzero element,SABMP,Sensors,signal
	processing,Signal processing algorithms,sparse matrices,sparse signal
	recovery,support agnostic Bayesian matching pursuit,time-frequency
	analysis,Vectors},
  pages = {283-290},
  owner = {afdidehf}
}

@article{Amblard2004,
  title = {Biomagnetic source detection by maximum entropy and graphical models},
  volume = {51},
  issn = {0018-9294},
  doi = {10.1109/TBME.2003.820999},
  abstract = {This article presents a new approach for detecting active sources
	in the cortex from magnetic field measurements on the scalp in magnetoencephalography
	(MEG). The solution of this ill-posed inverse problem is addressed
	within the framework of maximum entropy on the mean (MEM) principle
	introduced by Clarke and Janday. The main ingredient of this regularization
	technique is a reference probability measure on the random variables
	of interest. These variables are the intensity of current sources
	distributed on the cortical surface for which this measure encompasses
	all available prior information that could help to regularize the
	inverse problem. This measure introduces hidden Markov random variables
	associated with the activation state of predefined cortical regions.
	MEM approach is applied within this particular probabilistic framework
	and simulations show that the present methodology leads to a practical
	detection of cerebral activity from MEG data.},
  timestamp = {2016-07-08T11:38:32Z},
  number = {3},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Amblard, C. and Lapalme, E. and Lina, J.-M.},
  month = mar,
  year = {2004},
  keywords = {Algorithms,Biomagnetics,biomagnetic source detection,Brain Mapping,Brain
	models,cerebral activity,Cerebral Cortex,Computer-Assisted,Computer
	Simulation,cortical surface,Current measurement,Diagnosis,Entropy,Graphical
	models,hidden Markov models,hidden Markov random variables,hidden Markov random
	variables,Humans,ill-posed inverse problem,ill-posed
	inverse problem,imaging,inverse problems,magnetic field measurement,Magnetic
	field measurement,magnetoencephalography,Markov Chains,Markov
	Chains,maximum entropy,maximum entropy methods,maximum
	entropy methods,Models,Neurological,phantoms,Random variables,Random
	variables,regularization technique,regularization
	technique,Scalp,Statistical,Stochastic processes},
  pages = {427-442},
  owner = {afdidehf}
}

@article{Amin2015,
  title = {A sparsity-perspective to quadratic time--frequency distributions},
  issn = {1051-2004},
  doi = {http://dx.doi.org/10.1016/j.dsp.2015.06.011},
  abstract = {Abstract We examine nonstationary signals within the framework of
	compressive sensing and sparse reconstruction. Most of these signals,
	which arise in numerous applications, exhibit small relative occupancy
	in the time–frequency domain, casting them as sparse in a joint-variable
	representation. We present two general approaches to incorporate
	sparsity into time–frequency analysis, leading to what we refer
	to as sparsity-aware quadratic time–frequency distributions. Both
	approaches exploit time–frequency sparsity under full data and
	compressed observations. In the first approach, quadratic time–frequency
	distributions are derived based on optimal multi-task kernel design.
	In this case, sparseness in the time–frequency domain presents
	itself as a new design task, adding to the two fundamental tasks
	of auto-term preservation and cross-term suppression. In the second
	approach, sparse reconstruction is used, in lieu of the Fourier transform,
	to obtain quadratic time–frequency distributions from compressed
	measurements observed in the time domain or the joint-variable domain.
	It is shown that multiple measurement vector methods and block sparsity
	techniques play a fundamental role in improving signal local frequency
	representations. Examples of both approaches are provided. Analysis
	is supported by results based on simulated data, electromagnetic
	modeled data, and real Doppler and micro-Doppler data measurements
	of radar returns associated with human motions.},
  timestamp = {2016-10-03T12:59:50Z},
  journal = {Digital Signal Processing},
  author = {Amin, Moeness G. and Jokanovic, Branka and Zhang, Yimin D. and Ahmad, Fauzia},
  year = {2015},
  keywords = {Block,measurement,Micro-Doppler,Multiple,reconstruction,representation,Sparse,Sparsity,Time�frequency,vector},
  pages = {-},
  owner = {afdidehf}
}

@article{Amini2015,
  title = {Special Topics in Communications: Compressed Sensing (EE25140)},
  timestamp = {2016-07-11T16:55:23Z},
  author = {Amini, Arash},
  year = {2015},
  owner = {afdidehf}
}

@phdthesis{Amini1389,
  title = {Nemune Bardari Feshorde Ba Raveshhaye Gheyre Tasadofi},
  timestamp = {2016-07-10T06:49:12Z},
  school = {Sharif University of Technology},
  author = {Amini, Arash},
  year = {1389},
  owner = {afdidehf}
}

@article{Andrew,
  title = {Image classification by sparse coding},
  timestamp = {2016-07-08T12:45:54Z},
  author = {Andrew, Ng},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@inproceedings{Angelosante2011,
  title = {Group lassoing change-points in piecewise-stationary AR signals},
  doi = {10.1109/ICDSP.2011.6005007},
  abstract = {Regularizing the least-squares criterion with the total number of
	coefficient changes, it is possible to estimate time-varying (TV)
	autoregressive (AR) models with piecewise-constant coefficients.
	Such models emerge in various applications including speech segmentation,
	biomedical signal processing, and geophysics. To cope with the inherent
	lack of continuity and the high computational burden when dealing
	with high-dimensional data sets, this paper introduces a convex regularization
	approach enabling efficient and continuous estimation of TV-AR models.
	To this end, the problem is cast as a sparse regression one with
	grouped variables, and is solved by resorting to the group least-absolute
	shrinkage and selection operator (Lasso). The fresh look advocated
	here permeates benefits from advances in variable selection and compressive
	sampling to signal segmentation. An efficient block-coordinate descent
	algorithm is developed to implement the novel segmentation method.
	Issues regarding regularization and uniqueness of the solution are
	also discussed. Finally, an alternative segmentation technique is
	introduced to improve the detection of change instants. Numerical
	tests using synthetic and real data corroborate the merits of the
	developed segmentation techniques in identifying piecewise-constant
	TV-AR models.},
  timestamp = {2016-07-08T12:40:02Z},
  booktitle = {Digital Signal Processing (DSP), 2011 17th International Conference 	on},
  author = {Angelosante, D. and Giannakis, G.B.},
  month = jul,
  year = {2011},
  keywords = {autoregressive models,autoregressive processes,block-coordinate descent
	algorithm,brain models,compressive sampling,Computational modeling,continuous
	estimation,convex programming,convex regularization approach,Detectors,electroencephalography,group
	lassoing change points,least squares approximations,least squares
	criterion,Minimization,piecewise constant coefficients,piecewise
	constant techniques,piecewise stationary AR signals,regression analysis,signal
	sampling,signal segmentation,sparse regression,Spectral analysis,time-varying
	models,Tuning,TV-AR models},
  pages = {1-8},
  owner = {afdidehf}
}

@article{Arlot2010,
  title = {A survey of cross-validation procedures for model selection},
  volume = {4},
  doi = {10.1214/09-SS054},
  abstract = {Used to estimate the risk of an estimator or to perform model selection,
	cross-validation is a widespread strategy because of its simplic-
	ity and its (apparent) universality. Many results exist on model
	selection performances of cross-validation procedures. This survey
	intends to relate these results to the most recent advances of model
	selection theory, with a particular emphasis on distinguishing empirical
	statements from rigorous theoretical results. As a conclusion, guidelines
	are provided for choosing the best cross-validation procedure according
	to the particular features of the problem in hand.},
  timestamp = {2016-07-08T11:32:09Z},
  journal = {Statistics Surveys},
  author = {Arlot, Sylvain and Celisse, Alain},
  year = {2010},
  keywords = {cross-validation,leave-one-out.,Model selection},
  pages = {40�79},
  owner = {afdidehf}
}

@article{Arora2014,
  title = {New Algorithms for Learning Incoherent and Overcomplete Dictionaries},
  volume = {35},
  abstract = {In sparse recovery we are given a matrix A 2 Rnm (�the dictionary�)
	and a vector of the form AX where X is sparse, and the goal is to
	recover X. This is a central notion in signal processing, statistics
	and machine learning. But in applications such as sparse coding,
	edge detection, compression and super resolution, the dictionary
	A is unknown and has to be learned from random examples of the form
	Y = AX where X is drawn from an appropriate distribution � this
	is the dictionary learning problem. In most settings, A is overcomplete:
	it has more columns than rows. This paper presents a polynomial-time
	algorithm for learning overcomplete dictionaries; the only previously
	known algorithm with provable guarantees is the recent work of Spielman
	et al. (2012) who gave an algorithm for the undercomplete case, which
	is rarely the case in applications. Our algorithm applies to incoherent
	dictionaries which have been a central object of study since they
	were introduced in seminal work of Donoho and Huo (1999). In particular,
	a dictionary is -incoherent if each pair of columns has inner product
	at most = p n. The algorithm makes natural stochastic assumptions
	about the unknown sparse vector X, which can contain k  c min( p
	n= log n;m1=2?) non-zero entries (for any  > 0). This is close
	to the best k allowable by the best sparse recovery algorithms even
	if one knows the dictionary A exactly. Moreover, both the running
	time and sample complexity depend on log 1=, where  is the target
	accuracy, and so our algorithms converge very quickly to the true
	dictionary. Our algorithm can also tolerate substantial amounts of
	noise provided it is incoherent with respect to the dictionary (e.g.,
	Gaussian). In the noisy setting, our running time and sample complexity
	depend polynomially on 1=, and this is necessary.},
  timestamp = {2016-07-10T06:53:04Z},
  journal = {JMLR: Workshop and Conference Proceedings},
  author = {Arora, Sanjeev and Ge, Rong and Moitra, Ankur},
  year = {2014},
  pages = {1-28},
  owner = {Fardin}
}

@article{Asif2014,
  title = {Sparse Recovery of Streaming Signals Using $\ell _1$-Homotopy},
  volume = {62},
  issn = {1053-587X},
  doi = {10.1109/TSP.2014.2328981},
  abstract = {Most of the existing sparse-recovery methods assume a static system:
	the signal is a finite-length vector for which a fixed set of measurements
	and sparse representation are available and an l1 problem is solved
	for the reconstruction. However, the same representation and reconstruction
	framework is not readily applicable in a streaming system: the signal
	changes over time, and it is measured and reconstructed sequentially
	over small intervals. This is particularly desired when dividing
	signals into disjoint blocks and processing each block separately
	is infeasible or inefficient. In this paper, we discuss two streaming
	systems and a new homotopy algorithm for quickly solving the associated
	l1 problems: 1) recovery of smooth, time-varying signals for which,
	instead of using block transforms, we use lapped orthogonal transforms
	for sparse representation and 2) recovery of sparse, time-varying
	signals that follows a linear dynamic model. For both systems, we
	iteratively process measurements over a sliding interval and solve
	a weighted l1-norm minimization problem for estimating sparse coefficients.
	Since we estimate overlapping portions of the signal while adding
	and removing measurements, instead of solving a new l1 program as
	the system changes, we use available signal estimates as starting
	point in a homotopy formulation and update the solution in a few
	simple steps. We demonstrate with numerical experiments that our
	proposed streaming recovery framework provides better reconstruction
	compared to the methods that represent and reconstruct signals as
	independent, disjoint blocks, and that our proposed homotopy algorithm
	updates the solution faster than the current state-of-the-art solvers.},
  timestamp = {2016-07-11T16:51:06Z},
  number = {16},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Asif, M.S. and Romberg, J.},
  month = aug,
  year = {2014},
  keywords = {Basis pursuit denoising,compressed sensing,disjoint blocks,finite-length
	vector,Heuristic algorithms,homotopy algorithm,iterative method,iterative
	methods,Kalman filter,l1-homotopy,l1 problem,lapped orthogonal transforms,lapped
	transform,Lasso,linear dynamic model,minimisation,Minimization,reconstruction
	framework,Signal processing algorithms,signal reconstruction,signal
	representation,sliding interval,sparse coefficient estimation,sparse-recovery
	method,sparse representation,static system,streaming recovery framework,streaming
	signal,streaming system,Time measurement,time-varying signal recovery,transforms,Vectors,weighted
	$ell_1$ norm,weighted l1-norm minimization problem,Weight measurement},
  pages = {4209-4223},
  owner = {afdidehf}
}

@inproceedings{Attal2007,
  title = {Modeling and Detecting Deep Brain Activity with MEG \& EEG},
  doi = {10.1109/IEMBS.2007.4353448},
  abstract = {We introduce an anatomical and electrophysiological model of deep
	brain structures dedicated to magnetoencephalography (MEG) and electroencephalography
	(EEG) source imaging. So far, most imaging inverse models considered
	that MEG/EEG surface signals were predominantly produced by cortical,
	hence superficial, neural currents. Here we question whether crucial
	deep brain structures such as the basal ganglia and the hippocampus
	may also contribute to distant, scalp MEG and EEG measurements. We
	first design a realistic anatomical and electrophysiological model
	of these structures and subsequently run Monte-Carlo experiments
	to evaluate the respective sensitivity of the MEG and EEG to signals
	from deeper origins. Results indicate that MEG/EEG may indeed localize
	these deeper generators, which is confirmed here from experimental
	MEG data reporting on the modulation of alpha brain waves.},
  timestamp = {2016-07-09T20:14:23Z},
  booktitle = {Engineering in Medicine and Biology Society, 2007. EMBS 2007. 29th 	Annual International Conference of the IEEE},
  author = {Attal, Y. and Bhattacharjee, M. and Yelnik, J. and Cottereau, B. and Lefevre, J. and Okada, Y. and Bardinet, E. and Chupin, M. and Baillet, S.},
  month = aug,
  year = {2007},
  keywords = {alpha brain waves,anatomical model,Anatomical structure,Anatomy,basal ganglia,Basal
	Ganglia,bioelectric phenomena,Biological,Brain,Brain
	modeling,brain models,brain structures,deep brain activity,EEG,electroencephalography,electrophysiological
	model,Electrophysiology,hippocampus,Humans,Image segmentation,inverse
	models,inverse problems,Magnetic resonance imaging,magnetoencephalography,MEG,Models,Monte-Carlo
	experiments,Monte Carlo methods,Neurological,neurophysiology,Scalp,Visual
	Perception},
  pages = {4937-4940},
  owner = {afdidehf}
}

@article{Atto2009,
  title = {Sparsity Measure and the Detection of Significant Data},
  timestamp = {2016-07-11T16:55:11Z},
  journal = {SPARS�09 - Signal Processing with Adaptive Sparse Structured Representations},
  author = {Atto, Abdourrahmane and Pastor, Dominique and Mercier, Gr�goire},
  month = apr,
  year = {2009},
  owner = {Fardin}
}

@phdthesis{Ayaz2014,
  title = {Sparse Recovery with Fusion Frames},
  abstract = {Sparse signal structures have become increasingly important in signal
	processing applications as the technology progresses and a plethora
	of data needs to be handled. It has been shown in practice that various
	signals have fewer degrees of freedom compared to their actual sizes,
	i.e., can be expressed in terms of a small number of elements from
	some dictionary. Alongside an increase in applications, a recent
	theory of sparse and compressible signal recovery has been recently
	developed under the name of Compressed Sensing (CS). This approach
	states that a sparse signal can be eciently recovered from a small
	number of random linear measurements. Another powerful tool in signal
	processing is frames which provide redundant representations for
	signals. Such redundancy is desirable in many applications where
	resilience to errors and losses in data is important. The increase
	in data has also signicantly increased the demand to model applications
	requiring distributed processing which goes beyond the classical
	frames. The recent theory of Fusion Frames, which can be regarded
	as a generalization of classical frames, satises those needs by
	analyzing signals by projecting them onto multidimensional subspaces.
	Fusion frames provide a suitable mathematical framework to model
	large data systems such as sensor networks, transmission of data
	of communication networks, etc. In this thesis, we combine these
	two recent theories and consider the recovery of signals that have
	a sparse representation in a fusion frame. As in the classical CS,
	a sparse signal from a fusion frame can be sampled using very few
	random projections and eciently recovered using a convex optimization
	that minimizes the mixed `1=`2-norm. This problem has close connections
	with other similar recovery problems studied in CS literature such
	as block sparsity and joint sparsity problems. A key contribution
	in this thesis is to exploit the incoherence of the fusion frame
	subspaces in order to enhance the existing recovery results by incorporating
	this structure. In particular, we derive upper and lower bounds for
	the number of measurements required for the sparse recovery and the
	error derived by convex optimization. Aside from our results in the
	fusion frame setup, we also present results in the classical CS where
	we focus on improving constants appearing in the number of measurements
	required and prove optimal constants in the nonuniform setting with
	rather concise and simple proofs.},
  timestamp = {2017-06-23T09:52:54Z},
  school = {Rheinischen Friedrich-Wilhelms-Universit{\"a}t Bonn},
  author = {Ayaz, Ulas},
  year = {2014},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {afdidehf}
}

@article{Babaie-Zadeh2010,
  title = {On the Stable Recovery of the Sparsest Overcomplete Representations 	in Presence of Noise},
  volume = {58},
  issn = {1053-587X},
  doi = {10.1109/TSP.2010.2052357},
  abstract = {Let x be a signal to be sparsely decomposed over a redundant dictionary
	A, i.e., a sparse coefficient vector s has to be found such that
	x = As. It is known that this problem is inherently unstable against
	noise, and to overcome this instability, Donoho, Elad and Temlyakov
	["Stable recovery of sparse overcomplete representations in the presence
	of noise," IEEE Trans. Inf. Theory, vol. 52, no. 1, pp. 6-18, Jan.
	2006] have proposed to use an "approximate" decomposition, that is,
	a decomposition satisfying ?x - As?2 ? ? rather than satisfying the
	exact equality x = As. Then, they have shown that if there is a decomposition
	with ?s?0 <;; (1 + M-1)/2, where M denotes the coherence of the dictionary,
	this decomposition would be stable against noise. On the other hand,
	it is known that a sparse decomposition with ?s?0 <;; (1/2)spark(A)
	is unique. In other words, although a decomposition with ?s?0 <;;
	(1/2)spark(A) is unique, its stability against noise has been proved
	only for highly more restrictive decompositions satisfying ?s?0 <;;
	(1 + M-1)/2, because usually (1 + M-1)/2 ? (1/2)spark(A). This limitation
	maybe had not been very important before, because ?s?0 <;; (1 + M-1)/2
	is also the bound which guaranties that the sparse decomposition
	can be found via minimizing the l1 norm, a classic approach for sparse
	decomposition. However, with the availability of new algorithms for
	sparse decomposition, namely SL0 and robust-SLO, it would be important
	to know whether or not unique sparse decompositions with (1 + M-1)/2
	? ?s?0 <;; (1/2)spark(A) are stable. In this correspondence, we show
	that such decompositions are indeed stable. In other words, we extend
	the stability bou- - nd from ?s?0 <;; (1 + M-1)/2 to the whole uniqueness
	range ?s?0 <;; (1/2)spark(A). In summary, we show that all unique
	sparse decompositions are stably recoverable. Moreover, we see that
	sparser decompositions are "more stable".},
  timestamp = {2016-07-10T07:17:29Z},
  number = {10},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Babaie-Zadeh, M. and Jutten, C.},
  month = oct,
  year = {2010},
  keywords = {approximate decomposition,Collaborative work,compressed sensing,Dictionaries,International
	collaboration,Linear systems,matrix decomposition,Noise,overcomplete
	dictionaries,Permission,redundant dictionary,Senior members,Signal
	processing algorithms,signal representation,sparse component analysis
	(SCA),sparse decomposition,sparse recovery,sparse signal decomposition,sparsest
	overcomplete representation,Stability},
  pages = {5396-5400},
  owner = {Fardin}
}

@article{Bach2012,
  title = {Structured Sparsity through Convex Optimization},
  volume = {27},
  abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious
	representations of data or models. While naturally cast as a combinatorial
	optimization problem, variable or feature selection admits a convex
	relaxation through the regularization by the ? 1 -norm. In this paper,
	we consider situations where we are not only interested in sparsity,
	but where some structural prior knowledge is available as well. We
	show that the ? 1 -norm can then be extended to structured norms
	built on either disjoint or overlapping groups of variables, leading
	to a flexible framework that can deal with various structures. We
	present applications to unsupervised learning, for structured sparse
	principal component analysis and hierarchical dictionary learning,
	and to supervised learning in the context of nonlinear variable selection.},
  timestamp = {2016-07-11T16:58:38Z},
  number = {4},
  journal = {Statistical Science},
  author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
  year = {2012},
  keywords = {convex optimization,Sparsity},
  pages = {450-468},
  owner = {afdidehf}
}

@article{Bach2008,
  title = {Consistency of the Group Lasso and Multiple Kernel Learning},
  volume = {9},
  abstract = {We consider the least-square regression problem with regularization
	by a block `1-norm, that is, a sum of Euclidean norms over spaces
	of dimensions larger than one. This problem, referred to as the group
	Lasso, extends the usual regularization by the `1-norm where all
	spaces have dimension one, where it is commonly referred to as the
	Lasso. In this paper, we study the asymptotic group selection consistency
	of the group Lasso. We derive necessary and sufficient conditions
	for the consistency of group Lasso under practical assumptions, such
	as model misspecification. When the linear predictors and Euclidean
	norms are replaced by functions and reproducing kernel Hilbert norms,
	the problem is usually referred to as multiple kernel learning and
	is commonly used for learning from heterogeneous data sources and
	for non linear variable selection. Using tools from functional analysis,
	and in particular covariance operators, we extend the consistency
	results to this infinite dimensional case and also propose an adaptive
	scheme to obtain a consistent model estimate, even when the necessary
	condition required for the non adaptive scheme is not satisfied.},
  timestamp = {2016-07-08T12:03:05Z},
  journal = {Journal of Machine Learning Research},
  author = {Bach, Francis R.},
  year = {2008},
  keywords = {consistency,convex optimization,covariance operators,regularization,Sparsity},
  pages = {1179-1225},
  owner = {afdidehf}
}

@article{Bahrami2006,
  title = {Source localization for EEG and MEG},
  timestamp = {2016-07-10T08:23:22Z},
  author = {Bahrami, Bahador},
  year = {2006},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@inproceedings{Bahrampour2014,
  title = {Quality-Based Multimodal Classification Using Tree-Structured Sparsity},
  doi = {10.1109/CVPR.2014.524},
  abstract = {Recent studies have demonstrated advantages of information fusion
	based on sparsity models for multimodal classification. Among several
	sparsity models, tree-structured sparsity provides a flexible framework
	for extraction of cross-correlated information from different sources
	and for enforcing group sparsity at multiple granularities. However,
	the existing algorithm only solves an approximated version of the
	cost functional and the resulting solution is not necessarily sparse
	at group levels. This paper reformulates the tree-structured sparse
	model for multimodal classification task. An accelerated proximal
	algorithm is proposed to solve the optimization problem, which is
	an efficient tool for feature-level fusion among either homogeneous
	or heterogeneous sources of information. In addition, a (fuzzy-set-theoretic)
	possibilistic scheme is proposed to weight the available modalities,
	based on their respective reliability, in a joint optimization problem
	for finding the sparsity codes. This approach provides a general
	framework for quality-based fusion that offers added robustness to
	several sparsity-based multimodal classification algorithms. To demonstrate
	their efficacy, the proposed methods are evaluated on three different
	applications - multiview face recognition, multimodal face recognition,
	and target classification.},
  timestamp = {2016-07-10T07:39:12Z},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference 	on},
  author = {Bahrampour, S. and Ray, A. and Nasrabadi, N.M. and Jenkins, K.W.},
  month = jun,
  year = {2014},
  keywords = {accelerated proximal algorithm,cost functional,cross-correlated information
	extraction,Databases,Dictionaries,face recognition,feature-level
	fusion,fuzzy-set-theoretic possibilistic scheme,fuzzy set theory,heterogeneous
	information source,homogeneous information source,image classification,image
	fusion,information fusion,Joints,multimodal face recognition,multiview
	face recognition,optimisation,Optimization,optimization problem,possibility
	theory,quality-based multimodal classification,target classification,Training,trees
	(mathematics),tree-structured sparsity model,Vectors},
  pages = {4114-4121},
  owner = {Fardin}
}

@article{Bai2014,
  title = {Robust Visual Tracking Using Flexible Structured Sparse Representation},
  volume = {10},
  issn = {1551-3203},
  doi = {10.1109/TII.2013.2272089},
  abstract = {In this work, we propose a robust and flexible appearance model based
	on the structured sparse representation framework. In our method,
	we model the complex nonlinear appearance manifold and the occlusion
	as a sparse linear combination of structured union of subspaces in
	a basis library, which consists of multiple incremental learned target
	subspaces and a partitioned occlusion template set. In order to enhance
	the discriminative power of the model, a number of clustered background
	subspaces are also added into the basis library and updated during
	tracking. With the Block Orthogonal Matching Pursuit (BOMP) algorithm,
	we show that the new flexible structured sparse representation based
	appearance model facilitates the tracking performance compared with
	the prototype structured sparse representation model and other state
	of the art tracking algorithms.},
  timestamp = {2016-07-10T08:09:02Z},
  number = {1},
  journal = {Industrial Informatics, IEEE Transactions on},
  author = {Bai, Tianxiang and Li, Youfu},
  month = feb,
  year = {2014},
  keywords = {Appearance model,block orthogonal matching pursuit,block orthogonal
	matching pursuit algorithm,BOMP,clustered background subspaces,complex
	nonlinear appearance manifold,flexible appearance model,flexible
	structured sparse representation,image matching,image representation,learning
	(artificial intelligence),multiple incremental learned target subspaces,object
	tracking,partitioned occlusion template set,pattern clustering,robust
	visual tracking,set theory,sparse representation,visual tracking},
  pages = {538-547},
  owner = {afdidehf}
}

@inproceedings{Bai2012,
  title = {Structured compressive sensing for robust and fast visual tracking},
  doi = {10.1109/ICSENS.2012.6411584},
  abstract = {The application of compressive sensing to optical sensing has received
	significant attention recently. In this work, we propose a structured
	compressive sensing based tracking algorithm for intelligent optical
	sensing, which exploits the random feature reduction and the structured
	sparse representation of the target visual appearances. The robustness
	of the tracker can be achieved by seeking the structured sparse solution
	of the compressive sensing problem. The efficiency of the tracker
	is improved by a random feature reduction together with the Block
	Orthogonal Matching Pursuit (BOMP) algorithm. We conduct experiments
	and show that with an appropriate random reduction of feature dimension,
	the proposed method can achieve a more efficient tracking without
	losing the robustness compared with the reference trackers.},
  timestamp = {2016-07-11T16:57:14Z},
  booktitle = {Sensors, 2012 IEEE},
  author = {Bai, Tianxiang and Li, Youfu and Liu, Jianyang},
  month = oct,
  year = {2012},
  keywords = {Accuracy,block orthogonal matching pursuit algorithm,BOMP,compressed
	sensing,fast visual tracking algorithm,intelligent optical sensing,intelligent
	sensors,iterative methods,Matching pursuit algorithms,optical sensors,random
	feature dimension reduction,random feature reduction,random processes,Robustness,signal
	representation,structured compressive sensing,structured sparse representation,target tracking,Target
	tracking,target visual appearance,time-frequency
	analysis,Vectors,Visualization},
  pages = {1-4},
  owner = {afdidehf}
}

@inproceedings{Bai2013,
  title = {Learning appearance manifolds with structured sparse representation 	for robust visual tracking},
  doi = {10.1109/ICRA.2013.6631409},
  abstract = {This paper presents a novel algorithm for robust visual object tracking
	based on the structured sparse representation framework. Conventional
	structured sparse representation based tracker models the nonlinear
	appearance manifold with a single subspace that is difficult to handle
	significant pose and illumination changes. Different from the afore-mentioned
	method, the proposed algorithm approximates the nonlinear appearance
	manifold by multiple low dimensional subspaces computed by an incremental
	learning scheme based on the merging and insert strategy. In order
	to enhance the discriminative power of the model, a number of clustered
	background subspaces are also added into the basis library and updated
	during tracking. With the Block Orthogonal Matching Pursuit (BOMP)
	algorithm, we show that the complex nonlinear appearance manifold
	can effectively represent by a sparse linear combination of structured
	union of subspaces. Experiments on benchmark video sequences show
	that the new structured sparse representation model improves the
	robustness of tracking.},
  timestamp = {2016-07-09T19:54:58Z},
  booktitle = {Robotics and Automation (ICRA), 2013 IEEE International Conference 	on},
  author = {Bai, Tianxiang and Li, Y.F. and Shao, Zhanpeng},
  month = may,
  year = {2013},
  keywords = {benchmark video sequences,block orthogonal matching pursuit algorithm,BOMP
	algorithm,clustered background subspaces,illumination change,image
	representation,image sequences,incremental learning scheme,insert
	strategy,learning (artificial intelligence),Libraries,Manuals,merging
	strategy,multiple low dimensional subspaces,nonlinear appearance
	manifold learning,object tracking,pose change,robust visual object
	tracking,Silicon,structured sparse representation based tracker models,Target
	tracking,video signal processing},
  pages = {5788-5793},
  owner = {afdidehf}
}

@inproceedings{Bai2012a,
  title = {Flexible structured sparse representation for robust visual tracking},
  doi = {10.1109/MFI.2012.6343073},
  abstract = {In this work, we propose a robust and flexible appearance model based
	on the structured sparse representation framework. In our method,
	we model the complex nonlinear appearance manifold and occlusions
	as a sparse linear combination of structured union of subspaces in
	a basis library consisting of multiple learned low dimensional subspaces
	and a partitioned occlusion template set. In order to enhance the
	discriminative power of the model, a number of clustered background
	subspaces are also added into the basis library and updated during
	tracking. With the Block Orthogonal Matching Pursuit (BOMP) algorithm,
	we show that the new structured sparse representation based appearance
	model facilitates the tracking performance compared with the prototype
	model and other state of the art tracking algorithms.},
  timestamp = {2016-07-08T12:31:33Z},
  booktitle = {Multisensor Fusion and Integration for Intelligent Systems (MFI), 	2012 IEEE Conference on},
  author = {Bai, Tianxiang and Li, Y.F. and Tang, Yazhe},
  month = sep,
  year = {2012},
  keywords = {Appearance model,basis library,block orthogonal matching pursuit,BOMP
	algorithm,Clustering algorithms,complex nonlinear appearance manifold,Computational
	modeling,computer graphics,flexible structured sparse representation,image
	representation,Libraries,Manifolds,object tracking,partitioned occlusion
	template set,robust visual tracking,sparse representation,Target
	tracking,Vectors,Visualization,visual tracking},
  pages = {174-179},
  owner = {afdidehf}
}

@article{Baillet1997,
  title = {A Bayesian approach to introducing anatomo-functional priors in the 	EEG/MEG inverse problem},
  volume = {44},
  issn = {0018-9294},
  doi = {10.1109/10.568913},
  abstract = {We present a new approach to the recovering of dipole magnitudes in
	a distributed source model for magnetoencephalographic (MEG) and
	electroencephalographic (EEG) imaging. This method consists in introducing
	spatial and temporal a priori information as a cure to this ill-posed
	inverse problem. A nonlinear spatial regularization scheme allows
	the preservation of dipole moment discontinuities between some a
	priori noncorrelated sources, for instance, when considering dipoles
	located on both sides of a sulcus. Moreover, we introduce temporal
	smoothness constraints on dipole magnitude evolution at time scales
	smaller than those of cognitive processes. These priors are easily
	integrated into a Bayesian formalism, yielding a maximum a posteriori
	(MAP) estimator of brain electrical activity. Results from EEG simulations
	of our method are presented and compared with those of classical
	quadratic regularization and a now popular generalized minimum-norm
	technique called low-resolution electromagnetic tomography (LORETA).},
  timestamp = {2016-07-08T10:04:04Z},
  number = {5},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Baillet, S. and Garnero, L.},
  month = may,
  year = {1997},
  keywords = {Algorithms,anatomo-functional priors,Bayesian approach,Bayesian methods,Bayes
	methods,Bayes Theorem,brain electrical activity,Brain Mapping,Brain
	modeling,Cognition,cognitive processes,Computer-Assisted,dipole magnitude
	evolution,dipole magnitudes,dipole moment discontinuities,distributed
	source model,EEG/MEG inverse problem,Electrodes,electroencephalographic
	imaging,electroencephalography,generalized minimum-norm technique,Image
	Processing,Image resolution,inverse problems,LORETA,low-resolution
	electromagnetic tomography,Magnetic moments,Magnetic resonance imaging,magnetoencephalographic
	imaging,magnetoencephalography,MAP estimator,maximum a posteriori
	estimator,Maximum likelihood estimation,medical signal processing,Models,Neurological,noncorrelated
	sources,Nonlinear Dynamics,nonlinear spatial regularization,Positron
	emission tomography,Psychology,quadratic regularization,signal reconstruction,spatial
	a priori information,Spatial resolution,Statistical,sulcus,Surface
	Properties,temporal a priori information,temporal smoothness constraints},
  pages = {374-385},
  owner = {afdidehf}
}

@article{Baillet1999,
  title = {Combined MEG and EEG source imaging by minimization of mutual information},
  volume = {46},
  issn = {0018-9294},
  doi = {10.1109/10.759053},
  abstract = {Though very frequently assumed, the necessity to operate a joint processing
	of simultaneous magnetoencephalography (MEG) and electroencephalography
	(EEG) recordings for functional brain imaging has never been clearly
	demonstrated. However, the very last generation of MEG instruments
	allows the simultaneous recording of brain magnetic fields and electrical
	potentials on the scalp. But the general fear regarding the fusion
	between MEG and EEG data is that the drawbacks from one modality
	will systematically spoil the performances of the other one without
	any consequent improvement. This is the case for instance for the
	estimation of deeper or radial sources with MEG. In this paper, the
	authors propose a method for a cooperative processing of MEG and
	EEG in a distributed source model. First, the evaluation of the respective
	performances of each modality for the estimation of every dipole
	in the source pattern is made using a conditional entropy criterion.
	Then, the algorithm operates a preprocessing of the MEG and EEG gain
	matrices which minimizes the mutual information between these two
	transfer functions, by a selective weighting of the MEG and EEG lead
	fields. This new combined EEG/MEG modality brings major improvements
	to the localization of active sources, together with reduced sensitivity
	to perturbations on data.},
  timestamp = {2016-10-21T13:31:16Z},
  number = {5},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Baillet, S. and Garnero, L. and Marin, G. and Hugonin, J.-P.},
  month = may,
  year = {1999},
  note = {read},
  keywords = {Bayes Theorem,Brain,brain functional information,combined MEG/EEG
	source imaging,Computer-Assisted,conditional entropy criterion,cooperative
	processing,distributed source model,Electric potential,electroencephalography,Electromagnetic
	Fields,Entropy,Fusion power generation,Humans,Instruments,Magnetic
	fields,Magnetic recording,Magnetic resonance imaging,magnetoencephalography,medical
	diagnostic imaging,medical image processing,minimisation,Models,Mutual
	information,mutual information minimization,Neurological,Nonlinear
	Dynamics,perturbations sensitivity,Scalp,signal processing,Skull,source
	pattern dipole,source reconstruction},
  pages = {522--534},
  owner = {Fardin}
}

@article{Baillet2001,
  title = {Electromagnetic brain mapping},
  volume = {18},
  issn = {1053-5888},
  doi = {10.1109/79.962275},
  abstract = {There has been tremendous advances in our ability to produce images
	of human brain function. Applications of functional brain imaging
	extend from improving our understanding of the basic mechanisms of
	cognitive processes to better characterization of pathologies that
	impair normal function. Magnetoencephalography (MEG) and electroencephalography
	(EEG) (MEG/EEG) localize neural electrical activity using noninvasive
	measurements of external electromagnetic signals. Among the available
	functional imaging techniques, MEG and EEG uniquely have temporal
	resolutions below 100 ms. This temporal precision allows us to explore
	the timing of basic neural processes at the level of cell assemblies.
	MEG/EEG source localization draws on a wide range of signal processing
	techniques including digital filtering, three-dimensional image analysis,
	array signal processing, image modeling and reconstruction, and,
	blind source separation and phase synchrony estimation. We describe
	the underlying models currently used in MEG/EEG source estimation
	and describe the various signal processing steps required to compute
	these sources. In particular we describe methods for computing the
	forward fields for known source distributions and parametric and
	imaging-based approaches to the inverse problem},
  timestamp = {2016-07-08T12:20:41Z},
  number = {6},
  journal = {Signal Processing Magazine, IEEE},
  author = {Baillet, S. and Mosher, J.C. and Leahy, R.M.},
  month = nov,
  year = {2001},
  keywords = {array signal processing,blind source separation,Brain Mapping,Brain
	Mapping,Brain modeling,Brain
	modeling,cell assemblies,cognitive processes,digital filtering,digital
	filtering,Digital signal processing,Digital
	signal processing,EEG,electroencephalography,electromagnetic
	brain mapping,electromagnetic brain
	mapping,external electromagnetic signals,functional brain imaging,human
	brain function,Humans,image analysis,image modeling,Image reconstruction,Image
	resolution,magnetoencephalography,medical image processing,MEG,MEG/EEG,MEG/EEG
	source estimation,MEG/EEG source localization,neural electrical activity,neural
	process timing,noninvasive measurements,Noninvasive treatment,Pathology,Phased
	arrays,phase synchrony estimation,reviews,signal processing techniques,temporal
	resolutions},
  pages = {14-30},
  annote = {read}
}

@article{Bajwa2015,
  title = {Conditioning of Random Block Subdictionaries With Applications to 	Block-Sparse Recovery and Regression},
  volume = {61},
  issn = {0018-9448},
  doi = {10.1109/TIT.2015.2429632},
  abstract = {The linear model, in which a set of observations is assumed to be
	given by a linear combination of columns of a matrix (often termed
	a dictionary), has long been the mainstay of the statistics and signal
	processing literature. One particular challenge for inference under
	linear models is understanding the conditions on the dictionary under
	which reliable inference is possible. This challenge has attracted
	renewed attention in recent years, since many modern inference problems
	(e.g, high-dimensional statistics and compressed sensing) deal with
	the underdetermined setting, in which the number of observations
	is much smaller than the number of columns in the dictionary. This
	paper makes several contributions for this setting when the set of
	observations is given by a linear combination of a small number of
	groups of columns of the dictionary, termed the block-sparse case.
	First, it specifies conditions on the dictionary under which most
	block submatrices of the dictionary (often termed block subdictionaries)
	are well conditioned. This result is fundamentally different from
	prior work on block-sparse inference because: 1) it provides conditions
	that can be explicitly computed in polynomial time; 2) the given
	conditions translate into near-optimal scaling of the number of columns
	of the block subdictionaries as a function of the number of observations
	for a large class of dictionaries; and 3) it suggests that the spectral
	norm, rather than the column/block coherences of the dictionary,
	fundamentally limits the scaling of dimensions of the well-conditioned
	block subdictionaries. Second, in order to help understand the significance
	of this result in the context of block-sparse inference, this paper
	investigates the problems of block-sparse recovery and block-sparse
	regression in underdetermined settings. In both of these problems,
	this paper utilizes its result concerning conditioning of block subdictionaries
	and establishes that near-optimal block-sparse recovery and blo-
	k-sparse regression is possible for a large class of dictionaries
	as long as the dictionary satisfies easily computable conditions
	and the coefficients describing the linear combination of groups
	of columns can be modeled through a mild statistical prior. Third,
	the paper reports extensive numerical experiments that highlight
	the effects of different measures of the dictionary in block-sparse
	inference problems.},
  timestamp = {2016-07-08T12:02:23Z},
  number = {7},
  journal = {Information Theory, IEEE Transactions on},
  author = {Bajwa, W.U. and Duarte, M.F. and Calderbank, R.},
  month = jul,
  year = {2015},
  keywords = {Block-sparse inference,block-sparse inference problems,block-sparse
	recovery,block-sparse regression,block submatrices,Coherence,column-block
	coherences,compressed sensing,Context,Dictionaries,group lasso,group
	sparsity,high-dimensional statistics,inference mechanisms,linear
	combination,linear model,Linear regression,multiple measurement vectors,near-optimal
	block-sparse recovery,polynomial matrices,Polynomials,polynomial
	time,random block subdictionaries,Random variables,regression analysis,signal
	processing,spectral norm},
  pages = {4060-4079},
  owner = {afdidehf}
}

@inproceedings{Bajwa2009,
  title = {A restricted isometry property for structurally-subsampled unitary 	matrices},
  doi = {10.1109/ALLERTON.2009.5394883},
  abstract = {Subsampled (or partial) Fourier matrices were originally introduced
	in the compressive sensing literature by Candes et al. Later, in
	papers by Candes and Tao and Rudelson and Vershynin, it was shown
	that (random) subsampling of the rows of many other classes of unitary
	matrices also yield effective sensing matrices. The key requirement
	is that the rows of U, the unitary matrix, must be highly incoherent
	with the basis in which the signal is sparse. In this paper, we consider
	acquisition systems that - despite sensing sparse signals in an incoherent
	domain - cannot randomly subsample rows from U. We consider a general
	class of systems in which the sensing matrix corresponds to subsampling
	of the rows of matrices of the form ¿ = RU (instead of U), where
	R is typically a low-rank matrix whose structure reflects the physical/technological
	constraints of the acquisition system. We use the term ¿structurally-subsampled
	unitary matrices¿ to describe such sensing matrices. We investigate
	the restricted isometry property of a particular class of structurally-subsampled
	unitary matrices that arise naturally in application areas such as
	multiple-antenna channel estimation and sub-nyquist sampling. In
	addition, we discuss an immediate application of this work in the
	area of wireless channel estimation, where the main results of this
	paper can be applied to the estimation of multiple-antenna orthogonal
	frequency division multiplexing channels that have sparse impulse
	responses.},
  timestamp = {2016-07-08T11:27:14Z},
  booktitle = {Communication, Control, and Computing, 2009. Allerton 2009. 47th 	Annual Allerton Conference on},
  author = {Bajwa, W.U. and Sayeed, A.M. and Nowak, R.},
  month = sep,
  year = {2009},
  keywords = {acquisition systems,Area measurement,channel estimation,Compressive
	sensing,Fourier analysis,frequency estimation,Mathematics,matrix
	algebra,multiple antenna orthogonal frequency division multiplexing
	channel,OFDM,OFDM modulation,partial Fourier matrix,Reliability theory,restricted
	isometry property,Sampling methods,signal processing,signal sampling,sparse
	impulse response,sparse matrices,structurally subsampled unitary
	matrix,wireless channel estimation,Wireless sensor networks},
  pages = {1005-1012},
  owner = {Fardin}
}

@article{Balan2005,
  title = {Equivalence principle for optimization of sparse versus low-spread 	representations for signal estimation in noise},
  volume = {15},
  issn = {1098-1098},
  doi = {10.1002/ima.20034},
  abstract = {Estimation of a sparse signal representation, one with the minimum
	number of nonzero components, is hard. In this paper, we show that
	for a nontrivial set of the input data the corresponding optimization
	problem is equivalent to and can be solved by an algorithm devised
	for a simpler optimization problem. The simpler optimization problem
	corresponds to estimation of signals under a low-spread constraint.
	The goal of the two optimization problems is to minimize the Euclidian
	norm of the linear approximation error with an lp penalty on the
	coefficients, for p = 0 (sparse) and p = 1 (low-spread), respectively.
	The l0 problem is hard, whereas the l1 problem can be solved efficiently
	by an iterative algorithm. Here we precisely define the l0 optimization
	problem, construct an associated l1 optimization problem, and show
	that for a set with open interior of the input data the optimizers
	of the two optimization problems have the same support. The associated
	l1 optimization problem is used to find the support of the l0 optimizer.
	Once the support of the l0 problem is known, the actual solution
	is easily found by solving a linear system of equations. However,
	we point out our approach does not solve the harder optimization
	problem for all input data and thus may fail to produce the optimal
	solution in some cases. © 2005 Wiley Periodicals, Inc. Int J Imaging
	Syst Technol, 15, 10–17, 2005; Published online in Wiley InterScience
	(www.interscience.wiley.com). DOI 10.1002/ima.20034},
  timestamp = {2016-07-08T12:21:25Z},
  number = {1},
  journal = {International Journal of Imaging Systems and Technology},
  author = {Balan, Radu V. and Rosca, Justinian and Rickard, Scott},
  year = {2005},
  keywords = {l0 quasi-norm,l1 norm,Optimization,sparse signal},
  pages = {10-17},
  owner = {Fardin}
}

@article{Baldassarre2013,
  title = {Group-Sparse Model Selection: Hardness and Relaxations},
  volume = {abs/1303.3207},
  abstract = {Group-based sparsity models are proven instrumental in linear regression
	problems for recovering signals from much fewer measurements than
	standard compressive sensing. The main promise of these models is
	the recovery of �interpretable� signals through the identification
	of their constituent groups. In this paper, we establish a combinatorial
	framework for group-model selection problems and highlight the underlying
	tractability issues. In particular, we show that the group-model
	selection problem is equivalent to the well-known NP-hard weighted
	maximum coverage problem (WMC). Leveraging a graph-based understanding
	of group models, we describe group structures which enable correct
	model selection in polynomial time via dynamic programming. Furthermore,
	group structures that lead to totally unimodular constraints have
	tractable discrete as well as convex relaxations. We also present
	a generalization of the group-model that allows for within group
	sparsity, which can be used to model hierarchical sparsity. Finally,
	we study the Pareto frontier of group-sparse approximations for two
	tractable models, among which the tree sparsity model, and illustrate
	selection and computation trade-offs between our framework and the
	existing convex relaxations.},
  timestamp = {2016-07-08T12:40:06Z},
  journal = {CoRR},
  author = {Baldassarre, Luca and Bhan, Nirav and Cevher, Volkan and Kyrillidis, Anastasios T.},
  year = {2013},
  keywords = {Compressive Sensing.,Dynamic Programming,Interpretability,Signal Approximation,structured
	sparsity,Tractability},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1303-3207},
  owner = {afdidehf}
}

@inproceedings{Baldassarre2012,
  title = {A General Framework for Structured Sparsity via Proximal Optimization},
  abstract = {We study a generalized framework for structured sparsity. It extends
	the well known methods of Lasso and Group Lasso by incorporating
	additional constraints on the variables as part of a convex optimization
	problem. This framework provides a straightforward way of favouring
	prescribed sparsity patterns, such as orderings, contiguous regions
	and overlapping groups, among others. Available optimization methods
	are limited to specific constraint sets and tend to not scale well
	with sample size and dimensionality. We propose a first order proximal
	method, which builds upon results on fixed points and successive
	approximations. The algorithm can be applied to a general class of
	conic and norm constraints sets and relies on a proximity operator
	subproblem which can be computed numerically. Experiments on different
	regression problems demonstrate state-of-the-art statistical performance,
	which improves over Lasso, Group Lasso and StructOMP. They also demonstrate
	the efficiency of the optimization algorithm and its scalability
	with the size of the problem.},
  timestamp = {2016-07-08T10:17:30Z},
  booktitle = {15th International Conference on Artificial Intelligence and Statistics 	(AISTATS)},
  author = {Baldassarre, Luca and Morales, Jean and Argyriou, Andreas and Pontil, Massimiliano},
  year = {2012},
  owner = {afdidehf}
}

@inproceedings{Baldassarre2012a,
  title = {Structured Sparsity Models for Brain Decoding from fMRI Data},
  doi = {10.1109/PRNI.2012.31},
  abstract = {Structured sparsity methods have been recently proposed that allow
	to incorporate additional spatial and temporal information for estimating
	models for decoding mental states from fMRI data. These methods carry
	the promise of being more interpretable than simpler Lasso or Elastic
	Net methods. However, despite sparsity has often been advocated as
	leading to more interpretable models, we show that by itself sparsity
	and also structured sparsity could lead to unstable models. We present
	an extension of the Total Variation method and assess several other
	structured sparsity models on accuracy, sparsity and stability. Our
	results indicate that structured sparsity via the Sparse Total Variation
	can mitigate some of the instability inherent in simpler sparse methods,
	but more research is required to build methods that can reliably
	infer relevant activation patterns from fMRI data.},
  timestamp = {2016-07-11T16:58:11Z},
  booktitle = {Pattern Recognition in NeuroImaging (PRNI), 2012 International Workshop 	on},
  author = {Baldassarre, L. and Mourao-Miranda, J. and Pontil, M.},
  month = jul,
  year = {2012},
  keywords = {Accuracy,activation patterns,biomedical MRI,Brain,brain decoding,Brain
	models,Decoding,elastic net methods,fMRI,fMRI data,learning (artificial
	intelligence),medical image processing,sparse total variation method,spatial
	information,Stability,Stability criteria,Structured Sparsity,structured
	sparsity models,supervised machine learning techniques,temporal information,Vectors},
  pages = {5-8},
  owner = {afdidehf}
}

@inproceedings{Balouchestani2013,
  title = {Low sampling-rate approach for ECG signals with compressed sensing 	theory},
  doi = {10.1109/ICCME.2013.6548214},
  abstract = {A Wireless Body Area network (WBAN) is a special purpose of Wireless
	Sensor Networks (WSNs) to connect various Biomedical Wireless Sensors
	(BWSs) located inside and outside of the human body to collect and
	transmit vital signals. The collected biomedical data send out via
	Gate Way (GW) to external databases at the hospitals and medical
	centers for diagnostic and therapeutic purposes. The electrocardiogram
	(ECG) signals are widely used in health care systems because they
	are noninvasive mechanisms to establish medical diagnosis of heart
	diseases. In order to fully exploit the benefits of WBANs to Electronic
	Health (EH), Mobile Health (MH), and Ambulatory Health Monitoring
	Systems (AHMS) the power consumption and sampling rate should be
	restricted to a minimum. With this in mind, Compressed Sensing (CS)
	procedure and the collaboration of Block Sparse Bayesian Learning
	(BSBL)based on Dynamic Thresholding Approach (DTA) is used to provide
	a robust low sampling-rate approach for normal and abnormal ECG signals.
	Advanced WBANs based on our approach will be able to deliver healthcare
	not only to patients in hospital and medical centers; but also in
	their homes and workplaces thus offering cost saving, and improving
	the quality of life. Our simulation results illustrate 35% reduction
	of Percentage Root-mean-square Difference (PRD) and a good level
	of quality for Signal to Noise Ratio (SNR).},
  timestamp = {2016-07-09T19:58:32Z},
  booktitle = {Complex Medical Engineering (CME), 2013 ICME International Conference 	on},
  author = {Balouchestani, M. and Raahemifar, K. and Krishnan, S.},
  month = may,
  year = {2013},
  keywords = {AHMS,ambulatory health monitoring systems,belief networks,biomedical
	data,Biomedical measurement,biomedical wireless sensors,Bit error
	rate,block sparse Bayesian learning,body area networks,body sensor
	networks,BSBL,cardiology,compressed sensing,compressed sensing theory,diseases,DTA,dynamic
	thresholding approach,ECG Signal,ECG signals,EH,electrocardiogram
	signals,electrocardiography,electronic health,gate way,health care,heart
	diseases,hospitals,human body,learning (artificial intelligence),low
	sampling-rate approach,medical centers,medical diagnosis,medical
	diagnostic imaging,medical signal processing,MH,mobile health,percentage
	root-mean-square difference,power consumption,Sampling-rate,signal
	sampling,Signal-To-Noise Ratio,Signal to noise ratio,Simulation,therapeutic
	purposes,WBAN,wireless body area network,Wireless sensor networks},
  pages = {70-75},
  owner = {afdidehf}
}

@inproceedings{Balouchestani2013a,
  title = {New sampling approach for wireless ECG systems with compressed sensing 	theory},
  doi = {10.1109/MeMeA.2013.6549738},
  abstract = {Wireless Body Area Networks (WBANs) consist of small intelligent biomedical
	wireless sensors attached on or implanted to the body to collect
	vital biomedical data such as electrocardiogram (ECG) signals to
	provide continuous health monitoring systems for diagnostic and therapeutic
	purposes. ECG signals are widely used in health care systems because
	they are noninvasive mechanisms to establish medical diagnosis of
	heart diseases. In order to fully exploit the benefits of WBANs to
	Electronic Health (EH), Mobile Health (MH), and Ambulatory Health
	Monitoring Systems (AHMS) the power consumption and sampling rate
	should be restricted to a minimum. With this in mind, Compressed
	Sensing (CS) procedure and the collaboration of Block Sparse Bayesian
	Learning (BSBL) framework is used to provide new sampling approach
	for wireless ECG systems with CS theory. Advanced wireless ECG systems
	based on our approach will be able to deliver healthcare not only
	to patients in hospital and medical centers; but also in their homes
	and workplaces thus offering cost saving, and improving the quality
	of life. Our simulation results illustrate 25% reduction of Percentage
	Root-mean-square Difference (PRD) and a good level of quality for
	Signal to Noise Ratio (SNR), sampling-rate, and power consumption.},
  timestamp = {2016-07-10T06:53:24Z},
  booktitle = {Medical Measurements and Applications Proceedings (MeMeA), 2013 IEEE 	International Symposium on},
  author = {Balouchestani, M. and Raahemifar, K. and Krishnan, S.},
  month = may,
  year = {2013},
  keywords = {ambulatory health monitoring systems,block sparse Bayesian learning,Block
	Sparse Bayesian learning,body area networks,compressed sensing,compressed
	sensing theory,continuous health monitoring,diseases,electrocardiogram,electrocardiography,electronic
	health,health care,healthcare,heart disease,intelligent sensors,medical
	signal processing,mobile health,power consumption,Power demand,sampling
	approach,Sampling-rate,Sensors,Signal to noise ratio,Simulation,wireless
	body area networks,Wireless communication,Wireless ECG Signal,wireless
	ECG system,Wireless sensor networks},
  pages = {213-218},
  owner = {afdidehf}
}

@article{BANKS2009,
  title = {Inverse Problems Tutorial},
  timestamp = {2016-07-09T19:44:50Z},
  author = {BANKS, H. T. and DAVIDIAN, MARIE},
  year = {2009},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {N. C. STATE UNIVERSITY},
  owner = {Fardin}
}

@article{Baraniuk2010,
  title = {Model-Based Compressive Sensing},
  volume = {56},
  issn = {0018-9448},
  doi = {10.1109/TIT.2010.2040894},
  abstract = {Compressive sensing (CS) is an alternative to Shannon/Nyquist sampling
	for the acquisition of sparse or compressible signals that can be
	well approximated by just K ¿ N elements from an N -dimensional
	basis. Instead of taking periodic samples, CS measures inner products
	with M < N random vectors and then recovers the signal via a sparsity-seeking
	optimization or greedy algorithm. Standard CS dictates that robust
	signal recovery is possible from M = O(K log(N/K)) measurements.
	It is possible to substantially decrease M without sacrificing robustness
	by leveraging more realistic signal models that go beyond simple
	sparsity and compressibility by including structural dependencies
	between the values and locations of the signal coefficients. This
	paper introduces a model-based CS theory that parallels the conventional
	theory and provides concrete guidelines on how to create model-based
	recovery algorithms with provable performance guarantees. A highlight
	is the introduction of a new class of structured compressible signals
	along with a new sufficient condition for robust structured compressible
	signal recovery that we dub the restricted amplification property,
	which is the natural counterpart to the restricted isometry property
	of conventional CS. Two examples integrate two relevant signal models-wavelet
	trees and block sparsity-into two state-of-the-art CS recovery algorithms
	and prove that they offer robust recovery from just M = O(K) measurements.
	Extensive numerical simulations demonstrate the validity and applicability
	of our new theory and algorithms.},
  timestamp = {2016-07-09T20:13:49Z},
  number = {4},
  journal = {Information Theory, IEEE Transactions on},
  author = {Baraniuk, R.G. and Cevher, V. and Duarte, M.F. and Hegde, C.},
  month = apr,
  year = {2010},
  keywords = {block sparsity,compressibility,compressible signals,compressive sensing,Concrete,Costs,greedy
	algorithm,Greedy algorithms,Guidelines,image coding,information theory,Measurement
	standards,model-based compressive sensing,model-based recovery algorithms,Nyquist
	sampling,realistic signal models,restricted amplification property,restricted
	isometry property,Robustness,robust structured compressible signal
	recovery,Sampling methods,Shannon sampling,signal coefficients,signal
	model,sparse matrices,Sparsity,sparsity-seeking optimization,Sufficient
	conditions,Transform coding,union of subspaces,wavelet tree},
  pages = {1982-2001},
  owner = {afdidehf}
}

@article{Baraniuk2008,
  title = {A Simple Proof of the Restricted Isometry Property for Random Matrices},
  volume = {28},
  issn = {0176-4276},
  doi = {10.1007/s00365-007-9003-x},
  abstract = {We give a simple technique for verifying the Restricted Isometry Property
	(as introduced by Candes and Tao) for random matrices that underlies
	Compressed Sensing. Our approach has two main ingredients: (i) concentration
	inequalities for random inner products that have recently provided
	algorithmically simple proofs of the JohnsonLindenstrauss lemma;
	and (ii) covering numbers for finite-dimensional balls in Euclidean
	space. This leads to an elementary proof of the Restricted Isometry
	Property and brings out connections between Compressed Sensing and
	the Johnson Lindenstrauss lemma. As a result, we obtain simple
	and direct proofs of Kashins theorems on widths of finite balls
	in Euclidean space (and their improvements due to Gluskin) and proofs
	of the existence of optimal Compressed Sensing measurement matrices.
	In the process, we also prove that these measurements have a certain
	universality with respect to the sparsity-inducing basis.},
  language = {English},
  timestamp = {2017-06-23T13:03:35Z},
  number = {3},
  journal = {Constructive Approximation},
  author = {Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
  year = {2008},
  keywords = {15A52,15N2,60F10,94A12,94A20,compressed sensing,Concentration inequalities,random
	matrices,Sampling},
  pages = {253--263},
  owner = {afdidehf}
}

@mastersthesis{Barran2013,
  title = {Sparse Linear Models and Proximal Optimization},
  abstract = {Sparse models are growing in popularity as the amount of data available
	is becoming bigger. Feature selection methods have been around for
	many years, but the problems to solve evolve quite rapidly. If ten
	years ago a dataset with hundreds of variables was considered quite
	large, today datasets with ten thousands of variables are a common
	problem and easy to nd. Hence, sparse models are not evaluated only
	by the error of the t but also by its interpretability. This means
	that sometimes we might prefer a model with \worse" error if we can
	learn some structure of the problem from it. In this thesis we will
	provide an overview of the classic sparse models, starting with the
	Lasso and, from that point, we will dicuss the more modern proximal
	optimization theory. The main result is the FISTA algorithm, which
	is a general framework to optimize any combination of dierentiable
	function and convex regularization. An implementation of FISTA in
	ANSI C was also developed along with this thesis, and it is publicly
	available on the Internet. Finally we will present some experiments
	where all the previous models are compared using real-world datasets.},
  timestamp = {2016-07-11T16:49:01Z},
  author = {Barran, Alberto Torres},
  month = sep,
  year = {2013},
  owner = {Fardin}
}

@article{BattiniSonmez2013,
  title = {Critical parameters of the sparse representation-based classifier},
  volume = {7},
  issn = {1751-9632},
  doi = {10.1049/iet-cvi.2012.0127},
  abstract = {In recent years, the growing attention in the study of the compressive
	sensing (CS) theory suggested a novel classification algorithm called
	sparse representation-based classifier (SRC), which obtained promising
	results by casting classification as a sparse representation problem.
	Whereas SRC has been applied to different fields of applications
	and several variations of it have been proposed, less attention has
	been given to its critical parameters, that is, measurements correlated
	to its performance. This work underlines the differences between
	CS and SRC, it gives a mathematical definition of five measurements
	possible correlated to the performance of SRC and identifies three
	of them as critical parameters. The knowledge of the critical parameters
	is necessary to fuse multiple scores of SRC classifiers allowing
	for classification. The authors addressed the problem of two-dimensional
	face classification: using the Extended Yale B dataset to monitor
	the critical parameters and the Extended Cohn-Kanade database to
	test the robustness of SRC with emotional faces. Finally, the authors
	increased the initial performance of the holistic SRC with a block-based
	SRC, which uses one critical parameter for automatic selection of
	the most successful blocks.},
  timestamp = {2016-07-08T12:06:26Z},
  number = {6},
  journal = {Computer Vision, IET},
  author = {Battini Sonmez, E. and Albayrak, S.},
  month = dec,
  year = {2013},
  keywords = {block-based SRC,compressed sensing,compressive sensing theory,CS,emotional
	face,extended Cohn-Kanade database,extended Yale B dataset,face recognition,holistic
	SRC,image classification,sparse representation-based classifier,SRC
	classifiers,two-dimensional face classification},
  pages = {500-507},
  owner = {afdidehf}
}

@article{Becker2015,
  title = {Brain-Source Imaging: From sparse to tensor models},
  volume = {32},
  issn = {1053-5888},
  doi = {10.1109/MSP.2015.2413711},
  abstract = {A number of application areas such as biomedical engineering require
	solving an underdetermined linear inverse problem. In such a case,
	it is necessary to make assumptions on the sources to restore identifiability.
	This problem is encountered in brain-source imaging when identifying
	the source signals from noisy electroencephalographic or magnetoencephalographic
	measurements. This inverse problem has been widely studied during
	recent decades, giving rise to an impressive number of methods using
	different priors. Nevertheless, a thorough study of the latter, including
	especially sparse and tensor-based approaches, is still missing.
	In this article, we propose 1) a taxonomy of the algorithms based
	on methodological considerations; 2) a discussion of the identifiability
	and convergence properties, advantages, drawbacks, and application
	domains of various techniques; and 3) an illustration of the performance
	of seven selected methods on identical data sets. Directions for
	future research in the area of biomedical imaging are eventually
	provided.},
  timestamp = {2016-07-08T11:49:20Z},
  number = {6},
  journal = {Signal Processing Magazine, IEEE},
  author = {Becker, H. and Albera, L. and Comon, P. and Gribonval, R. and Wendling, F. and Merlet, I.},
  month = nov,
  year = {2015},
  keywords = {bioelectric potentials,biomedical engineering,Biomedical signal processing,Brain
	modeling,brain-source imaging,Distribution functions,electroencephalography,Graphical
	models,inverse problems,linear inverse problem,magnetoencephalography,medical
	image processing,methodological considerations,neurophysiology,Noise
	measurement,noisy electroencephalographic measurements,noisy magnetoencephalographic
	measurements,Signal processing algorithms,source signals,sparse matrices,sparse
	models,taxonomy algorithms,tensor models},
  pages = {100-112},
  owner = {afdidehf}
}

@inproceedings{Becker2014a,
  title = {A performance study of various brain source imaging approaches},
  doi = {10.1109/ICASSP.2014.6854729},
  abstract = {The objective of brain source imaging consists in reconstructing the
	cerebral activity everywhere within the brain based on EEG or MEG
	measurements recorded on the scalp. This requires solving an ill-posed
	linear inverse problem. In order to restore identifiability, additional
	hypotheses need to be imposed on the source distribution, giving
	rise to an impressive number of brain source imaging algorithms.
	However, a thorough comparison of different methodologies is still
	missing in the literature. In this paper, we provide an overview
	of priors that have been used for brain source imaging and conduct
	a comparative simulation study with seven representative algorithms
	corresponding to the classes of minimum norm, sparse, tensor-based,
	subspace-based, and Bayesian approaches. This permits us to identify
	new benchmark algorithms and promising directions for future research.},
  timestamp = {2016-07-08T11:26:30Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Becker, H. and Albera, L. and Comon, P. and Gribonval, R. and Wendling, F. and Merlet, I.},
  month = may,
  year = {2014},
  keywords = {Bayesian approaches,Bayes methods,benchmark algorithms,Brain modeling,brain
	source imaging algorithms,cerebral activity reconstruction,comparative
	simulation study,compressed sensing,EEG,EEG measurements,electroencephalography,identifiability
	restoration,Image reconstruction,imaging,inverse problem,Inverse
	problems,linear inverse problem,magnetoencephalography,medical signal
	processing,MEG,MEG measurements,minimum norm approaches,neurophysiology,Noise,overview,performance
	study,priors,scalp recording,source distribution hypothesis,Source
	localization,source separation,sparse approaches,subspace-based approaches,Surface
	reconstruction,tensor-based approaches,tensors},
  pages = {5869-5873},
  owner = {Fardin}
}

@article{Becker2014b,
  title = {EEG extended source localization: Tensor-based vs. conventional methods},
  volume = {96},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.03.043},
  abstract = {Abstract The localization of brain sources based on \{EEG\} measurements
	is a topic that has attracted a lot of attention in the last decades
	and many different source localization algorithms have been proposed.
	However, their performance is limited in the case of several simultaneously
	active brain regions and low signal-to-noise ratios. To overcome
	these problems, tensor-based preprocessing can be applied, which
	consists in constructing a space–time–frequency (STF) or space–time–wave–vector
	(STWV) tensor and decomposing it using the Canonical Polyadic (CP)
	decomposition. In this paper, we present a new algorithm for the
	accurate localization of extended sources based on the results of
	the tensor decomposition. Furthermore, we conduct a detailed study
	of the tensor-based preprocessing methods, including an analysis
	of their theoretical foundation, their computational complexity,
	and their performance for realistic simulated data in comparison
	to conventional source localization algorithms such as sLORETA, cortical
	\{LORETA\} (cLORETA), and 4-ExSo-MUSIC. Our objective consists, on
	the one hand, in demonstrating the gain in performance that can be
	achieved by tensor-based preprocessing, and, on the other hand, in
	pointing out the limits and drawbacks of this method. Finally, we
	validate the \{STF\} and \{STWV\} techniques on real measurements
	to demonstrate their usefulness for practical applications.},
  timestamp = {2016-07-08T12:16:38Z},
  number = {0},
  journal = {NeuroImage},
  author = {Becker, H. and Albera, L. and Comon, P. and Haardt, M. and Birot, G. and Wendling, F. and Gavaret, M. and Benar, C. G. and Merlet, I.},
  month = jun,
  year = {2014},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\,\\\\\\\\(EEG\\\\\\\\,\\\\(EEG\\\\,Distributed source localization,Space-Time-Frequency analysis,Space-Time-Frequency
	analysis,Space-Time-Wave-Vector analysis,Space-Time-Wave-Vector
	analysis,Tensor decomposition},
  pages = {143 - 157},
  owner = {Fardin}
}

@article{Ben-Haim2011,
  title = {Near-Oracle Performance of Greedy Block-Sparse Estimation Techniques 	From Noisy Measurements},
  volume = {5},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2011.2160250},
  abstract = {This paper examines the ability of greedy algorithms to estimate a
	block sparse parameter vector from noisy measurements. In particular,
	block sparse versions of the orthogonal matching pursuit and thresholding
	algorithms are analyzed under both adversarial and Gaussian noise
	models. In the adversarial setting, it is shown that estimation accuracy
	comes within a constant factor of the noise power. Under Gaussian
	noise, the Crame?r-Rao bound is derived, and it is shown that the
	greedy techniques come close to this bound at high signal-to-noise
	ratio. The guarantees are numerically compared with the actual performance
	of block and non-block algorithms, identifying situations in which
	block sparse techniques improve upon the scalar sparsity approach.
	Specifically, we show that block sparse methods are particularly
	successful when the atoms within each block are nearly orthogonal.},
  timestamp = {2016-07-10T06:48:47Z},
  number = {5},
  journal = {Selected Topics in Signal Processing, IEEE Journal of},
  author = {Ben-Haim, Z. and Eldar, Y.C.},
  month = sep,
  year = {2011},
  keywords = {Atomic measurements,block sparse parameter vector,block sparsity,Coherence,Crame?r-Rao
	bound,Dictionaries,Estimation,Gaussian noise,greedy
	block-sparse estimation technique,greedy block-sparse
	estimation technique,iterative methods,Matching pursuit algorithms,Matching pursuit
	algorithms,near-oracle performance,near-oracle
	performance,noisy measurement,orthogonal matching pursuit,orthogonal matching
	pursuit,orthogonal
	matching pursuit,performance guarantees,scalar sparsity approach,scalar
	sparsity approach,signal processing,signal
	processing,Signal-To-Noise Ratio,thresholding,thresholding algorithm,thresholding
	algorithm},
  pages = {1032-1047},
  owner = {afdidehf}
}

@article{Ben-Haim2010,
  title = {Coherence-Based Performance Guarantees for Estimating a Sparse Vector 	Under Random Noise},
  volume = {58},
  issn = {1053-587X},
  doi = {10.1109/TSP.2010.2052460},
  abstract = {We consider the problem of estimating a deterministic sparse vector
	x0 from underdetermined measurements A x0 + w, where w represents
	white Gaussian noise and A is a given deterministic dictionary. We
	provide theoretical performance guarantees for three sparse estimation
	algorithms: basis pursuit denoising (BPDN), orthogonal matching pursuit
	(OMP), and thresholding. The performance of these techniques is quantified
	as the l2 distance between the estimate and the true value of x0.
	We demonstrate that, with high probability, the analyzed algorithms
	come close to the behavior of the oracle estimator, which knows the
	locations of the nonzero elements in x0. Our results are non-asymptotic
	and are based only on the coherence of A, so that they are applicable
	to arbitrary dictionaries. This provides insight on the advantages
	and drawbacks of l1 relaxation techniques such as BPDN and the Dantzig
	selector, as opposed to greedy approaches such as OMP and thresholding.},
  timestamp = {2016-07-08T11:50:57Z},
  number = {10},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Ben-Haim, Z. and Eldar, Y.C. and Elad, M.},
  month = oct,
  year = {2010},
  keywords = {Algorithm design and analysis,arbitrary dictionaries,Basis pursuit,basis
	pursuit denoising,BPDN,coherence-based performance,Computer science,Dantzig
	selector,Dictionaries,Gaussian noise,matching pursuit,Matching
	pursuit algorithms,Matching pursuit
	algorithms,Noise measurement,Noise reduction,OMP,oracle,oracle estimator,oracle
	estimator,orthogonal matching pursuit,orthogonal
	matching pursuit,Permission,probability,Pursuit algorithms,Pursuit
	algorithms,random noise,random
	noise,signal denoising,Signal processing algorithms,sparse estimation,sparse
	estimation,sparse matrices,sparse
	matrices,sparse vector estimation,thresholding algorithm,thresholding
	algorithm,underdetermined measurements,underdetermined
	measurements},
  pages = {5030-5043},
  owner = {Fardin}
}

@article{Berg2010,
  title = {Theoretical and Empirical Results for Recovery From Multiple Measurements},
  volume = {56},
  issn = {0018-9448},
  doi = {10.1109/TIT.2010.2043876},
  abstract = {The joint-sparse recovery problem aims to recover, from sets of compressed
	measurements, unknown sparse matrices with nonzero entries restricted
	to a subset of rows. This is an extension of the single-measurement-vector
	(SMV) problem widely studied in compressed sensing. We study the
	recovery properties of two algorithms for problems with noiseless
	data and exact-sparse representation. First, we show that recovery
	using sum-of-norm minimization cannot exceed the uniform-recovery
	rate of sequential SMV using l 1 minimization, and that there are
	problems that can be solved with one approach, but not the other.
	Second, we study the performance of the ReMBo algorithm (M. Mishali
	and Y. Eldar, ¿Reduce and boost: Recovering arbitrary sets of jointly
	sparse vectors,¿ IEEE Trans. Signal Process., vol. 56, no. 10, 4692-4702,
	Oct. 2008) in combination with l 1 minimization, and show how recovery
	improves as more measurements are taken. From this analysis, it follows
	that having more measurements than the number of linearly independent
	nonzero rows does not improve the potential theoretical recovery
	rate.},
  timestamp = {2016-09-30T11:22:35Z},
  number = {5},
  journal = {Information Theory, IEEE Transactions on},
  author = {van den Berg, E. and Friedlander, M.P.},
  month = may,
  year = {2010},
  keywords = {compressed sensing,Computer science,convex optimization,Councils,Data
	Compression,exact-sparse representation,joint-sparse recovery problem,joint
	sparsity,minimisation,multiple channels,noiseless data,ReMBo algorithm,signal
	processing,Signal processing algorithms,single-measurement-vector,sparse
	matrices,sparse recovery,sum-of-norm minimization},
  pages = {2516--2527},
  owner = {afdidehf}
}

@inproceedings{Berinde2008,
  title = {Practical near-optimal sparse recovery in the L1 norm},
  doi = {10.1109/ALLERTON.2008.4797556},
  abstract = {We consider the approximate sparse recovery problem, where the goal
	is to (approximately) recover a high-dimensional vector x isin Rn
	from its lower-dimensional sketch Ax isin Rm. Specifically, we focus
	on the sparse recovery problem in the l1 norm: for a parameter k,
	given the sketch Ax, compute an approximation xcirc of x such that
	the l1 approximation error parx - xcircpar1 is close to minx' parx
	- x'par1, where x' ranges over all vectors with at most k terms.
	The sparse recovery problem has been subject to extensive research
	over the last few years. Many solutions to this problem have been
	discovered, achieving different trade-offs between various attributes,
	such as the sketch length, encoding and recovery times.},
  timestamp = {2016-07-10T07:32:46Z},
  booktitle = {Communication, Control, and Computing, 2008 46th Annual Allerton 	Conference on},
  author = {Berinde, R. and Indyk, P. and Ruzic, M.},
  month = sep,
  year = {2008},
  keywords = {approximate sparse recovery,Approximation error,computational complexity,Computer
	errors,Computer science,EMP radiation effects,encoding,Information
	theory,LI norm,Matching pursuit algorithms,matrix algebra,near-optimal
	sparse recovery,Pursuit algorithms,Signal processing algorithms,sketching
	matrices,sparse matching pursuit,sparse matrices,Vectors},
  pages = {198-205}
}

@inproceedings{Bhaskara2015,
  title = {Sparse Solutions to Nonnegative Linear Systems and Applications},
  volume = {38},
  timestamp = {2016-07-11T16:54:25Z},
  booktitle = {Proceedings of the 18th International Conference on Artificial Intelligence 	and Statistics (AISTATS)},
  author = {Bhaskara, Aditya and Suresh, Ananda Theertha and Zadimoghaddam, Morteza},
  year = {2015},
  owner = {Fardin}
}

@article{Bickel2007,
  title = {Discussion: The Dantzig Selector: Statistical Estimation When p is 	Much Larger Than n},
  volume = {35},
  doi = {10.1214/009053607000000424},
  timestamp = {2016-07-08T12:12:17Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Bickel, Peter J.},
  year = {2007},
  pages = {2352�2357},
  owner = {afdidehf}
}

@article{Biessmann2011,
  title = {Analysis of Multimodal Neuroimaging Data},
  volume = {4},
  issn = {1937-3333},
  doi = {10.1109/RBME.2011.2170675},
  abstract = {Each method for imaging brain activity has technical or physiological
	limits. Thus, combinations of neuroimaging modalities that can alleviate
	these limitations such as simultaneous recordings of neurophysiological
	and hemodynamic activity have become increasingly popular. Multimodal
	imaging setups can take advantage of complementary views on neural
	activity and enhance our understanding about how neural information
	processing is reflected in each modality. However, dedicated analysis
	methods are needed to exploit the potential of multimodal methods.
	Many solutions to this data integration problem have been proposed,
	which often renders both comparisons of results and the choice of
	the right method for the data at hand difficult. In this review we
	will discuss different multimodal neuroimaging setups, the advances
	achieved in basic research and clinical application and the methods
	used. We will provide a comprehensive overview of mathematical tools
	reoccurring in multimodal neuroimaging studies for artifact removal,
	data-driven and model-driven analyses, enabling the practitioner
	to try established or new combinations from these algorithmic building
	blocks.},
  timestamp = {2016-10-21T13:48:04Z},
  journal = {Biomedical Engineering, IEEE Reviews in},
  author = {Biessmann, F. and Plis, S. and Meinecke, F.C. and Eichele, T. and Muller, K.},
  year = {2011},
  keywords = {artifact removal,biomedical imaging,Brain,brain activity,brain
	activity,Brain modeling,clinical application,clinical
	application,Computer-Assisted,data analysis,Data
	analysis,data-driven analysis,data integration,data
	integration,Diffusion Tensor Imaging,EEG-functional
	magnetic resonance imaging (fMRI),EEG-functional magnetic resonance
	imaging (fMRI),Electroencephalograms (EEG),electroencephalography,Electrophysiology,fMRI,haemodynamics,hemodynamic
	activity,Hemodynamics,Humans,Image Processing,Magnetic resonance
	imaging,magnetoencephalograms (MEG),magnetoencephalography,mathematical
	tool,MEG-fMRI,model-driven analysis,multimodal,multimodal neuroimaging
	data analysis,Near-Infrared,near infrared spectroscopy (NIRS),Neuroimaging,neurophysiological
	activity,neurophysiology,Optics and Photonics,Positron-Emission Tomography,Spatial
	resolution,Spectroscopy},
  pages = {26--58},
  owner = {Fardin}
}

@inproceedings{Bioglio2015,
  title = {On the fly estimation of the sparsity degree in Compressed Sensing 	using sparse sensing matrices},
  doi = {10.1109/ICASSP.2015.7178682},
  abstract = {In this paper, we propose a mathematical model to estimate the sparsity
	degree k of exactly k-sparse signals acquired through Compressed
	Sensing (CS). Our method does not need to recover the signal to estimate
	its sparsity, and is based on the use of sparse sensing matrices.
	We exploit this model to propose a CS acquisition system where the
	number of measurements is calculated on-the-fly depending on the
	estimated signal sparsity. Experimental results on block-based CS
	acquisition of black and white images show that the proposed adaptive
	technique outperforms classical CS acquisition methods where the
	number of measurements is set a priori.},
  timestamp = {2016-07-10T07:16:49Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International 	Conference on},
  author = {Bioglio, V. and Bianchi, T. and Magli, E.},
  month = apr,
  year = {2015},
  keywords = {Adaptive Sensing,adaptive technique,block-based CS acquisition system,compressed sensing,compressed
	sensing,Estimation theory,exactly k-sparse signals,Matching
	pursuit algorithms,mathematical model,Maximum likelihood estimation,Sensors,signal
	detection,signal sparsity estimation,sparse matrices,sparse sensing matrices,sparse
	sensing matrices,Sparse Sensing
	Matrices,sparsity degree on-the-fly estimation,sparsity degree on-the-fly
	estimation,Sparsity Estimation,Sparsity
	Estimation,Upper bound},
  pages = {3801-3805},
  owner = {afdidehf}
}

@article{Blomgren1998,
  title = {Color TV: total variation methods for restoration of vector-valued 	images},
  volume = {7},
  issn = {1057-7149},
  doi = {10.1109/83.661180},
  abstract = {We propose a new definition of the total variation (TV) norm for vector-valued
	functions that can be applied to restore color and other vector-valued
	images. The new TV norm has the desirable properties of (1) not penalizing
	discontinuities (edges) in the image, (2) being rotationally invariant
	in the image space, and (3) reducing to the usual TV norm in the
	scalar case. Some numerical experiments on denoising simple color
	images in red-green-blue (RGB) color space are presented},
  timestamp = {2016-07-08T11:51:59Z},
  number = {3},
  journal = {Image Processing, IEEE Transactions on},
  author = {Blomgren, P. and Chan, T.F.},
  month = mar,
  year = {1998},
  keywords = {Color,Colored noise,color image denoising,color restoration,color
	TV,Humans,image colour analysis,image discontinuities,Image edge
	detection,image edges,image restoration,image space,Multidimensional
	systems,Multispectral imaging,Noise,Noise level,Noise reduction,numerical
	experiments,RGB color space,rotationally invariant method,telecommunication
	channels,total variation methods,total variation norm,TV,vector-valued
	functions,vector-valued image restoration},
  pages = {304-309},
  owner = {afdidehf}
}

@article{Blumensath2011,
  title = {Sampling and Reconstructing Signals From a Union of Linear Subspaces},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2146550},
  abstract = {In this paper, we study the problem of sampling and reconstructing
	signals which are assumed to lie on or close to one of several subspaces
	of a Hilbert space. Importantly, we here consider a very general
	setting in which we allow infinitely many subspaces in infinite dimensional
	Hilbert spaces. This general approach allows us to unify many results
	derived recently in areas such as compressed sensing, affine rank
	minimization, analog compressed sensing and structured matrix decompositions.},
  timestamp = {2016-09-30T11:24:49Z},
  number = {7},
  journal = {Information Theory, IEEE Transactions on},
  author = {Blumensath, T.},
  month = jul,
  year = {2011},
  keywords = {affine rank minimization,affine transforms,analog compressed sensing,Analytical
	models,Approximation algorithms,Approximation methods,bi-Lipschitz
	embedding condition,compressed sensing,Computational modeling,Hilbert
	space,Hilbert spaces,Image reconstruction,infinite dimensional Hilbert
	spaces,inverse problems,linear subspace,matrix decomposition,nonconvexly
	constrained optimization,projected landweber algorithm,Sampling,sampling
	operator,signal reconstruction,signal recovery,signal sampling,signal
	sampling procedure,structured matrix decomposition,union of subspaces},
  pages = {4660--4671},
  owner = {afdidehf}
}

@article{Blumensath2009,
  title = {Sampling Theorems for Signals From the Union of Finite-Dimensional 	Linear Subspaces},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2013003},
  abstract = {Compressed sensing is an emerging signal acquisition technique that
	enables signals to be sampled well below the Nyquist rate, given
	that the signal has a sparse representation in an orthonormal basis.
	In fact, sparsity in an orthonormal basis is only one possible signal
	model that allows for sampling strategies below the Nyquist rate.
	In this paper, we consider a more general signal model and assume
	signals that live on or close to the union of linear subspaces of
	low dimension. We present sampling theorems for this model that are
	in the same spirit as the Nyquist-Shannon sampling theorem in that
	they connect the number of required samples to certain model parameters.
	Contrary to the Nyquist-Shannon sampling theorem, which gives a necessary
	and sufficient condition for the number of required samples as well
	as a simple linear algorithm for signal reconstruction, the model
	studied here is more complex. We therefore concentrate on two aspects
	of the signal model, the existence of one to one maps to lower dimensional
	observation spaces and the smoothness of the inverse map. We show
	that almost all linear maps are one to one when the observation space
	is at least of the same dimension as the largest dimension of the
	convex hull of the union of any two subspaces in the model. However,
	we also show that in order for the inverse map to have certain smoothness
	properties such as a given finite Lipschitz constant, the required
	observation dimension necessarily depends logarithmically on the
	number of subspaces in the signal model. In other words, while unique
	linear sampling schemes require a small number of samples depending
	only on the dimension of the subspaces involved, in order to have
	stable sampling methods, the number of samples depends necessarily
	logarithmically on the number of subspaces in the model. These results
	are then applied to two examples, the standard compressed sensing
	signal model i- - n which the signal has a sparse representation
	in an orthonormal basis and to a sparse signal model with additional
	tree structure.},
  timestamp = {2016-09-30T11:25:16Z},
  number = {4},
  journal = {Information Theory, IEEE Transactions on},
  author = {Blumensath, T. and Davies, M.E.},
  month = apr,
  year = {2009},
  keywords = {additional tree structure,Bandwidth,compressed sensing,compressed
	sensing signal model,Councils,embedding and restricted isometry,finite-dimensional
	linear subspaces,Focusing,Image reconstruction,linear sampling schemes,Nyquist
	criterion,Nyquist rate,Nyquist-Shannon sampling theorem,Sampling
	methods,sampling theorems,signal acquisition,signal processing,signal
	reconstruction,signal sampling,sparse representation,Sufficient conditions,Tree
	data structures,trees (mathematics),unions of linear subspaces},
  pages = {1872--1882},
  owner = {afdidehf}
}

@article{Blumensath2009a,
  title = {Iterative hard thresholding for compressed sensing},
  volume = {27},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2009.04.002},
  abstract = {Compressed sensing is a technique to sample compressible signals below
	the Nyquist rate, whilst still allowing near optimal reconstruction
	of the signal. In this paper we present a theoretical analysis of
	the iterative hard thresholding algorithm when applied to the compressed
	sensing recovery problem. We show that the algorithm has the following
	properties (made more precise in the main text of the paper)• It
	gives near-optimal error guarantees. • It is robust to observation
	noise. • It succeeds with a minimum number of observations. •
	It can be used with any sampling operator for which the operator
	and its adjoint can be computed. • The memory requirement is linear
	in the problem size. • Its computational complexity per iteration
	is of the same order as the application of the measurement operator
	or its adjoint. • It requires a fixed number of iterations depending
	only on the logarithm of a form of signal to noise ratio of the signal.
	• Its performance guarantees are uniform in that they only depend
	on properties of the sampling operator and signal sparsity.},
  timestamp = {2016-07-09T19:45:21Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Blumensath, Thomas and Davies, Mike E.},
  year = {2009},
  keywords = {Algorithms},
  pages = {265 - 274},
  owner = {afdidehf}
}

@inproceedings{Bo2013,
  title = {Multipath Sparse Coding Using Hierarchical Matching Pursuit},
  doi = {10.1109/CVPR.2013.91},
  abstract = {Complex real-world signals, such as images, contain discriminative
	structures that differ in many aspects including scale, invariance,
	and data channel. While progress in deep learning shows the importance
	of learning features through multiple layers, it is equally important
	to learn features through multiple paths. We propose Multipath Hierarchical
	Matching Pursuit (M-HMP), a novel feature learning architecture that
	combines a collection of hierarchical sparse features for image classification
	to capture multiple aspects of discriminative structures. Our building
	blocks are MI-KSVD, a codebook learning algorithm that balances the
	reconstruction error and the mutual incoherence of the codebook,
	and batch orthogonal matching pursuit (OMP), we apply them recursively
	at varying layers and scales. The result is a highly discriminative
	image representation that leads to large improvements to the state-of-the-art
	on many standard benchmarks, e.g., Caltech-101, Caltech-256, MITScenes,
	Oxford-IIIT Pet and Caltech-UCSD Bird-200.},
  timestamp = {2016-07-09T20:16:44Z},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference 	on},
  author = {Bo, Liefeng and Ren, Xiaofeng and Fox, D.},
  month = jun,
  year = {2013},
  keywords = {batch OMP,codebook learning algorithm,Computer architecture,data channel,Deep
	Learning,encoding,feature extraction,Feature Learning,feature learning
	architecture,hierarchical sparse feature collection,image classification,Image
	coding,image matching,Image reconstruction,image representation,iterative
	methods,learning (artificial intelligence),Matching pursuit algorithms,M-HMP,MI-KSVD,multipath
	hierarchical matching pursuit,multipath sparse coding,Object Recognition,orthogonal
	matching pursuit,reconstruction error,SIFT,Sparse Coding,sparse matrices,Vectors,Visualization},
  pages = {660-667},
  owner = {afdidehf}
}

@article{Boccuto2002,
  title = {Some Inequalities in Classical Spaces with Mixed Norms},
  volume = {6},
  issn = {1385-1292},
  doi = {10.1023/A:1021353215312},
  abstract = {We consider some inequalities in such classical Banach Function Spaces
	as Lorentz, Marcinkiewicz, and Orlicz spaces. Our aim is to explore
	connections between the norm of a function of two variables on the
	product space and the mixed norm of the same function, where mixed
	norm is calculated in function spaces on coordinate spaces, first
	in one variable, then in the other. This issue is motivated by various
	problems of functional analysis and theory of functions. We will
	currently mention just geometry of spaces of vector-valued functions
	and embedding theorems for Sobolev and Besov spaces generated by
	metrics which differ from Lp. Our main results are actually counterexamples
	for Lorentz spaces versus the natural intuition that arises from
	the easier case of Orlicz spaces (Section 2). In the Appendix we
	give a proof for the Kolmogorov�Nagumo theorem on change of order
	of mixed norm calculation in its most general form. This result shows
	that Lp is the only space where it is possible to change this order.},
  language = {English},
  timestamp = {2016-07-10T08:21:53Z},
  number = {4},
  journal = {Positivity},
  author = {Boccuto, Antonio and Bukhvalov, Alexander V. and Sambucini, Anna
	Rita},
  year = {2002},
  keywords = {Banach Function Spaces,Lorentz spaces,mixed norm inequalities for
	functions of many variables,spaces with mixed norms},
  pages = {393-411},
  owner = {afdidehf}
}

@article{Boyd2008,
  title = {Least-norm solutions of undetermined equations},
  timestamp = {2016-07-09T19:56:23Z},
  author = {Boyd, Stephen},
  year = {2008},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Brodersen2013,
  title = {Variational Bayesian mixed-effects inference for classification studies},
  volume = {76},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2013.03.008},
  abstract = {Abstract Multivariate classification algorithms are powerful tools
	for predicting cognitive or pathophysiological states from neuroimaging
	data. Assessing the utility of a classifier in application domains
	such as cognitive neuroscience, brain–computer interfaces, or clinical
	diagnostics necessitates inference on classification performance
	at more than one level, i.e., both in individual subjects and in
	the population from which these subjects were sampled. Such inference
	requires models that explicitly account for both fixed-effects (within-subjects)
	and random-effects (between-subjects) variance components. While
	models of this sort are standard in mass-univariate analyses of fMRI
	data, they have not yet received much attention in multivariate classification
	studies of neuroimaging data, presumably because of the high computational
	costs they entail. This paper extends a recently developed hierarchical
	model for mixed-effects inference in multivariate classification
	studies and introduces an efficient variational Bayes approach to
	inference. Using both synthetic and empirical fMRI data, we show
	that this approach is equally simple to use as, yet more powerful
	than, a conventional t-test on subject-specific sample accuracies,
	and computationally much more efficient than previous sampling algorithms
	and permutation tests. Our approach is independent of the type of
	underlying classifier and thus widely applicable. The present framework
	may help establish mixed-effects inference as a future standard for
	classification group analyses.},
  timestamp = {2016-07-11T17:10:31Z},
  journal = {NeuroImage},
  author = {Brodersen, Kay H. and Daunizeau, Jean and Mathys, Christoph and Chumbley, Justin R. and Buhmann, Joachim M. and Stephan, Klaas E.},
  year = {2013},
  keywords = {Balanced accuracy,Bayesian inference,Fixed effects,Group studies,Normal-binomial,Random
	effects,Variational Bayes},
  pages = {345 - 361},
  owner = {afdidehf}
}

@inproceedings{Bruckstein2008,
  title = {Sparse non-negative solution of a linear system of equations is unique},
  doi = {10.1109/ISCCSP.2008.4537325},
  abstract = {We consider an underdetermined linear system of equations Ax = b with
	non-negative entries of A and b, and the solution x being also required
	to be non-negative. We show that if there exists a sufficiently sparse
	solution to this problem, it is necessarily unique. Furthermore,
	we present a greedy algorithm - a variant of the matching pursuit
	- that is guaranteed to find this sparse solution. The result mentioned
	above is obtained by extending the existing theoretical analysis
	of the basis pursuit problem, i.e. min ||x||1 s.t. Ax = b, by studying
	conditions for perfect recovery of sparse enough solutions. Considering
	a matrix A with arbitrary column norms, and an arbitrary monotone
	element-wise concave penalty replacing the lscr1-norm objective function,
	we generalize known equivalence results, and use those to derive
	the above uniqueness claim.},
  timestamp = {2016-07-11T16:49:34Z},
  booktitle = {Communications, Control and Signal Processing, 2008. ISCCSP 2008. 	3rd International Symposium on},
  author = {Bruckstein, A.M. and Elad, M. and Zibulevsky, M.},
  month = mar,
  year = {2008},
  keywords = {basis pursuit problem,Computer science,Equations,greedy algorithm,Greedy
	algorithms,Image Processing,Lead,linear system of equations,Linear
	systems,matching pursuit,Matching pursuit algorithms,matrix algebra,monotone
	element-wise concave penalty lscr1-norm objective function,Pursuit
	algorithms,signal processing,sparse matrices,sparse nonnegative solution},
  pages = {762-767},
  owner = {Fardin}
}

@article{Bruckstein2008a,
  title = {On the Uniqueness of Nonnegative Sparse Solutions to Underdetermined 	Systems of Equations},
  volume = {54},
  issn = {0018-9448},
  doi = {10.1109/TIT.2008.929920},
  abstract = {An underdetermined linear system of equations Ax = b with nonnegativity
	constraint x ges 0 is considered. It is shown that for matrices A
	with a row-span intersecting the positive orthant, if this problem
	admits a sufficiently sparse solution, it is necessarily unique.
	The bound on the required sparsity depends on a coherence property
	of the matrix A. This coherence measure can be improved by applying
	a conditioning stage on A, thereby strengthening the claimed result.
	The obtained uniqueness theorem relies on an extended theoretical
	analysis of the lscr0 - lscr1 equivalence developed here as well,
	considering a matrix A with arbitrary column norms, and an arbitrary
	monotone element-wise concave penalty replacing the lscr1-norm objective
	function. Finally, from a numerical point of view, a greedy algorithm-a
	variant of the matching pursuit-is presented, such that it is guaranteed
	to find this sparse solution. It is further shown how this algorithm
	can benefit from well-designed conditioning of A .},
  timestamp = {2016-07-10T07:17:33Z},
  number = {11},
  journal = {Information Theory, IEEE Transactions on},
  author = {Bruckstein, A.M. and Elad, M. and Zibulevsky, M.},
  month = nov,
  year = {2008},
  keywords = {$ell _{1}$,Basis pursuit,coherence measure,Constraint theory,Equations,greedy algorithm,greedy
	algorithm,Greedy algorithms,Image Processing,Image
	Processing,Image recognition,Image
	recognition,linear equations,linear system,Linear systems,Linear
	systems,Matching pursuit algorithms,Matching
	pursuit algorithms,monotone element-wise concave penalty,monotone element-wise concave
	penalty,nonnegative sparse solution,nonnegative
	sparse solution,orthogonal matching pursuit,positive orthant,positive
	orthant,Pursuit algorithms,Pursuit
	algorithms,signal processing,sparse matrices,sparse
	matrices,sparse solution,sparse
	solution,underdetermined linear system,underdetermined
	linear system,uniqueness},
  pages = {4813-4820},
  owner = {Fardin}
}

@article{Buhlmann2006,
  title = {Boosting For High-dimensional Linear Models},
  volume = {34},
  doi = {10.1214/009053606000000092},
  abstract = {We prove that boosting with the squared error loss, L2Boosting, is
	consistent for very high-dimensional linear models, where the number
	of predictor variables is allowed to grow essentially as fast as
	O(exp(sample size)), assuming that the true underlying regression
	function is sparse in terms of the ?1-norm of the regression coefficients.
	In the language of signal processing, this means consistency for
	de-noising using a strongly overcomplete dictionary if the underlying
	signal is sparse in terms of the ?1-norm.We also propose here an
	AICbased method for tuning, namely for choosing the number of boosting
	iterations. This makes L2Boosting computationally attractive since
	it is not required to run the algorithm multiple times for cross-validation
	as commonly used so far. We demonstrate L2Boosting for simulated
	data, in particular where the predictor dimension is large in comparison
	to sample size, and for a difficult tumor-classification problem
	with gene expression microarray data.},
  timestamp = {2016-07-08T11:49:00Z},
  number = {2},
  journal = {The Annals of Statistics},
  author = {Buhlmann, Peter},
  year = {2006},
  keywords = {Binary classification,gene expression,Lasso,matching pursuit,overcomplete
	dictionary,Sparsity,variable selection,weak greedy algorithm.},
  pages = {559�583},
  owner = {Fardin}
}

@article{Bullinaria2014,
  title = {Bias and Variance, Under-Fitting and Over-Fitting},
  timestamp = {2016-07-08T11:37:52Z},
  author = {Bullinaria, John A.},
  year = {2014},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Bunea2007,
  title = {Sparsity oracle inequalities for the Lasso},
  volume = {1},
  doi = {10.1214/07-EJS008},
  abstract = {This paper studies oracle properties of ?1-penalized least squares
	in nonparametric regression setting with random design. We show that
	the penalized least squares estimator satisfies sparsity oracle inequalities,
	i.e., bounds in terms of the number of non-zero components of the
	oracle vec- tor. The results are valid even when the dimension of
	the model is (much) larger than the sample size and the regression
	matrix is not positive definite. They can be applied to high-dimensional
	linear regression, to nonparamet- ric adaptive regression estimation
	and to the problem of aggregation of arbitrary estimators.},
  timestamp = {2016-07-11T16:55:13Z},
  journal = {Electronic Journal of Statistics},
  author = {Bunea, Florentina and Tsybakov, Alexandre and Wegkamp, Marten},
  year = {2007},
  keywords = {adaptive estimation.,aggregation,dimension reduction,Lasso,mutual
	coherence,nonparametric regression,oracle inequalities,penalized
	least squares,Sparsity},
  pages = {169�194},
  owner = {afdidehf}
}

@article{Buzsaki2012,
  title = {The origin of extracellular fields and currents --- EEG, ECoG, LFP 	and spikes},
  volume = {13},
  doi = {10.1038/nrn3241},
  abstract = {Neuronal activity in the brain gives rise to transmembrane currents
	that can be measured in the extracellular medium. Although the major
	contributor of the extracellular signal is the synaptic transmembrane
	current, other sources � including Na+ and Ca2+ spikes, ionic fluxes
	through voltage- and ligand-gated channels, and intrinsic membrane
	oscillations � can substantially shape the extracellular field.
	High-density recordings of field activity in animals and subdural
	grid recordings in humans, combined with recently developed data
	processing tools and computational modelling, can provide insight
	into the cooperative behaviour of neurons, their average synaptic
	input and their spiking output, and can increase our understanding
	of how these processes contribute to the extracellular signal.},
  timestamp = {2016-07-11T17:02:23Z},
  journal = {Nature Reviews Neuroscience},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Anastassiou, Costas A. and Koch, Christof},
  month = jun,
  year = {2012},
  pages = {407-420},
  owner = {Fardin}
}

@article{Cai2011,
  title = {Orthogonal Matching Pursuit for Sparse Signal Recovery With Noise},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2146090},
  abstract = {We consider the orthogonal matching pursuit (OMP) algorithm for the
	recovery of a high-dimensional sparse signal based on a small number
	of noisy linear measurements. OMP is an iterative greedy algorithm
	that selects at each step the column, which is most correlated with
	the current residuals. In this paper, we present a fully data driven
	OMP algorithm with explicit stopping rules. It is shown that under
	conditions on the mutual incoherence and the minimum magnitude of
	the nonzero components of the signal, the support of the signal can
	be recovered exactly by the OMP algorithm with high probability.
	In addition, we also consider the problem of identifying significant
	components in the case where some of the nonzero components are possibly
	small. It is shown that in this case the OMP algorithm will still
	select all the significant components before possibly selecting incorrect
	ones. Moreover, with modified stopping rules, the OMP algorithm can
	ensure that no zero components are selected.},
  timestamp = {2016-07-10T07:20:32Z},
  number = {7},
  journal = {Information Theory, IEEE Transactions on},
  author = {Cai, T.T. and Wang, Lie},
  month = jul,
  year = {2011},
  keywords = {$ell_{1}$ minimization,Algorithm design and analysis,compressed sensing,compressed
	sensing,data driven OMP algorithm,Eigenvalues and eigenfunctions,Equations,explicit
	stopping rules,Gaussian noise,Greedy algorithms,high-dimensional
	sparse signal,iterative greedy algorithm,iterative methods,Matching
	pursuit algorithms,mutual incoherence,noisy linear measurements,orthogonal
	matching pursuit algorithm,orthogonal matching pursuit (OMP),Signal
	processing algorithms,signal reconstruction,sparse signal recovery,support
	recovery},
  pages = {4680-4688},
  owner = {afdidehf}
}

@article{Cai2010,
  title = {Shifting Inequality and Recovery of Sparse Signals},
  volume = {58},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2034936},
  abstract = {In this paper, we present a concise and coherent analysis of the constrained
	??1 minimization method for stable recovering of high-dimensional
	sparse signals both in the noiseless case and noisy case. The analysis
	is surprisingly simple and elementary, while leads to strong results.
	In particular, it is shown that the sparse recovery problem can be
	solved via ??1 minimization under weaker conditions than what is
	known in the literature. A key technical tool is an elementary inequality,
	called Shifting Inequality, which, for a given nonnegative decreasing
	sequence, bounds the ??2 norm of a subsequence in terms of the ??1
	norm of another subsequence by shifting the elements to the upper
	end.},
  timestamp = {2016-09-29T16:36:16Z},
  number = {3},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Cai, T.T. and Wang, Lie and Xu, Guangwu},
  month = mar,
  year = {2010},
  keywords = {$ell_{1}$ minimization,constrained minimization method,high-dimensional
	sparse signals,minimisation,nonnegative decreasing sequence,restricted
	isometry property,shifting inequality,signal processing,signal reconstruction,sparse
	recovery,sparse recovery problem,sparse signals recovery},
  pages = {1300--1308},
  annote = {PDF \& PPT, read},
  annote = {PDF \& PPT, read},
  annote = {PDF \& PPT, read},
  annote = {PDF \& PPT, read},
  annote = {PDF \& PPT, read},
  owner = {Fardin}
}

@article{Cai2010a,
  title = {New Bounds for Restricted Isometry Constants},
  volume = {56},
  issn = {0018-9448},
  doi = {10.1109/TIT.2010.2054730},
  abstract = {This paper discusses new bounds for restricted isometry constants
	in compressed sensing. Let ? be an n � p real matrix and A; be
	a positive integer with k ? n. One of the main results of this paper
	shows that if the restricted isometry constant ?k of ? satisfies
	?k <; 0.307 then k-sparse signals are guaranteed to be recovered
	exactly via ?1 minimization when no noise is present and k-sparse
	signals can be estimated stably in the noisy case. It is also shown
	that the bound cannot be substantially improved. An explicit example
	is constructed in which ?k = k-1/2k-1 <; 0.5, but it is impossible
	to recover certain k-sparse signals.},
  timestamp = {2016-07-10T06:53:18Z},
  number = {9},
  journal = {Information Theory, IEEE Transactions on},
  author = {Cai, T.T. and Wang, Lie and Xu, Guangwu},
  month = sep,
  year = {2010},
  keywords = {$ell_1$ minimization,compressed sensing,Computer aided instruction,k-sparse
	signal,Linear matrix inequalities,Mathematics,Measurement errors,minimisation,Minimization,Minimization
	methods,Noise,Noise measurement,positive integer,real matrix,restricted
	isometry constant,restricted isometry property,signal processing,sparse
	matrices,sparse signal recovery,Statistics,Upper bound,Vectors},
  pages = {4388-4394},
  owner = {Fardin}
}

@article{Cai2009,
  title = {On Recovery of Sparse Signals Via $\ell_1$ Minimization},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2021377},
  abstract = {This paper considers constrained lscr1 minimization methods in a unified
	framework for the recovery of high-dimensional sparse signals in
	three settings: noiseless, bounded error, and Gaussian noise. Both
	lscr1 minimization with an lscrinfin constraint (Dantzig selector)
	and lscr1 minimization under an llscr2 constraint are considered.
	The results of this paper improve the existing results in the literature
	by weakening the conditions and tightening the error bounds. The
	improvement on the conditions shows that signals with larger support
	can be recovered accurately. In particular, our results illustrate
	the relationship between lscr1 minimization with an llscr2 constraint
	and lscr1 minimization with an lscrinfin constraint. This paper also
	establishes connections between restricted isometry property and
	the mutual incoherence property. Some results of Candes, Romberg,
	and Tao (2006), Candes and Tao (2007), and Donoho, Elad, and Temlyakov
	(2006) are extended.},
  timestamp = {2016-09-29T16:35:27Z},
  number = {7},
  journal = {Information Theory, IEEE Transactions on},
  author = {Cai, T.T. and Xu, Guangwu and Zhang, Jun},
  month = jul,
  year = {2009},
  keywords = {compressed sensing,constrained minimization methods,Dantzig selector,Dantzig
	selector$ell _{1} $ minimization,Equations,error bounds,Gaussian noise,Gaussian
	noise,Helium,isometry property,Least squares methods,Linear
	regression,minimisation,Minimization methods,mutual incoherence property,Noise
	measurement,restricted isometry property,signal processing,sparse
	recovery,sparse signal recovery,Sparsity,Statistics,Vectors},
  pages = {3388--3397},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Cai2014,
  title = {Sparse Representation of a Polytope and Recovery of Sparse Signals 	and Low-Rank Matrices},
  volume = {60},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2288639},
  abstract = {This paper considers compressed sensing and affine rank minimization
	in both noiseless and noisy cases and establishes sharp restricted
	isometry conditions for sparse signal and low-rank matrix recovery.
	The analysis relies on a key technical tool, which represents points
	in a polytope by convex combinations of sparse vectors. The technique
	is elementary while yielding sharp results. It is shown that for
	any given constant t ? 4/3, in compressed sensing, ?tkA <; ?((t-1)/t)
	guarantees the exact recovery of all k sparse signals in the noiseless
	case through the constrained l1 minimization, and similarly, in affine
	rank minimization, ?trM <; ?((t-1)/t) ensures the exact reconstruction
	of all matrices with rank at most r in the noiseless case via the
	constrained nuclear norm minimization. In addition, for any ? > 0,
	?tkA <; ?(t-1/t) + ? is not sufficient to guarantee the exact recovery
	of all k-sparse signals for large k. Similar results also hold for
	matrix recovery. In addition, the conditions ?tkA <; ?((t-)1/t) and
	?trM <; ?((t-1)/t) are also shown to be sufficient, respectively,
	for stable recovery of approximately sparse signals and low-rank
	matrices in the noisy case.},
  timestamp = {2016-07-11T16:52:01Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Cai, T.T. and Zhang, Anru},
  month = jan,
  year = {2014},
  keywords = {affine rank minimization,compressed sensing,constrained
	$ell_{1}$ minimization,constrained $ell_{1}$
	minimization,constrained l1 minimization,constrained nuclear norm minimization,constrained nuclear
	norm minimization,constrained nuclear norm
	minimization,k-sparse
	signal recovery,k-sparse signal
	recovery,low-rank matrix recovery,matrix algebra,matrix
	algebra,minimisation,Minimization methods,Minimization
	methods,Noise,Noise measurement,restricted isometry,restricted
	isometry,sharp restricted isometry conditions,sharp restricted
	isometry conditions,signal representation,sparse matrices,sparse
	matrices,sparse polytope representation,sparse
	polytope representation,sparse signal recovery,sparse vectors,sparse
	vectors,Vectors},
  pages = {122-132},
  owner = {afdidehf}
}

@article{Cai2007,
  title = {Discussion: the dantzig selector: statistical estimation when p is 	much larger than n},
  volume = {35},
  doi = {10.1214/009053607000000442},
  timestamp = {2016-07-08T12:12:21Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Cai, T. Tony and Lv, Jinchi},
  year = {2007},
  pages = {2365�2369},
  owner = {afdidehf}
}

@inproceedings{Caiafa2012,
  title = {Block sparse representations of tensors using Kronecker bases},
  doi = {10.1109/ICASSP.2012.6288476},
  abstract = {In this paper, we consider sparse representations of multidimensional
	signals (tensors) by generalizing the one-dimensional case (vectors).
	A new greedy algorithm, namely the Tensor-OMP algorithm, is proposed
	to compute a block-sparse representation of a tensor with respect
	to a Kronecker basis where the non-zero coefficients are restricted
	to be located within a sub-tensor (block). It is demonstrated, through
	simulation examples, the advantage of considering the Kronecker structure
	together with the block-sparsity property obtaining faster and more
	precise sparse representations of tensors compared to the case of
	applying the classical OMP (Orthogonal Matching Pursuit).},
  timestamp = {2016-07-08T11:42:05Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International 	Conference on},
  author = {Caiafa, C.F. and Cichocki, A.},
  month = mar,
  year = {2012},
  keywords = {block sparse representations,compressed sensing,Dictionaries,greedy
	algorithm,Greedy algorithms,Kronecker Bases,Kronecker structure,Matching
	pursuit algorithms,multidimensional signals,nonzero coefficients,one-dimensional
	case,orthogonal matching pursuit,orthogonal matching pursuit (OMP),Signal
	processing algorithms,signal representation,sparse matrices,sparse
	representations,subtensor,Tensile stress,tensor-OMP algorithm,tensors,Vectors},
  pages = {2709-2712},
  owner = {afdidehf}
}

@article{Calderbank1996,
  title = {Good Quantum Error-Correcting Codes Exist},
  volume = {54},
  abstract = {A quantum error-correcting code is defined to be a unitary mapping
	(encoding) of k qubits (2-state quantum systems) into a subspace
	of the quantum state space of n qubits such that if any t of the
	qubits undergo arbitrary decoherence, not necessarily independently,
	the resulting n qubits can be used to faithfully reconstruct the
	original quantum state of the k encoded qubits. Quantum error-correcting
	codes are shown to exist with asymptotic rate k/n = 1 - 2H(2t/n)
	where H(p) is the binary entropy function -p log p - (1-p) log (1-p).
	Upper bounds on this asymptotic rate are given.},
  timestamp = {2016-07-08T12:38:31Z},
  number = {2},
  journal = {Phys. Rev. A},
  author = {Calderbank, A. R. and Sho, Peter W.},
  year = {1996},
  pages = {1098-1106},
  owner = {Fardin}
}

@article{Calderbank2015,
  title = {On block coherence of frames},
  volume = {38},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2014.03.003},
  abstract = {Abstract Block coherence of matrices plays an important role in analyzing
	the performance of block compressed sensing recovery algorithms (Bajwa
	and Mixon, 2012). In this paper, we characterize two block coherence
	metrics: worst-case and average block coherence. First, we present
	lower bounds on worst-case block coherence, in both the general case
	and also when the matrix is constrained to be a union of orthobases.
	We then present deterministic matrix constructions based upon Kronecker
	products which obtain these lower bounds. We also characterize the
	worst-case block coherence of random subspaces. Finally, we present
	a flipping algorithm that can improve the average block coherence
	of a matrix, while maintaining the worst-case block coherence of
	the original matrix. We provide numerical examples which demonstrate
	that our proposed deterministic matrix construction performs well
	in block compressed sensing.},
  timestamp = {2016-07-10T07:11:43Z},
  number = {1},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Calderbank, Robert and Thompson, Andrew and Xie, Yao},
  year = {2015},
  keywords = {Block,Coherence,Compressedsensing,Frameconstructions,Grassmannpacking,Randomsubspaces},
  pages = {50 - 71},
  owner = {afdidehf}
}

@article{Calhoun2014,
  title = {Neuroimage: Special issue on multimodal data fusion},
  volume = {102, Part 1},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.04.070},
  timestamp = {2016-07-10T06:50:01Z},
  number = {0},
  journal = {NeuroImage},
  author = {Calhoun, Vince D. and Lemieux, Louis},
  year = {2014},
  note = {Multimodal Data Fusion},
  pages = {1 - 2},
  owner = {Fardin}
}

@article{Cand`es2008,
  title = {Highly Robust Error Correction byConvex Programming},
  volume = {54},
  issn = {0018-9448},
  doi = {10.1109/TIT.2008.924688},
  abstract = {This paper discusses a stylized communications problem where one wishes
	to transmit a real-valued signal (a block of pieces of information)
	to a remote receiver. We ask whether it is possible to transmit this
	information reliably when a fraction of the transmitted codeword
	is corrupted by arbitrary gross errors, and when in addition, all
	the entries of the codeword are contaminated by smaller errors (e.g.,
	quantization errors). We show that if one encodes the information
	as where is a suitable coding matrix, there are two decoding schemes
	that allow the recovery of the block of pieces of information with
	nearly the same accuracy as if no gross errors occurred upon transmission
	(or equivalently as if one had an oracle supplying perfect information
	about the sites and amplitudes of the gross errors). Moreover, both
	decoding strategies are very concrete and only involve solving simple
	convex optimization programs, either a linear program or a second-order
	cone program. We complement our study with numerical simulations
	showing that the encoder/decoder pair performs remarkably well.},
  timestamp = {2016-07-08T12:43:04Z},
  number = {7},
  journal = {Information Theory, IEEE Transactions on},
  author = {Cand{\`e}s, E.J. and Randall, P.A.},
  month = jul,
  year = {2008},
  keywords = {$ell _{1}$ minimization,codeword transmission,coding matrix,Concrete,convex
	optimization programs,convex programming,Decoding,Decoding of (random)
	linear codes,decoding schemes,encoder-decoder,Error correction,Error
	correction codes,Gaussian random matrices and random projections,Linear
	code,linear codes,linear programming,matrix algebra,numerical analysis,Numerical
	Analysis,Numerical simulation,Numerical
	simulation,numerical simulations,Quantization,remote receiver,remote
	receiver,restricted orthonormality,restricted
	orthonormality,robust error correction,Robustness,second-order
	cone program,second-order cone
	program,second-order cone programming,sparse matrices,sparse
	solutions to underdetermined systems,sparse solutions
	to underdetermined systems,the Dantzig selector,Vectors},
  pages = {2829-2840},
  owner = {afdidehf}
}

@article{Cand`es2007,
  title = {Sparsity and Incoherence in Compressive Sampling},
  volume = {23},
  abstract = {We consider the problem of reconstructing a sparse signal x0 2 Rn
	from a limited number of linear measurements. Given m randomly selected
	samples of Ux0, where U is an orthonormal matrix, we show that `1
	minimization recovers x0 exactly when the number of measurements
	exceeds m  Const � ?2(U) � S � log n, where S is the number
	of nonzero components in x0, and ? is the largest entry in U properly
	normalized: ?(U) = p n � maxk,j |Uk,j |. The smaller ?, the fewer
	samples needed. The result holds for �most� sparse signals x0
	supported on a fixed (but arbitrary) set T. Given T, if the sign
	of x0 for each nonzero entry on T and the observed values of Ux0
	are drawn at random, the signal is recovered with overwhelming probability.
	Moreover, there is a sense in which this is nearly optimal since
	any method succeeding with the same probability would require just
	about this many samples.},
  timestamp = {2016-07-11T16:54:55Z},
  journal = {Inverse Problems},
  author = {Cand{\`e}s, Emmanuel and Romberg, Justin},
  year = {2007},
  keywords = {Basis pursuit,discrete Fourier transform.,l1-minimization,restricted
	orthonormality,singular values of random matrices,Sparsity,wavelets},
  pages = {969-985},
  owner = {afdidehf}
}

@inproceedings{Cand`es2005,
  address = {San Jose, CA},
  title = {Practical Signal Recovery from Random Projections},
  abstract = {Can we recover a signal f 2 RN from a small number of linear measurements?
	A series of recent papers developed a collection of results showing
	that it is surprisingly possible to reconstruct certain types of
	signals accurately from limited measurements. In a nutshell, suppose
	that f is compressible in the sense that it is well-approximated
	by a linear combination of M vectors taken from a known basis . Then
	not knowing anything in advance about the signal, f can (very nearly)
	be recovered from about M logN generic nonadaptive measurements only.
	The recovery procedure is concrete and consists in solving a simple
	convex optimization program. In this paper, we show that these ideas
	are of practical significance. Inspired by theoretical developments,
	we propose a series of practical recovery procedures and test them
	on a series of signals and images which are known to be well approximated
	in wavelet bases. We demonstrate empirically that it is possible
	to recover an object from about 3M5M projections onto generically
	chosen vectors with the same accuracy as the ideal M-term wavelet
	approximation. We briefly discuss possible implications in the areas
	of data compression and medical imaging.},
  timestamp = {2017-06-23T13:08:46Z},
  booktitle = {Computational Imaging III: Proc. SPIE International Symposium on 	Electronic Imaging},
  author = {Cand{\`e}s, Emmanuel and Romberg, Justin},
  month = jan,
  year = {2005},
  pages = {76--86},
  owner = {afdidehf}
}

@article{Cand`es2006,
  title = {Stable signal recovery from incomplete and inaccurate measurements},
  volume = {59},
  doi = {10.1002/cpa.20124},
  timestamp = {2016-09-29T16:36:39Z},
  number = {8},
  journal = {Communications on Pure and Applied Mathematics},
  author = {Cand{\`e}s, Emmanuel and Romberg, Justin and Tao, Terence},
  month = aug,
  year = {2006},
  pages = {1207--1223},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Cand`es2006a,
  title = {Robust uncertainty principles: exact signal reconstruction from highly 	incomplete frequency information},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.862083},
  abstract = {This paper considers the model problem of reconstructing an object
	from incomplete frequency samples. Consider a discrete-time signal
	f?CN and a randomly chosen set of frequencies ?. Is it possible to
	reconstruct f from the partial knowledge of its Fourier coefficients
	on the set ?? A typical result of this paper is as follows. Suppose
	that f is a superposition of |T| spikes f(t)=???Tf(?)?(t-?) obeying
	|T|?CM (log N)-1   |?| for some constant CM>0. We do not know
	the locations of the spikes nor their amplitudes. Then with probability
	at least 1-O(N-M), f can be reconstructed exactly as the solution
	to the ?1 minimization problem. In short, exact recovery may be obtained
	by solving a convex optimization problem. We give numerical values
	for CM which depend on the desired probability of success. Our result
	may be interpreted as a novel kind of nonlinear sampling theorem.
	In effect, it says that any signal made out of |T| spikes may be
	recovered by convex programming from almost every set of frequencies
	of size O(|T| logN). Moreover, this is nearly optimal in the sense
	that any method succeeding with probability 1-O(N-M) would in general
	require a number of frequency samples at least proportional to |T| logN.
	The methodology extends to a variety of other situations and higher
	dimensions. For example, we show how one can reconstruct a piecewise
	constant (one- or two-dimensional) object from incomplete frequency
	samples - provided that the number of jumps (discontinuities) obeys
	the condition above - by minimizing other convex functionals such
	as the total variation of f.},
  timestamp = {2016-09-29T16:31:55Z},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Cand{\`e}s, E.J. and Romberg, J. and Tao, T.},
  month = feb,
  year = {2006},
  keywords = {biomedical imaging,convex optimization,convex programming,convex
	programming,discrete-time signal,discrete-time
	signal,duality in optimization,Fourier analysis,Fourier
	analysis,Fourier coefficient,free probability,free
	probability,Frequency,Image reconstruction,image sampling,image
	sampling,incomplete frequency information,incomplete
	frequency information,indeterminancy,linear programming,linear
	programming,Mathematics,minimisation,minimization problem,minimization
	problem,nonlinear sampling theorem,nonlinear
	sampling theorem,piecewise constant object,piecewise constant techniques,piecewise
	constant techniques,probability,probability value,probability
	value,Random matrices,Robustness,robust uncertainty principle,robust
	uncertainty principle,Sampling methods,Sampling
	methods,signal processing,signal reconstruction,signal
	sampling,sparse matrices,sparse random matrix,Sparsity,total-variation
	minimization,trigonometric expansion,trigonometric expansions,Uncertainty,uncertainty
	principle},
  pages = {489--509},
  owner = {Fardin}
}

@article{Cand`es2007a,
  title = {The Dantzig selector: statistical estimation when p is much larger 	than n},
  volume = {35},
  doi = {10.1214/009053606000001523},
  abstract = {In many important statistical applications, the number of variables
	or parameters p is much larger than the number of observations n.
	Suppose then that we have observations y = X? +z, where ? ? Rp is
	a parameter vector of interest, X is a data matrix with possibly
	far fewer rows than columns, np, and the zi s are i.i.d. N(0,?2).
	Is it possible to estimate ? reliably based on the noisy data y?
	To estimate ?, we introduce a new estimatorwe call it the Dantzig
	selector which is a solution to the 1-regularization problem
	where r is the residual vector y ? X  ? and t is a positive scalar.
	We show that if X obeys a uniform uncertainty principle (with unit-normed
	columns) and if the true parameter vector ? is sufficiently sparse
	(which here roughly guarantees that the model is identifiable), then
	with very large probability, Our results are nonasymptotic and we
	give values for the constant C. Even though nmay be much smaller
	than p, our estimator achieves a loss within a logarithmic factor
	of the ideal mean squared error one would achieve with an oracle
	which would supply perfect information about which coordinates are
	nonzero, and which were above the noise level. In multivariate regression
	and from a model selection viewpoint, our result says that it is
	possible nearly to select the best subset of variables by solving
	a very simple convex program, which, in fact, can easily be recast
	as a convenient linear program (LP).},
  timestamp = {2017-06-23T13:10:04Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Cand{\`e}s, Emmanuel and Tao, Terence},
  year = {2007},
  keywords = {1-minimization,geometry in high dimensions,ideal estimation,linear
	programming,Model selection,oracle inequalities,random matrices.,restricted
	orthonormality,sparse solutions to underdetermined systems,Statistical
	linear model},
  pages = {2313--2351},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Cand`es2006b,
  title = {Near-Optimal Signal Recovery From Random Projections: Universal Encoding 	Strategies?},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2006.885507},
  abstract = {Suppose we are given a vector f in a class FsubeRopfN , e.g., a class
	of digital signals or digital images. How many linear measurements
	do we need to make about f to be able to recover f to within precision
	epsi in the Euclidean (lscr2) metric? This paper shows that if the
	objects of interest are sparse in a fixed basis or compressible,
	then it is possible to reconstruct f to within very high accuracy
	from a small number of random measurements by solving a simple linear
	program. More precisely, suppose that the nth largest entry of the
	vector f (or of its coefficients in a fixed basis) obeys ,
	where R0 and p0. Suppose that we take measurements yk=langf ,Xkrang,k=1,...,K,
	where the Xk are N-dimensional Gaussian vectors with independent
	standard normal entries. Then for each f obeying the decay estimate
	above for some 0<p<1 and with overwhelming probability, our reconstruction
	ft, defined as the solution to the constraints yk=langf ,Xkrang
	with minimal lscr1 norm, obeys parf-fparlscr2lesCp middotRmiddot(K/logN)-r,
	r=1/p-1/2. There is a sense in which this result is optimal; it is
	generally impossible to obtain a higher accuracy from any set of
	K measurements whatsoever. The methodology extends to various other
	random measurement ensembles; for example, we show that similar results
	hold if one observes a few randomly sampled Fourier coefficients
	of f. In fact, the results are quite general and require only two
	hypotheses on the measurement ensemble which are detailed},
  timestamp = {2017-06-23T13:28:23Z},
  number = {12},
  journal = {IEEE Trans. Inf. Theory},
  author = {Cand{\`e}s, E.J. and Tao, T.},
  month = dec,
  year = {2006},
  keywords = {Concentration of measure,Concrete,convex optimization,Digital images,duality
	in optimization,encoding,Geometry,image coding,Image reconstruction,linear
	measurement,linear program,linear programming,Mathematics,Measurement
	standards,N-dimensional Gaussian vector,Random matrices,random projection,random
	projections,signal reconstruction,signal recovery,singular
	values of random matrices,singular values
	of random matrices,Sparsity,trigonometric expansions,uncertainty
	principle,universal encoding strategy,Vectors},
  pages = {5406--5425},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  owner = {Fardin}
}

@article{Cand`es2005b,
  title = {Decoding by linear programming},
  volume = {51},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.858979},
  abstract = {This paper considers a natural error correcting problem with real
	valued input/output. We wish to recover an input vector f?Rn from
	corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix
	and e is an arbitrary and unknown vector of errors. Is it possible
	to recover f exactly from the data y? We prove that under suitable
	conditions on the coding matrix A, the input f is the unique solution
	to the ?1-minimization problem (||x||?1:=?i|xi|) min(g?Rn) ||y -
	Ag||?1 provided that the support of the vector of errors is not too
	large, ||e||?0:=|{i:ei ? 0}|??m for some ?>0. In short, f can
	be recovered exactly by solving a simple convex optimization problem
	(which one can recast as a linear program). In addition, numerical
	experiments suggest that this recovery procedure works unreasonably
	well; f is recovered exactly even in situations where a significant
	fraction of the output is corrupted. This work is related to the
	problem of finding sparse solutions to vastly underdetermined systems
	of linear equations. There are also significant connections with
	the problem of recovering signals from highly incomplete measurements.
	In fact, the results introduced in this paper improve on our earlier
	work. Finally, underlying the success of ?1 is a crucial property
	we call the uniform uncertainty principle that we shall describe
	in detail.},
  timestamp = {2017-06-23T13:09:07Z},
  number = {12},
  journal = {IEEE Trans. Inf. Theory},
  author = {Cand{\`e}s, E.J. and Tao, T.},
  month = dec,
  year = {2005},
  keywords = {Basis pursuit,convex programming,Decoding,Decoding of (random) linear
	codes,duality in optimization,Equations,Error correction,error correction
	codes,Gaussian processes,Gaussian random matrices,Gaussian random
	matrix,indeterminancy,information theory,Linear code,linear code
	decoding,linear codes,linear programming,Mathematics,minimisation,minimization
	problem,natural error correcting problem,principal angles,random
	codes,restricted orthonormality,simple convex optimization problem,singular
	values of random matrices,sparse matrices,sparse solution,sparse
	solutions to underdetermined systems,uncertainty principle,Vectors},
  pages = {4203--4215},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  owner = {Fardin}
}

@article{Cand`es1999,
  title = {Ridgelets: a key to higher-dimensional intermittency?},
  abstract = {In dimensions two and higher, wavelets can efficiently represent only
	a small range of the full diversity of interesting behavior. In effect,
	wavelets are welladapted for pointlike phenomena, whereas in dimensions
	greater than one, interesting phenomena can be organized along lines,
	hyperplanes, and other nonpointlike structures, for which wavelets
	are poorly adapted. We discuss in this paper a new subject, ridgelet
	analysis, which can effectively deal with linelike phenomena in
	dimension 2, planelike phenomena in dimension 3 and so on. It encompasses
	a collection of tools which all begin from the idea of analysis by
	ridge functions (u1x1+: : :+unxn) whose ridge proles  are
	wavelets, or alternatively from performing a wavelet analysis in
	the Radon domain. The paper reviews recent work on the continuous
	ridgelet transform (CRT), ridgelet frames, ridgelet orthonormal bases,
	ridgelets and edges and describes a new notion of smoothness naturally
	attached to this new representation.},
  timestamp = {2017-06-23T13:07:34Z},
  journal = {Royal Society Typescript},
  author = {Cand{\`e}s, Emmanuel J. and Donoho, David L.},
  year = {1999},
  owner = {afdidehf}
}

@article{Cand`es2006c,
  title = {Quantitative Robust Uncertainty Principles and Optimally Sparse Decompositions},
  volume = {6},
  issn = {1615-3375},
  doi = {10.1007/s10208-004-0162-x},
  abstract = {In this paper we develop a robust uncertainty principle for finite
	signals in CN which states that, for nearly all choices T ,  ? {0,
	. . . , N ? 1} such that |T| + ||  (log N) ?1/2  N, there is
	no signal f supported on T whose discrete Fourier transform  f
	is supported on . In fact, we can make the above uncertainty principle
	quantitative in the sense that if f is supported on T , then only
	a small percentage of the energy (less than half, say) of  f is
	concentrated on . As an application of this robust uncertainty principle
	(QRUP), we consider the problem of decomposing a signal into a sparse
	superposition of spikes and complex sinusoids f (s) = t?T ?1(t)?(s
	? t) + ?? ?2(?)ei2??s/N / ? N. We show that if a generic signal
	f has a decomposition (?1, ?2) using spike and frequency locations
	in T and , respectively, and obeying |T| + || ? Const  (log
	N) ?1/2 N, then (?1, ?2) is the unique sparsest possible decomposition
	(all other decompositions have more nonzero terms). In addition,
	if |T| + || ? Const  (log N) ?1 N, then the sparsest (?1,
	?2) can be found by solving a convex optimization problem. Underlying
	our results is a new probabilistic approach which insists on finding
	the correct uncertainty relation, or the optimally sparse solution
	for nearly all subsets but not necessarily all of them, and allows
	us to considerably sharpen previously known results [9], [10]. In
	fact, we show that the fraction of sets (T,) for which the above
	properties do not hold can be upper bounded by quantities like N??
	for large values of ?. The QRUP (and the application to finding sparse
	representations) can be extended to general pairs of orthogonal bases2
	of CN . For nearly all choices1, 2 ? {0, . . . , N ? 1} obeying
	|(log N) ?m, where m ? 6, there is no
	signal f such that 1 f is supported on2 f is supported on2
	where ?() is the mutual coherence between 2.},
  language = {English},
  timestamp = {2017-06-23T09:47:19Z},
  number = {2},
  journal = {Foundations of Computational Mathematics},
  author = {Cand{\`e}s, Emmanuel J. and Romberg, Justin},
  year = {2006},
  keywords = {Applications of uncertainty principles,Basis pursuit,convex optimization,duality
	in optimization,Eigenvalues of random matrices,linear programming,random
	matrices,Sparsity,trigonometric expansion,uncertainty principle,wavelets},
  pages = {227--254},
  owner = {afdidehf}
}

@article{Cand`es2008b,
  title = {Enhancing Sparsity by Reweighted l1 Minimization},
  volume = {14},
  abstract = {It is now well understood that (1) it is possible to reconstruct sparse
	signals exactly from what appear to be highly incomplete sets of
	linear measurements and (2) that this can be done by constrained
	1 minimization. In this paper, we study a novel method for sparse
	signal recovery that in many situations outperforms 1 minimization
	in the sense that substantially fewer measurements are needed for
	exact recovery. The algorithm consists of solving a sequence of weighted
	1-minimization problems where the weights used for the next iteration
	are computed from the value of the current solution. We present a
	series of experiments demonstrating the remarkable performance and
	broad applicability of this algorithm in the areas of sparse signal
	recovery, statistical estimation, error correction and image processing.
	Interestingly, superior gains are also achieved when our method is
	applied to recover signals with assumed near-sparsity in overcomplete
	representations�not by reweighting the 1 norm of the coefficient
	sequence as is common, but by reweighting the 1 norm of the transformed
	object. An immediate consequence is the possibility of highly efficient
	data acquisition protocols by improving on a technique known as Compressive
	Sensing.},
  timestamp = {2016-07-08T12:21:14Z},
  journal = {J Fourier Anal Appl},
  author = {Cand{\`e}s, Emmanuel J. and Wakin, Michael B. and Boyd, Stephen P.},
  year = {2008},
  keywords = {�,Compressive,Dantzig,Equations,FOCUSS,Iterative,l1-minimization,linear,of,reweighting,selector,sensing,Sparsity,systems,underdetermined},
  pages = {877�905},
  owner = {afdidehf}
}

@article{Caruana2006,
  title = {An Empirical Comparison of Supervised Learning Algorithms},
  abstract = {A number of supervised learning methods have been introduced in the
	last decade. Un- fortunately, the last comprehensive empiri- cal
	evaluation of supervised learning was the Statlog Project in the
	early 90's. We present a large-scale empirical comparison between
	ten supervised learning methods: SVMs, neural nets, logistic regression,
	naive bayes, memory-based learning, random forests, de- cision trees,
	bagged trees, boosted trees, and boosted stumps. We also examine
	the eect that calibrating the models via Platt Scaling and Isotonic
	Regression has on their perfor- mance. An important aspect of our
	study is the use of a variety of performance criteria to evaluate
	the learning methods.},
  timestamp = {2016-07-08T10:31:15Z},
  journal = {Proceedings of the 23 rd International Conference on Machine Learning, 	Pittsburgh, PA},
  author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
  year = {2006},
  owner = {Fardin}
}

@article{Castano-Candamil2015,
  title = {Solving the EEG inverse problem based on space-time-frequency structured 	sparsity constraints},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2015.05.052},
  abstract = {Abstract We introduce \{STOUT\} (spatio-temporal unifying tomography),
	a novel method for the source analysis of electroencephalograpic
	(EEG) recordings, which is based on a physiologically-motivated source
	representation. Our method assumes that only a small number of brain
	sources are active throughout a measurement, where each of the sources
	exhibits focal (smooth but localized) characteristics in space, time
	and frequency. This structure is enforced through an expansion of
	the source current density into appropriate spatio-temporal basis
	functions in combination with sparsity constraints. This approach
	combines the main strengths of two existing methods, namely Sparse
	Basis Field Expansions (Haufe et al., 2011) and Time–Frequency
	Mixed-Norm Estimates (Gramfort et al., 2013). By adjusting the ratio
	between two regularization terms, \{STOUT\} is capable of trading
	temporal for spatial reconstruction accuracy and vice versa, depending
	on the requirements of specific analyses and the provided data. Due
	to allowing for non-stationary source activations, \{STOUT\} is particularly
	suited for the localization of event-related potentials (ERP) and
	other evoked brain activity. We demonstrate its performance on simulated
	\{ERP\} data for varying signal-to-noise ratios and numbers of active
	sources. Our analysis of the generators of visual and auditory evoked
	\{N200\} potentials reveals that the most active sources originate
	in the temporal and occipital lobes, in line with the literature
	on sensory processing.},
  timestamp = {2016-07-10T08:21:21Z},
  journal = {NeuroImage},
  author = {Casta{\~n}o-Candamil, Sebasti{\'a}n and H{\"o}hne, Johannes and Mart{\'i}nez-Vargas, Juan-David and An, Xing-Wei and Castellanos-Dom{\'i}nguez, German and Haufe, Stefan},
  year = {2015},
  keywords = {EEG},
  pages = {-},
  owner = {afdidehf}
}

@article{Castrodad2011,
  title = {Learning Discriminative Sparse Representations for Modeling, Source 	Separation, and Mapping of Hyperspectral Imagery},
  volume = {49},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2011.2163822},
  abstract = {A method is presented for subpixel modeling, mapping, and classification
	in hyperspectral imagery using learned block-structured discriminative
	dictionaries, where each block is adapted and optimized to represent
	a material in a compact and sparse manner. The spectral pixels are
	modeled by linear combinations of subspaces defined by the learned
	dictionary atoms, allowing for linear mixture analysis. This model
	provides flexibility in source representation and selection, thus
	accounting for spectral variability, small-magnitude errors, and
	noise. A spatial-spectral coherence regularizer in the optimization
	allows pixel classification to be influenced by similar neighbors.
	We extend the proposed approach for cases for which there is no knowledge
	of the materials in the scene, unsupervised classification, and provide
	experiments and comparisons with simulated and real data. We also
	present results when the data have been significantly undersampled
	and then reconstructed, still retaining high-performance classification,
	showing the potential role of compressive sensing and sparse modeling
	techniques in efficient acquisition/transmission missions for hyperspectral
	imagery.},
  timestamp = {2016-07-09T19:55:03Z},
  number = {11},
  journal = {Geoscience and Remote Sensing, IEEE Transactions on},
  author = {Castrodad, A. and Xing, Zhengming and Greer, J.B. and Bosch, E. and Carin, L. and Sapiro, G.},
  month = nov,
  year = {2011},
  keywords = {acquisition mission,Data models,Dictionaries,encoding,geophysical
	image processing,geophysical techniques,hyperspectral image classification,hyperspectral
	image mapping,Hyperspectral imaging,image classification,Image reconstruction,Image
	reconstruction,learned block-structured discriminative dictionaries,learned
	block-structured discriminative dictionaries,linear mixture analysis,linear
	mixture analysis,Materials,optimisation,scene classification,scene
	classification,sparse modeling,sparse
	modeling,spatial-spectral coherence analysis,spectral pixel model,spectral
	pixel model,spectral unmixing,spectral
	unmixing,spectral variability analysis,subpixel model,subpixel
	model,transmission mission,transmission
	mission},
  pages = {4263-4281},
  owner = {afdidehf}
}

@inproceedings{Cevher2009,
  address = {Marseille},
  title = {Recovery of clustered sparse signals from compressive measurements},
  abstract = {We introduce a new signal model, called (K,C)-sparse, to capture K-sparse
	signals in N dimensions whose nonzero coefficients are contained
	within at most C clusters, with C < K << N. In contrast to the existing
	work in the sparse approximation and compressive sensing literature
	on block sparsity, no prior knowledge of the locations and sizes
	of the clusters is assumed. We prove that O (K + C log(N/C)) random
	projections are sufficient for (K,C)-model sparse signal recovery
	based on subspace enumeration. We also provide a robust polynomial-time
	recovery algorithm for (K,C)-model sparse signals with provable estimation
	guarantees.},
  timestamp = {2016-07-10T07:48:15Z},
  booktitle = {Sampling Theory and Applications (SAMPTA)},
  author = {Cevher, Volkan and Indyk, Piotr and Hegde, Chinmay and Baraniuk, Richard G.},
  month = may,
  year = {2009},
  owner = {Fardin}
}

@inproceedings{Chalasani2013,
  title = {A fast proximal method for convolutional sparse coding},
  doi = {10.1109/IJCNN.2013.6706854},
  abstract = {Sparse coding, an unsupervised feature learning technique, is often
	used as a basic building block to construct deep networks. Convolutional
	sparse coding is proposed in the literature to overcome the scalability
	issues of sparse coding techniques to large images. In this paper
	we propose an efficient algorithm, based on the fast iterative shrinkage
	thresholding algorithm (FISTA), for learning sparse convolutional
	features. Through numerical experiments, we show that the proposed
	convolutional extension of FISTA can not only lead to faster convergence
	compared to existing methods but can also easily generalize to other
	cost functions.},
  timestamp = {2016-07-08T10:17:14Z},
  booktitle = {Neural Networks (IJCNN), The 2013 International Joint Conference 	on},
  author = {Chalasani, R. and Principe, J.C. and Ramakrishnan, N.},
  month = aug,
  year = {2013},
  keywords = {Convergence,Convolution,convolutional codes,convolutional extension,convolutional
	extension,convolutional sparse coding,convolutional
	sparse coding,Cost function,Dictionaries,encoding,fast
	iterative shrinkage thresholding algorithm,fast iterative
	shrinkage thresholding algorithm,fast proximal method,feature extraction,Feature
	Extraction,FISTA,image coding,Image segmentation,iterative methods,iterative
	methods,scalability issues,scalability
	issues,Sparse Coding,sparse convolutional features,sparse matrices,sparse
	matrices,unsupervised feature learning technique,unsupervised
	feature learning technique,unsupervised learning},
  pages = {1-5},
  owner = {afdidehf}
}

@article{Chandrasekaran2009,
  title = {Representation and Compression of Multidimensional Piecewise Functions 	Using Surflets},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2008.2008153},
  abstract = {We study the representation, approximation, and compression of functions
	in M dimensions that consist of constant or smooth regions separated
	by smooth (M-1)-dimensional discontinuities. Examples include images
	containing edges, video sequences of moving objects, and seismic
	data containing geological horizons. For both function classes, we
	derive the optimal asymptotic approximation and compression rates
	based on Kolmogorov metric entropy. For piecewise constant functions,
	we develop a multiresolution predictive coder that achieves the optimal
	rate-distortion performance; for piecewise smooth functions, our
	coder has near-optimal rate-distortion performance. Our coder for
	piecewise constant functions employs surflets, a new multiscale geometric
	tiling consisting of M-dimensional piecewise constant atoms containing
	polynomial discontinuities. Our coder for piecewise smooth functions
	uses surfprints, which wed surflets to wavelets for piecewise smooth
	approximation. Both of these schemes achieve the optimal asymptotic
	approximation performance. Key features of our algorithms are that
	they carefully control the potential growth in surflet parameters
	at higher smoothness and do not require explicit estimation of the
	discontinuity. We also extend our results to the corresponding discrete
	function spaces for sampled data. We provide asymptotic performance
	results for both discrete function spaces and relate this asymptotic
	performance to the sampling rate and smoothness orders of the underlying
	functions and discontinuities. For approximation of discrete data,
	we propose a new scale-adaptive dictionary that contains few elements
	at coarse and fine scales, but many elements at medium scales. Simulation
	results on synthetic signals provide a comparison between surflet-based
	coders and previously studied approximation schemes based on wedgelets
	and wavelets.},
  timestamp = {2016-07-10T07:54:24Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Chandrasekaran, V. and Wakin, M.B. and Baron, D. and Baraniuk, R.G.},
  month = jan,
  year = {2009},
  keywords = {approximation theory,asymptotic approximation,compression,data compression,Dictionaries,discontinuities,discrete
	data approximation,Entropy,Geology,image coding,images edges,Instruments,metric
	entropy,moving objects,multidimensional piecewise functions compression,multidimensional
	piecewise functions representation,multidimensional signals,Multidimensional
	systems,multiresolution predictive coder,multiscale geometric tiling,multiscale
	representations,nonlinear approximation,piecewise smooth approximation,piecewise
	smooth functions,polynomial discontinuities,Polynomials,rate�distortion,Sampling
	methods,seismic data,signal resolution,sparse representations,surflets,Two
	dimensional displays,video sequences,wavelets},
  pages = {374-400},
  owner = {afdidehf}
}

@inproceedings{Chandrasekaran2004,
  title = {Surflets: a sparse representation for multidimensional functions 	containing smooth discontinuities},
  doi = {10.1109/ISIT.2004.1365602},
  abstract = {Discontinuities in data often provide vital information, and representing
	these discontinuities sparsely is an important goal for approximation
	and compression algorithms. Little work has been done on efficient
	representations for higher dimensional functions containing arbitrarily
	smooth discontinuities. We consider the N-dimensional Horizon class-N-dimensional
	functions containing a CK smooth (N-1)-dimensional singularity separating
	two constant regions. We derive the optimal rate-distortion function
	for this class and introduce the multiscale surflet representation
	for sparse piecewise approximation of these functions. We propose
	a compression algorithm using surflets that achieves the optimal
	asymptotic rate-distortion performance for Horizon functions. This
	algorithm can be implemented using knowledge of only the N-dimensional
	function, without explicitly estimating the (N-1)-dimensional discontinuity.},
  timestamp = {2016-07-11T16:59:05Z},
  booktitle = {Information Theory, 2004. ISIT 2004. Proceedings. International Symposium 	on},
  author = {Chandrasekaran, V. and Wakin, M.B. and Baron, D. and Baraniuk, R.G.},
  month = jun,
  year = {2004},
  keywords = {arbitrarily smooth discontinuity,Bit rate,compression algorithm,Compression
	algorithms,data compression,Dictionaries,Hypercubes,Instruments,multidimensional-horizon
	class function,Multidimensional systems,multiscale surflet representation,optimal
	rate-distortion function,optimisation,Polynomials,Quantization,Rate-distortion,rate
	distortion theory,smoothing methods,sparse piecewise approximation,Video
	compression},
  pages = {563-},
  owner = {afdidehf}
}

@article{Chang2014,
  title = {An Improved RIP-Based Performance Guarantee for Sparse Signal Recovery 	via Orthogonal Matching Pursuit},
  volume = {60},
  issn = {0018-9448},
  doi = {10.1109/TIT.2014.2338314},
  abstract = {A sufficient condition reported very recently for perfect recovery
	of a K-sparse vector via orthogonal matching pursuit (OMP) in K iterations
	(when there is no noise) is that the restricted isometry constant
	(RIC) of the sensing matrix satisfies ?K+1 <; (1/?(K) + 1). In the
	noisy case, this RIC upper bound along with a requirement on the
	minimal signal entry magnitude is known to guarantee exact support
	identification. In this paper, we show that, in the presence of noise,
	a relaxed RIC upper bound ?K+1 <; (?(4K + 1) - 1/2K) together with
	a relaxed requirement on the minimal signal entry magnitude suffices
	to achieve perfect support identification using OMP. In the noiseless
	case, our result asserts that such a relaxed RIC upper bound can
	ensure exact support recovery in K iterations: this narrows the gap
	between the so far best known bound ?K+1 <; (1/?(K( + 1)) and the
	ultimate performance guarantee ?K+1 = (1/(K)). Our approach relies
	on a newly established near orthogonality condition, characterized
	via the achievable angles between two orthogonal sparse vectors upon
	compression, and, thus, better exploits the knowledge about the geometry
	of the compressed space. The proposed near orthogonality condition
	can be also exploited to derive less restricted sufficient conditions
	for signal reconstruction in two other compressive sensing problems,
	namely, compressive domain interference cancellation and support
	identification via the subspace pursuit algorithm.},
  timestamp = {2016-07-08T11:24:00Z},
  number = {9},
  journal = {Information Theory, IEEE Transactions on},
  author = {Chang, Ling-Hua and Wu, Jwo-Yuh},
  month = sep,
  year = {2014},
  keywords = {compressed sensing,compressive domain interference cancellation,Compressive
	sensing,compressive sensing problems,interference cancellation,interference
	suppression,iterative methods,K-sparse vector,Matching pursuit algorithms,near
	orthogonality condition,Noise,OMP,orthogonal matching pursuit,orthogonal
	matching pursuit,restricted isometry constant,restricted isometry
	constant (RIC),restricted isometry property,restricted isometry property
	(RIP),RIC,RIP based performance guarantee,Sensors,signal reconstruction,signal
	reconstruction,sparse matrices,sparse signal recovery,subspace pursuit,subspace
	pursuit algorithm,support identification,Upper bound,Vectors},
  pages = {5702-5715},
  owner = {Fardin}
}

@inproceedings{Chardon2014,
  title = {A block-sparse music algorithm for the localization and the identification 	of directive sources},
  doi = {10.1109/ICASSP.2014.6854343},
  abstract = {We introduce a generalization of the MUSIC algorithm to treat block-sparse
	signals in a multi-measurement vector framework. We show, through
	theoretical analysis and numerical experiments, that the requirements
	in terms of number of snapshots and number of measurements depend
	not only on the sparsity and on the size of the blocks, but also
	on the rank of the matrices of coefficients for each block. We apply
	this algorithm to the localization of directive sources, which can
	be modeled by block-sparsity in a dictionary of multipoles, and show
	that it compares favorably to a greedy approach based on the same
	model.},
  timestamp = {2016-07-08T10:07:04Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Chardon, G.},
  month = may,
  year = {2014},
  keywords = {Algorithm design and analysis,Arrays,block-sparse MUSIC algorithm,block-sparse
	signals,block sparsity,Dictionaries,directive source identification,directive
	source localization,greedy approach,matrices of coefficients,matrix
	algebra,multi-measurement vector,multimeasurement vector framework,Multiple signal classification,Multiple
	signal classification,multipole dictionary,signal
	classification,source localization,Standards,Vectors},
  pages = {3953-3957},
  owner = {afdidehf}
}

@article{Chartrand2007,
  title = {Exact Reconstruction of Sparse Signals via Nonconvex Minimization},
  volume = {14},
  issn = {1070-9908},
  doi = {10.1109/LSP.2007.898300},
  abstract = {Several authors have shown recently that It is possible to reconstruct
	exactly a sparse signal from fewer linear measurements than would
	be expected from traditional sampling theory. The methods used involve
	computing the signal of minimum lscr1 norm among those having the
	given measurements. We show that by replacing the lscr1 norm with
	the lscrp norm with p < 1, exact reconstruction is possible with
	substantially fewer measurements. We give a theorem in this direction,
	and many numerical examples, both in one complex dimension, and larger-scale
	examples in two real dimensions.},
  timestamp = {2016-09-29T16:32:35Z},
  number = {10},
  journal = {IEEE Signal Process. Letters},
  author = {Chartrand, R.},
  month = oct,
  year = {2007},
  keywords = {compressed sensing,concave programming,Frequency measurement,Gaussian
	distribution,image coding,Image reconstruction,image sampling,image
	sampling,minimisation,nonconvex minimization,nonconvex
	minimization,nonconvex optimization,Sampling methods,Sampling
	methods,signal reconstruction,sparse signal exact reconstruction,sparse
	signal exact reconstruction,Surges,Terminology},
  pages = {707--710},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@inproceedings{Chartrand2013,
  title = {A nonconvex ADMM algorithm for group sparsity with sparse groups},
  doi = {10.1109/ICASSP.2013.6638818},
  abstract = {We present an efficient algorithm for computing sparse representations
	whose nonzero coefficients can be divided into groups, few of which
	are nonzero. In addition to this group sparsity, we further impose
	that the nonzero groups themselves be sparse. We use a nonconvex
	optimization approach for this purpose, and use an efficient ADMM
	algorithm to solve the nonconvex problem. The efficiency comes from
	using a novel shrinkage operator, one that minimizes nonconvex penalty
	functions for enforcing sparsity and group sparsity simultaneously.
	Our numerical experiments show that combining sparsity and group
	sparsity improves signal reconstruction accuracy compared with either
	property alone. We also find that using nonconvex optimization significantly
	improves results in comparison with convex optimization.},
  timestamp = {2016-07-08T11:24:40Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Chartrand, R. and Wohlberg, B.},
  month = may,
  year = {2013},
  keywords = {alternating direction method of multipliers,compressed sensing,concave
	programming,convex optimization,Dictionaries,group sparsity,Minimization,nonconvex
	ADMM algorithm,nonconvex optimization,nonconvex optimization approach,nonconvex
	penalty functions,nonzero coefficients,Optimization,shrinkage,shrinkage
	operator,Signal processing algorithms,signal reconstruction,signal
	representation,sparse groups,sparse matrices,sparse representations,sparse
	signal representations,Vectors},
  pages = {6009-6013},
  owner = {afdidehf}
}

@inproceedings{Chavali2011,
  title = {A low-complexity sparsity-based multi-target tracking algorithm for 	urban environments},
  doi = {10.1109/RADAR.2011.5960549},
  abstract = {In this paper, we propose a low-complexity sparsity based multi-target
	tracking algorithm. We develop a finite dimensional representation
	of the received signal when the radar is operating in an urban environment.
	The dimensionality of the representation denotes the extra degrees
	of freedom that an urban environment offers. We employ spread-spectrum
	signaling to exploit the full diversity offered by the environment.
	We then develop a block-sparse measurement model by discretizing
	the delay-Doppler plane and prove that the dictionary of the block
	sparse model exhibits a special structure under spread-spectrum signaling.
	This structure enables an efficient support recovery of the sparse
	vector, by projecting the measurement vector on the row space of
	the dictionary. Numerical simulations show that our tracking procedure
	takes significantly less time, while giving good tracking performance.},
  timestamp = {2016-07-08T10:25:46Z},
  booktitle = {Radar Conference (RADAR), 2011 IEEE},
  author = {Chavali, P. and Nehorai, Arye},
  month = may,
  year = {2011},
  keywords = {Delay,Dictionaries,diversity reception,Doppler effect,Doppler radar,finite
	dimensional representation,Indexes,low complexity sparsity-based
	multitarget tracking algorithm,low-complexity support recovery,measurement
	vector,Multi-target tracking,Numerical simulation,radar tracking,sparse
	vector,spread spectrum radar,spread-spectrum signaling,target tracking,Target
	tracking,tracking procedure,urban environment},
  pages = {309-314},
  owner = {afdidehf}
}

@article{Chavez-Roman2014,
  title = {Super Resolution Image Generation Using Wavelet Domain Interpolation 	With Edge Extraction via a Sparse Representation},
  volume = {11},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2014.2308905},
  abstract = {This letter addresses the problem of generating a super-resolution
	(SR) image from a single low-resolution (LR) input image in the wavelet
	domain. To achieve a sharper image, an intermediate stage for estimating
	the high-frequency (HF) subbands has been proposed. This stage includes
	an edge preservation procedure and mutual interpolation between the
	input LR image and the HF subband images, as performed via the discrete
	wavelet transform (DWT). Sparse mixing weights are calculated over
	blocks of coefficients in an image, which provides a sparse signal
	representation in the LR image. All of the subband images are used
	to generate the new high-resolution image using the inverse DWT.
	Experimental results indicated that the proposed approach outperforms
	existing methods in terms of objective criteria and subjective perception
	improving the image resolution.},
  timestamp = {2016-07-11T16:59:00Z},
  number = {10},
  journal = {Geoscience and Remote Sensing Letters, IEEE},
  author = {Chavez-Roman, H. and Ponomaryov, V.},
  month = oct,
  year = {2014},
  keywords = {Discrete wavelet transforms,edge detection,edge extraction,edge
	extraction,Hafnium,HF subband image,HF
	subband image,high-frequency subband image,Image edge detection,Image
	edge detection,image representation,image
	representation,Image resolution,interpolation,inverse
	discrete wavelet transform,inverse discrete wavelet
	transform,inverse DWT,inverse transforms,PSNR,single low-resolution
	input image,single
	low-resolution input image,single LR input image,sparse mixing estimators,sparse
	mixing weight calculation,sparse signal representation,SR image generation,super
	resolution image generation,super resolution (SR),Wavelet domain,Wavelet
	domain,wavelet domain mutual interpolation},
  pages = {1777-1781},
  owner = {afdidehf}
}

@inproceedings{Chen2014,
  title = {O(1) Algorithms for Overlapping Group Sparsity},
  doi = {10.1109/ICPR.2014.291},
  abstract = {Sparsity based techniques have become very popular in machine learning,
	medical imaging and computer vision. Recently, with the emerging
	and development of structured sparsity, signals can be recovered
	more accurately. However, solving structured sparsity problems often
	involves much higher computational complexity. Few of existing works
	can reduce the computational complexity of such problems. Especially
	for overlapping group sparsity, the computational complexity for
	each entry is linear to the degree of overlapping, making it infeasible
	for large-scale problems. In this paper, we propose novel algorithms
	to efficiently address this issue, where the computational complexity
	for each entry is always O(1) and independent to the degree of overlapping.
	Experiments on 1D signal and 2D image demonstrate the effectiveness
	and efficiency of our methods. This work may inspire more scalable
	algorithms for structured sparsity.},
  timestamp = {2016-07-10T07:10:33Z},
  booktitle = {Pattern Recognition (ICPR), 2014 22nd International Conference on},
  author = {Chen, Chen and Peng, Zhongxing and Huang, Junzhou},
  month = aug,
  year = {2014},
  keywords = {1D signal,2D image,Approximation algorithms,Clustering algorithms,computational complexity,Computational
	complexity,Computational efficiency,Image
	Processing,Noise reduction,O(1) algorithms,overlapping group sparsity,pattern
	clustering,signal processing,Signal processing algorithms},
  pages = {1645-1650},
  owner = {afdidehf}
}

@inproceedings{Chen2014a,
  title = {Medical Image Feature Extraction and Fusion Algorithm Based on K-SVD},
  doi = {10.1109/3PGCIC.2014.142},
  abstract = {In order to better fuse the CT and MR images, based on the classical
	image fusion method, an image feature extraction and fusion algorithm
	based on K-SVD is presented. The images are sparse representation.
	The images are divided into blocks via the sliding window. The dictionary
	is compiled the column vectors. The redundant dictionary is learned
	by the K-singular value decomposition (K-SVD) algorithm. Then we
	solve the sparse coefficient matrix for each original image. And
	combining sparse coefficient of nonzero elements realizes the image
	feature fusion. Finally, the reconstructed fusion image is obtained
	from the combined sparse coefficients and the overcomplete dictionary.
	The method in this paper is capable of extracting image features
	and the strong anti noise interference. Experiments show that this
	method better preserves the useful information in the original image
	and the fusion image details are clear. Compared with other fusion
	algorithms, the results show that the proposed method has better
	fusion performance in both noiseless and noisy situations and is
	superior.},
  timestamp = {2016-07-09T20:10:05Z},
  booktitle = {P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC), 2014 	Ninth International Conference on},
  author = {Chen, Hongli and Huang, Zhaohua},
  month = nov,
  year = {2014},
  keywords = {antinoise interference,biomedical MRI,column vectors,Computed tomography,computerised
	tomography,CT images,Dictionaries,Discrete wavelet transforms,Feature
	Extraction,fusion algorithm,image denoising,image feature fusion,image fusion,image
	fusion,Image reconstruction,image representation,K-singular
	value decomposition algorithm,K-SVD algorithm,medical image feature
	extraction,medical image processing,MR images,nonzero elements,reconstructed
	fusion image,redundant dictionary,Singular Value Decomposition,sliding
	window,sparse coefficient matrix,sparse matrices,sparse representation,Vectors},
  pages = {333-337},
  owner = {afdidehf}
}

@article{Chen2006,
  title = {Theoretical Results on Sparse Representations of Multiple-Measurement 	Vectors},
  volume = {54},
  issn = {1053-587X},
  doi = {10.1109/TSP.2006.881263},
  abstract = {The sparse representation of a multiple-measurement vector (MMV) is
	a relatively new problem in sparse representation. Efficient methods
	have been proposed. Although many theoretical results that are available
	in a simple case-single-measurement vector (SMV)-the theoretical
	analysis regarding MMV is lacking. In this paper, some known results
	of SMV are generalized to MMV. Some of these new results take advantages
	of additional information in the formulation of MMV. We consider
	the uniqueness under both an lscr0-norm-like criterion and an lscr1-norm-like
	criterion. The consequent equivalence between the lscr0-norm approach
	and the lscr1-norm approach indicates a computationally efficient
	way of finding the sparsest representation in a redundant dictionary.
	For greedy algorithms, it is proven that under certain conditions,
	orthogonal matching pursuit (OMP) can find the sparsest representation
	of an MMV with computational efficiency, just like in SMV. Simulations
	show that the predictions made by the proved theorems tend to be
	very conservative; this is consistent with some recent advances in
	probabilistic analysis based on random matrix theory. The connections
	will be discussed},
  timestamp = {2016-09-30T11:20:03Z},
  number = {12},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Chen, Jie and Huo, X.},
  month = dec,
  year = {2006},
  keywords = {Analytical models,Basis pursuit,Computational efficiency,Computational
	modeling,Dictionaries,Equations,Greedy algorithms,iterative methods,l0-norm-like
	criterion,l1-norm-like criterion,Magnetic analysis,Matching pursuit
	algorithms,matrix algebra,multiple-measurement vector (MMV),multiple-measurement
	vectors,orthogonal matching pursuit,orthogonal matching pursuit (OMP),Predictive
	models,probabilistic analysis,random matrix theory,redundant dictionary,signal
	representation,single-measurement vector,sparse matrices,sparse representation,sparse
	representations,statistical analysis,time-frequency analysis},
  pages = {4634--4643},
  owner = {Fardin}
}

@inproceedings{Chen2005,
  title = {Sparse representations for multiple measurement vectors (MMV) in 	an over-complete dictionary},
  volume = {4},
  doi = {10.1109/ICASSP.2005.1415994},
  abstract = {The multiple measurement vector (MMV), a newly emerged problem in
	sparse representation in an over-complete dictionary motivated by
	a neuro-magnetic inverse problem that arises in magnetoencephalography
	(MEG) - a modality for imaging the possible activation regions in
	the brain, poses new challenges. Efficient methods have been designed
	to search for sparse representations; however, we have not seen substantial
	development in the theoretical analysis, considering what has been
	done in a simpler case - single measurement vector (SMV) - in which
	many theoretical results are known. This paper extends the known
	results of SMV to MMV. Our theoretical results show the fundamental
	limitation on when a sparse representation is unique. Moreover, the
	relation between the solutions of ?0-norm minimization and the solutions
	of ?1-norm minimization indicates a computationally efficient approach
	to find a sparse representation. Interestingly, simulations show
	that the predictions made by these theorems tend to be conservative.},
  timestamp = {2016-07-11T16:52:56Z},
  booktitle = {Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP 	'05). IEEE International Conference on},
  author = {Chen, Jie and Huo, X.},
  month = mar,
  year = {2005},
  keywords = {?0-norm minimization,?1-norm minimization,biomedical imaging,brain
	activation region imaging,Brain modeling,Computational modeling,Design
	methodology,Dictionaries,Equations,inverse problems,magnetoencephalography,Matching
	pursuit algorithms,MEG,minimisation,MMV,multiple measurement vector,neuro-magnetic
	inverse problem,over-complete dictionary,Predictive models,signal
	representation,single measurement vector,SMV,sparse matrices,sparse
	representation,Systems engineering and theory,Vectors},
  pages = {iv/257-iv/260 Vol. 4},
  owner = {Fardin}
}

@inproceedings{Chen2013,
  title = {Distributed sparse canonical correlation analysis in clustering sensor 	data},
  doi = {10.1109/ACSSC.2013.6810359},
  abstract = {The problem of determining information-bearing sensors in the presence
	of multiple field sources and (non-)linear data models is considered.
	To this end, a novel canonical correlation analysis (CCA) framework
	combined with norm-one regularization is introduced to identify correlated
	measurements across the distributed sensors and cluster the sensor
	data based on their source content. A distributed algorithm is also
	put forth for informative sensor identification in nonlinear settings
	using the novel CCA approach. Toward this end, the sparsity-aware
	CCA framework is reformulated as a separable constrained minimization
	problem which is solved by utilizing block coordinate descent techniques
	combined with the alternating direction method of multipliers. Numerical
	tests demonstrate that the distributed sparse CCA scheme put forth
	outperforms existing alternatives when it comes to clustering the
	sensor data based on their source content.},
  timestamp = {2016-07-08T12:15:11Z},
  booktitle = {Signals, Systems and Computers, 2013 Asilomar Conference on},
  author = {Chen, Jia and Schizas, I.D.},
  month = nov,
  year = {2013},
  keywords = {alternating direction method of multipliers,block coordinate descent
	techniques,canonical correlation analysis,correlated measurements,Correlation,correlation
	methods,Cost function,Data models,distributed algorithm,distributed
	algorithms,Distributed databases,Distributed processing,distributed sensors,distributed
	sensors,distributed sparse CCA scheme,information-bearing
	sensors,informative sensor identification,Minimization,multiple field
	sources,Noise,nonlinear data models,nonlinear settings,norm-one regularization,novel
	canonical correlation analysis,pattern clustering,sensor data clustering,separable
	constrained minimization problem,source content,Sparsity,sparsity-aware
	CCA framework,Standards},
  pages = {639-643},
  owner = {afdidehf}
}

@article{Chen2015,
  title = {On the Null Space Constant for $\ell_p$ Minimization},
  volume = {22},
  issn = {1070-9908},
  doi = {10.1109/LSP.2015.2416003},
  abstract = {The literature on sparse recovery often adopts the lp �norm� (
	p ? [0,1]) as the penalty to induce sparsity of the signal satisfying
	an underdetermined linear system. The performance of the corresponding
	lp minimization problem can be characterized by its null space constant.
	In spite of the NP-hardness of computing the constant, its properties
	can still help in illustrating the performance of lp minimization.
	In this letter, we show the strict increase of the null space constant
	in the sparsity level k and its continuity in the exponent p. We
	also indicate that the constant is strictly increasing in p with
	probability 1 when the sensing matrix A is randomly generated. Finally,
	we show how these properties can help in demonstrating the performance
	of lp minimization, mainly in the relationship between the the exponent
	p and the sparsity level k.},
  timestamp = {2016-07-10T07:16:57Z},
  number = {10},
  journal = {Signal Processing Letters, IEEE},
  author = {Chen, Laming and Gu, Yuantao},
  month = oct,
  year = {2015},
  keywords = {${ell _p}$ minimization,compressed sensing,computational complexity,Continuity,lp
	minimization problem,Materials,minimisation,Minimization,monotonicity,NP-hardness,Null
	space,null space constant,probability,Probability distribution,sensing
	matrix,Sensors,signal sparse recovery,sparse recovery,Standards,underdetermined
	linear system,Vectors},
  pages = {1600-1603},
  owner = {afdidehf}
}

@article{Chen2001,
  title = {Atomic Decomposition by Basis Pursuit},
  volume = {43},
  abstract = {The time-frequency and time-scale communities have recently developed
	a large number of overcomplete waveform dictionaries stationary
	wavelets, wavelet packets, cosine packets, chirplets, and warplets,
	to name a few. Decomposition into overcomplete systems is not unique,
	and several methods for decomposition have been proposed, including
	the method of frames (MOF), matching pursuit (MP), and, for special
	dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP)
	is a principle for decomposing a signal into an optimal superposition
	of dictionary elements, where optimal means having the smallest l1
	norm of coefficients among all such decompositions. We give examples
	exhibiting several advantages over MOF, MP, and BOB, including better
	sparsity and superresolution. BP has interesting relations to ideas
	in areas as diverse as ill-posed problems, abstract harmonic analysis,
	total variation denoising, and multiscale edge denoising. BP in highly
	overcomplete dictionaries leads to large-scale optimization problems.
	With signals of length 8192 and a wavelet packet dictionary, one
	gets an equivalent linear program of size 8192 by 212,992. Such problems
	can be attacked successfully only because of recent advances in linear
	and quadratic programming by interior-point methods. We obtain reasonable
	success with a primal-dual logarithmic barrier method and conjugategradient
	solver.},
  timestamp = {2017-06-23T13:08:19Z},
  number = {1},
  journal = {SIAM J. Sci. Comput.},
  author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
  year = {2001},
  keywords = {cosine packets,denoising,interior-point methods for linear programming,l1
	norm optimization,matching pursuit,MATLAB code,multiscale edges,overcomplete
	signal representation,time-frequency analysis,time-scale analysis,total
	variation denoising,wavelet packets,wavelets},
  pages = {129--159},
  annote = {SIAM Journal on Scientific Computing},
  annote = {SIAM Journal on Scientific Computing},
  annote = {SIAM Journal on Scientific Computing},
  annote = {SIAM Journal on Scientific Computing},
  annote = {SIAM Journal on Scientific Computing},
  owner = {afdidehf}
}

@inproceedings{Chen2013a,
  title = {Multiple dipolar sources localization for MEG using Bayesian particle 	filtering},
  doi = {10.1109/ICASSP.2013.6637789},
  abstract = {Electromagnetic source localization is a technique that enables the
	study of neural dynamical activities on a millisecond timescale using
	Magnetoencephalography (MEG) or Electroencephalography (EEG) data.
	It aims to reveal neural activities in the brain cortical region
	which cannot be seen with imaging methods that operate on a slower
	timescale such as fMRI. In this paper, we model the problem under
	a Bayesian multi-target tracking framework. A multi-target detection
	and particle filtering algorithm is developed to estimate the dipolar
	source dynamics, and a minimum norm (MN) based estimation method
	is incorporated to construct the birth-death move for the dynamical
	number of dipolar sources. The algorithm is tested using both simulated
	and experimental data1. The results demonstrate that the proposed
	algorithm performs better than that in previous works in terms of
	both localization accuracy and computational cost.},
  timestamp = {2016-07-09T20:16:51Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Chen, Xi and Godsill, S.},
  month = may,
  year = {2013},
  keywords = {Bayesian,Bayesian multitarget tracking framework,Bayesian particle
	filtering,Bayes methods,belief networks,birth-death move,brain cortical
	region,Brain modeling,computational cost,Computational modeling,dipolar
	source dynamics,dipolar sources,EEG data,electrocardiography,electroencephalography,electroencephalography
	data,electromagnetic source localization,Estimation,Estimation theory,fMRI,Heuristic
	algorithms,imaging methods,localization,localization accuracy,magnetoencephalography,magnetoencephalography
	data,medical signal detection,MEG data,MEG/EEG,millisecond timescale,minimum
	norm based estimation method,multiple dipolar sources localization,multitarget
	detection,neural activity,neural dynamical activity,object detection,particle
	filter,particle filtering algorithm,particle filtering (numerical
	methods),target tracking,Vectors},
  pages = {949-953},
  owner = {afdidehf}
}

@inproceedings{Chen2012,
  title = {Structured Sparse Canonical Correlation Analysis},
  abstract = {In this paper, we propose to apply sparse canonical correlation analysis
	(sparse CCA) to an important genome-wide association study problem,
	eQTL mapping. Existing sparse CCA models do not incorporate structural
	information among variables such as pathways of genes. This work
	extends the sparse CCA so that it could exploit either the pre-given
	or unknown group structure via the structured-sparsity-inducing penalty.
	Such structured penalty poses new challenge on optimization techniques.
	To address this challenge, by specializing the excessive gap framework,
	we develop a scalable primal-dual optimization algorithm with a fast
	rate of convergence. Empirical results show that the proposed optimization
	algorithm is more efficient than existing state-of-the-art methods.
	We also demonstrate the effectiveness of the structured sparse CCA
	on both simulated and genetic datasets.},
  timestamp = {2016-07-11T16:57:38Z},
  booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence 	and Statistics (AISTATS)},
  author = {Chen, Xi and Liu, Han and Carbonell, Jaime G.},
  year = {2012},
  owner = {afdidehf}
}

@inproceedings{Chen2015a,
  title = {SegBOMP: An efficient algorithm for block non-sparse signal recovery},
  doi = {10.1109/ICME.2015.7177503},
  abstract = {Block sparse signal recovery methods have attracted great interests
	which take the block structure of the nonzero coefficients into account
	when clustering. Compared with traditional compressive sensing methods,
	it can obtain better recovery performance with fewer measurements
	by utilizing the block-sparsity explicitly. In this paper we propose
	a segmented-version of the block orthogonal matching pursuit algorithm
	in which it divides any vector into several sparse sub-vectors. By
	doing this, the original method can be significantly accelerated
	due to the dimension reduction of measurements for each segmented
	vector. Experimental results showed that with low complexity the
	proposed method yielded identical or even better reconstruction performance
	than the conventional methods which treated the signal in the standard
	block-sparsity fashion. Furthermore, in the specific case, where
	not all segments contain nonzero blocks, the performance improvement
	can be interpreted as a gain in �effective SNR� in noisy environment.},
  timestamp = {2016-07-10T08:10:25Z},
  booktitle = {Multimedia and Expo (ICME), 2015 IEEE International Conference on},
  author = {Chen, Xushan and Zhang, Xiongwei and Yang, Jibin and Sun, Meng and Zeng, Li},
  month = jun,
  year = {2015},
  keywords = {block orthogonal matching pursuit algorithm,block sparse signal recovery
	method,Block-sparsity,block-sparsity explicitly,compressed sensing,Compressive
	sensing,compressive sensing method,Computational modeling,Dimension
	Reduction,Matching pursuit algorithms,Noise measurement,nonzero coefficient,orthogonal
	matching pursuit,pattern clustering,signal reconstruction,Signal
	to noise ratio,SNR,Standards},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Chen2010,
  title = {Robust face recognition using locally adaptive sparse representation},
  doi = {10.1109/ICIP.2010.5652203},
  abstract = {This paper presents a block-based face-recognition algorithm based
	on a sparse linear-regression subspace model via locally adaptive
	dictionary constructed from past observable data (training samples).
	The local features of the algorithm provide an immediate benefit
	- the increase in robustness level to various registration errors.
	Our proposed approach is inspired by the way human beings often compare
	faces when presented with a tough decision: we analyze a series of
	local discriminative features (do the eyes match? how about the nose?
	what about the chin?...) and then make the final classification decision
	based on the fusion of local recognition results. In other words,
	our algorithm attempts to represent a block in an incoming test image
	as a linear combination of only a few atoms in a dictionary consisting
	of neighboring blocks in the same region across all training samples.
	The results of a series of these sparse local representations are
	used directly for recognition via either maximum likelihood fusion
	or a simple democratic majority voting scheme. Simulation results
	on standard face databases demonstrate the effectiveness of the proposed
	algorithm in the presence of multiple mis-registration errors such
	as translation, rotation, and scaling.},
  timestamp = {2016-09-30T11:28:03Z},
  booktitle = {Image Processing (ICIP), 2010 17th IEEE International Conference 	on},
  author = {Chen, Yi and Do, T.T. and Tran, T.D.},
  month = sep,
  year = {2010},
  keywords = {Databases,Dictionaries,Face,face recognition,locally adaptive sparse
	representation,Maximum likelihood estimation,maximum likelihood fusion,regression
	analysis,robust face recognition,Robustness,sparse linear-regression
	subspace model,sparse matrices,Training,Training data},
  pages = {1657--1660},
  owner = {afdidehf}
}

@inproceedings{Cheng2014,
  title = {LorSLIM: Low Rank Sparse Linear Methods for Top-N Recommendations},
  doi = {10.1109/ICDM.2014.112},
  abstract = {In this paper, we notice that sparse and low-rank structures arise
	in the context of many collaborative filtering applications where
	the underlying graphs have block-diagonal adjacency matrices. Therefore,
	we propose a novel Sparse and Low-Rank Linear Method (Lor SLIM) to
	capture such structures and apply this model to improve the accuracy
	of the Top-N recommendation. Precisely, a sparse and low-rank aggregation
	coefficient matrix W is learned from Lor SLIM by solving an l1-norm
	and nuclear norm regularized optimization problem. We also develop
	an efficient alternating augmented Lagrangian method (ADMM) to solve
	the optimization problem. A comprehensive set of experiments is conducted
	to evaluate the performance of Lor SLIM. The experimental results
	demonstrate the superior recommendation quality of the proposed algorithm
	in comparison with current state-of-the-art methods.},
  timestamp = {2016-07-09T19:58:14Z},
  booktitle = {Data Mining (ICDM), 2014 IEEE International Conference on},
  author = {Cheng, Yao and Yin, Li'ang and Yu, Yong},
  month = dec,
  year = {2014},
  keywords = {ADMM,alternating augmented Lagrangian method,block-diagonal adjacency
	matrices,Collaboration,collaborative filtering,collaborative filtering
	applications,Equations,l1-norm,L_1-norm regularization,LorSLIM,Lor
	SLIM,low-rank aggregation coefficient matrix,low-rank linear method,low
	rank sparse linear methods,low-rank structures,mathematical model,matrix
	algebra,nuclear norm regularization,nuclear norm regularized optimization
	problem,optimisation,Optimization,recommender systems,Sparse and
	Low-Rank Linear Method,sparse matrices,top-n recommendations,Top-N
	Recommender Systems,Vectors},
  pages = {90-99},
  owner = {afdidehf}
}

@article{Chesneau2008,
  title = {Some theoretical results on the Grouped Variables Lasso},
  volume = {17},
  issn = {1066-5307},
  doi = {10.3103/S1066530708040030},
  abstract = {We consider the linear regression model with Gaussian error. We estimate
	the unknown parameters by a procedure inspired by the Group Lasso
	estimator introduced in [22]. We show that this estimator satisfies
	a sparsity inequality, i.e., a bound in terms of the number of non-zero
	components of the oracle regression vector. We prove that this bound
	is better, in some cases, than the one achieved by the Lasso and
	the Dantzig selector.},
  language = {English},
  timestamp = {2016-07-10T08:22:32Z},
  number = {4},
  journal = {Mathematical Methods of Statistics},
  author = {Chesneau, Ch. and Hebiri, M.},
  year = {2008},
  keywords = {group lasso,Lasso,penalized least squares,Sparsity,variable selection},
  pages = {317-326},
  owner = {afdidehf}
}

@inproceedings{Chi2013,
  title = {Block and Group Regularized Sparse Modeling for Dictionary Learning},
  doi = {10.1109/CVPR.2013.55},
  abstract = {This paper proposes a dictionary learning framework that combines
	the proposed block/group (BGSC) or reconstructed block/group (R-BGSC)
	sparse coding schemes with the novel Intra-block Coherence Suppression
	Dictionary Learning algorithm. An important and distinguishing feature
	of the proposed framework is that all dictionary blocks are trained
	simultaneously with respect to each data group while the intra-block
	coherence being explicitly minimized as an important objective. We
	provide both empirical evidence and heuristic support for this feature
	that can be considered as a direct consequence of incorporating both
	the group structure for the input data and the block structure for
	the dictionary in the learning process. The optimization problems
	for both the dictionary learning and sparse coding can be solved
	efficiently using block-gradient descent, and the details of the
	optimization algorithms are presented. We evaluate the proposed methods
	using well-known datasets, and favorable comparisons with state-of-the-art
	dictionary learning methods demonstrate the viability and validity
	of the proposed framework.},
  timestamp = {2016-07-08T11:40:38Z},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference 	on},
  author = {Chi, Yu-Tseh and Ali, M. and Rajwade, A. and Ho, J.},
  month = jun,
  year = {2013},
  keywords = {BGSC,block-gradient descent,block group,Coherence,Dictionaries,dictionary
	blocks,dictionary learning,encoding,Error analysis,handwriting recognition,hand-written
	digit recognition,image coding,intra-block coherence,learning (artificial
	intelligence),linear programming,novel intra-block coherence suppression
	dictionary learning algorithm,optimisation,optimization problems,R-BGSC,reconstructed
	block group sparse coding schemes,Sparse Coding,Training,Vectors},
  pages = {377-382},
  owner = {afdidehf}
}

@inproceedings{Chi2013a,
  title = {Affine-Constrained Group Sparse Coding and Its Application to Image-Based 	Classifications},
  doi = {10.1109/ICCV.2013.90},
  abstract = {This paper proposes a novel approach for sparse coding that further
	improves upon the sparse representation-based classification (SRC)
	framework. The proposed framework, Affine-Constrained Group Sparse
	Coding (ACGSC), extends the current SRC framework to classification
	problems with multiple input samples. Geometrically, the affineconstrained
	group sparse coding essentially searches for the vector in the convex
	hull spanned by the input vectors that can best be sparse coded using
	the given dictionary. The resulting objective function is still convex
	and can be efficiently optimized using iterative block-coordinate
	descent scheme that is guaranteed to converge. Furthermore, we provide
	a form of sparse recovery result that guarantees, at least theoretically,
	that the classification performance of the constrained group sparse
	coding should be at least as good as the group sparse coding. We
	have evaluated the proposed approach using three different recognition
	experiments that involve illumination variation of faces and textures,
	and face recognition under occlusions. Preliminary experiments have
	demonstrated the effectiveness of the proposed approach, and in particular,
	the results from the recognition/occlusion experiment are surprisingly
	accurate and robust.},
  timestamp = {2016-07-08T10:17:18Z},
  booktitle = {Computer Vision (ICCV), 2013 IEEE International Conference on},
  author = {Chi, Yu-Tseh and Ali, M. and Rushdi, M. and Ho, J.},
  month = dec,
  year = {2013},
  keywords = {ACGSC,affine,affine-constrained group sparse coding,classification,convex
	hull,Dictionaries,encoding,face recognition,group,illumination variation,illumination
	variation,image-based classifications,image-based
	classifications,image classification,image coding,Image
	coding,image representation,image texture,image
	texture,iterative block-coordinate descent scheme,iterative block-coordinate
	descent scheme,iterative methods,lighting,objective function,objective
	function,Sparse Coding,Sparse
	coding,sparse matrices,sparse representation-based classification
	framework,sparse representation-based
	classification framework,SRC,textures,Training,Vectors},
  pages = {681-688},
  owner = {afdidehf}
}

@inproceedings{Chiou2012,
  title = {Efficient image/video deblocking via sparse representation},
  doi = {10.1109/VCIP.2012.6410838},
  abstract = {Blocking artifact, characterized by visually noticeable changes in
	pixel values along block boundaries, is a common problem in block-based
	image/video compression, especially at low bitrate coding. Various
	post-processing techniques have been proposed to reduce blocking
	artifacts, but they usually introduce excessive blurring or ringing
	effects. This paper proposes a self-learning-based image/ video deblocking
	framework via properly formulating deblocking as an MCA (morphological
	component analysis)-based image decomposition problem via sparse
	representation. The proposed method first decomposes an image/video
	frame into the low-frequency and high-frequency parts by applying
	BM3D (block-matching and 3D filtering) algorithm. The high-frequency
	part is then decomposed into a �blocking component� and a �non-blocking
	component� by performing dictionary learning and sparse coding
	based on MCA. As a result, the blocking component can be removed
	from the image/video frame successfully while preserving most original
	image/video details. Experimental results demonstrate the efficacy
	of the proposed algorithm.},
  timestamp = {2016-07-08T12:19:00Z},
  booktitle = {Visual Communications and Image Processing (VCIP), 2012 IEEE},
  author = {Chiou, Yi-Wen and Yeh, Chia-Hung and Kang, Li-Wei and Lin, Chia-Wen and Fan-Jiang, Shu-Jhen},
  month = nov,
  year = {2012},
  keywords = {3D filtering algorithm,block-based image-video compression,block boundaries,Blocking
	artifact,blocking artifact reduction,block-matching filtering algorithm,BM3D
	algorithm,data compression,deblocking,Dictionaries,dictionary learning,excessive
	blurring effects,filtering theory,image coding,Image decomposition,image
	matching,Image reconstruction,image representation,image-video deblocking,low
	bitrate coding,MCA analysis-based image decomposition problem,morphological
	component analysis,morphological component analysis-based image decomposition
	problem,nonblocking component,pixel values,post-processing techniques,ringing
	effects,self-learning-based image-video deblocking framework,sparse
	representation,Streaming media,Training,Transform coding,video coding},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Chouzenoux2011,
  title = {A Memory Gradient algorithm for l2-l0 regularization with applications 	to image restoration},
  doi = {10.1109/ICIP.2011.6116230},
  abstract = {In this paper, we consider a class of differentiable criteria for
	sparse image recovery problems. The regularization is applied to
	a linear transform of the target image. As special cases, it includes
	edge preserving measures or frame analysis potentials. As shown by
	our asymptotic results, the considered ?2 - ?0 penalties may be employed
	to approximate solutions to ?0 penalized optimization problems. One
	of the advantages of the approach is that it allows us to derive
	an efficient Majorize-Minimize Memory Gradient algorithm. The fast
	convergence properties of the proposed optimization algorithm are
	illustrated through image restoration examples.},
  timestamp = {2016-07-08T10:27:24Z},
  booktitle = {Image Processing (ICIP), 2011 18th IEEE International Conference 	on},
  author = {Chouzenoux, E. and Pesquet, J. and Talbot, H. and Jezierska, A.},
  month = sep,
  year = {2011},
  keywords = {Convergence,deblurring,denoising,differentiable criteria,edge preservation,edge
	preserving measures,fast convergence property,frame analysis potentials,gradient
	methods,Image edge detection,image restoration,image restoration
	examples,inverse problems,linear transform,Majorize-Minimize algorithms,majorize-minimize
	memory gradient algorithm,Minimization,Noise reduction,non-convex
	optimization,optimisation,Optimization,optimization algorithm,penalized
	optimization problems,regularization,sparse image recovery problems,sparse
	representations,target image,transforms},
  pages = {2717-2720},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@article{Chowdhury2015,
  title = {MEG-EEG Information Fusion and Electromagnetic Source Imaging: From 	Theory to Clinical Application in Epilepsy},
  issn = {0896-0267},
  doi = {10.1007/s10548-015-0437-3},
  abstract = {The purpose of this study is to develop and quantitatively assess
	whether fusion of EEG and MEG (MEEG) data within the maximum entropy
	on the mean (MEM) framework increases the spatial accuracy of source
	localization, by yielding better recovery of the spatial extent and
	propagation pathway of the underlying generators of inter-ictal epileptic
	discharges (IEDs). The key element in this study is the integration
	of the complementary information from EEG and MEG data within the
	MEM framework. MEEG was compared with EEG and MEG when localizing
	single transient IEDs. The fusion approach was evaluated using realistic
	simulation models involving one or two spatially extended sources
	mimicking propagation patterns of IEDs. We also assessed the impact
	of the number of EEG electrodes required for an efficient EEG�MEG
	fusion. MEM was compared with minimum norm estimate, dynamic statistical
	parametric mapping, and standardized low-resolution electromagnetic
	tomography. The fusion approach was finally assessed on real epileptic
	data recorded from two patients showing IEDs simultaneously in EEG
	and MEG. Overall the localization of MEEG data using MEM provided
	better recovery of the source spatial extent, more sensitivity to
	the source depth and more accurate detection of the onset and propagation
	of IEDs than EEG or MEG alone. MEM was more accurate than the other
	methods. MEEG proved more robust than EEG and MEG for single IED
	localization in low signal-tonoise ratio conditions. We also showed
	that only few EEG electrodes are required to bring additional relevant
	information to MEG during MEM fusion.},
  language = {English},
  timestamp = {2016-10-21T14:00:46Z},
  journal = {Brain Topography},
  author = {Chowdhury, RashedaArman and Zerouali, Younes and Hedrich, Tanguy and Heers, Marcel and Kobayashi, Eliane and Lina, Jean-Marc and Grova, Christophe},
  month = may,
  year = {2015},
  keywords = {Electro-encephalography,Fusion,Inter-ictal epileptic discharges,Magneto-encephalography,Maximum
	entropy on the mean framework,Spatio-temporal propagation},
  pages = {1--28},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {afdidehf}
}

@article{Cintron-Arias2007,
  title = {Linear Inverse Problems- A MATLAB Tutorial},
  timestamp = {2016-07-09T19:57:19Z},
  author = {Cintron-Arias, Ariel},
  month = may,
  year = {2007},
  howpublished = {SAMSI/CRSC Undergraduate Workshop},
  owner = {Fardin}
}

@Article{Cohen2009,
  author    = {Cohen, A. and Dahmen, W. and Devore, R.},
  title     = {Compressed sensing and best k -term approximation},
  journal   = {Journal of The American Mathematical Society},
  year      = {2009},
  volume    = {22},
  number    = {1},
  pages     = {211--231},
  month     = jul,
  abstract  = {Compressed sensing is a new concept in signal processing where one
	seeks to minimize the number of measurements to be taken from signals
	while still retaining the information necessary to approximate them
	well. The ideas have their origins in certain abstract results from
	functional analysis and approximation theory by Kashin [23] but were
	recently brought into the forefront by the work of Cand`es, Romberg
	and Tao [7, 5, 6] and Donoho [9] who constructed concrete algorithms
	and showed their promise in application. There remain several fundamental
	questions on both the theoretical and practical side of compressed
	sensing. This paper is primarily concerned about one of these theoretical
	issues revolving around just how well compressed sensing can approximate
	a given signal from a given budget of fixed linear measurements,
	as compared to adaptive linear measurements. More precisely, we consider
	discrete signals x 2 IRN, allocate n < N linear measurements of x,
	and we describe the range of k for which these measurements encode
	enough information to recover x in the sense of `p to the accuracy
	of best k-term approximation. We also consider the problem of having
	such accuracy only with high probability.},
  doi       = {10.1090/S0894-0347-08-00610-3},
  keywords  = {best k-term approximation,compressed sensing,instance,instance optimality,mixed,norm estimates.,null space property,optimality in probability,restricted isometry property},
  masid     = {4476754},
  owner     = {afdidehf},
  timestamp = {2016-09-30T11:06:58Z},
}

@incollection{Coifman1995,
  series = {Lecture Notes in Statistics},
  title = {Translation-Invariant De-Noising},
  volume = {103},
  isbn = {978-0-387-94564-4},
  language = {English},
  timestamp = {2016-07-11T17:09:12Z},
  booktitle = {Wavelets and Statistics},
  publisher = {{\{Springer New York\}}},
  author = {Coifman, R.R. and Donoho, D.L.},
  editor = {Antoniadis, Anestis and Oppenheim, Georges},
  year = {1995},
  pages = {125-150},
  owner = {afdidehf}
}

@article{Cooke,
  title = {Constrained Optimization},
  timestamp = {2016-07-08T12:03:09Z},
  author = {Cooke, Dudley},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Trinity College Dublin},
  owner = {Fardin}
}

@article{Correa2010,
  title = {Canonical Correlation Analysis for Data Fusion and Group Inferences},
  volume = {27},
  issn = {1053-5888},
  doi = {10.1109/MSP.2010.936725},
  abstract = {We have presented two CCA-based approaches for data fusion and group
	analysis of biomedical imaging data and demonstrated their utility
	on fMRI, sMRI, and EEG data. The results show that CCA and M-CCA
	are powerful tools that naturally allow the analysis of multiple
	data sets. The data fusion and group analysis methods presented are
	completely data driven, and use simple linear mixing models to decompose
	the data into their latent components. Since CCA and M-CCA are based
	on second-order statistics they provide a relatively lessstrained
	solution as compared to methods based on higherorder statistics such
	as ICA. While this can be advantageous, the flexibility also tends
	to lead to solutions that are less sparse than those obtained using
	assumptions of non-Gaussianity-in particular superGaussianity-at
	times making the results more difficult to interpret. Thus, it is
	important to note that both approaches provide complementary perspectives,
	and hence it is beneficial to study the data using different analysis
	techniques.},
  timestamp = {2016-07-08T11:49:52Z},
  number = {4},
  journal = {Signal Processing Magazine, IEEE},
  author = {Correa, N.M. and Adali, T. and Li, Yi-Ou and Calhoun, V.D.},
  year = {2010},
  keywords = {biomedical imaging,canonical correlation analysis,correlation methods,data analysis,Data
	analysis,data fusion,electroencephalography,feature-based
	approach,group inference,image analysis,Independent component analysis,Magnetic
	analysis,Magnetic resonance imaging,matrix decomposition,medical
	imaging data,medical signal processing,Performance analysis,sensor
	fusion,Spatial resolution},
  pages = {39-50},
  owner = {Fardin}
}

@article{Correa2010a,
  title = {Multi-set canonical correlation analysis for the fusion of concurrent 	single trial ERP and functional MRI},
  volume = {50},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.01.062},
  abstract = {Functional magnetic resonance imaging (fMRI) data and electroencephalography
	(EEG) data provide complementary spatio-temporal information about
	brain function. Methods to couple the relative strengths of these
	modalities usually involve two stages: first forming a feature set
	from each dataset based on one criterion followed by exploration
	of connections among the features using a second criterion. We propose
	a data fusion method for simultaneously acquired fMRI and \{EEG\}
	data that combines these steps using a single criterion for finding
	the cross-modality associations and performing source separation.
	Using multi-set canonical correlation analysis (M-CCA), we obtain
	a decomposition of the two modalities, into spatial maps for fMRI
	data and a corresponding temporal evolution for \{EEG\} data, based
	on trial-to-trial covariation across the two modalities. Additionally,
	the analysis is performed on data from a group of subjects in order
	to make group inferences about the covariation across modalities.
	Being multivariate, the proposed method facilitates the study of
	brain connectivity along with localization of brain function. M-CCA
	can be easily extended to incorporate different data types and additional
	modalities. We demonstrate the promise of the proposed method in
	finding covarying trial-to-trial amplitude modulations (AMs) in an
	auditory task involving implicit pattern learning. The results show
	approximately linear decreasing trends in \{AMs\} for both modalities
	and the corresponding spatial activations occur mainly in motor,
	frontal, temporal, inferior parietal, and orbito-frontal areas that
	are linked both to sensory function as well as learning and expectation—all
	of which match activations related to the presented paradigm.},
  timestamp = {2016-07-10T06:47:47Z},
  number = {4},
  journal = {NeuroImage},
  author = {Correa, Nicolle M. and Eichele, Tom and Adal?, T{\"u}lay and Li, Yi-Ou and Calhoun, Vince D.},
  year = {2010},
  pages = {1438 - 1445},
  owner = {Fardin}
}

@inproceedings{Cotter2010,
  title = {Sparse Representation for accurate classification of corrupted and 	occluded facial expressions},
  doi = {10.1109/ICASSP.2010.5494903},
  abstract = {Facial expression recognition remains a challenging problem especially
	when the face is partially corrupted or occluded. We propose using
	a new classification method, termed Sparse Representation based Classification
	(SRC), to accurately recognize expressions under these conditions.
	A test vector is representable as a linear combination of vectors
	from its own class and so its representation as a linear combination
	of all available training vectors is sparse. Efficient methods have
	been developed in the area of compressed sensing to recover this
	sparse representation. SRC gives state of the art performance on
	clean and noise corrupted images matching the recognition rate obtained
	using Gabor based features. When test images are occluded by square
	black blocks, SRC improves significantly on the performance obtained
	using Gabor features; SRC increases the recognition rate by 6.6%
	when the block occlusion length is 30 and by 11.2% when the block
	length is 40.},
  timestamp = {2016-07-11T16:51:26Z},
  booktitle = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International 	Conference on},
  author = {Cotter, S.F.},
  month = mar,
  year = {2010},
  keywords = {classification,compressed sensing,corrupted facial expressions,Educational
	institutions,Emotion recognition,face recognition,Facial
	expression recognition,Facial expression
	recognition,feature extraction,Gabor based features,hidden
	feature removal,hidden feature
	removal,Humans,image classification,image matching,Image recognition,Image
	recognition,occluded facial expressions,occluded
	facial expressions,occlusion,Pixel,Principal component analysis,Principal
	component analysis,sparse representation,sparse
	representation,sparse representation based classification,sparse representation based
	classification,Testing,Vectors},
  pages = {838-841},
  owner = {afdidehf}
}

@article{Cotter2005,
  title = {Sparse solutions to linear inverse problems with multiple measurement 	vectors},
  volume = {53},
  issn = {1053-587X},
  doi = {10.1109/TSP.2005.849172},
  abstract = {We address the problem of finding sparse solutions to an underdetermined
	system of equations when there are multiple measurement vectors having
	the same, but unknown, sparsity structure. The single measurement
	sparse solution problem has been extensively studied in the past.
	Although known to be NP-hard, many single-measurement suboptimal
	algorithms have been formulated that have found utility in many different
	applications. Here, we consider in depth the extension of two classes
	of algorithms-Matching Pursuit (MP) and FOCal Underdetermined System
	Solver (FOCUSS)-to the multiple measurement case so that they may
	be used in applications such as neuromagnetic imaging, where multiple
	measurement vectors are available, and solutions with a common sparsity
	structure must be computed. Cost functions appropriate to the multiple
	measurement problem are developed, and algorithms are derived based
	on their minimization. A simulation study is conducted on a test-case
	dictionary to show how the utilization of more than one measurement
	vector improves the performance of the MP and FOCUSS classes of algorithm,
	and their performances are compared.},
  timestamp = {2016-09-30T11:19:42Z},
  number = {7},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Cotter, S.F. and Rao, B.D. and Engan, K. and Kreutz-Delgado, K.},
  month = jul,
  year = {2005},
  keywords = {algorithms-matching pursuit,Computational modeling,Cost function,Dictionaries,Equations,focal
	underdetermined system solver,Focusing,inverse problems,linear inverse
	problem,measurement vector,Minimization methods,neuromagnetic imaging,Pursuit
	algorithms,signal processing,suboptimal algorithm,Testing,Vectors},
  pages = {2477--2488},
  owner = {Fardin}
}

@article{Cplex,
  title = {Mathematical programming system},
  timestamp = {2016-07-09T20:07:35Z},
  author = {Cplex, Ilog},
  owner = {afdidehf}
}

@article{Cuffin1998,
  title = {EEG dipole source localization},
  volume = {17},
  issn = {0739-5175},
  doi = {10.1109/51.715495},
  abstract = {Considers using inverse solutions for determining source locations.
	Dipole source localization using EEGs recorded from the scalp is
	widely used to make estimates of the locations of sources of electrical
	activity in the brain. Such information can be very useful for both
	clinical and research applications. For example, in clinical applications,
	accurate information about the location of an epileptic focus (source)
	in the brain can be used to plan surgery for its removal. Likewise,
	information about the regions of the brain that process various signals
	(e.g., an auditory tone) can provide valuable research information
	about the functioning of the brain. It is necessary to assume a model
	of the source and a model of the head in order to estimate the location
	of a source. Once these models are selected, an inverse solution
	can be calculated for the location of the source in the head model.
	This source is then assumed to represent the actual source. The accuracy
	with which a source can be located by this method is affected by
	a number of factors including source-modeling errors, head-modeling
	errors, measurement-location errors, and EEG noise. This article
	presents a review of the research dealing with the effects of these
	factors on localization accuracy.},
  timestamp = {2016-07-08T12:16:33Z},
  number = {5},
  journal = {Engineering in Medicine and Biology Magazine, IEEE},
  author = {Cuffin, B.Neil},
  month = sep,
  year = {1998},
  keywords = {Active noise reduction,Anatomic,auditory tone,biomedical engineering,Brain,brain
	functioning,Brain modeling,brain models,brain regions,Conductivity,Displays,EEG
	dipole source localization,electrodiagnostics,electroencephalography,epileptic
	focus,head model,Humans,inverse problems,Magnetic heads,Magnetic
	heads,Models,Muscles,Neurological,research review,research
	review,reviews,Scalp,scalp-recorded EEGs,scalp-recorded
	EEGs,Skull,surgery planning},
  pages = {118-122},
  owner = {Fardin}
}

@article{Custo2014,
  title = {EEG source imaging of brain states using spatiotemporal regression},
  volume = {96},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.04.002},
  abstract = {Abstract Relating measures of electroencephalography (EEG) back to
	the underlying sources is a long-standing inverse problem. Here we
	propose a new method to estimate the \{EEG\} sources of identified
	electrophysiological states that represent spontaneous activity,
	or are evoked by a stimulus, or caused by disease or disorder. Our
	method has the unique advantage of seamlessly integrating a statistical
	significance of the source estimate while efficiently eliminating
	artifacts (e.g., due to eye blinks, eye movements, bad electrodes).
	After determining the electrophysiological states in terms of stable
	topographies using established methods (e.g.: ICA, PCA, k-means,
	epoch average), we propose to estimate these states' time courses
	through spatial regression of a General Linear Model (GLM). These
	time courses are then used to find \{EEG\} sources that have a similar
	time-course (using temporal regression of a second GLM). We validate
	our method using both simulated and experimental data. Simulated
	data allows us to assess the difference between source maps obtained
	by the proposed method and those obtained by applying conventional
	source imaging of the state topographies. Moreover, we use data from
	7 epileptic patients (9 distinct epileptic foci localized by intracranial
	EEG) and 2 healthy subjects performing an eyes-open/eyes-closed task
	to elicit activity in the alpha frequency range. Our results indicate
	that the proposed \{EEG\} source imaging method accurately localizes
	the sources for each of the electrical brain states. Furthermore,
	our method is particularly suited for estimating the sources of \{EEG\}
	resting states or otherwise weak spontaneous activity states, a problem
	not adequately solved before.},
  timestamp = {2016-07-08T12:18:05Z},
  journal = {NeuroImage},
  author = {Custo, Anna and Vulliemoz, Serge and Grouiller, Frederic and Ville, Dimitri Van De and Michel, Christoph},
  year = {2014},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\,\\\\\\\\(EEG\\\\\\\\,\\\\(EEG\\\\,imaging,Source},
  pages = {106 - 116},
  owner = {afdidehf}
}

@book{Daelemans2008,
  edition = {1},
  series = {Lecture Notes in Computer Science 5211 : Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2008, Antwerp, Belgium, September 15-19, 2008, Proceedings, 	Part I},
  isbn = {3-540-87478-X 978-3-540-87478-2},
  timestamp = {2016-07-09T20:00:17Z},
  publisher = {{\{Springer-Verlag Berlin Heidelberg\}}},
  author = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
  year = {2008},
  owner = {Fardin}
}

@article{Dale1993,
  title = {Improved Localizadon of Cortical Activity by Combining EEG and MEG 	with MRI Cortical Surface Reconstruction: A Linear Approach},
  volume = {5},
  issn = {0898-929X},
  doi = {10.1162/jocn.1993.5.2.162},
  abstract = {We describe a comprehensive linear approach to the problem of imaging
	brain activity with high temporal as well as spatial resolution based
	on combining EEG and MEG data with anatomical constraints derived
	from MRI images. The "inverse problem" of estimating the distribution
	of dipole strengths over the cortical surface is highly underdetermined,
	even given closely spaced EEG and MEG recordings. We have obtained
	much better solutions to this problem by explicitly incorporating
	both local cortical orientation as well as spatial covariance of
	sources and sensors into our formulation. An explicit polygonal model
	of the cortical manifold is first constructed as follows: (1) slice
	data in three orthogonal planes of section (needle-shaped voxels)
	are combined with a linear deblurring technique to make a single
	high-resolution 3-D image (cubic voxels), (2) the image is recursively
	flood-filled to determine the topology of the gray-white matter border,
	and (3) the resulting continuous surface is refined by relaxing it
	against the original 3-D gray-scale image using a deformable template
	method, which is also used to computationally flatten the cortex
	for easier viewing. The explicit solution to an error minimization
	formulation of an optimal inverse linear operator (for a particular
	cortical manifold, sensor placement, noise and prior source covariance)
	gives rise to a compact expression that is practically computable
	for hundreds of sensors and thousands of sources. The inverse solution
	can then be weighted for a particular (averaged) event using the
	sensor covariance for that event. Model studies suggest that we may
	be able to localize multiple cortical sources with spatial resolution
	as good as PET with this technique, while retaining a much finer
	grained picture of activity over time.},
  timestamp = {2016-10-21T13:26:15Z},
  number = {2},
  journal = {Cognitive Neuroscience, Journal of},
  author = {Dale, A and Sereno, M},
  month = apr,
  year = {1993},
  pages = {162--176},
  owner = {Fardin}
}

@inproceedings{DallAnese2011,
  title = {Group sparse total least-squares for cognitive spectrum sensing},
  doi = {10.1109/SPAWC.2011.5990487},
  abstract = {The present paper develops a collaborative scheme whereby cognitive
	radios cooperate to localize active primary transmitters and reconstruct
	the power spectral density (PSD) maps (one per frequency band) portraying
	the power distribution across space. The sensing scheme relies on
	a parsimonious linear system model that accounts for the narrow-band
	nature of transmit-PSDs compared to the large swath of sensed frequencies,
	and for the group sparsity emerging when adopting a spatial grid
	of candidate primary user locations. Combining the merits of Lasso,
	group Lasso, and total least-squares (TLS), the proposed group sparse
	(GS) TLS approach yields hierarchically-sparse PSD estimates, and
	copes with model uncertainty induced by channel randomness and grid
	mismatch effects. Taking advantage of a novel low-complexity solver
	for the GS-Lasso, a block coordinate descent scheme is developed
	to solve the formulated GS-TLS problem. Simulations demonstrate the
	superior localization and PSD-estimation performance of GS-TLS compared
	to approaches that do not account for model uncertainties.},
  timestamp = {2016-07-08T12:40:41Z},
  booktitle = {Signal Processing Advances in Wireless Communications (SPAWC), 2011 	IEEE 12th International Workshop on},
  author = {Dall'Anese, E. and Bazerque, J. and Zhu, Hao and Giannakis, G.B.},
  month = jun,
  year = {2011},
  keywords = {block coordinate descent scheme,channel estimation,cognitive radio,cognitive
	radios,cognitive spectrum sensing,Convergence,Fading,grid mismatch
	effects,group sparse total least-squares,least squares approximations,parsimonious
	linear system model,power distribution,power spectral density,PSD,Sensors,Shadow
	mapping,sparse matrices,TLS approach,Uncertainty},
  pages = {96-100},
  owner = {afdidehf}
}

@article{Daneshmand2015,
  title = {Hybrid Random/Deterministic Parallel Algorithms for Convex and Nonconvex 	Big Data Optimization},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2436357},
  abstract = {We propose a decomposition framework for the parallel optimization
	of the sum of a differentiable (possibly nonconvex) function and
	a nonsmooth (possibly nonseparable), convex one. The latter term
	is usually employed to enforce structure in the solution, typically
	sparsity. The main contribution of this work is a novel parallel,
	hybrid random/deterministic decomposition scheme wherein, at each
	iteration, a subset of (block) variables is updated at the same time
	by minimizing a convex surrogate of the original nonconvex function.
	To tackle huge-scale problems, the (block) variables to be updated
	are chosen according to a mixed random and deterministic procedure,
	which captures the advantages of both pure deterministic and random
	update-based schemes. Almost sure convergence of the proposed scheme
	is established. Numerical results show that on huge-scale problems
	the proposed hybrid random/deterministic algorithm compares favorably
	to random and deterministic schemes on both convex and nonconvex
	problems.},
  timestamp = {2016-07-08T12:44:42Z},
  number = {15},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Daneshmand, A. and Facchinei, F. and Kungurtsev, V. and Scutari, G.},
  month = aug,
  year = {2015},
  keywords = {Big data,block variables,concave programming,Convergence,convergence
	of numerical methods,convex big data optimization,convex programming,deterministic
	algorithms,deterministic parallel algorithms,deterministic procedure,hybrid
	random algorithms,Image Processing,Indexes,Jacobi method,mixed random
	procedure,nonconvex big data optimization,nonconvex problems,Optimization,Parallel
	algorithms,parallel and distributed methods,pure deterministic schemes,random
	selections,random update-based schemes,Signal processing algorithms,sparse
	solution},
  pages = {3914-3929},
  owner = {afdidehf}
}

@article{Daniel1998,
  title = {Breast disease: dynamic spiral MR imaging},
  volume = {209},
  doi = {10.1148/radiology.209.2.9807580},
  abstract = {To compare various subjective, empiric, and pharmacokinetic methods
	for interpreting findings at dynamic magnetic resonance (MR) imaging
	of the breast. MATERIALS AND METHODS: Dynamic spiral breast MR imaging
	was performed in 52 women suspected of having or with known breast
	disease. Gadolinium-enhanced images were obtained at 12 locations
	through the whole breast every 7.8 seconds for 8.5 minutes after
	bolus injection of contrast material. Time-signal intensity curves
	from regions of interest corresponding to 57 pathologically proved
	lesions were analyzed by means of a two-compartment pharmacokinetic
	model, and the diagnostic performance of various parameters was analyzed.
	RESULTS: Findings included invasive carcinoma in 17 patients, isolated
	ductal carcinoma in situ (DCIS) in six, and benign lesions in 34.
	Although some overlap between carcinomas and benign diagnoses was
	noted for all parameters, receiver operating characteristic analysis
	indicated that the exchange rate constant had the greatest overall
	ability to discriminate benign and malignant disease. The elimination
	rate constant and washout were the most specific parameters. The
	exchange rate constant, wash-in, and extrapolation point were the
	most sensitive parameters. DCIS was not consistently distinguished
	from benign disease with any method. CONCLUSION: Dynamic spiral breast
	MR imaging proved an excellent method with which to collect contrast
	enhancement data rapidly enough that accurate comparisons can be
	made between many analytic methods.},
  timestamp = {2016-09-29T14:46:10Z},
  number = {2},
  journal = {Radiology},
  author = {Daniel, B. L. and Yen, Y. F. and Glover, G. H. and Ikeda, D. M. and Birdwell, R. L. and Sawyer-Glover, A. M. and Black, J. W. and Plevritis, S. K. and Jeffrey, S. S. and Herfkens, R. J.},
  year = {1998},
  pages = {499--509},
  annote = {http://dx.doi.org/10.1148/radiology.209.2.9807580},
  annote = {http://dx.doi.org/10.1148/radiology.209.2.9807580},
  annote = {http://dx.doi.org/10.1148/radiology.209.2.9807580},
  annote = {http://dx.doi.org/10.1148/radiology.209.2.9807580},
  annote = {http://dx.doi.org/10.1148/radiology.209.2.9807580},
  owner = {afdidehf},
  pmid = {9807580}
}

@article{Darvas2004,
  title = {Mapping human brain function with MEG and EEG: methods and validation},
  volume = {23, Supplement 1},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2004.07.014},
  abstract = {We survey the field of magnetoencephalography (MEG) and electroencephalography
	(EEG) source estimation. These modalities offer the potential for
	functional brain mapping with temporal resolution in the millisecond
	range. However, the limited number of spatial measurements and the
	ill-posedness of the inverse problem present significant limits to
	our ability to produce accurate spatial maps from these data without
	imposing major restrictions on the form of the inverse solution.
	Here we describe approaches to solving the forward problem of computing
	the mapping from putative inverse solutions into the data space.
	We then describe the inverse problem in terms of low dimensional
	solutions, based on the equivalent current dipole (ECD), and high
	dimensional solutions, in which images of neural activation are constrained
	to the cerebral cortex. We also address the issue of objective assessment
	of the relative performance of inverse procedures by the free-response
	receiver operating characteristic (FROC) curve. We conclude with
	a discussion of methods for assessing statistical significance of
	experimental results through use of the bootstrap for determining
	confidence regions in dipole-fitting methods, and random field (RF)
	and permutation methods for detecting significant activation in cortically
	constrained imaging studies.},
  timestamp = {2016-07-09T20:07:20Z},
  number = {0},
  journal = {NeuroImage},
  author = {Darvas, F. and Pantazis, D. and Kucukaltun-Yildirim, E. and Leahy, R. M.},
  year = {2004},
  note = {Mathematics in Brain Imaging},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(MEG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(MEG\\\\\\\\\\\\\\\\,\\\\\\\\(MEG\\\\\\\\,\\\\(MEG\\\\,Dipole fitting,EEG,imaging methods,Inverse methods,Inverse
	methods,ROC analysis,ROC
	analysis,Statistical parametric tests,Statistical
	parametric tests},
  pages = {S289 - S299},
  owner = {afdidehf}
}

@book{Daubechies1992,
  title = {Ten Lectures on Wavelets},
  abstract = {Wavelets are a mathematical development that may revolutionize the
	world of information storage and retrieval according to many experts.
	They are a fairly simple mathematical tool now being applied to the
	compression of data such as fingerprints, weather satellite photographs,
	and medical x-rays that were previously thought to be impossible
	to condense without losing crucial details. This monograph contains
	10 lectures presented by Dr. Daubechies as the principal speaker
	at the 1990 CBMS-NSF Conference on Wavelets and Applications. The
	author has worked on several aspects of the wavelet transform and
	has developed a collection of wavelets that are remarkably efficient.
	The opening chapter provides an overview of the main problems presented
	in the book. Following chapters discuss the theoretical and practical
	aspects of wavelet theory, including wavelet transforms, orthonormal
	bases of wavelets, and characterization of functional spaces by means
	of wavelets. The last chapter presents several topics under active
	research, as multidimensional wavelets, wavelet packet bases, and
	a construction of wavelets tailored to decompose functions defined
	in a finite interval. Because of their interdisciplinary origins,
	wavelets appeal to scientists and engineers of many different backgrounds},
  timestamp = {2017-06-23T13:10:40Z},
  author = {Daubechies, Ingrid},
  year = {1992},
  keywords = {bandlimited functions,Fourier series,Fourier transform,frames,orthonormal
	bases,Shannon's theorem,time-frequency functions,wavelets},
  owner = {afdidehf}
}

@article{Daubechies2008a,
  title = {Accelerated Projected Gradient Method for Linear Inverse Problems 	with Sparsity Constraints},
  volume = {14},
  issn = {1069-5869},
  doi = {10.1007/s00041-008-9039-8},
  abstract = {Regularization of ill-posed linear inverse problems via ?1 pe- nalization
	has been proposed for cases where the solution is known to be (al-
	most) sparse. One way to obtain the minimizer of such an ?1 penalized
	func- tional is via an iterative soft-thresholding algorithm. We
	propose an alternative implementation to ?1-constraints, using a
	gradient method, with projection on ?1-balls. The corresponding algorithm
	uses again iterative soft-thresholding, now with a variable thresholding
	parameter. We also propose accelerated ver- sions of this iterative
	method, using ingredients of the (linear) steepest descent method.
	We prove convergence in norm for one of these projected gradient
	methods, without and with acceleration.},
  language = {English},
  timestamp = {2016-07-08T10:07:38Z},
  number = {5-6},
  journal = {Journal of Fourier Analysis and Applications},
  author = {Daubechies, Ingrid and Fornasier, Massimo and Loris, Ignace},
  year = {2008},
  keywords = {15A29,49M30,52A41,65F22,65K10,90C25,Linear inverse problems,Projected
	gradient method,sparse recovery},
  pages = {764-792},
  owner = {Fardin}
}

@article{Daubechies1991,
  title = {A Simple Wilson Orthonormal Basis with Exponential Decay},
  volume = {22},
  doi = {10.1137/0522035},
  abstract = {Following a basic idea of Wilson [ Generalized Wannier functions, 
	preprint] orthonormal bases for $L^2 (\mathbb{R})$
 which are a variation
	on the Gabor scheme are constructed. More precisely, $\phi \in L^2
	(\mathbb{R})$ is constructed such that the $\psi _{ln } $, $l \in
	\mathbb{N}$, $n \in \mathbb{Z}$, defined by \[ \begin{gathered} \psi
	_{0n} (x) = \phi \left( {x - n} \right) \hfill \\ \psi _{in} (x)
	= \sqrt 2 \phi \left( {x - \frac{n} {2}} \right)\cos (2\pi lx)\,{\text{if}}\,l
	\ne 0,\,l + n \in 2\mathbb{Z} \hfill \\ = \sqrt 2 \phi \left( {x
	- \frac{n} {2}} \right)\sin (2\pi lx)\,{\text{if}}\,l \ne 0,\,l +
	n \in 2\mathbb{Z} + 1, \hfill \\ \end{gathered} \] constitute an
	orthonormal basis. Explicit examples are given in which both $\phi
	$ and its Fourier transform $\hat \phi $ have exponential decay.
	In the examples $\phi $ is constructed as an infinite superposition
	of modulated Gaussians, with coefficients that decrease exponentially
	fast. It is believed that such orthonormal bases could be useful
	in many contexts where lattices of modulated Gaussian functions are
	now used. Read More: http://epubs.siam.org/doi/abs/10.1137/0522035?journalCode=sjmaah},
  timestamp = {2016-09-29T16:21:13Z},
  number = {2},
  journal = {SIAM J. Math. Anal.},
  author = {Daubechies, Ingrid and Jaffard, St\'{e}phane and Journ\'{e}, Jean-Lin},
  year = {1991},
  keywords = {orthonormal bases,phase space localization,time-frequency analysis},
  pages = {554--573},
  owner = {afdidehf}
}

@article{Daunizeau2009,
  title = {EEG-MEG source reconstruction},
  timestamp = {2016-07-08T12:17:46Z},
  author = {Daunizeau, Jean},
  year = {2009},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@article{David2003,
  title = {A neural mass model for MEG/EEG: coupling and neuronal dynamics},
  volume = {20},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2003.07.015},
  abstract = {Although MEG/EEG signals are highly variable, systematic changes in
	distinct frequency bands are commonly encountered. These frequency-specific
	changes represent robust neural correlates of cognitive or perceptual
	processes (for example, alpha rhythms emerge on closing the eyes).
	However, their functional significance remains a matter of debate.
	Some of the mechanisms that generate these signals are known at the
	cellular level and rest on a balance of excitatory and inhibitory
	interactions within and between populations of neurons. The kinetics
	of the ensuing population dynamics determine the frequency of oscillations.
	In this work we extended the classical nonlinear lumped-parameter
	model of alpha rhythms, initially developed by Lopes da Silva and
	colleagues [Kybernetik 15 (1974) 27], to generate more complex dynamics.
	We show that the whole spectrum of MEG/EEG signals can be reproduced
	within the oscillatory regime of this model by simply changing the
	population kinetics. We used the model to examine the influence of
	coupling strength and propagation delay on the rhythms generated
	by coupled cortical areas. The main findings were that (1) coupling
	induces phase-locked activity, with a phase shift of 0 or π when
	the coupling is bidirectional, and (2) both coupling and propagation
	delay are critical determinants of the MEG/EEG spectrum. In forthcoming
	articles, we will use this model to (1) estimate how neuronal interactions
	are expressed in MEG/EEG oscillations and establish the construct
	validity of various indices of nonlinear coupling, and (2) generate
	event-related transients to derive physiologically informed basis
	functions for statistical modelling of average evoked responses.},
  timestamp = {2016-07-08T10:31:18Z},
  number = {3},
  journal = {NeuroImage},
  author = {David, Olivier and Friston, Karl J.},
  year = {2003},
  pages = {1743 - 1755},
  owner = {Fardin}
}

@article{Davis1997,
  title = {Adaptive greedy approximations},
  volume = {13},
  issn = {0176-4276},
  doi = {10.1007/BF02678430},
  abstract = {The problem of optimally approximating a function with a linear expansion
	over a redundant dictionary of waveforms is NP-hard. The greedy matching
	pursuit algorithm and its orthogonalized variant produce suboptimal
	function expansions by iteratively choosing dictionary waveforms
	that best match the function's structures. A matching pursuit provides
	a means of quickly computing compact, adaptive function approximations.
	Numerical experiments show that the approximation errors from matching
	pursuits initially decrease rapidly, but the asymptotic decay rate
	of the errors is slow. We explain this behavior by showing that matching
	pursuits are chaotic, ergodic maps. The statistical properties of
	the approximation errors of a pursuit can be obtained from the invariant
	measure of the pursuit. We characterize these measures using group
	symmetries of dictionaries and by constructing a stochastic differential
	equation model. We derive a notion of the coherence of a signal with
	respect to a dictionary from our characterization of the approximation
	errors of a pursuit. The dictionary elements selected during the
	initial iterations of a pursuit correspond to a function's coherent
	structures. The tail of the expansion, on the other hand, corresponds
	to a noise which is characterized by the invariant measure of the
	pursuit map. When using a suitable dictionary, the expansion of a
	function into its coherent structures yields a compact approximation.
	We demonstrate a denoising algorithm based on coherent function expansions.},
  language = {English},
  timestamp = {2016-09-30T10:44:14Z},
  number = {1},
  journal = {Constructive Approximation},
  author = {Davis, G. and Mallat, S. and Avellaneda, M.},
  year = {1997},
  keywords = {41A10,Adaptive approximations,denoising,Greedy algorithms,matching
	pursuit,overcomplete signal representation,Time frequency analysis},
  pages = {57--98},
  owner = {afdidehf}
}

@article{Davis1994,
  title = {Adaptive time-frequency decompositions},
  volume = {33},
  timestamp = {2016-07-08T10:12:21Z},
  number = {7},
  journal = {Optical-Engineering},
  author = {Davis, G. and Mallat, S. and Zhang, Z.},
  year = {1994},
  pages = {2183�91},
  owner = {Fardin}
}

@article{DeBrunner1997,
  title = {Lapped multiple bases algorithms for still image compression without 	blocking effect},
  volume = {6},
  issn = {1057-7149},
  doi = {10.1109/83.623194},
  abstract = {We describe a system for still image compression that uses several
	transform sets in a multiple bases realization algorithm. Our algorithms
	reduce the number of encoded transform coefficients 20% beyond DCT-only
	compression. We extend these algorithms to use several newly developed
	lapped orthogonal transform (LOT) bases, resulting in useful algorithms
	for low bit rate (high compression) operation without blocking effect},
  timestamp = {2016-09-29T16:28:27Z},
  number = {9},
  journal = {Image Processing, IEEE Transactions on},
  author = {DeBrunner, V.E. and Chen, Lixiang and Li, Hong-Jian},
  month = sep,
  year = {1997},
  keywords = {Bit rate,block transform coding,data compression,discrete cosine transforms,Discrete
	transforms,image coding,Image reconstruction,ISO standards,lapped
	multiple bases algorithms,lapped orthogonal transform,low bit rate,Pixel,signal
	representations,Standards development,still image compression,Transform coding,transform
	coding,transform coefficients,transforms},
  pages = {1316--1321},
  owner = {Fardin}
}

@article{Dehghan2015,
  title = {Restricted Isometry Property on Banded Block Toeplitz Matrices with 	Application to Multi-Channel Convolutive Source Separation},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2457391},
  abstract = {In compressive sensing (CS), the restricted isometry property (RIP)
	is an important condition on measurement matrices which guarantees
	the recovery of sparse signals with undersampled measurements. It
	has been proved in the prior works that both random (e.g., i.i.d.
	Gaussian, Bernoulli, \ldots ) and Toeplitz matrices satisfy the RIP
	with high probability. However, structured matrices, such as banded
	Toeplitz matrices have drawn more attention since their structures
	have the advantage of fast matrix multiplication which may decrease
	the computational complexity of recovery algorithms. In this paper,
	we show that banded block Toeplitz matrices satisfy the RIP condition
	with high probability. Banded block Toeplitz matrices can be used
	in the sparse multi-channel source separation. The banded block Toeplitz
	matrices decrease the computational complexity while they have fewer
	number of non-zero entries in comparison to the same dimensional
	banded Toeplitz matrices. Furthermore, our simulation results show
	that banded block Toeplitz matrices outperform banded Toeplitz matrices
	in signal estimation. The analytical RIP bound for banded block Toeplitz
	matrices is provided in this paper and the RIP bound of sparse Gaussian
	matrices is also obtained as an upper bound for banded block Toeplitz
	matrices. Our simulation and analytical results show that sparse
	Gaussian random matrices do satisfy the RIP condition with high probability.
	The probability of satisfying the RIP depends on the probability
	of zero entries.},
  timestamp = {2016-07-10T08:02:50Z},
  number = {21},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Dehghan, H. and Dansereau, R.M. and Chan, A.D.C.},
  month = nov,
  year = {2015},
  keywords = {Banded block Toeplitz matrices,compressed sensing,computational complexity,Electromyography,Matrix
	decomposition,restricted isometry property,source separation,sparse
	Gaussian random matrices,sparse matrices,Time-domain analysis},
  pages = {5665-5676},
  owner = {afdidehf}
}

@article{Demissie2014,
  title = {High-resolution range-Doppler processing by coherent block-sparse 	estimation},
  volume = {50},
  issn = {0018-9251},
  doi = {10.1109/TAES.2013.120455},
  abstract = {This work contemplates advanced signal processing techniques for narrow
	band pulsed radar systems. If we assume a sparse scene of point-like
	targets and formulate the data model for the received signal in Fourier
	space, we can produce a block sparse estimation problem for the range
	profile spectra in all relevant Doppler channels. The range profile
	spectra are jointly estimated by coherent versions of block matching
	pursuit and basis pursuit algorithms. Then, in each Doppler channel,
	we compute the delays using a parametric high-resolution method.
	The mixed sparse/parametric approach overcomes the disadvantages
	of matched filters concerning resolution and ambiguities and has
	less computational complexity than full two-dimensional sparse estimation
	methods. We show results from numerical simulations and experimental
	measurements recorded with a passive bistatic radar using Global
	System for Mobil Communications (GSM) base stations as illuminators
	of opportunity.},
  timestamp = {2016-07-08T12:43:34Z},
  number = {2},
  journal = {Aerospace and Electronic Systems, IEEE Transactions on},
  author = {Demissie, B. and Berger, C.R.},
  month = apr,
  year = {2014},
  keywords = {basis pursuit algorithms,block matching pursuit,coherent block-sparse
	estimation,compressed sensing,Data models,Dictionaries,Doppler channels,Doppler
	effect,Doppler radar,Estimation,Estimation theory,Fourier space,Global
	System for Mobil Communications,GSM base stations,high-resolution
	range-Doppler processing,iterative methods,matched filters,Matching
	pursuit algorithms,narrow band pulsed radar systems,parametric high-resolution
	method,passive bistatic radar,passive radar,point-like targets,profile
	spectra,Radar,radar resolution,signal processing techniques,time-frequency
	analysis,Vectors},
  pages = {843-857},
  owner = {afdidehf}
}

@article{DeVore1996,
  title = {Some remarks on greedy algorithms},
  volume = {5},
  issn = {1019-7168},
  doi = {10.1007/BF02124742},
  abstract = {Estimates are given for the rate of approximation of a function by
	means of greedy algorithms. The estimates apply to approximation
	from an arbitrary dictionary of functions. Three greedy algorithms
	are discussed: the Pure Greedy Algorithm, an Orthogonal Greedy Algorithm,
	and a Relaxed Greedy Algorithm.},
  language = {English},
  timestamp = {2016-07-10T08:22:27Z},
  number = {1},
  journal = {Advances in Computational Mathematics},
  author = {DeVore, R.A. and Temlyakov, V.N.},
  year = {1996},
  pages = {173-187},
  owner = {Fardin}
}

@inproceedings{Dhakal2011,
  title = {Sparse space codes for multi-antenna systems},
  doi = {10.1109/CWIT.2011.5872147},
  abstract = {Sparse space codes (SSC) are proposed as a novel transmission scheme
	for an under-determined MIMO channel. Each SSC codeword is a sparse
	vector of the size of the number of transmit antennas. The information
	is imparted through: (i) uncertainty in the positions of non-zero
	elements, and (ii) uncertainty in the symbol-space of non-zero elements.
	Basis-pursuit (BPD) and LASSO detectors are used with knowledge of
	code sparsity to detect SSC. However, their performance is found
	to be degraded compared to the Maximum Likelihood Detector (MLD).
	A runner-up basis pursuit algorithm is proposed that provides MLD-like
	performance with a small increment in complexity over BPD. Analytical
	and simulation results show that SSC outperforms orthogonal space
	time block codes in terms of word error rate at varying SNR ranges.},
  timestamp = {2016-07-11T16:54:27Z},
  booktitle = {Information Theory (CWIT), 2011 12th Canadian Workshop on},
  author = {Dhakal, S. and Bayesteh, A.},
  month = may,
  year = {2011},
  keywords = {Antenna arrays,Basis pursuit,basis-pursuit detector,Binary phase shift
	keying,codes,Complexity theory,compressive sampling,Detectors,error
	statistics,LASSO detectors,maximum likelihood detector,MIMO,MIMO
	channel,MIMO communication,multiantenna systems,nonzero elements,orthogonal
	space time block codes,Receiving antennas,runner-up basis pursuit
	algorithm,Signal to noise ratio,sparse space codes,symbol-space,transmission
	scheme,transmit antennas,Transmitting antennas,word error rate,word
	error rate},
  pages = {159-164},
  owner = {afdidehf}
}

@article{Dias2013,
  title = {Introduction to Inverse Problems},
  timestamp = {2016-07-08T12:50:35Z},
  author = {Dias, Jos� Bioucas},
  year = {2013},
  owner = {Fardin}
}

@inproceedings{Ding2008,
  title = {A novel sparse source imaging in reconstructing extended cortical 	current sources},
  doi = {10.1109/IEMBS.2008.4650226},
  abstract = {We have developed a new sparse source imaging (SSI) method with the
	use of the L1-norm in EEG inverse problems to reconstruct extended
	cortical current sources. The new SSI method explores the sparseness
	in cortical current density variation maps (the transform domain)
	other than in the cortical current density maps (the original domain)
	from previously reported SSI methods. The new SSI is assessed by
	a series of computer simulations. The performance of SSI is compared
	with the well-known L2-norm MNE using the AUC metric. Our present
	simulation data indicate that the new SSI has significantly improved
	performance in reconstructing extended cortical current sources and
	estimating their cortical extents. The L2-norm MNE shows relatively
	poor performance in the same source imaging tasks. The new SSI method
	is also applicable to MEG source imaging.},
  timestamp = {2016-07-08T11:25:42Z},
  booktitle = {Engineering in Medicine and Biology Society, 2008. EMBS 2008. 30th 	Annual International Conference of the IEEE},
  author = {Ding, Lei},
  month = aug,
  year = {2008},
  keywords = {Action Potentials,Algorithms,Area Under Curve,Brain,Brain Mapping,Brain
	modeling,Cerebral Cortex,Charge coupled devices,Computer-Assisted,Computer
	Simulation,Current density,Current distribution,Current measurement,EEG,electroencephalography,Electrophysiology,Humans,Image
	Processing,Image reconstruction,inverse problems,l1-norm,Magnetic
	Resonance Imaging,Neurons,ROC Curve,SCOP,Sensitivity and Specificity,sparseness
	regularization,sparse source imaging,Surface reconstruction,transform
	domain},
  pages = {4555-4558},
  owner = {afdidehf}
}

@inproceedings{Ding2012,
  title = {Sparse electromagnetic source imaging using EEG and MEG},
  doi = {10.1109/EMBC.2012.6347416},
  abstract = {The present study proposed the combined use of EEG and MEG data in
	a new sparse electromagnetic source imaging (ESI) technique, i.e.,
	variation-based sparse cortical current density (VB-SCCD) method.
	Monte Carlo simulations were conducted to investigate the performance
	of the proposed approach in multiple extended brain activations (up
	to ten) that were randomly generated. Experimental EEG and MEG data
	from a face recognition task were further used to evaluate the performance
	of VB-SCCD. The present results indicate that the proposed approach
	can accurately reconstruct multiple brain activations and their spatial
	extents. The source imaging results from real data further demonstrate
	it is capable to recover networked brain activations involving multiple
	cortical regions, which are consistent with results from functional
	magnetic resonance imaging in same task paradigm. The present results
	further indicate the capability of the proposed approach in reconstructing
	deep brain sources and temporal dynamics of brain sources at millisecond
	resolutions. It thus suggests that sparse ESI using combined EEG
	and MEG is a promising technique probing detailed spatiotemporal
	brain activations.},
  timestamp = {2016-10-21T13:49:07Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), Annual International 	Conference of the IEEE},
  author = {Ding, Lei and Yuan, Han},
  month = aug,
  year = {2012},
  keywords = {biomedical MRI,Brain,Brain modeling,Current density,deep brain source
	reconstruction,EEG,electroencephalography,face recognition,face
	recognition task,face recognition
	task,functional magnetic resonance imaging,Humans,Image reconstruction,Image
	reconstruction,Magnetic resonance imaging,magnetoencephalography,medical
	image processing,MEG,millisecond resolutions,Monte Carlo Method,Monte
	Carlo methods,Monte Carlo simulations,multiple brain activation reconstruction,multiple
	cortical regions,multiple extended brain activations,networked brain
	activations,source imaging results,sparse electromagnetic source
	imaging,spatiotemporal brain activations,spatiotemporal phenomena,task
	paradigm,temporal dynamics,variation-based sparse cortical current
	density},
  pages = {6224--6227},
  owner = {afdidehf}
}

@inproceedings{Ding2011,
  title = {Investigation of EEG and MEG source imaging accuracy in reconstructing 	extended cortical sources},
  doi = {10.1109/IEMBS.2011.6091773},
  abstract = {Electroencephalography (EEG) and magneto-encephalography (MEG) are
	both currently used to reconstruct brain activity. The performance
	of inverse source reconstructions is dependent on the modality of
	signals in use as well as inverse techniques. Here we used a recently
	proposed sparse source imaging technique, i.e., the variation-based
	sparse cortical current density (VB-SCCD) algorithm to compare the
	use of EEG or MEG data in reconstructing extended cortical sources.
	We conducted Monte Carlo simulations as comparison to two other widely
	used source imaging techniques. The VB-SCCD technique was further
	evaluated in experimental EEG and MEG data. Our present results indicate
	that EEG and MEG have similar reconstruction performance as indicated
	by a metric, the area under the receiver operating characteristic
	curve (AUC). Furthermore, EEG and MEG have different advantages and
	limitations in revealing different aspects of features of extended
	cortical sources, which are complimentary to each other. A simultaneous
	EEG and MEG analysis framework is thus promising to produce much
	improved source reconstructions.},
  timestamp = {2016-07-09T19:45:15Z},
  booktitle = {Engineering in Medicine and Biology Society, EMBC, 2011 Annual International 	Conference of the IEEE},
  author = {Ding, Lei and Yuan, Han},
  month = aug,
  year = {2011},
  keywords = {Accuracy,Algorithms,Area Under Curve,Brain,brain activity,Brain Mapping,Brain
	modeling,Cerebral Cortex,Charge coupled devices,Computer-Assisted,Computer
	Simulation,EEG,electroencephalography,extended cortical sources,feature extraction,Feature
	Extraction,Humans,Image Processing,Image reconstruction,Inverse
	problems,inverse source reconstructions,Magnetic resonance imaging,magnetoencephalography,Magneto-encephalography,medical
	signal processing,MEG,Models,Monte Carlo Method,Monte Carlo methods,Monte
	Carlo simulations,Normal Distribution,receiver operating characteristic
	curve,Reproducibility of Results,ROC Curve,sensitivity analysis,signal
	reconstruction,source imaging accuracy,sparse source imaging technique,Statistical,variation-based
	sparse cortical current density},
  pages = {7013-7016},
  owner = {afdidehf}
}

@inproceedings{Ding2013a,
  title = {Wavelet based sparse source imaging technique},
  doi = {10.1109/EMBC.2013.6610774},
  abstract = {The present study proposed a novel multi-resolution wavelet to efficiently
	compress cortical current densities on the highly convoluted cortical
	surface. The basis function of the proposed wavelet is supported
	on triangular faces of the cortical mesh and it is thus named as
	the face-based wavelet to be distinguished from other vertex-based
	wavelets. The proposed face-based wavelet was used as a transform
	to gain the sparse representation of cortical sources and then was
	integrated into the framework of L1-norm regularizations with the
	purpose to improve the performance of sparse source imaging (SSI)
	in solving EEG/MEG inverse problems. Monte Carlo simulations were
	conducted with multiple extended sources (up to ten) at random locations.
	Experimental MEG data from an auditory induced language task was
	further adopted to evaluate the performance of the proposed wavelet
	based SSI technique. The present results indicated that the face-based
	wavelet can efficiently compress cortical current densities and has
	better performance than the vertex-based wavelet in helping inverse
	source reconstructions in terms of estimation accuracies in source
	localization and source extent. Experimental results further indicated
	improved detection performance of the face-based wavelet as compared
	with the vertex-based wavelet in the framework of SSI. It thus suggests
	the proposed wavelet based SSI can become a promising tool in studying
	brain functions and networks.},
  timestamp = {2016-07-11T17:11:39Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual 	International Conference of the IEEE},
  author = {Ding, Lei and Zhu, Min and Liao, Ke},
  month = jul,
  year = {2013},
  keywords = {Accuracy,auditory induced language task,brain functions,Brain modeling,brain
	networks,Convolution,cortical current density compression,Current
	density,EEG-MEG inverse problems,electroencephalography,face-based
	wavelet,highly convoluted cortical surface,Image reconstruction,Inverse
	problems,inverse source reconstructions,inverse transforms,L1-norm
	regularization,magnetoencephalography,medical image processing,Monte
	Carlo methods,Monte Carlo simulations,multiple extended sources,multiresolution
	wavelet,random locations,sparse source imaging,Surface waves,triangular
	cortical mesh faces,vertex-based wavelet transform,Wavelet analysis,wavelet
	based sparse source imaging technique,wavelet transforms},
  pages = {5418-5421},
  owner = {afdidehf}
}

@article{Do2005,
  title = {The contourlet transform: an efficient directional multiresolution 	image representation},
  volume = {14},
  issn = {1057-7149},
  doi = {10.1109/TIP.2005.859376},
  abstract = {The limitations of commonly used separable extensions of one-dimensional
	transforms, such as the Fourier and wavelet transforms, in capturing
	the geometry of image edges are well known. In this paper, we pursue
	a "true" two-dimensional transform that can capture the intrinsic
	geometrical structure that is key in visual information. The main
	challenge in exploring geometry in images comes from the discrete
	nature of the data. Thus, unlike other approaches, such as curvelets,
	that first develop a transform in the continuous domain and then
	discretize for sampled data, our approach starts with a discrete-domain
	construction and then studies its convergence to an expansion in
	the continuous domain. Specifically, we construct a discrete-domain
	multiresolution and multidirection expansion using nonseparable filter
	banks, in much the same way that wavelets were derived from filter
	banks. This construction results in a flexible multiresolution, local,
	and directional image expansion using contour segments, and, thus,
	it is named the contourlet transform. The discrete contourlet transform
	has a fast iterated filter bank algorithm that requires an order
	N operations for N-pixel images. Furthermore, we establish a precise
	link between the developed filter bank and the associated continuous-domain
	contourlet expansion via a directional multiresolution analysis framework.
	We show that with parabolic scaling and sufficient directional vanishing
	moments, contourlets achieve the optimal approximation rate for piecewise
	smooth functions with discontinuities along twice continuously differentiable
	curves. Finally, we show some numerical experiments demonstrating
	the potential of contourlets in several image processing applications.},
  timestamp = {2016-07-11T17:00:26Z},
  number = {12},
  journal = {Image Processing, IEEE Transactions on},
  author = {Do, M.N. and Vetterli, M.},
  month = dec,
  year = {2005},
  keywords = {Algorithms,Artificial Intelligence,channel bank filters,Computer-Assisted,Computer
	Graphics,Contourlets,contourlet transform,contours,contour segments,Convergence,Curvelets,directional
	multiresolution analysis,discrete-domain construction,discrete-domain
	multiresolution,Discrete Fourier transforms,Discrete transforms,Discrete
	wavelet transforms,Filter bank,filter banks,Fourier transform,Fourier
	transforms,geometric image processing,Geometry,image edges,Image
	Enhancement,image expansion,Image Interpretation,Image reconstruction,image representation,image
	representation,Image resolution,Information
	Storage and Retrieval,multidirection,multidirection expansion,multiresolution,multiresolution
	image representation,nonseparable filter banks,numerical analysis,signal
	processing,sparse representation,wavelets,wavelet transform,wavelet
	transforms},
  pages = {2091-2106},
  owner = {afdidehf}
}

@article{Do2003,
  title = {The Finite Ridgelet Transform for Image Representation},
  volume = {12},
  issn = {1057-7149},
  doi = {10.1109/TIP.2002.806252},
  abstract = {The ridgelet transform was introduced as a sparse expansion for functions
	on continuous spaces that are smooth away from discontinuities along
	lines. We propose an orthonormal version of the ridgelet transform
	for discrete and finite-size images. Our construction uses the finite
	Radon transform (FRAT) as a building block. To overcome the periodization
	effect of a finite transform, we introduce a novel ordering of the
	FRAT coefficients. We also analyze the FRAT as a frame operator and
	derive the exact frame bounds. The resulting finite ridgelet transform
	(FRIT) is invertible, nonredundant and computed via fast algorithms.
	Furthermore, this construction leads to a family of directional and
	orthonormal bases for images. Numerical results show that the FRIT
	is more effective than the wavelet transform in approximating and
	denoising images with straight edges.},
  timestamp = {2016-09-30T13:54:35Z},
  number = {1},
  journal = {Image Processing, IEEE Transactions on},
  author = {Do, M.N. and Vetterli, M.},
  month = jan,
  year = {2003},
  keywords = {continuous spaces,Continuous wavelet transforms,Digital images,Discrete
	cosine transforms,discrete images,Discrete transforms,Discrete wavelet
	transforms,exact frame bounds,fast algorithms,finite Radon transform,finite
	ridgelet transform,finite-size images,Frame operator,FRAT coefficients
	ordering,image denoising,Image Processing,image representation,images
	denoising,invertible transform,Laboratories,Noise reduction,nonredundant
	transform,Radon transforms,sparse function expansion,straight edges,Wavelet
	domain,wavelet transform},
  pages = {16--28},
  owner = {afdidehf}
}

@inproceedings{Dong2014,
  title = {A novel distributed compressive video sensing based on hybrid sparse 	basis},
  doi = {10.1109/VCIP.2014.7051569},
  abstract = {Distributed compressive video sensing (DCVS) is a new emerging video
	codec that incorporates advantages of distributed video coding (DVC)
	and compressive sensing (CS). However, due to the absence of a good
	sparse basis, the DCVS does not achieve ideal compressing efficiency
	compared with the traditional video codec, such as MPEG-4, H.264,
	etc. This paper proposes a new hybrid sparse basis, which combines
	the image-block prediction and DCT basis. Adaptive block-based prediction
	is employed to learn block-prediction basis by exploiting temporal
	correlation among successive frames. Based on linear DCT basis and
	predicted basis, the hybrid sparse basis can achieve sparser representation
	with lower complexity. The experiment results indicate that the proposal
	outperforms the state-of-the-art DCVS schemes on both visual quality
	and average PSNR. In addition, an iterative fashion proposed in the
	decoder can enhance the sparsity of the hybrid sparse basis and improve
	the rate-distortion performance significantly.},
  timestamp = {2016-07-08T11:25:38Z},
  booktitle = {Visual Communications and Image Processing Conference, 2014 IEEE},
  author = {Dong, Haifeng and Zhuang, Bojin and Su, Fei and Zhao, Zhicheng},
  month = dec,
  year = {2014},
  keywords = {adaptive block-based prediction,block-prediction,compressed sensing,Compressive
	sensing,DCT basis,Dictionaries,discrete cosine transforms,distributed
	compressed video sensing,distributed compressive video sensing,distributed
	video coding,encoding,H.264,image-block prediction,Image reconstruction,MPEG-4,Proposals,Sensors,Silicon,sparse
	basis,temporal correlation,video codec,video codecs},
  pages = {320-323},
  owner = {afdidehf}
}

@techreport{Dong2006,
  title = {Methods for Constrained Optimization},
  abstract = {This discussion focuses on the constrained optimization problem and
	looks into different methods for solving it. Constrained optimization
	is approached somewhat differently from unconstrained optimization
	because the goal is not to find the global optima. Often, constrained
	optimization methods use unconstrained optimization as a sub-step.
	In this paper, I first set up the constrained optimization problem,
	introduce several optimization methods, and apply it to a toy problem.
	Then I introduce the real-world application of data classification,
	which utilizes constrained optimization. I apply the applicable methods
	to a 2-D classification problem before demonstrating the full capability
	on handwritten numeral identification using real data.},
  timestamp = {2016-07-09T20:11:44Z},
  author = {Dong, Shuonan},
  year = {2006},
  owner = {Fardin}
}

@article{Donoho2012,
  title = {Sparsity in Modern High-Dimensional Statistics},
  timestamp = {2016-07-11T16:55:09Z},
  author = {Donoho, David},
  month = sep,
  year = {2012},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {SAMSI Astrostatistics},
  owner = {Fardin}
}

@article{Donoho2006,
  title = {Compressed sensing},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2006.871582},
  abstract = {Suppose x is an unknown vector in Ropfm (a digital image or signal);
	we plan to measure n general linear functionals of x and then reconstruct.
	If x is known to be compressible by transform coding with a known
	transform, and we reconstruct via the nonlinear procedure defined
	here, the number of measurements n can be dramatically smaller than
	the size m. Thus, certain natural classes of images with m pixels
	need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful
	recovery, as opposed to the usual m pixel samples. More specifically,
	suppose x has a sparse representation in some orthonormal basis (e.g.,
	wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients
	belong to an lscrp ball for 0<ples1. The N most important coefficients
	in that expansion allow reconstruction with lscr2 error O(N1/2-1p/).
	It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing
	reconstruction with accuracy comparable to that attainable with direct
	knowledge of the N most important coefficients. Moreover, a good
	approximation to those N important coefficients is extracted from
	the n measurements by solving a linear program-Basis Pursuit in signal
	processing. The nonadaptive measurements have the character of "random"
	linear combinations of basis/frame elements. Our results use the
	notions of optimal recovery, of n-widths, and information-based complexity.
	We estimate the Gel'fand n-widths of lscrp balls in high-dimensional
	Euclidean space in the case 0<ples1, and give a criterion identifying
	near- optimal subspaces for Gel'fand n-widths. We show that "most"
	subspaces are near-optimal, and show that convex optimization (Basis
	Pursuit) is a near-optimal way to extract information derived from
	these near-optimal subspaces},
  timestamp = {2017-06-23T13:26:18Z},
  number = {4},
  journal = {IEEE Trans. Inf. Theory},
  author = {Donoho, D.L.},
  month = apr,
  year = {2006},
  keywords = {Adaptive sampling,almost-spherical sections of Banach spaces,Basis
	Pursuit,compressed sensing,convex optimization,convex programming,Data
	Compression,Data mining,Digital images,Eigenvalues of random matrices,Euclidean
	space,Gel'fand,general linear functional measurement,image coding,Image reconstruction,Image
	reconstruction,image sampling,image sensors,information-based
	complexity,integrated sensing and processing,minimum,nonadaptive
	nonpixel sampling,optimal recovery,Pixel,Quotient-of-a-Subspace theorem,sensing
	compression,signal processing,Size measurement,sparse matrices,sparse
	representation,sparse solution of linear equations,Transform coding,Vectors},
  pages = {1289--1306},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  owner = {afdidehf}
}

@article{Donoho2013,
  title = {Accurate Prediction of Phase Transitions in Compressed Sensing via 	a Connection to Minimax Denoising},
  volume = {59},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2239356},
  abstract = {Compressed sensing posits that, within limits, one can undersample
	a sparse signal and yet reconstruct it accurately. Knowing the precise
	limits to such undersampling is important both for theory and practice.
	We present a formula that characterizes the allowed undersampling
	of generalized sparse objects. The formula applies to approximate
	message passing (AMP) algorithms for compressed sensing, which are
	here generalized to employ denoising operators besides the traditional
	scalar soft thresholding denoiser. This paper gives several examples
	including scalar denoisers not derived from convex penalization-the
	firm shrinkage nonlinearity and the minimax nonlinearity-and also
	nonscalar denoisers-block thresholding, monotone regression, and
	total variation minimization. Let the variables ? = k/N and ? = n/N
	denote the generalized sparsity and undersampling fractions for sampling
	the k-generalized-sparse N-vector x0 according to y=Ax0. Here, A
	is an n�N measurement matrix whose entries are iid standard Gaussian.
	The formula states that the phase transition curve ? = ?(?) separating
	successful from unsuccessful reconstruction of x0 by AMP is given
	by ? = M(?|Denoiser) where M(?|Denoiser) denotes the per-coordinate
	minimax mean squared error (MSE) of the specified, optimally tuned
	denoiser in the directly observed problem y = x + z. In short, the
	phase transition of a noiseless undersampling problem is identical
	to the minimax MSE in a denoising problem. We prove that this formula
	follows from state evolution and present numerical results validating
	it in a wide range of settings. The above formula generates numerous
	new insights, both in the scalar and in the nonscalar cases.},
  timestamp = {2016-07-08T10:07:58Z},
  number = {6},
  journal = {Information Theory, IEEE Transactions on},
  author = {Donoho, D.L. and Johnstone, I. and Montanari, A.},
  month = jun,
  year = {2013},
  keywords = {AMP algorithm,approximate message passing,Approximate message passing
	(AMP),Approximation algorithms,block thresholding,compressed sensing,compressed
	sensing,firm shrinkage nonlinearity,Gaussian processes,generalized
	sparse N vector,generalized sparsity,group lasso,iid standard Gaussian,James�Stein,joint
	sparsity,Lasso,least mean squares methods,matrix algebra,measurement
	matrix,message passing,minimax denoising,minimax mean squared error,minimax
	nonlinearity,minimax risk of firm thresholding,minimax risk of soft
	thresholding,minimax risk over nearly black objects,minimax shrinkage,minimisation,Minimization,MMSE,monotone regression,monotone
	regression,noiseless undersampling problem,Noise
	reduction,nonconvex penalization,nonscalar denoiser,optimally tuned
	denoiser,Partitioning algorithms,phase transition curve,phase transition
	prediction,regression analysis,scalar soft thresholding denoiser,signal
	denoising,signal reconstruction,signal sampling,sparse signal reconstruction,state
	evolution,total variation minimization,undersampling fraction,undersampling
	fraction,Vectors},
  pages = {3396-3433},
  owner = {afdidehf}
}

@article{Donoho2008,
  title = {Fast Solution of $\ell_1$-Norm Minimization Problems When the Solution 	May Be Sparse},
  volume = {54},
  issn = {0018-9448},
  doi = {10.1109/TIT.2008.929958},
  abstract = {The minimum lscr1-norm solution to an underdetermined system of linear
	equations y=Ax is often, remarkably, also the sparsest solution to
	that system. This sparsity-seeking property is of interest in signal
	processing and information transmission. However, general-purpose
	optimizers are much too slow for lscr1 minimization in many large-scale
	applications.In this paper, the Homotopy method, originally proposed
	by Osborne et al. and Efron et al., is applied to the underdetermined
	lscr1-minimization problem min parxpar1 subject to y=Ax. Homotopy
	is shown to run much more rapidly than general-purpose LP solvers
	when sufficient sparsity is present. Indeed, the method often has
	the following k-step solution property: if the underlying solution
	has only k nonzeros, the Homotopy method reaches that solution in
	only k iterative steps. This k-step solution property is demonstrated
	for several ensembles of matrices, including incoherent matrices,
	uniform spherical matrices, and partial orthogonal matrices. These
	results imply that Homotopy may be used to rapidly decode error-correcting
	codes in a stylized communication system with a computational budget
	constraint. The approach also sheds light on the evident parallelism
	in results on lscr1 minimization and orthogonal matching pursuit
	(OMP), and aids in explaining the inherent relations between Homotopy,
	least angle regression (LARS), OMP, and polytope faces pursuit.},
  timestamp = {2016-07-08T12:29:17Z},
  number = {11},
  journal = {Information Theory, IEEE Transactions on},
  author = {Donoho, D.L. and Tsaig, Y.},
  month = nov,
  year = {2008},
  keywords = {$ell _{1}$ minimization,Basis pursuit,communication system,Equations,error-correcting
	codes,error correction codes,Homotopy method,Homotopy methods,incoherent
	matrices,information transmission,Iterative decoding,iterative method,iterative
	methods,Large-scale systems,Lasso,least angle regression,Least Angle
	Regression (Lars),least squares approximations,linear equation,lscr1-norm
	minimization problem,Matching pursuit algorithms,matrix algebra,minimisation,Minimization
	methods,Optimization methods,orthogonal matching pursuit,Parallel
	processing,partial orthogonal matrices,polytope faces pursuit,regression
	analysis,signal processing,sparse representations,sparsity-seeking
	property,underdetermined system,Underdetermined systems of linear
	equations,uniform spherical matrices},
  pages = {4789-4812},
  owner = {afdidehf}
}

@article{Donoho2012a,
  title = {Sparse Solution of Underdetermined Systems of Linear Equations by 	Stagewise Orthogonal Matching Pursuit},
  volume = {58},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2173241},
  abstract = {Finding the sparsest solution to underdetermined systems of linear
	equations y = ?x is NP-hard in general. We show here that for systems
	with �typical�/�random� ?, a good approximation to the sparsest
	solution is obtained by applying a fixed number of standard operations
	from linear algebra. Our proposal, Stagewise Orthogonal Matching
	Pursuit (StOMP), successively transforms the signal into a negligible
	residual. Starting with initial residual r0 = y, at the s -th stage
	it forms the �matched filter� ?Trs-1, identifies all coordinates
	with amplitudes exceeding a specially chosen threshold, solves a
	least-squares problem using the selected coordinates, and subtracts
	the least-squares fit, producing a new residual. After a fixed number
	of stages (e.g., 10), it stops. In contrast to Orthogonal Matching
	Pursuit (OMP), many coefficients can enter the model at each stage
	in StOMP while only one enters per stage in OMP; and StOMP takes
	a fixed number of stages (e.g., 10), while OMP can take many (e.g.,
	n). We give both theoretical and empirical support for the large-system
	effectiveness of StOMP. We give numerical examples showing that StOMP
	rapidly and reliably finds sparse solutions in compressed sensing,
	decoding of error-correcting codes, and overcomplete representation.},
  timestamp = {2016-07-11T16:54:22Z},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Donoho, D.L. and Tsaig, Y. and Drori, I. and Starck, J.-L.},
  month = feb,
  year = {2012},
  keywords = {$ell _{1}$ minimization,Approximation methods,compressed sensing,Computational
	complexity,Decoding,decoding error-correcting codes,Equations,Error
	correction codes,false alarm rate,false discovery rate,Gaussian noise,iterative
	methods,iterative thresholding,linear equations,Linear systems,Matching
	pursuit algorithms,Minimization,mutual access interference,NP hard,phase
	transition,sparse matrices,sparse overcomplete representation,sparse
	solution,stagewise orthogonal matching pursuit,stepwise regression,time-frequency
	analysis,underdetermined systems,Vectors},
  pages = {1094-1121},
  owner = {Fardin}
}

@article{Donoho1992,
  title = {Superresolution via Sparsity Constraints},
  volume = {23},
  doi = {10.1137/0523074},
  abstract = {Consider the problem of recovering a measure p supported on a lattice
	of span A, when measurements are only available concerning the Fourier
	Transform &(w) at frequencies Iwi < Q. If Q is much smaller than
	the Nyquist frequency 7r/A and the measurements are noisy, then,
	in general, stable recovery of p is impossible. In this paper we
	show that if, in addition, we know that the measure p satisfies certain
	sparsity constraints, then stable recovery is possible. Say that
	a set has Rayleigh inder less than or equal to R if in any interval
	of length 4ir/Q R there are at most R elements. Indeed, if the (unknoivn)
	support of p is known, a priori, to have Rayleigh index at most R,
	then stable recovery is possible with a stability coefficient that
	grows at most like A-4R-1 as A - 0. This result validates certain
	practical efforts, in spectroscopy, seismic prospecting, and astronomy,
	to provide super-resolution by imposing support limitations in reconstruction.
	Our results amount to inequalities for interpolation of entire functions
	of exponential type from values at special point sets which are irregular,
	yet internally balanced, uniformly discrete, and of uniform density
	1.},
  timestamp = {2016-09-29T16:16:02Z},
  number = {5},
  journal = {SIAM J. Math. Anal.},
  author = {Donoho, David L.},
  year = {1992},
  keywords = {Balayage.,criterion.,Diffraction-limited,Entire,Exponential,Functions,imaging.,Interpolation.,Inverse,Nonlinear,Nyquist,of,Problems.,Rate.,Rayleigh,recovery.,Spectroscopy.,Super-Resolution.,Type.},
  pages = {1309--1331},
  annote = {http://dx.doi.org/10.1137/0523074 SIAM Journal on Mathematical Analysis},
  annote = {http://dx.doi.org/10.1137/0523074 SIAM Journal on Mathematical Analysis},
  annote = {http://dx.doi.org/10.1137/0523074 SIAM Journal on Mathematical Analysis},
  annote = {http://dx.doi.org/10.1137/0523074 SIAM Journal on Mathematical Analysis},
  annote = {http://dx.doi.org/10.1137/0523074 SIAM Journal on Mathematical Analysis},
  owner = {Fardin}
}

@article{Donoho2006c,
  title = {On the stability of the basis pursuit in the presence of noise},
  volume = {86},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2005.05.027},
  abstract = {Given a signal S ∈ R N and a full-rank matrix D ∈ R N × L with
	N &lt; L , we define the signal's over-complete representation as
	α ∈ R L satisfying S = D α . Among the infinitely many solutions
	of this under-determined linear system of equations, we have special
	interest in the sparsest representation, i.e., the one minimizing
	∥ α ∥ 0 . This problem has a combinatorial flavor to it, and
	its direct solution is impossible even for moderate L. Approximation
	algorithms are thus required, and one such appealing technique is
	the basis pursuit (BP) algorithm. This algorithm has been the focus
	of recent theoretical research effort. It was found that if indeed
	the representation is sparse enough, \{BP\} finds it accurately.
	When an error is permitted in the composition of the signal, we no
	longer require exact equality S = D α . The \{BP\} has been extended
	to treat this case, leading to a denoizing algorithm. The natural
	question to pose is how the above-mentioned theoretical results generalize
	to this more practical mode of operation. In this paper we propose
	such a generalization. The behavior of the basis pursuit in the presence
	of noise has been the subject of two independent very wide contributions
	released for publication very recently. This paper is another contribution
	in this direction, but as opposed to the others mentioned, this paper
	aims to present a somewhat simplified picture of the topic, and thus
	could be referred to as a primer to this field. Specifically, we
	establish here the stability of the \{BP\} in the presence of noise
	for sparse enough representations. We study both the case of a general
	dictionary D , and a special case where D is built as a union of
	orthonormal bases. This work is a direct generalization of noiseless
	\{BP\} study, and indeed, when the noise power is reduced to zero,
	we obtain the known results of the noiseless BP.},
  timestamp = {2016-07-10T07:17:25Z},
  number = {3},
  journal = {Signal Processing},
  author = {Donoho, David L. and Elad, Michael},
  year = {2006},
  note = {Sparse Approximations in Signal and Image ProcessingSparse Approximations
	in Signal and Image Processing},
  keywords = {Basis,denoizing,Pursuit},
  pages = {511 - 532},
  owner = {Fardin}
}

@Article{Donoho2003,
  author               = {Donoho, D.L. and Elad, M.},
  title                = {Optimally sparse representation in general (nonorthogonal) dictionaries via $\ell^1$ minimization},
  journal              = {in Proc. Natl. Acad. Sci.},
  year                 = {2003},
  volume               = {100},
  number               = {5},
  pages                = {2197--2202},
  month                = mar,
  issn                 = {1091-6490},
  abstract             = {Given a dictionary D = {dk} of vectors dk, we seek to represent a
	signal S as a linear combination S = ∑k γ(k)dk, with scalar coefficients
	γ(k). In particular, we aim for the sparsest representation possible.
	In general, this requires a combinatorial optimization process. Previous
	work considered the special case where D is an overcomplete system
	consisting of exactly two orthobases and has shown that, under a
	condition of mutual incoherence of the two bases, and assuming that
	S has a sufficiently sparse representation, this representation is
	unique and can be found by solving a convex optimization problem:
	specifically, minimizing the l1 norm of the coefficients γ̱. In
	this article, we obtain parallel results in a more general setting,
	where the dictionary D can arise from two or several bases, frames,
	or even less structured systems. We sketch three applications: separating
	linear features from planar ones in 3D data, noncooperative multiuser
	encoding, and identification of over-complete independent component
	models.},
  annote               = {read http://dx.doi.org/10.1073/pnas.0437847100 Proceedings of the National Academy of Sciences},
  citeulike-article-id = {3823938},
  citeulike-linkout-0  = {http://dx.doi.org/10.1073/pnas.0437847100},
  citeulike-linkout-1  = {http://www.pnas.org/content/100/5/2197.full.abstract},
  citeulike-linkout-2  = {http://www.pnas.org/content/100/5/2197.full.full.pdf},
  citeulike-linkout-3  = {http://www.pnas.org/cgi/content/abstract/100/5/2197},
  citeulike-linkout-4  = {http://view.ncbi.nlm.nih.gov/pubmed/16576749},
  citeulike-linkout-5  = {http://www.hubmed.org/display.cgi?uids=16576749},
  day                  = {04},
  doi                  = {10.1073/pnas.0437847100},
  keywords             = {basis_pursuit,compressive_sensing,l1_minimization,Sparsity},
  owner                = {Fardin},
  pmid                 = {16576749},
  posted-at            = {2010-06-21 09:28:19},
  timestamp            = {2016-09-29T16:09:51Z},
}

@inproceedings{Donoho2002,
  title = {Multiscale Geometric Analysis for 3-D Catalogues},
  abstract = {We have developed tools for analysis of 3-D volumetric data which
	allow sensitive characterizations of point clouds containing lamentary
	structures. Our approach involves multiscale X-ray transforms of
	the data volume. The idea is that cubes of all different locations
	and scales are extracted from the data volume and then analyzed by
	integrating along lines (X-Rays) of all different orientations and
	scales. The underlying principle is that point clouds with different
	amounts of lamentarity will lead to different distributions for
	the X-ray coefficients when viewed at the right scale. The multiscale
	approach guarantees that information from all scales is available;
	by extracting the information from the transform in a statistically
	appropriate fashion, we can sensitively resolve differences in details
	of the lamentarity. We will describe the algorithm and the results
	of comparing different simulated galaxy distributions.},
  timestamp = {2017-06-23T13:11:31Z},
  booktitle = {Proceedings of SPIE on Astronomical Telescopes Instrumentations (International 	Society for Optical Engineering, Bellingham, WA)},
  author = {Donoho, D. L. and Levi, O. and Starck, J.-L. and Martinez, V. J.},
  year = {2002},
  keywords = {3-D Beamlet Transform,filaments,galaxy distribution,large scale structures,statistical
	methods},
  owner = {afdidehf}
}

@Article{Donoho1989,
  author    = {Donoho, D.L. and Stark, P.B.},
  title     = {Uncertainty Principles and Signal Recovery},
  journal   = {SIAM J. Appl. Math.},
  year      = {1989},
  volume    = {49},
  number    = {3},
  pages     = {906--931},
  abstract  = {The uncertainty principle can easily be generalized to cases where
	the sets of concentration are not intervals. Such generalizations
	are presented for continuous and discrete-time functions, and for
	several measures of concentration (e.g., $L_2 $ and $L_1 $
	measures). The generalizations explain interesting phenomena in signal
	recovery problems where there is an interplay of missing data, sparsity,
	and bandlimiting. Read More: http://epubs.siam.org/doi/abs/10.1137/0149053},
  keywords  = {bandlimiting timelimiting,L1-methods,signal recovery,sparse spike trains,stable recovery,uncertainty principle,unique recovery},
  owner     = {afdidehf},
  timestamp = {2017-06-23T13:12:09Z},
}

@inproceedings{Duarte2009,
  title = {Recovery of compressible signals in unions of subspaces},
  doi = {10.1109/CISS.2009.5054712},
  abstract = {Compressive sensing (CS) is an alternative to Shannon/Nyquist sampling
	for acquisition of sparse or compressible signals; instead of taking
	periodic samples, we measure inner products with M < N random vectors
	and then recover the signal via a sparsity-seeking optimization or
	greedy algorithm. Initial research has shown that by leveraging stronger
	signal models than standard sparsity, the number of measurements
	required for recovery of an structured sparse signal can be much
	lower than that of standard recovery. In this paper, we introduce
	a new framework for structured compressible signals based on the
	unions of subspaces signal model, along with a new sufficient condition
	for their recovery that we dub the restricted amplification property
	(RAmP). The RAmP is the natural counterpart to the restricted isometry
	property (RIP) of conventional CS. Numerical simulations demonstrate
	the validity and applicability of our new framework using wavelet-tree
	compressible signals as an example.},
  timestamp = {2016-07-10T07:48:19Z},
  booktitle = {Information Sciences and Systems, 2009. CISS 2009. 43rd Annual Conference 	on},
  author = {Duarte, M.F. and Hegde, C. and Cevher, V. and Baraniuk, R.G.},
  month = mar,
  year = {2009},
  keywords = {compressible signal recovery,compressible signals,compressive sensing,Computational
	efficiency,data compression,Electric variables measurement,greedy
	algorithm,Greedy algorithms,Instruments,Lead,Measurement standards,Numerical
	simulation,numerical simulations,optimisation,random vectors,restricted
	amplification property,restricted isometry property,Sampling methods,Shannon-Nyquist
	sampling,signal processing,signal sampling,sparsity-seeking optimization,subspaces
	signal model,Sufficient conditions,trees (mathematics),unions of
	subspaces,wavelet transforms,wavelet-tree compressible signals},
  pages = {175-180},
  owner = {afdidehf}
}

@article{Duff1977,
  title = {A survey of sparse matrix research},
  volume = {65},
  issn = {0018-9219},
  doi = {10.1109/PROC.1977.10514},
  abstract = {This paper surveys the state of the art in sparse matrix research
	in January 1976. Much of the survey deals with the solution of sparse
	simultaneous linear equations, including the storage of such matrices
	and the effect of paging on sparse matrix algorithms. In the symmetric
	case, relevant terms from graph theory are defined. Band systems
	and matrices arising from the discretization of partial differential
	equations are treated as separate cases. Preordering techniques are
	surveyed with particular emphasis on partitioning (to block triangular
	form) and tearing (to bordered block triangular form). Methods for
	solving the least squares problem and for sparse linear programming
	are also reviewed. The sparse eigenproblem is discussed with particular
	reference to some fairly recent iterative methods. There is a short
	discussion of general iterative techniques, and reference is made
	to good standard texts in this field. Design considerations when
	implementing sparse matrix algorithms are examined and finally comments
	are made concerning the availability of codes in this area.},
  timestamp = {2016-07-08T11:33:02Z},
  number = {4},
  journal = {Proceedings of the IEEE},
  author = {Duff, I.S.},
  month = apr,
  year = {1977},
  keywords = {Algorithm design and analysis,Graph theory,Iterative algorithms,iterative
	methods,Least squares methods,linear programming,Partial differential
	equations,Partitioning algorithms,sparse matrices,Symmetric matrices},
  pages = {500-535},
  owner = {afdidehf}
}

@inproceedings{Dvijotham2010,
  title = {A nullspace analysis of the nuclear norm heuristic for rank minimization},
  doi = {10.1109/ICASSP.2010.5495918},
  abstract = {The problem of minimizing the rank of a matrix subject to linear equality
	constraints arises in applications in machine learning, dimensionality
	reduction, and control theory, and is known to be NP-hard. A popular
	heuristic minimizes the nuclear norm (sum of the singular values)
	of the matrix instead of the rank, and was recently shown to give
	an exact solution in several scenarios. In this paper, we present
	a new analysis for this heuristic based on a property of the nullspace
	of the operator defining the constraints, called the spherical section
	property. We give conditions for the exact recovery of all matrices
	up to a certain rank, and show that these conditions hold with high
	probability for operators generated from random Gaussian ensembles.
	Our analysis provides simpler proofs than existing isometry-based
	methods, as well as robust recovery results when the matrix is not
	exactly low-rank.},
  timestamp = {2016-07-08T11:25:56Z},
  booktitle = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International 	Conference on},
  author = {Dvijotham, K. and Fazel, M.},
  month = mar,
  year = {2010},
  keywords = {Application software,Collaboration,compressed sensing,Computational
	complexity,Computer science,Constraint optimization,Constraint theory,Control
	theory,convex optimization,exact recovery,Gaussian processes,isometry-based
	methods,linear equality constraints,Machine learning,matrix algebra,Matrix
	rank minimization,matrix subject,minimisation,NP-hard,nuclear norm
	heuristic,nullspace analysis,random Gaussian ensembles,random processes,rank
	minimization,Robustness,spherical section property,System identification},
  pages = {3586-3589},
  owner = {afdidehf}
}

@book{Eden2013,
  title = {The Science of Large Data Sets: Spikes, Fields, and Voxels},
  timestamp = {2016-07-11T17:04:35Z},
  publisher = {{\{Society for Neuroscience\}}},
  author = {Eden, Uri},
  year = {2013},
  owner = {Fardin}
}

@article{Efron2004,
  title = {Least angle regression (with discussion)},
  volume = {32},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward
	Selection and Backward Elimination is to choose a linear model on
	the basis of the same set of data to which the model will be applied.
	Typically we have available a large collection of possible covariates
	from which we hope to select a parsimonious set for the efficient
	prediction of a response variable. Least Angle Regression (LARS),
	a new model selection algorithm, is a useful and less greedy version
	of traditional forward selection methods. Three main properties are
	derived: (1) A simple modification of the LARS algorithm implements
	the Lasso, an attractive version of ordinary least squares that constrains
	the sum of the absolute regression coefficients; the LARS modification
	calculates all possible Lasso estimates for a given problem, using
	an order of magnitude less computer time than previous methods. (2)
	A different LARS modification efficiently implements Forward Stagewise
	linear regression, another promising new model selection method;
	this connection explains the similar numerical results previously
	observed for the Lasso and Stagewise, and helps us understand the
	properties of both methods, which are seen as constrained versions
	of the simpler LARS algorithm. (3) A simple approximation for the
	degrees of freedom of a LARS estimate is available, from which we
	derive a Cp estimate of prediction error; this allows a principled
	choice among the range of possible LARS estimates. LARS and its variants
	are computationally efficient: the paper describes a publicly available
	algorithm that requires only the same order of magnitude of computational
	effort as ordinary least squares applied to the full set of covariates.},
  timestamp = {2016-07-09T19:56:19Z},
  number = {2},
  journal = {The Annals of Statistics},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  year = {2004},
  keywords = {boosting,coefficient paths,Lasso,Linear regression,variable selection},
  pages = {407�499},
  owner = {afdidehf}
}

@article{Efron2007,
  title = {Discussion: the dantzig selector: statistical estimation when p is 	much larger than n},
  volume = {35},
  doi = {10.1214/009053607000000433},
  abstract = {This is a fascinating paper on an important topic: the choice of predictor
	variables in large-scale linear models. A previous paper in these
	pages attacked the same problem using the �LARS� algorithm (Efron,
	Hastie, Johnstone and Tibshirani [3]); actually three algorithms
	including the Lasso as middle case. There are tantalizing similarities
	between the Dantzig Selector (DS) and the LARS methods, but they
	are not the same and produce somewhat different models. We explore
	this relationship with the Lasso and LARS here.},
  timestamp = {2016-07-08T12:12:25Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Efron, Bradley and Hastie, Trevor and Tibshirani, Robert},
  year = {2007},
  pages = {2358�2364},
  owner = {afdidehf}
}

@article{Eftekhari2015a,
  title = {The restricted isometry property for random block diagonal matrices},
  volume = {38},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2014.02.001},
  abstract = {Abstract In Compressive Sensing, the Restricted Isometry Property
	(RIP) ensures that robust recovery of sparse vectors is possible
	from noisy, undersampled measurements via computationally tractable
	algorithms. It is by now well-known that Gaussian (or, more generally,
	sub-Gaussian) random matrices satisfy the \{RIP\} under certain conditions
	on the number of measurements. Their use can be limited in practice,
	however, due to storage limitations, computational considerations,
	or the mismatch of such matrices with certain measurement architectures.
	These issues have recently motivated considerable effort towards
	studying the \{RIP\} for structured random matrices. In this paper,
	we study the \{RIP\} for block diagonal measurement matrices where
	each block on the main diagonal is itself a sub-Gaussian random matrix.
	Our main result states that such matrices can indeed satisfy the
	\{RIP\} but that the requisite number of measurements depends on
	certain properties of the basis in which the signals are sparse.
	In the best case, these matrices perform nearly as well as dense
	Gaussian random matrices, despite having many fewer nonzero entries.},
  timestamp = {2016-07-11T17:04:27Z},
  number = {1},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Eftekhari, Armin and Yap, Han Lun and Rozell, Christopher J. and Wakin, Michael B.},
  year = {2015},
  keywords = {Block,Compressive,diagonal,isometry,matrices,property,Restricted,sensing},
  pages = {1 - 31},
  owner = {afdidehf}
}

@inproceedings{Eiwen2010,
  title = {Group sparsity methods for compressive channel estimation in doubly 	dispersive multicarrier systems},
  doi = {10.1109/SPAWC.2010.5670986},
  abstract = {We propose advanced compressive estimators of doubly dispersive channels
	within multicarrier communication systems (including classical OFDM
	systems). The performance of compressive channel estimation has been
	shown to be limited by leakage components impairing the channel's
	effective delay-Doppler sparsity. We demonstrate a group sparse structure
	of these leakage components and apply recently proposed recovery
	techniques for group sparse signals. We also present a basis optimization
	method for enhancing group sparsity. Statistical knowledge about
	the channel can be incorporated in the basis optimization if available.
	The proposed estimators outperform existing compressive estimators
	with respect to estimation accuracy and, in one instance, also computational
	complexity.},
  timestamp = {2016-07-08T12:41:04Z},
  booktitle = {Signal Processing Advances in Wireless Communications (SPAWC), 2010 	IEEE Eleventh International Workshop on},
  author = {Eiwen, D. and Taubock, G. and Hlawatsch, F. and Feichtinger, H.G.},
  month = jun,
  year = {2010},
  keywords = {block sparsity,channel estimation,compressed sensing,compressive channel
	estimation,data compression,delay-Doppler sparsity,Discrete Fourier
	transforms,Dispersion,dispersive channels,doubly dispersive channel,doubly
	dispersive channels,doubly selective channel,group sparsity,group
	sparsity methods,multicarrier communication systems,multicarrier
	modulation,OFDM,optimisation,Optimization,Signal to noise ratio,statistical
	analysis,statistical knowledge,union of subspaces,Wireless communication},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Eksioglu2011,
  title = {A clustering based framework for dictionary block structure identification},
  doi = {10.1109/ICASSP.2011.5947240},
  abstract = {Sparse representations over redundant dictionaries offer an efficient
	paradigm for signal representation. Recently block-sparsity has been
	put forward as a prior condition for some sparse representation applications,
	where the coefficients of the sparse representation occur in blocks
	rather than being distributed randomly over the sparse vector. Block-sparse
	representation algorithms, which are extensions of the regular sparse
	representation algorithms have been developed. However, these algorithms
	work under the assumption that both the dictionary and its corresponding
	block structure are known. In this paper, we consider the problem
	of recovering the optimally block sparsifying block structure for
	a given data set and dictionary pair. We propose a block structure
	identification framework employing a clustering step which can be
	realized using the standard clustering schemes from the literature.
	The block structure identification algorithm works efficiently, and
	for synthetically generated block-sparse data the underlying block
	structure is retrieved even for comparably short data records.},
  timestamp = {2016-07-08T10:08:11Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International 	Conference on},
  author = {Eksioglu, E.M.},
  month = may,
  year = {2011},
  keywords = {block sparse representations,Block-sparsity,block structure identification,clustering,Clustering
	algorithms,clustering schemes,Couplings,Dictionaries,dictionary block
	structure,Matching pursuit algorithms,measurement,pattern clustering,redundant
	dictionaries,Signal processing algorithms,signal representation,signal
	representations,sparse vector},
  pages = {4044-4047},
  annote = {read},
  owner = {afdidehf}
}

@inproceedings{ElBadawy2015,
  title = {Relative group sparsity for non-negative matrix factorization with 	application to on-the-fly audio source separation},
  doi = {10.1109/ICASSP.2015.7177971},
  abstract = {We consider dictionary-based signal decompositions with group sparsity,
	a variant of structured sparsity. We point out that the group sparsity-inducing
	constraint alone may not be sufficient in some cases when we know
	that some bigger groups or so-called supergroups cannot vanish completely.
	To deal with this problem we introduce the notion of relative group
	sparsity preventing the supergroups from vanishing. In this paper
	we formulate practical criteria and algorithms for relative group
	sparsity as applied to non-negative matrix factorization and investigate
	its potential benefit within the on-the-fly audio source separation
	framework we recently introduced. Experimental evaluation shows that
	the proposed relative group sparsity leads to performance improvement
	over group sparsity in both supervised and semi-supervised on-the-fly
	audio source separation settings.},
  timestamp = {2016-07-10T07:51:30Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International 	Conference on},
  author = {El Badawy, D. and Ozerov, A. and Duong, N.Q.K.},
  month = apr,
  year = {2015},
  keywords = {Acoustics,audio signal processing,audio source separation,compressed
	sensing,Dictionaries,dictionary-based signal decompositions,group
	sparsity,group sparsity-inducing constraint,learning (artificial
	intelligence),matrix decomposition,nonnegative matrix factorization,non-negative
	matrix factorization,relative group sparsity,semi-supervised on-the-fly
	audio source separation settings,Signal processing algorithms,source
	separation,Speech,Structured Sparsity,supergroups,universal model},
  pages = {256-260},
  owner = {afdidehf}
}

@book{Elad2010,
  title = {Sparse and Redundant Representations: From Theory to Applications 	in Signal and Image Processing},
  timestamp = {2016-07-10T08:24:13Z},
  publisher = {{\{Springer\}}},
  author = {Elad, Michael},
  editor = {{{Springer}}},
  year = {2010},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Elad2006,
  title = {Sparse Representations Are Most Likely To Be The Sparsest Possible},
  doi = {10.1155/ASP/2006/96247},
  abstract = {Given a signal S ? RN and a full-rankmatrix D ? RN�L withN < L,
	we define the signal�s overcomplete representations as all ? ?
	RL satisfying S = D?. Among all the possible solutions, we have special
	interest in the sparsest one�the one minimizing ?0. Previous
	work has established that a representation is unique if it is sparse
	enough, requiring ?0 < Spark(D)/2. The measure Spark(D) stands
	for the minimal number of columns from D that are linearly dependent.
	This bound is tight�examples can be constructed to show that with
	Spark(D)/2 or more nonzero entries, uniqueness is violated. In this
	paper we study the behavior of overcomplete representations beyond
	the above bound. While tight from a worst-case standpoint, a probabilistic
	point of view leads to uniqueness of representations satisfying ?0
	< Spark(D). Furthermore, we show that even beyond this point, uniqueness
	can still be claimed with high confidence. This new result is important
	for the study of the average performance of pursuit algorithms�when
	trying to show an equivalence between the pursuit result and the
	ideal solution, one must also guarantee that the ideal result is
	indeed the sparsest.},
  timestamp = {2016-07-11T16:52:55Z},
  number = {96247},
  journal = {EURASIP Journal on Applied Signal Processing},
  author = {Elad, Michael},
  year = {2006},
  owner = {Fardin}
}

@article{Elad2002,
  title = {Sparse Representations and the Basis Pursuit Algorithm},
  timestamp = {2016-07-11T16:52:34Z},
  author = {Elad, Michael},
  month = nov,
  year = {2002},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@article{Elad2002a,
  title = {A generalized uncertainty principle and sparse representation in 	pairs of bases},
  volume = {48},
  issn = {0018-9448},
  doi = {10.1109/TIT.2002.801410},
  abstract = {An elementary proof of a basic uncertainty principle concerning pairs
	of representations of RN vectors in different orthonormal bases is
	provided. The result, slightly stronger than stated before, has a
	direct impact on the uniqueness property of the sparse representation
	of such vectors using pairs of orthonormal bases as overcomplete
	dictionaries. The main contribution in this paper is the improvement
	of an important result due to Donoho and Huo (2001) concerning the
	replacement of the l0 optimization problem by a linear programming
	(LP) minimization when searching for the unique sparse representation.},
  timestamp = {2016-09-29T16:09:06Z},
  number = {9},
  journal = {IEEE Trans. Inf. Theory},
  author = {Elad, M. and Bruckstein, A.M.},
  month = sep,
  year = {2002},
  keywords = {Cities and towns,Computer science,Dictionaries,Dynamic Programming,Electronic
	mail,generalized uncertainty principle,image coding,Image Processing,Linear
	algebra,linear programming,linear programming minimization,minimisation,NSP,null space property,null
	space property,Optimization,orthonormal bases,orthonormal
	bases,overcomplete dictionaries,pairs of bases,pairs
	of bases,signal representation,sparse representation,sparse
	representation,Uncertainty,uniqueness property,uniqueness
	property,Vectors},
  pages = {2558--2567},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  owner = {Fardin}
}

@article{Elad2001,
  title = {On sparse signal representations},
  volume = {1},
  doi = {10.1109/ICIP.2001.958936},
  abstract = {An elementary proof of a basic uncertainty principle concerning pairs
	of representations of ?N vectors in different orthonormal bases is
	provided. The result, slightly stronger than stated before, has a
	direct impact on the uniqueness property of the sparse representation
	of such vectors using pairs of orthonormal bases as overcomplete
	dictionaries. The main contribution in this paper is the improvement
	of an important result due to Donoho and Huo (1999) concerning the
	replacement of the l0 optimization problem by a linear programming
	minimization when searching for the unique sparse representation},
  timestamp = {2016-07-10T07:14:04Z},
  journal = {Image Processing, 2001. Proceedings. 2001 International Conference 	on},
  author = {Elad, M. and Bruckstein, A.M.},
  year = {2001},
  keywords = {Cities and towns,Computer science,Dictionaries,Equations,linear programming,linear
	programming minimization,minimisation,optimization problem,orthonormal
	bases,overcomplete dictionaries,Signal generators,signal processing,signal
	representation,signal representations,sparse representation,Uncertainty,uncertainty
	principle,uniqueness property,unique sparse representation search,Vectors},
  pages = {3-6},
  owner = {Fardin}
}

@inproceedings{Elad2006a,
  title = {Image Denoising with Shrinkage and Redundant Representations},
  volume = {2},
  doi = {10.1109/CVPR.2006.143},
  abstract = {Shrinkage is a well known and appealing denoising technique. The use
	of shrinkage is known to be optimal for Gaussian white noise, provided
	that the sparsity on the signal�s representation is enforced using
	a unitary transform. Still, shrinkage is also practiced successfully
	with nonunitary, and even redundant representations. In this paper
	we shed some light on this behavior. We show that simple shrinkage
	could be interpreted as the first iteration of an algorithm that
	solves the basis pursuit denoising (BPDN) problem. Thus, this work
	leads to a novel iterative shrinkage algorithm that can be considered
	as an effective pursuit method. We demonstrate this algorithm, both
	on synthetic data, and for the image denoising problem, where we
	learn the image prior parameters directly from the given image. The
	results in both cases are superior to several popular alternatives.},
  timestamp = {2016-07-08T12:46:43Z},
  booktitle = {Computer Vision and Pattern Recognition, 2006 IEEE Computer Society 	Conference on},
  author = {Elad, M. and Matalon, B. and Zibulevsky, M.},
  year = {2006},
  keywords = {Computer science,image denoising,Iterative algorithms,iterative methods,Maximum
	a posteriori estimation,Noise reduction,Pursuit algorithms,Table
	lookup,wavelet transforms,White noise},
  pages = {1924-1931},
  owner = {afdidehf}
}

@article{Eldar2009,
  title = {Uncertainty Relations for Shift-Invariant Analog Signals},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2032711},
  abstract = {The past several years have witnessed a surge of research investigating
	various aspects of sparse representations and compressed sensing.
	Most of this work has focused on the finite-dimensional setting in
	which the goal is to decompose a finite-length vector into a given
	finite dictionary. Underlying many of these results is the conceptual
	notion of an uncertainty principle: a signal cannot be sparsely represented
	in two different bases. Here, we extend these ideas and results to
	the analog, infinite-dimensional setting by considering signals that
	lie in a finitely generated shift-invariant (SI) space. This class
	of signals is rich enough to include many interesting special cases
	such as multiband signals and splines. By adapting the notion of
	coherence defined for finite dictionaries to infinite SI representations,
	we develop an uncertainty principle similar in spirit to its finite
	counterpart. We demonstrate tightness of our bound by considering
	a bandlimited lowpass train that achieves the uncertainty principle.
	Building upon these results and similar work in the finite setting,
	we show how to find a sparse decomposition in an overcomplete dictionary
	by solving a convex optimization problem. The distinguishing feature
	of our approach is the fact that even though the problem is defined
	over an infinite domain with infinitely many variables and constraints,
	under certain conditions on the dictionary spectrum our algorithm
	can find the sparsest representation by solving a finite-dimensional
	problem.},
  timestamp = {2016-09-30T11:25:40Z},
  number = {12},
  journal = {Information Theory, IEEE Transactions on},
  author = {Eldar, Y.C.},
  month = dec,
  year = {2009},
  keywords = {analog compressed sensing,bandlimited lowpass train,Coherence,compressed
	sensing,convex optimization problem,convex programming,data compression,Dictionaries,finite
	dictionary,finite-dimensional problem,finite-length vector,Frequency,infinite-dimensional
	setting,matrix decomposition,Mechanical variables measurement,overcomplete
	dictionary spectrum,Position measurement,Quantum mechanics,shift-invariant
	analog signal,shift-invariant signals,Signal generators,Signal processing
	algorithms,signal representation,sparse decomposition,sparse decompositions,sparse
	matrices,sparse representation,sparsest representation,Surges,Uncertainty,uncertainty relations,uncertainty
	relations,Vectors},
  pages = {5742--5757},
  owner = {afdidehf}
}

@article{Eldar2009a,
  title = {Compressed Sensing of Analog Signals in Shift-Invariant Spaces},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2020750},
  abstract = {A traditional assumption underlying most data converters is that the
	signal should be sampled at a rate exceeding twice the highest frequency.
	This statement is based on a worst-case scenario in which the signal
	occupies the entire available bandwidth. In practice, many signals
	are sparse so that only part of the bandwidth is used. In this paper,
	we develop methods for low-rate sampling of continuous-time sparse
	signals in shift-invariant (SI) spaces, generated by m kernels with
	period T . We model sparsity by treating the case in which only k
	out of the m generators are active, however, we do not know which
	k are chosen. We show how to sample such signals at a rate much lower
	than m/T, which is the minimal sampling rate without exploiting sparsity.
	Our approach combines ideas from analog sampling in a subspace with
	a recently developed block diagram that converts an infinite set
	of sparse equations to a finite counterpart. Using these two components
	we formulate our problem within the framework of finite compressed
	sensing (CS) and then rely on algorithms developed in that context.
	The distinguishing feature of our results is that in contrast to
	standard CS, which treats finite-length vectors, we consider sampling
	of analog signals for which no underlying finite-dimensional model
	exists. The proposed framework allows to extend much of the recent
	literature on CS to the analog domain.},
  timestamp = {2016-09-30T11:27:11Z},
  number = {8},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Eldar, Y.C.},
  month = aug,
  year = {2009},
  keywords = {analog compressed sensing,analog compressed sensing signal,Bandwidth,Data
	Compression,data converter,finite compressed sensing,finite-dimensional
	model,finite-length vector,low-rate sampling,matrix algebra,shift-invariant
	spaces,signal sampling,sparse equation,Sparsity,subNyquist sampling,sub-Nyquist
	sampling},
  pages = {2986--2997},
  owner = {afdidehf}
}

@inproceedings{Eldar2008,
  title = {Uncertainty relations and sparse decompositions of analog signals},
  doi = {10.1109/EEEI.2008.4736676},
  abstract = {We consider uncertainty principles for analog signals that lie in
	a finitely-generated shift-invariant (SI) space. By adapting the
	notion of coherence defined for finite dictionaries to infinite SI
	representations, we develop an uncertainty principle similar in spirit
	to its finite counterpart. Building upon these results and similar
	work in the finite setting, we show how to find a sparse decomposition
	of an analog signal in an overcomplete dictionary by solving a convex
	optimization problem. The distinguishing feature of our approach
	is the fact that even though the problem is defined over an infinite
	domain with infinitely many variables and constraints, under certain
	conditions on the dictionary spectrum our algorithm can find the
	sparsest representation by solving a finite dimensional problem.},
  timestamp = {2016-09-30T11:25:50Z},
  booktitle = {Electrical and Electronics Engineers in Israel, 2008. IEEEI 2008. 	IEEE 25th Convention of},
  author = {Eldar, Y.C.},
  month = dec,
  year = {2008},
  keywords = {analog signal sparse decomposition,analog signal uncertainty principles,Character
	generation,compressed sensing,Concrete,convex optimization problem,convex
	programming,Dictionaries,finite dictionaries,finite dimensional problem,finitely-generated
	shift-invariant space,Multidimensional systems,Signal generators,signal
	representation,Space technology,sparse decompositions,Surges,Uncertainty,uncertainty
	principle},
  pages = {147--151},
  owner = {afdidehf}
}

@inproceedings{Eldar2009b,
  title = {Block-sparsity: Coherence and efficient recovery},
  doi = {10.1109/ICASSP.2009.4960226},
  abstract = {We consider compressed sensing of block-sparse signals, i.e., sparse
	signals that have nonzero coefficients occurring in clusters. Based
	on an uncertainty relation for block-sparse signals, we define a
	block-coherence measure and show that a block-version of the orthogonal
	matching pursuit algorithm recovers block k-sparse signals in no
	more than k steps if the block-coherence is sufficiently small. The
	same condition on block-sparsity is shown to guarantee successful
	recovery through a mixed lscr2/lscr1 optimization approach. The significance
	of the results lies in the fact that making explicit use of block-sparsity
	can yield better reconstruction properties than treating the signal
	as being sparse in the conventional sense, thereby ignoring the additional
	structure in the problem.},
  timestamp = {2016-09-30T13:32:16Z},
  booktitle = {Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE 	International Conference on},
  author = {Eldar, Y.C. and Bolcskei, H.},
  month = apr,
  year = {2009},
  keywords = {block-coherence measure,block k-sparse signals,Block-sparsity,Clustering
	algorithms,Coherence,compressed sensing,Context,Matching pursuit
	algorithms,orthogonal matching pursuit algorithm,Pursuit algorithms,Robustness,signal
	processing,Sufficient conditions,Uncertainty,uncertainty relations,Wireless
	communication},
  pages = {2885--2888},
  owner = {afdidehf}
}

@Article{Eldar2010,
  author    = {Eldar, Y. and Kuppinger, P. and B{\"o}lcskei, H.},
  title     = {Block-sparse signals: Uncertainty relations and efficient recovery},
  journal   = {IEEE Trans. Signal Process.},
  year      = {2010},
  volume    = {58},
  number    = {6},
  pages     = {3042--3054},
  month     = jun,
  annote    = {read http://www.nari.ee.ethz.ch/commth//pubs/p/Block2009 block-coherence IEEE Transactions on Signal Processing},
  keywords  = {Basis pursuit,Block-sparsity,compressed sensing,matching pursuit},
  owner     = {Fardin},
  timestamp = {2016-09-30T13:34:19Z},
}

@book{Eldar2012,
  title = {Compressed Sensing: Theory and Applications},
  isbn = {978-1-107-00558-7},
  abstract = {Compressed sensing is an exciting, rapidly growing field, attracting
	considerable attention in electrical engineering, applied mathematics,
	statistics and computer science. This book provides the first detailed
	introduction to the subject, highlighting recent theoretical advances
	and a range of applications, as well as outlining numerous remaining
	research challenges. After a thorough review of the basic theory,
	many cutting-edge techniques are presented, including advanced signal
	modeling, sub-Nyquist sampling of analog signals, non-asymptotic
	analysis of random matrices, adaptive sensing, greedy algorithms
	and use of graphical models. All chapters are written by leading
	researchers in the field, and consistent style and notation are utilized
	throughout. Key background information and clear definitions make
	this an ideal resource for researchers, graduate students and practitioners
	wanting to join this exciting research area. It can also serve as
	a supplementary textbook for courses on computer vision, coding theory,
	signal processing, image processing and algorithms for efficient
	data processing},
  timestamp = {2016-07-08T11:57:26Z},
  publisher = {{\{Cambridge University Press\}}},
  author = {Eldar, Y.C. and Kutyniok, G.},
  year = {2012},
  owner = {Fardin}
}

@article{Eldar2009c,
  title = {Robust Recovery of Signals From a Structured Union of Subspaces},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2030471},
  abstract = {Traditional sampling theories consider the problem of reconstructing
	an unknown signal x from a series of samples. A prevalent assumption
	which often guarantees recovery from the given measurements is that
	x lies in a known subspace. Recently, there has been growing interest
	in nonlinear but structured signal models, in which x lies in a union
	of subspaces. In this paper, we develop a general framework for robust
	and efficient recovery of such signals from a given set of samples.
	More specifically, we treat the case in which x lies in a sum of
	k subspaces, chosen from a larger set of m possibilities. The samples
	are modeled as inner products with an arbitrary set of sampling functions.
	To derive an efficient and robust recovery algorithm, we show that
	our problem can be formulated as that of recovering a block-sparse
	vector whose nonzero elements appear in fixed blocks. We then propose
	a mixed lscr2/lscr1 program for block sparse recovery. Our main result
	is an equivalence condition under which the proposed convex algorithm
	is guaranteed to recover the original signal. This result relies
	on the notion of block restricted isometry property (RIP), which
	is a generalization of the standard RIP used extensively in the context
	of compressed sensing. Based on RIP, we also prove stability of our
	approach in the presence of noise and modeling errors. A special
	case of our framework is that of recovering multiple measurement
	vectors (MMV) that share a joint sparsity pattern. Adapting our results
	to this context leads to new MMV recovery methods as well as equivalence
	conditions under which the entire set can be determined efficiently.},
  timestamp = {2016-09-30T11:22:16Z},
  number = {11},
  journal = {IEEE Trans. Inf. Theory},
  author = {Eldar, Y.C. and Mishali, M.},
  month = nov,
  year = {2009},
  keywords = {Block restricted isometry property,block sparse vector recovery,block
	sparsity,compressed sensing,Frequency,History,joint sparsity pattern,joint
	sparsity pattern,mixed-norm recovery,mixed-norm
	recovery,multiple measurement vector recovery,multiple measurement vector
	recovery,multiple measurement vectors (MMV),multiple measurement
	vectors (MMV),nonlinear signal model,restricted isometry property,restricted
	isometry property,Robustness,sampling functions,sampling
	functions,Sampling methods,sampling theory,sampling
	theory,signal processing,Signal processing algorithms,Signal
	processing algorithms,signal reconstruction,signal recovery,signal
	recovery,signal sampling,signal
	sampling,Stability,subspace structured union,union of linear subspaces,union
	of linear subspaces,Vectors},
  pages = {5302--5316},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  owner = {afdidehf}
}

@inproceedings{Eldar2009d,
  title = {Block sparsity and sampling over a union of subspaces},
  doi = {10.1109/ICDSP.2009.5201211},
  abstract = {Sparse signal representations have gained wide popularity in recent
	years. In many applications the data can be expressed using only
	a few nonzero elements in an appropriate expansion. In this paper,
	we study a block-sparse model, in which the nonzero coefficients
	are arranged in blocks. To exploit this structure, we redefine the
	standard (NP-hard) sparse recovery problem, based on which we propose
	a convex relaxation in the form of a mixed lscr2=lscr1 program. Isometry-based
	analysis is used to prove equivalence of the solution to that of
	the optimal program, under certain mild conditions. We further establish
	the robustness of our algorithm to mismodeling and bounded noise.
	We then present theoretical arguments and numerical experiments demonstrating
	the improved recovery performance of our method in comparison with
	sparse reconstruction that does not incorporate a block structure.
	The results are then applied to two related problems. The first is
	that of simultaneous sparse approximation. Our results can be used
	to prove isometry-based equivalence properties for this setting.
	In addition, we propose an alternative approach to acquire the measurements,
	that leads to performance improvement over standard methods. Finally,
	we show how our results can be used to sample signals in a finite
	structured union of subspaces, leading to robust and efficient recovery
	algorithms.},
  timestamp = {2016-09-30T11:26:37Z},
  booktitle = {Digital Signal Processing, 2009 16th International Conference on},
  author = {Eldar, Y.C. and Mishali, M.},
  month = jul,
  year = {2009},
  keywords = {block-sparse model,block sparsity,Coherence,compressed sensing,Computational
	complexity,isometry-based analysis,Matching pursuit algorithms,Measurement
	standards,Minimization methods,multiple measurement vectors (MMV),Noise
	robustness,nonzero coefficients,NP-hard,Particle measurements,restricted
	isometry property,Sampling methods,signal denoising,signal representation,signal
	representations,sparse approximation,sparse matrices,sparse signal
	representations,subspaces union,union of subspaces},
  pages = {1--8},
  owner = {afdidehf}
}

@article{Eldar2010a,
  title = {Average Case Analysis of Multichannel Sparse Recovery Using Convex 	Relaxation},
  volume = {56},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2034789},
  abstract = {This paper considers recovery of jointly sparse multichannel signals
	from incomplete measurements. Several approaches have been developed
	to recover the unknown sparse vectors from the given observations,
	including thresholding, simultaneous orthogonal matching pursuit
	(SOMP), and convex relaxation based on a mixed matrix norm. Typically,
	worst case analysis is carried out in order to analyze conditions
	under which the algorithms are able to recover any jointly sparse
	set of vectors. However, such an approach is not able to provide
	insights into why joint sparse recovery is superior to applying standard
	sparse reconstruction methods to each channel individually. Previous
	work considered an average case analysis of thresholding and SOMP
	by imposing a probability model on the measured signals. Here, the
	main focus is on analysis of convex relaxation techniques. In particular,
	the mixed l 2,1 approach to multichannel recovery is investigated.
	Under a very mild condition on the sparsity and on the dictionary
	characteristics, measured for example by the coherence, it is shown
	that the probability of recovery failure decays exponentially in
	the number of channels. This demonstrates that most of the time,
	multichannel sparse recovery is indeed superior to single channel
	methods. The probability bounds are valid and meaningful even for
	a small number of signals. Using the tools developed to analyze the
	convex relaxation technique, also previous bounds for thresholding
	and SOMP recovery are tightened.},
  timestamp = {2016-09-30T11:23:15Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Eldar, Y.C. and Rauhut, H.},
  month = jan,
  year = {2010},
  keywords = {Algorithm design and analysis,average case analysis,Average performance,convex
	relaxation techniques,Dictionaries,incomplete measurements,jointly
	sparse multichannel signals,joint sparse recovery,Matching pursuit
	algorithms,Mathematics,mixed matrix norm,mixed-norm optimization,multichannel
	recovery,multichannel sparse recovery,probability,probability bounds,probability
	model,Radar signal processing,Reconstruction algorithms,recovery
	failure decays,relaxation theory,Signal analysis,Signal processing
	algorithms,signal reconstruction,simultaneous orthogonal matching pursuit,simultaneous orthogonal matching
	pursuit,single channel methods,sparse
	matrices,sparse vectors,standard sparse reconstruction methods,thresholding,Vectors},
  pages = {505--519},
  owner = {afdidehf}
}

@phdthesis{Elhamifar2012,
  address = {United States -- Maryland},
  title = {Sparse modeling for high-dimensional multi-manifold data analysis},
  abstract = {High-dimensional data are ubiquitous in many areas of science and
	engineering, such as machine learning, signal and image processing,
	computer vision, pattern recognition, bioinformatics, etc. Often,
	high-dimensional data are not distributed uniformly in the ambient
	space; instead they lie in or close to a union of low-dimensional
	manifolds. Recovering such low-dimensional structures in the data
	helps to not only significantly reduce the computational cost and
	memory requirements of algorithms that deal with the data, but also
	reduce the effect of the high-dimensional noise in the data and improve
	the performance of inference and learning tasks. There are three
	fundamental tasks related to the multi-manifold data: clustering,
	dimensionality reduction, and classification. While the area of machine
	learning has seen great advances in these areas, the applicability
	of current algorithms are limited due to several challenges. First,
	in many problems, manifolds are spatially close or even intersect,
	while existing methods work only when manifolds are sufficiently
	separated. Second, most algorithms require to know the dimensions
	or the number of manifolds a priori, while in real-world problems
	such quantities are often unknown. Third, most existing algorithms
	have difficulty in effectively dealing with data nuisances, such
	as noise, outliers, and missing entries, as well as manifolds of
	different intrinsic dimensions. In this thesis, we present new frameworks
	based on sparse representation techniques for the problems of clustering,
	dimensionality reduction and classification of multi-manifold data
	that effectively address the aforementioned challenges. The key idea
	behind the proposed algorithms is what we call the self-expressiveness
	property of the data. This property states that in an appropriate
	dictionary formed from the given data points in multiple manifolds,
	a sparse representation of a data point corresponds to selecting
	other points from the same manifold. Our goal is then to search for
	such sparse representations and use them in appropriate frameworks
	to cluster, embed, and classify multi-manifold data. We propose sparse
	optimization programs to find such desired representations and develop
	theoretical guarantees for the success of the proposed algorithms.
	By extensive experiments on synthetic and real data, we demonstrate
	that the proposed algorithms significantly improve the state-of-the-art
	results.},
  timestamp = {2016-07-11T16:49:03Z},
  school = {The Johns Hopkins University},
  author = {Elhamifar, Ehsan},
  year = {2012},
  keywords = {Applied sciences,block-sparse recovery,block-sparse representation,embedding,face
	clustering,face recognition,Manifold learning,motion segmentation,multi-manifold
	classification,multi-manifold clustering,Multi-manifold data,multi-manifold
	dimensionality reduction,multi-manifold embedding,Pure sciences,real
	data,Sparse Manifold Clustering,sparse representation,sparse subspace
	clustering,subspace clustering,Subspace segmentation,Subspace-sparse
	recovery theory,synthetic data},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Elhamifar2012a,
  title = {Learning All by Selecting A Few},
  timestamp = {2016-07-09T19:54:52Z},
  author = {Elhamifar, Ehsan and Vidal, Rene},
  year = {2012},
  note = {Center for Imaging Science Johns Hopkins University (PPT)},
  owner = {Fardin}
}

@Article{Elhamifar2012b,
  author    = {Elhamifar, E. and Vidal, R.},
  title     = {Block-Sparse Recovery via Convex Optimization},
  journal   = {IEEE Transactions on Signal Processing},
  year      = {2012},
  volume    = {60},
  number    = {8},
  pages     = {4094--4107},
  abstract  = {Given a dictionary that consists of multiple blocks and a signal that
	lives in the range space of only a few blocks, we study the problem
	of finding a block-sparse representation of the signal, i.e., a representation
	that uses the minimum number of blocks. Motivated by signal/image
	processing and computer vision applications, such as face recognition,
	we consider the block-sparse recovery problem in the case where the
	number of atoms in each block is arbitrary, possibly much larger
	than the dimension of the underlying subspace. To find a blocksparse
	representation of a signal, we propose two classes of non-convex
	optimization programs, which aim to minimize the number of nonzero
	coefficient blocks and the number of nonzero reconstructed vectors
	from the blocks, respectively. Since both classes of problems are
	NP-hard, we propose convex relaxations and derive conditions under
	which each class of the convex programs is equivalent to the original
	non-convex formulation. Our conditions depend on the notions of mutual
	and cumulative subspace coherence of a dictionary, which are natural
	generalizations of existing notions of mutual and cumulative coherence.
	We evaluate the performance of the proposed convex programs through
	simulations as well as real experiments on face recognition. We show
	that treating the face recognition problem as a block-sparse recovery
	problem improves the state-of-the-art results by 10% with only 25%
	of the training data.},
  annote    = {read},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/tsp/ElhamifarV12},
  doi       = {10.1109/TSP.2012.2196694},
  keywords  = {block-sparse signals,convex optimization,face recognition.,principal angles,subspaces},
  owner     = {Fardin},
  timestamp = {2016-09-30T11:45:59Z},
}

@inproceedings{Elhamifar2009,
  title = {Sparse subspace clustering},
  doi = {10.1109/CVPR.2009.5206547},
  abstract = {We propose a method based on sparse representation (SR) to cluster
	data drawn from multiple low-dimensional linear or affine subspaces
	embedded in a high-dimensional space. Our method is based on the
	fact that each point in a union of subspaces has a SR with respect
	to a dictionary formed by all other data points. In general, finding
	such a SR is NP hard. Our key contribution is to show that, under
	mild assumptions, the SR can be obtained `exactly' by using l1 optimization.
	The segmentation of the data is obtained by applying spectral clustering
	to a similarity matrix built from this SR. Our method can handle
	noise, outliers as well as missing data. We apply our subspace clustering
	algorithm to the problem of segmenting multiple motions in video.
	Experiments on 167 video sequences show that our approach significantly
	outperforms state-of-the-art methods.},
  timestamp = {2016-09-30T11:48:04Z},
  booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference 	on},
  author = {Elhamifar, E. and Vidal, R.},
  month = jun,
  year = {2009},
  keywords = {Clustering algorithms,Dictionaries,image coding,image motion analysis,Image
	segmentation,information theory,iterative methods,matrix algebra,motion
	segmentation,optimisation,pattern clustering,Polynomials,Principal
	component analysis,sparse representation,sparse subspace clustering,spectral
	clustering,Strontium,video sequences},
  pages = {2790--2797},
  owner = {Fardin}
}

@inproceedings{Ellenrieder2010,
  title = {Electromagnetic source imaging for sparse cortical activation patterns},
  doi = {10.1109/IEMBS.2010.5626203},
  abstract = {We propose modifications to the Automatic Relevance Determination
	(ARD) algorithm for solving the EEG/MEG inverse problem when the
	activation map of the cortex is known to be sparse. We propose to
	include a term to account for the background noise activity, i.e.
	electric activity of sources not in the cortex. Also, we prune the
	results of the ARD algorithm using a Model Selection criterion to
	get sparser results. Simulations with a realistic head model show
	a very important reduction of the number of sources incorrectly detected
	as active.},
  timestamp = {2016-07-08T12:20:45Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 2010 Annual International 	Conference of the IEEE},
  author = {von Ellenrieder, N. and Hurtado, M. and Muravchik, C.H.},
  month = aug,
  year = {2010},
  keywords = {Algorithms,ARD algorithm,automatic relevance determination,background
	noise activity,Brain modeling,Cerebral Cortex,Computational modeling,Covariance
	matrix,Data models,EEG inverse problem,electroencephalography,Electromagnetic
	Fields,electromagnetic source imaging,Humans,inverse problems,magnetoencephalography,medical
	signal processing,MEG inverse problem,model selection criterion,Signal
	to noise ratio,sparse cortex activation map,sparse cortical activation
	patterns},
  pages = {4316-4319}
}

@inproceedings{Erdogan2013,
  title = {Group sparsity based sparse coding for region covariances},
  doi = {10.1109/SIU.2013.6531422},
  abstract = {In the recent years, there has been an increasing interest in using
	sparse representations for image processing and computer vision.
	The main reason behind their popularity is that they could provide
	a more robust and efficient way of reconstructing a target by means
	of a limited number of atoms in a dictionary. The common practice
	in sparse coding is to use dictionary atoms which live in Euclidean
	spaces. In recent years, some studies proposed to use region covariance
	based dictionary atoms to come up with more effective sparse coding
	schemes. The optimization schemes suggested by these studies are
	fundamentally different than those of the standard methods since
	covariance matrices live in a special Riemannian manifold. In this
	study, we propose to enrich such a sparse coding scheme proposed
	by Sivalingram et al. with a group sparsity constraint. The experimental
	results on a face recognition task reveals that considering group
	sparsity improves the recognition rate.},
  timestamp = {2016-07-08T12:40:56Z},
  booktitle = {Signal Processing and Communications Applications Conference (SIU), 	2013 21st},
  author = {Erdogan, H.T. and Erdem, E. and Erdem, A.},
  month = apr,
  year = {2013},
  keywords = {computer vision,covariance-based representations,covariance matrices,Covariance
	matrices,Dictionaries,Euclidean spaces,Euclidean
	spaces,face recognition,group sparsity,group
	sparsity based sparse coding,group sparsity based sparse
	coding,image coding,Image Processing,Image reconstruction,Image
	reconstruction,image representation,region covariances,region
	covariances,Robustness,Sparse Coding,Sparse
	coding,sparse representations,special Riemannian manifold,special
	Riemannian manifold,Standards},
  pages = {1-4},
  owner = {afdidehf}
}

@inproceedings{Eslahi2014,
  title = {Block compressed sensing images using Curvelet transform},
  doi = {10.1109/IranianCEE.2014.6999788},
  abstract = {Due to the optimal sparse representation of objects with edges by
	the multiscale and directional Curvelet Transform, its application
	have been increasingly interested over the past years. In this paper,
	we investigate how the block-based compressed sensing (BCS) can be
	improved to an efficient recovery algorithm, by employing the iterative
	Curvelet thresholding (ICT). Also, we consider two accelerated iterative
	shrinkage thresholding (IST) methods, including the following: 1)
	Beck and Teboulle's fast iterative shrinkage thresholding algorithm
	(FISTA); 2) Bioucas-Dias and Figueiredo's two-step iterative shrinkage
	thresholding (TwIST) algorithm, to increase the execution speed of
	the proposed methods rather than simple ICT. To compare our experimental
	results with the results of some other methods, we employ pick signal
	to noise ratio (PSNR) and structural similarity (SSIM) index as the
	quality assessor. Numerical results show good performance of the
	new proposed BCS using accelerated ICT methods, in terms of these
	two quality assessments.},
  timestamp = {2016-07-08T11:41:13Z},
  booktitle = {Electrical Engineering (ICEE), 2014 22nd Iranian Conference on},
  author = {Eslahi, N. and Aghagolzadeh, A. and Andargoli, S.M.H.},
  month = may,
  year = {2014},
  keywords = {Accelerated Iteratitive Shrinkage Thresholdig,BCS,block-based compressed
	sensing image,compressed sensing,curvelet transform,curvelet transforms,fast
	iterative shrinkage thresholding algorithm,FISTA algorithm,ICT methods,information
	and communication technology,Iterative Curvelet Thresholding,iterative
	methods,landweber iteration,PSNR,quality assessor,signal-to-noise
	ratio,Sparsity,SSIM index,structural similarity index,TwIST algorithm,two-step
	iterative shrinkage thresholding algorithm},
  pages = {1581-1586},
  owner = {afdidehf}
}

@inproceedings{Eslahi2014a,
  title = {Recovery of compressive video sensing via dictionary learning and 	forward prediction},
  doi = {10.1109/ISTEL.2014.7000819},
  abstract = {In this paper, we propose a new framework for compressive video sensing
	(CVS) that exploits the inherent spatial and temporal redundancies
	of a video sequence, effectively. The proposed method splits the
	video sequence into the key and non-key frames followed by dividing
	each frame into the small non-overlapping blocks of equal sizes.
	At the decoder side, the key frames are reconstructed using adaptively
	learned sparsifying (ALS) basis via �o minimization, in order to
	exploit the spatial redundancy. Also, three well-known dictionary
	learning algorithms are investigated in our method. For recovery
	of the non-key frames, a prediction of the current frame is initialized,
	by using the previous reconstructed frame, in order to exploit the
	temporal redundancy. The prediction is employed in a proper optimization
	problem to recover the current non-key frame. To compare our experimental
	results with the results of some other methods, we employ pick signal
	to noise ratio (PSNR) and structural similarity (SSIM) index as the
	quality assessor. The numerical results show the adequacy of our
	proposed method in CVS.},
  timestamp = {2016-07-10T07:48:22Z},
  booktitle = {Telecommunications (IST), 2014 7th International Symposium on},
  author = {Eslahi, N. and Aghagolzadeh, A. and Andargoli, S.M.H.},
  month = sep,
  year = {2014},
  keywords = {adaptively-learned sparsifying basis,ALS basis,compressed sensing,Compressive
	Video Sensing,compressive video sensing recovery,CVS,decoder,Decoding,Dictionaries,dictionary
	learning,dictionary learning algorithm,forward prediction,Image reconstruction,Image
	reconstruction,image sequences,inherent spatial redundancy,key frame
	reconstruction,Minimization,nonkey frame frame,nonoverlapping block,optimisation,optimization
	problem,PSNR,PSNR index,redundancy,Sensors,sparse recovery,Spatial/Temporal
	Redundancy,Split Bregman Iteration,SSIM index,structural similarity
	index,temporal redundancy,video coding,video sequence},
  pages = {833-838},
  owner = {afdidehf}
}

@inproceedings{Facchinei2014,
  title = {Flexible parallel algorithms for big data optimization},
  doi = {10.1109/ICASSP.2014.6854999},
  abstract = {We propose a decomposition framework for the parallel optimization
	of the sum of a differentiable function and a (block) separable nonsmooth,
	convex one. The latter term is typically used to enforce structure
	in the solution as, for example, in LASSO problems. Our framework
	is very flexible and includes both fully parallel Jacobi schemes
	and Gauss-Seidel (Southwell-type) ones, as well as virtually all
	possibilities in between (e.g., gradient- or Newton-type methods)
	with only a subset of variables updated at each iteration. Our theoretical
	convergence results improve on existing ones, and numerical results
	show that the new method compares favorably to existing algorithms.},
  timestamp = {2016-07-08T12:31:22Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Facchinei, F. and Sagratella, S. and Scutari, G.},
  month = may,
  year = {2014},
  keywords = {Approximation algorithms,Approximation methods,Big data,Big Data optimization,Convergence,decomposition
	framework,flexible parallel algorithms,fully parallel Jacobi schemes,Gauss-Seidel
	schemes,iterative methods,Jacobian matrices,Jacobi method,Lasso,LASSO
	problems,minimisation,Minimization,Optimization,Parallel algorithms,Parallel
	algorithms,parallel optimization,sparse solution},
  pages = {7208-7212},
  owner = {afdidehf}
}

@article{Facchinei2015,
  title = {Parallel Selective Algorithms for Nonconvex Big Data Optimization},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2399858},
  abstract = {We propose a decomposition framework for the parallel optimization
	of the sum of a differentiable (possibly nonconvex) function and
	a (block) separable nonsmooth, convex one. The latter term is usually
	employed to enforce structure in the solution, typically sparsity.
	Our framework is very flexible and includes both fully parallel Jacobi
	schemes and Gauss-Seidel (i.e., sequential) ones, as well as virtually
	all possibilities �in between� with only a subset of variables
	updated at each iteration. Our theoretical convergence results improve
	on existing ones, and numerical results on LASSO, logistic regression,
	and some nonconvex quadratic problems show that the new method consistently
	outperforms existing algorithms.},
  timestamp = {2016-07-10T07:25:25Z},
  number = {7},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Facchinei, F. and Scutari, G. and Sagratella, S.},
  month = apr,
  year = {2015},
  keywords = {Approximation methods,Big data,Convergence,decomposition framework,differentiable
	function,distributed methods,fully parallel Jacobi schemes,Gauss-Seidel
	schemes,Jacobian matrices,Jacobi method,Lasso,logistic regression,nonconvex
	big data optimization,nonconvex quadratic problems,optimisation,Optimization,Parallel
	algorithms,parallel optimization,parallel selective algorithms,regression
	analysis,separable nonsmooth convex function,Signal processing algorithms,sparse
	solution,Standards,variables selection,Vectors},
  pages = {1874-1889},
  owner = {afdidehf}
}

@incollection{Fadili2012,
  title = {Curvelets and Ridgelets},
  isbn = {978-1-4614-1799-6},
  language = {English},
  timestamp = {2016-07-08T12:09:47Z},
  booktitle = {Computational Complexity},
  publisher = {{\{Springer New York\}}},
  author = {Fadili, Jalal and Starck, Jean-Luc},
  editor = {Meyers, Robert A.},
  year = {2012},
  pages = {754-773},
  owner = {Fardin}
}

@article{Fadili2010,
  title = {Image Decomposition and Separation Using Sparse Representations: 	An Overview},
  volume = {98},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2009.2024776},
  abstract = {This paper gives essential insights into the use of sparsity and morphological
	diversity in image decomposition and source separation by reviewing
	our recent work in this field. The idea to morphologically decompose
	a signal into its building blocks is an important problem in signal
	processing and has far-reaching applications in science and technology.
	Starck , proposed a novel decomposition method-morphological component
	analysis (MCA)-based on sparse representation of signals. MCA assumes
	that each (monochannel) signal is the linear mixture of several layers,
	the so-called morphological components, that are morphologically
	distinct, e.g., sines and bumps. The success of this method relies
	on two tenets: sparsity and morphological diversity. That is, each
	morphological component is sparsely represented in a specific transform
	domain, and the latter is highly inefficient in representing the
	other content in the mixture. Once such transforms are identified,
	MCA is an iterative thresholding algorithm that is capable of decoupling
	the signal content. Sparsity and morphological diversity have also
	been used as a novel and effective source of diversity for blind
	source separation (BSS), hence extending the MCA to multichannel
	data. Building on these ingredients, we will provide an overview
	the generalized MCA introduced by the authors in and as a fast and
	efficient BSS method. We will illustrate the application of these
	algorithms on several real examples. We conclude our tour by briefly
	describing our software toolboxes made available for download on
	the Internet for sparse signal and image decomposition and separation.},
  timestamp = {2016-07-08T12:46:12Z},
  number = {6},
  journal = {Proceedings of the IEEE},
  author = {Fadili, M.J. and Starck, J.-L. and Bobin, J. and Moudden, Y.},
  month = jun,
  year = {2010},
  keywords = {Application software,blind source separation,Image decomposition,Image
	decomposition,Image Processing,Image
	Processing,Independent component analysis,Internet,Iterative algorithms,Iterative
	algorithms,iterative thresholding algorithm,iterative
	thresholding algorithm,morphological component analysis,morphological component
	analysis,Signal analysis,signal content decoupling,signal
	content decoupling,signal processing,Signal processing algorithms,Signal
	processing algorithms,Software tools,Software
	tools,source separation,sparse representation,sparse
	representation,sparse representations,sparse signal decomposition,sparse
	signal decomposition},
  pages = {983-994},
  owner = {afdidehf}
}

@book{Faloutsos2010,
  edition = {1},
  series = {Lecture Notes in Computer Science 6321 : Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, 	Part I},
  isbn = {3-642-15879-X 978-3-642-15879-7},
  timestamp = {2016-10-24T16:39:47Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Faloutsos, Christos},
  editor = {Balc�zar, Jos� Luis and Bonchi, Francesco and Gionis, Aristides and Sebag, Mich�le},
  year = {2010},
  owner = {Fardin}
}

@inproceedings{Fang2012,
  title = {Block-sparsity pattern recovery from noisy observations},
  doi = {10.1109/ICASSP.2012.6288626},
  abstract = {We study the problem of recovering the sparsity pattern of block-sparse
	signals from noise-corrupted measurements. A simple, efficient recovery
	method, namely, a block-version of the orthogonal matching pursuit
	(OMP) method, is considered in this paper and its behavior for recovering
	the block-sparsity pattern is analyzed. We provide sufficient conditions
	under which the block-version of the OMP can successfully recover
	the block-sparse representations in the presence of noise. Our analysis
	reveals that exploiting block-sparsity can improve the recovery ability
	and lead to a guaranteed recovery for a higher sparsity level. Numerical
	results are presented to corroborate our theoretical claim.},
  timestamp = {2016-07-08T11:48:19Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International 	Conference on},
  author = {Fang, Jun and Li, Hongbin},
  month = mar,
  year = {2012},
  keywords = {block sparse representations,block sparse signals,Block-sparsity,block
	sparsity pattern recovery,Coherence,compressed sensing,Dictionaries,iterative
	methods,Matching pursuit algorithms,Noise,noise corrupted measurements,Noise
	measurement,noisy observations,orthogonal matching pursuit,Pollution
	measurement,recovery ability,signal representation,sparsity level,Vectors},
  pages = {3321-3324},
  owner = {afdidehf}
}

@article{Fang2015,
  title = {Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse 	Signals},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2014.2375133},
  abstract = {We consider the problem of recovering block-sparse signals whose cluster
	patterns are unknown a priori. Block-sparse signals with nonzero
	coefficients occurring in clusters arise naturally in many practical
	scenarios. However, the knowledge of the block partition is usually
	unavailable in practice. In this paper, we develop a new sparse Bayesian
	learning method for recovery of block-sparse signals with unknown
	cluster patterns. A pattern-coupled hierarchical Gaussian prior is
	introduced to characterize the pattern dependencies among neighboring
	coefficients, where a set of hyperparameters are employed to control
	the sparsity of signal coefficients. The proposed hierarchical model
	is similar to that for the conventional sparse Bayesian learning.
	However, unlike the conventional sparse Bayesian learning framework
	in which each individual hyperparameter is associated independently
	with each coefficient, in this paper, the prior for each coefficient
	not only involves its own hyperparameter, but also its immediate
	neighbor hyperparameters. In doing this way, the sparsity patterns
	of neighboring coefficients are related to each other and the hierarchical
	model has the potential to encourage structured-sparse solutions.
	The hyperparameters are learned by maximizing their posterior probability.
	We exploit an expectation-maximization (EM) formulation to develop
	an iterative algorithm that treats the signal as hidden variables
	and iteratively maximizes a lower bound on the posterior probability.
	In the M-step, a simple suboptimal solution is employed to replace
	a gradient-based search to maximize the lower bound. Numerical results
	are provided to illustrate the effectiveness of the proposed algorithm.},
  timestamp = {2016-07-10T07:31:38Z},
  number = {2},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Fang, J. and Shen, Y. and Li, H. and Wang, P.},
  month = jan,
  year = {2015},
  keywords = {Bayes methods,Block-sparse signal recovery,Clustering algorithms,Electronic
	mail,iterative methods,Partitioning algorithms,pattern-coupled hierarchical
	model,Signal processing algorithms,sparse Bayesian learning,Vectors},
  pages = {360-372},
  owner = {afdidehf}
}

@inproceedings{Fang2012a,
  title = {Graph-Oriented Learning via Automatic Group Sparsity for Data Analysis},
  doi = {10.1109/ICDM.2012.82},
  abstract = {The key task in graph-oriented learning is constructing an informative
	graph to model the geometrical and discriminant structure of a data
	manifold. Since traditional graph construction methods are sensitive
	to noise and less datum-adaptive to changes in density, a new graph
	construction method so-called ?1-Graph has been proposed [1] recently.
	A graph construction method needs to have two important properties:
	sparsity and locality. However, the ?1-Graph is strong in sparsity
	property, but weak in locality. In order to overcome such limitation,
	we propose a new method of constructing an informative graph using
	automatic group sparse regularization based on the work of ?1-Graph,
	which is called as group sparse graph (GroupSp-Graph). The newly
	developed GroupSp-Graph has the same noise-insensitive property as
	?1-Graph, and also can successively preserve the group and local
	information in the graph. In other words, the proposed group sparse
	graph has both properties of sparsity and locality simultaneously.
	Furthermore, we integrate the proposed graph with several graph-oriented
	learning algorithms: spectral embedding, spectral clustering, subspace
	learning and manifold regularized non-negative matrix factorization.
	The empirical studies on benchmark data sets show that the proposed
	algorithms achieve considerable improvement over classic graph constructing
	methods and the ?1-Graph method in various learning task.},
  timestamp = {2016-07-08T12:38:40Z},
  booktitle = {Data Mining (ICDM), 2012 IEEE 12th International Conference on},
  author = {Fang, Yuqiang and Wang, Ruili and Dai, Bin},
  month = dec,
  year = {2012},
  keywords = {?1-graph,automatic group sparse regularization,automatic group sparsity,Clustering
	algorithms,data analysis,data manifold,discriminant structure,Educational
	institutions,Equations,geometrical structure,Geometry,graph construction,graph
	learning,graph-oriented learning,Graph theory,group sparse graph,GroupSp-graph,informative
	graph,Laplace equations,learning (artificial intelligence),learning
	task,manifold regularized nonnegative matrix factorization,Manifolds,Matrix
	decomposition,Noise,noise-insensitive property,non-negative matrix
	factoriza-tion,pattern clustering,sparse matrices,sparse representation,sparsity
	property,spectral clustering,spectral embedding,subspace learning},
  pages = {251-259},
  owner = {afdidehf}
}

@article{Farjoun2008,
  title = {MATLAB Tutorial: Advanced matrix operations: Sparse matrices},
  timestamp = {2016-07-09T20:08:16Z},
  author = {Farjoun, Yossi},
  year = {2008},
  owner = {Fardin}
}

@phdthesis{Feldman2003,
  title = {Decoding Error-Correcting Codes via Linear Programming},
  abstract = {Error-correcting codes are fundamental tools used to transmit digital
	information over unreliable channels. Their study goes back to the
	work of Hamming [Ham50] and Shannon [Sha48], who used them as the
	basis for the field of information theory. The problem of decoding
	the original information up to the full error-correcting potential
	of the system is often very complex, especially for modern codes
	that approach the theoretical limits of the communication channel.
	In this thesis we investigate the application of linear programming
	(LP) relaxation to the problem of decoding an error-correcting code.
	Linear programming relaxation is a standard technique in approximation
	algorithms and operations research, and is central to the study of
	efficient algorithms to find good (albeit suboptimal) solutions to
	very difficult optimization problems. Our new "LP decoders" have
	tight combinatorial characterizations of decoding success that can
	be used to analyze error-correcting performance. Furthermore, LP
	decoders have the desirable (and rare) property that whenever they
	output a result, it is guaranteed to be the optimal result: the most
	likely (ML) information sent over the channel. We refer to this property
	as the ML certificate property. We provide specific LP decoders for
	two major families of codes: turbo codes and low-density parity-check
	(LDPC) codes. These codes have received a great deal of attention
	recently due to their unprecedented error-correcting performance.(cont.)
	Our decoder is particularly attractive for analysis of these codes
	because the standard message-passing algorithms used for decoding
	are often difficult to analyze. For turbo codes, we give a relaxation
	very close to min-cost flow, and show that the success of the decoder
	depends on the costs in a certain residual graph. For the case of
	rate-1/2 repeat-accumulate codes (a certain type of turbo code),
	we give an inverse polynomial upper bound on the probability of decoding
	failure. For LDPC codes (or any binary linear code), we give a relaxation
	based on the factor graph representation of the code. We introduce
	the concept of fractional distance, which is a function of the relaxation,
	and show that LP decoding always corrects a number of errors up to
	half the fractional distance. We show that the fractional distance
	is exponential in the girth of the factor graph. Furthermore, we
	give an efficient algorithm to compute this fractional distance.
	We provide experiments showing that the performance of our decoders
	are comparable to the standard message-passing decoders. We also
	give new provably convergent message-passing decoders based on linear
	programming duality that have the ML certificate property.},
  timestamp = {2016-07-08T12:10:08Z},
  author = {Feldman, Jon},
  year = {2003},
  annote = {http://hdl.handle.net/1721.1/42831 Massachusetts Institute of Technology.
	Dept. of Electrical Engineering and Computer Science.},
  annote = {http://hdl.handle.net/1721.1/42831 Massachusetts Institute of Technology.
	Dept. of Electrical Engineering and Computer Science.},
  annote = {http://hdl.handle.net/1721.1/42831 Massachusetts Institute of Technology.
	Dept. of Electrical Engineering and Computer Science.},
  annote = {http://hdl.handle.net/1721.1/42831 Massachusetts Institute of Technology.Dept.
	of Electrical Engineering and Computer Science.},
  annote = {http://hdl.handle.net/1721.1/42831 Massachusetts Institute of Technology.	Dept. of Electrical Engineering and Computer Science.},
  owner = {afdidehf}
}

@article{Feldman2007,
  title = {LP Decoding Corrects a Constant Fraction of Errors},
  volume = {53},
  issn = {0018-9448},
  doi = {10.1109/TIT.2006.887523},
  abstract = {We show that for low-density parity-check (LDPC) codes whose Tanner
	graphs have sufficient expansion, the linear programming (LP) decoder
	of Feldman, Karger, and Wainwright can correct a constant fraction
	of errors. A random graph will have sufficient expansion with high
	probability, and recent work shows that such graphs can be constructed
	efficiently. A key element of our method is the use of a dual witness:
	a zero-valued dual solution to the decoding linear program whose
	existence proves decoding success. We show that as long as no more
	than a certain constant fraction of the bits are flipped by the channel,
	we can find a dual witness. This new method can be used for proving
	bounds on the performance of any LP decoder, even in a probabilistic
	setting. Our result implies that the word error rate of the LP decoder
	decreases exponentially in the code length under the binary-symmetric
	channel (BSC). This is the first such error bound for LDPC codes
	using an analysis based on "pseudocodewords." Recent work by Koetter
	and Vontobel shows that LP decoding and min-sum decoding of LDPC
	codes are closely related by the "graph cover" structure of their
	pseudocodewords; in their terminology, our result implies that that
	there exist families of LDPC codes where the minimum BSC pseudoweight
	grows linearly in the block length},
  timestamp = {2016-09-29T15:34:19Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Feldman, J. and Malkin, T. and Servedio, R.A. and Stein, C. and Wainwright, M.J.},
  month = jan,
  year = {2007},
  keywords = {binary-symmetric channel,BSC,channel coding,constant fraction,constant
	fraction,Decoding,Engineering profession,Engineering
	profession,Error analysis,Error correction,error correction codes,Error
	correction codes,factor graphs,factor
	graphs,graph cover structure,Graph theory,Industrial engineering,Industrial
	engineering,Iterative decoding,Iterative
	decoding,LDPC,linear programming,linear programming decoder,linear
	programming decoder,low-density parity-check code,low-density
	parity-check code,low-density parity-check (LDPC) codes,low-density parity-check
	(LDPC) codes,message passing,minsum decoding,minsum
	decoding,Operations research,parity check codes,parity
	check codes,Performance analysis,Performance
	analysis,probabilistic setting,probabilistic
	setting,probability,pseudocodeword,random graph,random
	graph,Tanner graph,Tanner graphs,Tanner
	graphs,zero-valued dual solution},
  pages = {82--89},
  owner = {afdidehf}
}

@article{Feldman2005,
  title = {LP Decoding Achieves Capacity},
  abstract = {We give a linear programming (LP) decoder that achieves the capacity
	(optimal rate) of a wide range of probabilistic binary communication
	channels. This is the first such result for LP decoding. More generally,
	as far as the authors are aware this is the first known polynomial-time
	capacity-achieving decoder with the maximum-likelihood (ML) certificate
	property---where output codewords come with a proof of optimality.
	Additionally, this result extends the capacity-achieving property
	of expander codes beyond the binary symmetric channel to a larger
	family of communication channels.Perhaps most importantly, since
	LP decoding performs well in practice on turbo codes and low-density
	parity-check (LDPC) codes (comparable to the popular "belief propagation"
	algorithm), this result exhibits the power of a new, widely applicable
	"dual witness" technique (Feldman, Malkin, Servedio, Stein and Wainwright,
	ISIT '04) for bounding decoder performance.For expander codes over
	an adversarial channel, we prove that LP decoding corrects a constant
	fraction of errors. To show this, we provide a new combinatorial
	characterization of error events that is of independent interest,
	and which we expect will lead to further improvements.},
  timestamp = {2016-07-09T19:58:58Z},
  author = {Feldman, Jon and Stein, Cliff},
  year = {2005},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Columbia University},
  owner = {afdidehf}
}

@article{Feldman2005a,
  title = {Using linear programming to Decode Binary linear codes},
  volume = {51},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.842696},
  abstract = {A new method is given for performing approximate maximum-likelihood
	(ML) decoding of an arbitrary binary linear code based on observations
	received from any discrete memoryless symmetric channel. The decoding
	algorithm is based on a linear programming (LP) relaxation that is
	defined by a factor graph or parity-check representation of the code.
	The resulting "LP decoder" generalizes our previous work on turbo-like
	codes. A precise combinatorial characterization of when the LP decoder
	succeeds is provided, based on pseudocodewords associated with the
	factor graph. Our definition of a pseudocodeword unifies other such
	notions known for iterative algorithms, including "stopping sets,"
	"irreducible closed walks," "trellis cycles," "deviation sets," and
	"graph covers." The fractional distance dfrac of a code is introduced,
	which is a lower bound on the classical distance. It is shown that
	the efficient LP decoder will correct up to ?dfrac/2?-1 errors and
	that there are codes with dfrac=?(n1-?). An efficient algorithm to
	compute the fractional distance is presented. Experimental evidence
	shows a similar performance on low-density parity-check (LDPC) codes
	between LP decoding and the min-sum and sum-product algorithms. Methods
	for tightening the LP relaxation to improve performance are also
	provided.},
  timestamp = {2016-09-29T15:27:58Z},
  number = {3},
  journal = {Information Theory, IEEE Transactions on},
  author = {Feldman, J. and Wainwright, M.J. and Karger, D.R.},
  month = mar,
  year = {2005},
  keywords = {arbitrary binary linear code,Associate members,Belief propagation
	(BP),binary codes,combinatorial characterization,Computer science,deviation
	set,discrete memoryless symmetric channel,discrete systems,Error
	correction codes,factor graph,fractional distance,Graph theory,irreducible
	closed walk,iterative algorithm,Iterative algorithms,Iterative decoding,LDPC
	code,Linear code,linear codes,linear programming,linear
	programming (LP),linear programming
	(LP),low-density parity-check code,low-density parity-check
	(LDPC) codes,low-density parity-check (LDPC)
	codes,LP decoder,LP decoding,maximum likelihood decoding,maximum-likelihood
	decoding,memoryless systems,minimum distance,min-sum-sum-product
	algorithm,parity check codes,Performance analysis,pseudocodeword,pseudocodewords,stopping
	set,trellis codes,trellis cycle,turbo codes,turbo-like code},
  pages = {954--972},
  owner = {afdidehf}
}

@article{Fevotte2015,
  title = {Nonlinear Hyperspectral Unmixing With Robust Nonnegative Matrix Factorization},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2468177},
  abstract = {We introduce a robust mixing model to describe hyperspectral data
	resulting from the mixture of several pure spectral signatures. The
	new model extends the commonly used linear mixing model by introducing
	an additional term accounting for possible nonlinear effects, that
	are treated as sparsely distributed additive outliers. With the standard
	nonnegativity and sum-to-one constraints inherent to spectral unmixing,
	our model leads to a new form of robust nonnegative matrix factorization
	with a group-sparse outlier term. The factorization is posed as an
	optimization problem, which is addressed with a block-coordinate
	descent algorithm involving majorization-minimization updates. Simulation
	results obtained on synthetic and real data show that the proposed
	strategy competes with the state-of-the-art linear and nonlinear
	unmixing methods.},
  timestamp = {2016-07-10T06:59:36Z},
  number = {12},
  journal = {Image Processing, IEEE Transactions on},
  author = {Fevotte, C. and Dobigeon, N.},
  month = dec,
  year = {2015},
  keywords = {Approximation methods,block-coordinate descent algorithm,Extraterrestrial
	measurements,geophysical image processing,group-sparse outlier term,group-sparsity,group
	theory,hyperspectral data,Hyperspectral imagery,Hyperspectral imaging,Hyperspectral
	imaging,linear programming,majorization-minimization updates,Matrix
	decomposition,minimisation,nonlinear hyperspectral unmixing method,nonlinear
	unmixing,Optimization,robust mixing model,Robustness,robust nonnegative matrix factorization,robust nonnegative
	matrix factorization,sparsely
	distributed additive outliers,spectral signatures},
  pages = {4810-4819},
  owner = {afdidehf}
}

@article{Fiori2013,
  title = {Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching},
  abstract = {Graph matching is a challenging problem with very important applications
	in a wide range of fields, from image and video analysis to biological
	and biomedical problems. We propose a robust graph matching algorithm
	inspired in sparsityrelated techniques. We cast the problem, resembling
	group or collaborative sparsity formulations, as a non-smooth convex
	optimization problem that can be efficiently solved using augmented
	Lagrangian techniques. The method can deal with weighted or unweighted
	graphs, as well as multimodal data, where different graphs represent
	different types of data. The proposed approach is also naturally
	integrated with collaborative graph inference techniques, solving
	general network inference problems where the observed variables,
	possibly coming from different modalities, are not in correspondence.
	The algorithm is tested and compared with state-of-the-art graph
	matching techniques in both synthetic and real graphs. We also present
	results on multimodal graphs and applications to collaborative inference
	of brain connectivity from alignment-free functional magnetic resonance
	imaging (fMRI) data. The code is publicly available.},
  timestamp = {2016-07-10T08:08:22Z},
  journal = {ArXiv e-prints},
  author = {Fiori, M. and Sprechmann, P. and Vogelstein, J. and Mus{\'e}, P. and Sapiro, G.},
  month = nov,
  year = {2013},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics
	- Machine Learning},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1311.6425F},
  archiveprefix = {arXiv},
  owner = {Fardin},
  primaryclass = {math.OC}
}

@inproceedings{Firat2014,
  title = {Deep learning for brain decoding},
  doi = {10.1109/ICIP.2014.7025563},
  abstract = {Learning low dimensional embedding spaces (manifolds) for efficient
	feature representation is crucial for complex and high dimensional
	input spaces. Functional magnetic resonance imaging (fMRI) produces
	high dimensional input data and with a less then ideal number of
	labeled samples for a classification task. In this study, we explore
	deep learning methods for fMRI classification tasks in order to reduce
	dimensions of feature space, along with improving classification
	performance for brain decoding. We employ sparse autoencoders for
	unsupervised feature learning, leveraging unlabeled fMRI data to
	learn efficient, non-linear representations as the building blocks
	of a deep learning architecture by stacking them. Proposed method
	is tested on a memory encoding/retrieval experiment with ten classes.
	The results support the efficiency compared to the baseline multi-voxel
	pattern analysis techniques.},
  timestamp = {2016-07-08T12:10:25Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Firat, O. and Oztekin, L. and Yarman Vural, F.T.},
  month = oct,
  year = {2014},
  keywords = {baseline multi-voxel pattern analysis techniques,biomedical MRI,Brain,brain
	decoding,brain state decoding,complex input spaces,Computer architecture,Decoding,Deep
	Learning,deep learning architecture,deep learning methods,efficient
	feature representation,encoding,feature extraction,feature space
	dimension,fMRI,fMRI classification tasks,functional magnetic resonance
	imaging,high dimensional input data,high dimensional input spaces,Image
	classification,low dimensional embedding spaces,Magnetic resonance
	imaging,Manifolds,medical image processing,memory encoding,memory
	retrieval,MVPA,neurophysiology,nonlinear representations,Pattern
	analysis,sample classification,sparse autoencoders,Stacked Autoencoders,unlabeled
	fMRI data,unsupervised feature learning,unsupervised learning},
  pages = {2784-2788},
  owner = {afdidehf}
}

@article{Fletcher2009,
  title = {Necessary and Sufficient Conditions for Sparsity Pattern Recovery},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2032726},
  abstract = {The paper considers the problem of detecting the sparsity pattern
	of a k -sparse vector in BBR n from m random noisy measurements.
	A new necessary condition on the number of measurements for asymptotically
	reliable detection with maximum-likelihood (ML) estimation and Gaussian
	measurement matrices is derived. This necessary condition for ML
	detection is compared against a sufficient condition for simple maximum
	correlation (MC) or thresholding algorithms. The analysis shows that
	the gap between thresholding and ML can be described by a simple
	expression in terms of the total signal-to-noise ratio (SNR), with
	the gap growing with increasing SNR. Thresholding is also compared
	against the more sophisticated Lasso and orthogonal matching pursuit
	(OMP) methods. At high SNRs, it is shown that the gap between Lasso
	and OMP over thresholding is described by the range of powers of
	the nonzero component values of the unknown signals. Specifically,
	the key benefit of Lasso and OMP over thresholding is the ability
	of Lasso and OMP to detect signals with relatively small components.},
  timestamp = {2016-07-10T06:48:53Z},
  number = {12},
  journal = {Information Theory, IEEE Transactions on},
  author = {Fletcher, A.K. and Rangan, S. and Goyal, V.K.},
  month = dec,
  year = {2009},
  keywords = {Additive noise,asymptotically reliable detection,compressed sensing,convex
	optimization,Gaussian measurement matrices,Gaussian processes,k-sparse
	vector,Lasso,Matching pursuit algorithms,matrix algebra,maximum correlation,maximum
	likelihood detection,Maximum likelihood estimation,maximum-likelihood
	estimation,maximum-likelihood (ML) estimation,Noise level,Noise measurement,orthogonal
	matching pursuit,orthogonal matching pursuit (OMP),Random matrices,random
	noisy measurement,random projections,signal detection,signal processing,signal-to-noise
	ratio,sparse approximation,sparse matrices,sparsity pattern recovery,subset
	selection,Sufficient conditions,thresholding,thresholding algorithm,Vectors},
  pages = {5758-5772},
  owner = {Fardin}
}

@inproceedings{Fletcher2005,
  title = {Analysis of denoising by sparse approximation with random frame asymptotics},
  doi = {10.1109/ISIT.2005.1523636},
  abstract = {If a signal x is known to have a sparse representation with respect
	to a frame, the signal can be estimated from a noise-corrupted observation
	y by finding the best sparse approximation to y. This paper analyzes
	the mean squared error (MSE) of this denoising scheme and the probability
	that the estimate has the same sparsity pattern as the original signal.
	The first main result is an MSE bound that depends on a new bound
	on approximating a Gaussian signal as a linear combination of elements
	of an overcomplete dictionary. This bound may be of independent interest
	for source coding. Further analyses are for dictionaries generated
	randomly according to a spherically-symmetric distribution and signals
	expressible with single dictionary elements. Easily-computed approximations
	for the probability of selecting the correct dictionary element and
	the MSE are given. In the limit of large dimension, these approximations
	have simple forms. The asymptotic expressions reveal a critical input
	signal-to-noise ratio (SNR) for signal recovery},
  timestamp = {2016-07-08T10:28:45Z},
  booktitle = {Information Theory, 2005. ISIT 2005. Proceedings. International Symposium 	on},
  author = {Fletcher, A.K. and Rangan, S. and Goyal, V.K. and Ramchandran, K.},
  month = sep,
  year = {2005},
  keywords = {denoising scheme,Dictionaries,Estimation error,Gaussian processes,Gaussian
	signal,Geometry,mean squared error,mean square error methods,noise-corrupted
	observation,Noise reduction,Pattern analysis,random frame asymptotics,Signal
	analysis,signal denoising,Signal generators,signal recovery,Signal-To-Noise Ratio,Signal to noise ratio,Signal
	to noise ratio,signal-to-noise
	ratio,Solid modeling,source coding,source
	coding,sparse approximation,sparse
	approximation,spherically-symmetric distribution},
  pages = {1706-1710},
  owner = {Fardin}
}

@article{Fletcher2006,
  title = {Denoising by Sparse Approximation: Error Bounds Based on Rate-Distortion 	Theory},
  volume = {2006},
  doi = {10.1155/ASP/2006/26318},
  timestamp = {2016-09-29T16:12:18Z},
  number = {26318},
  journal = {EURASIP Journal on Applied Signal Processing},
  author = {Fletcher, Alyson K. and Rangan, Sundeep and Goyal, Vivek K. and Ramchandran, Kannan},
  year = {2006},
  pages = {1--19},
  owner = {Fardin}
}

@article{Folland1997,
  title = {The uncertainty principle: A mathematical survey},
  volume = {3},
  issn = {1069-5869},
  doi = {10.1007/BF02649110},
  abstract = {We survey various mathematical aspects of the uncertainty principle,
	including Heisenberg�s inequality and its variants, local uncertainty
	inequalities, logarithmic uncertainty inequalities, results relating
	to Wigner distributions, qualitative uncertainty principles, theorems
	on approximate concentration, and decompositions of phase space.},
  language = {English},
  timestamp = {2016-07-11T17:05:55Z},
  number = {3},
  journal = {Journal of Fourier Analysis and Applications},
  author = {Folland, Gerald B. and Sitaram, Alladi},
  year = {1997},
  keywords = {Fourier transform,Heisenberg's inequality,Landau-Pollak-Slepian theory,logarithmic
	inequalities,phase space,uncertainty principle,Wigner distribution},
  pages = {207-238},
  owner = {afdidehf}
}

@inproceedings{Forero2014,
  title = {Broadband underwater source localization via multitask learning},
  doi = {10.1109/CISS.2014.6814098},
  abstract = {Passive sonar is an attractive technology for stealthy underwater
	source localization. Notwithstanding its appeal, passive-sonar-based
	localization is challenging due to the complexities of underwater
	acoustic propagation. This work casts broadband underwater source
	localization as a multitask learning (MTL) problem, where each task
	refers to a robust sparse signal approximation problem over a single
	frequency. MTL provides a framework for exchanging information across
	the individual regression problems and constructing an aggregate
	(across frequencies) source localization map. Efficient algorithms
	based on block coordinate descent are developed for solving the localization
	problem. Numerical tests on the SWellEX-3 dataset illustrate and
	compare the localization performance of the proposed algorithm to
	the one of competitive alternatives.},
  timestamp = {2016-07-08T11:49:41Z},
  booktitle = {Information Sciences and Systems (CISS), 2014 48th Annual Conference 	on},
  author = {Forero, P.A.},
  month = mar,
  year = {2014},
  keywords = {acoustic signal processing,block coordinate descent,block coordinate
	descent,broadband underwater source localization,Convex functions,group
	sparsity,multi-task learning,multitask learning problem,passive sonar,regression
	problems,robust sparse signal approximation problem,sonar,Sonar equipment,source
	localization map,stealthy underwater source localization,underwater
	acoustic propagation,underwater sound,Underwater source localization,Vectors,Weaving},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Forero2015,
  title = {Structured outlier models for robust dictionary learning},
  doi = {10.1109/CISS.2015.7086814},
  abstract = {Robust dictionary learning algorithms seek to learn a dictionary while
	being robust to the presence of outliers in the training set. Often,
	the elements of the training set have an underlying structure due
	to, for example, their spatial relation or their similarity. When
	outliers are present as elements of the training set, they often
	inherit the underlying structure of the training set. This work capitalizes
	on such structure, encoded as an undirected graph connecting elements
	of the training set, and on sparsity-aware outlier modeling tools
	to develop robust dictionary learning algorithms. Not only do these
	algorithms yield a robust dictionary, but they also identify the
	outliers in the training set. Computationally efficient algorithms
	based on block coordinate descent and proximal gradient methods are
	developed.},
  timestamp = {2016-07-11T16:57:30Z},
  booktitle = {Information Sciences and Systems (CISS), 2015 49th Annual Conference 	on},
  author = {Forero, P.A. and Shafer, S. and Harguess, J.},
  month = mar,
  year = {2015},
  keywords = {block coordinate descent method,Convergence,Dictionaries,dictionary
	learning,Graph theory,Laplace equations,Laplacian regularization,learning
	(artificial intelligence),Optimization,proximal gradient algorithms,proximal
	gradient method,robust dictionary learning algorithm,Robustness,sparse
	matrices,sparsity-aware outlier modeling tool,structured outlier
	model,Training,undirected graph},
  pages = {1-6},
  owner = {afdidehf}
}

@incollection{Foucart2012,
  series = {Springer Proceedings in Mathematics},
  title = {Sparse Recovery Algorithms: Sufficient Conditions in Terms of Restricted 	Isometry Constants},
  volume = {13},
  isbn = {978-1-4614-0771-3},
  language = {English},
  timestamp = {2016-07-11T16:50:39Z},
  booktitle = {Approximation Theory XIII: San Antonio 2010},
  publisher = {{\{Springer New York\}}},
  author = {Foucart, Simon},
  editor = {Neamtu, Marian and Schumaker, Larry},
  year = {2012},
  pages = {65-77},
  owner = {Fardin}
}

@article{Foucart2010a,
  title = {Real versus complex null space properties for sparse vector recovery},
  volume = {348},
  doi = {10.1016/j.crma.2010.07.024},
  timestamp = {2016-10-24T16:21:09Z},
  number = {15-16},
  journal = {Comptes rendus de l'académie des sciences, Mathématiques},
  author = {Foucart, Simon and Gribonval, Rémi},
  month = aug,
  year = {2010},
  pages = {863--865},
  hal_id = {inria-00539612},
  hal_version = {v1},
  owner = {afdidehf}
}

@article{Foucart2015,
  title = {Sparse disjointed recovery from noninflating measurements},
  volume = {39},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2015.04.005},
  abstract = {Abstract We investigate the minimal number of linear measurements
	needed to recover sparse disjointed vectors robustly in the presence
	of measurement error. First, we analyze an iterative hard thresholding
	algorithm relying on a dynamic program computing sparse disjointed
	projections to upper-bound the order of the minimal number of measurements.
	Next, we show that this order cannot be reduced by any robust algorithm
	handling noninflating measurements. As a consequence, we conclude
	that there is no benefit in knowing the simultaneity of sparsity
	and disjointedness over knowing only one of these structures.},
  timestamp = {2016-07-11T16:48:36Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Foucart, Simon and Minner, Michael F. and Needham, Tom},
  year = {2015},
  keywords = {Compressive,Disjointed,Dynamic,hard,isometry,Iterative,matrices,Models,programming,property,Restricted,sensing,Simultaneously,Sparse,structured,Subgaussian,thresholding,Vectors},
  pages = {558 - 567},
  owner = {afdidehf}
}

@article{Foygel2014,
  title = {Corrupted Sensing: Novel Guarantees for Separating Structured Signals},
  volume = {60},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2293654},
  abstract = {We study the problem of corrupted sensing, a generalization of compressed
	sensing in which one aims to recover a signal from a collection of
	corrupted or unreliable measurements. While an arbitrary signal cannot
	be recovered in the face of arbitrary corruption, tractable recovery
	is possible when both signal and corruption are suitably structured.
	We quantify the relationship between signal recovery and two geometric
	measures of structure, the Gaussian complexity of a tangent cone,
	and the Gaussian distance to a subdifferential. We take a convex
	programming approach to disentangling signal and corruption, analyzing
	both penalized programs that tradeoff between signal and corruption
	complexity, and constrained programs that bound the complexity of
	signal or corruption when prior information is available. In each
	case, we provide conditions for exact signal recovery from structured
	corruption and stable signal recovery from structured corruption
	with added unstructured noise. Our simulations demonstrate close
	agreement between our theoretical recovery bounds and the sharp phase
	transitions observed in practice. In addition, we provide new interpretable
	bounds for the Gaussian complexity of sparse vectors, block-sparse
	vectors, and low-rank matrices, which lead to sharper guarantees
	of recovery when combined with our results and those in the literature.},
  timestamp = {2016-07-08T12:05:56Z},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Foygel, R. and Mackey, L.},
  month = feb,
  year = {2014},
  keywords = {$ell_{1}$ minimization,arbitrary corruption,arbitrary signal,atomic
	norms,block sparse vectors,block sparsity,Complexity theory,compressed sensing,compressed
	sensing,convex programming approach,corrupted
	sensing,Deconvolution,Error correction,Gaussian complexity,Gaussian
	distance,Gaussian processes,Geometry,low rank,low rank matrices,Noise,Noise
	measurement,Sensors,signal recovery,signal restoration,source separation,sparse
	matrices,Sparsity,structured corruption,structured signal,structured
	signals,tangent cone,tractable recovery,Vectors},
  pages = {1223-1247},
  owner = {afdidehf}
}

@article{Friedman2010,
  title = {A note on the group lasso and a sparse group lasso},
  abstract = {We consider the group lasso penalty for the linear model. We note
	that the standard algorithm for solving the problem assumes that
	the model matrices in each group are orthonormal. Here we consider
	a more general penalty that blends the lasso (L1) with the group
	lasso (\two-norm"). This penalty yields solutions that are sparse
	at both the group and individual feature levels. We derive an ecient
	algorithm for the resulting convex problem based on coordinate descent.
	This algorithm can also be used to solve the general form of the
	group lasso, with non-orthonormal model matrices.},
  timestamp = {2016-07-08T11:25:34Z},
  journal = {ArXiv e-prints},
  author = {Friedman, J. and Hastie, T. and Tibshirani, R.},
  month = jan,
  year = {2010},
  keywords = {-,Mathematics,Statistics},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1001.0736F},
  archiveprefix = {arXiv},
  owner = {Fardin}
}

@article{Friston2008,
  title = {Multiple sparse priors for the M/EEG inverse problem},
  volume = {39},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2007.09.048},
  abstract = {This paper describes an application of hierarchical or empirical Bayes
	to the distributed source reconstruction problem in electro- and
	magnetoencephalography (EEG and MEG). The key contribution is the
	automatic selection of multiple cortical sources with compact spatial
	support that are specified in terms of empirical priors. This obviates
	the need to use priors with a specific form (e.g., smoothness or
	minimum norm) or with spatial structure (e.g., priors based on depth
	constraints or functional magnetic resonance imaging results). Furthermore,
	the inversion scheme allows for a sparse solution for distributed
	sources, of the sort enforced by equivalent current dipole (ECD)
	models. This means the approach automatically selects either a sparse
	or a distributed model, depending on the data. The scheme is compared
	with conventional applications of Bayesian solutions to quantify
	the improvement in performance.},
  timestamp = {2016-07-09T20:17:11Z},
  number = {3},
  journal = {NeuroImage},
  author = {Friston, Karl and Harrison, Lee and Daunizeau, Jean and Kiebel, Stefan and Phillips, Christophe and Trujillo-Barreto, Nelson and Henson, Richard and Flandin, Guillaume and Mattout, J�r�mie},
  year = {2008},
  keywords = {Automatic relevance,determination,Expectation maximization,Free energy,Model
	selection,Restricted maximum likelihood,Sparse priors,Variational
	Bayes},
  pages = {1104 - 1120},
  owner = {afdidehf}
}

@article{Friston2006,
  title = {Bayesian estimation of evoked and induced responses},
  volume = {27},
  abstract = {We describe an extension of our empirical Bayes approach to magnetoencephalography/
	electroencephalography (MEG/EEG) source reconstruction that covers
	both evoked and induced responses. The estimation scheme is based
	on classical covariance component estimation using restricted maximum
	likelihood (ReML). We have focused previously on the estimation of
	spatial covariance components under simple assumptions about the
	temporal correlations. Here we extend the scheme, using temporal
	basis functions to place constraints on the temporal form of the
	responses. We show how the same scheme can estimate evoked responses
	that are phase-locked to the stimulus and induced responses that
	are not. For a single trial the model is exactly the same. In the
	context of multiple trials, however, the inherent distinction between
	evoked and induced responses calls for different treatments of the
	underlying hierarchical multitrial model. We derive the respective
	models and show how they can be estimated efficiently using ReML.
	This enables the Bayesian estimation of evoked and induced changes
	in power or, more generally, the energy of wavelet coefficients.},
  timestamp = {2016-07-08T11:37:15Z},
  number = {9},
  journal = {Human Brain Mapping},
  author = {Friston, Karl and Henson, Richard and Phillips, Christophe and Mattout, Jeremie},
  year = {2006},
  pages = {722�735},
  owner = {afdidehf}
}

@article{Friston2007,
  title = {Variational free energy and the Laplace approximation},
  volume = {34},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2006.08.035},
  abstract = {This note derives the variational free energy under the Laplace approximation,
	with a focus on accounting for additional model complexity induced
	by increasing the number of model parameters. This is relevant when
	using the free energy as an approximation to the log-evidence in
	Bayesian model averaging and selection. By setting restricted maximum
	likelihood (ReML) in the larger context of variational learning and
	expectation maximisation (EM), we show how the ReML objective function
	can be adjusted to provide an approximation to the log-evidence for
	a particular model. This means ReML can be used for model selection,
	specifically to select or compare models with different covariance
	components. This is useful in the context of hierarchical models
	because it enables a principled selection of priors that, under simple
	hyperpriors, can be used for automatic model selection and relevance
	determination (ARD). Deriving the ReML objective function, from basic
	variational principles, discloses the simple relationships among
	Variational Bayes, \{EM\} and ReML. Furthermore, we show that \{EM\}
	is formally identical to a full variational treatment when the precisions
	are linear in the hyperparameters. Finally, we also consider, briefly,
	dynamic models and how these inform the regularisation of free energy
	ascent schemes, like \{EM\} and ReML.},
  timestamp = {2016-07-11T17:10:35Z},
  number = {1},
  journal = {NeuroImage},
  author = {Friston, Karl and Mattout, J�r�mie and Trujillo-Barreto, Nelson and Ashburner, John and Penny, Will},
  year = {2007},
  keywords = {automatic relevance determination,Expectation maximisation,Free energy,Model
	selection,Relevance vector machines,Restricted maximum likelihood,Variational
	Bayes},
  pages = {220 - 234},
  owner = {afdidehf}
}

@article{Friston2002,
  title = {Classical and Bayesian Inference in Neuroimaging: Theory},
  volume = {16},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1006/nimg.2002.1090},
  abstract = {This paper reviews hierarchical observation models, used in functional
	neuroimaging, in a Bayesian light. It emphasizes the common ground
	shared by classical and Bayesian methods to show that conventional
	analyses of neuroimaging data can be usefully extended within an
	empirical Bayesian framework. In particular we formulate the procedures
	used in conventional data analysis in terms of hierarchical linear
	models and establish a connection between classical inference and
	parametric empirical Bayes (PEB) through covariance component estimation.
	This estimation is based on an expectation maximization or \{EM\}
	algorithm. The key point is that hierarchical models not only provide
	for appropriate inference at the highest level but that one can revisit
	lower levels suitably equipped to make Bayesian inferences. Bayesian
	inferences eschew many of the difficulties encountered with classical
	inference and characterize brain responses in a way that is more
	directly predicated on what one is interested in. The motivation
	for Bayesian approaches is reviewed and the theoretical background
	is presented in a way that relates to conventional methods, in particular
	restricted maximum likelihood (ReML). This paper is a technical and
	theoretical prelude to subsequent papers that deal with applications
	of the theory to a range of important issues in neuroimaging. These
	issues include; (i) Estimating nonsphericity or variance components
	in fMRI time-series that can arise from serial correlations within
	subject, or are induced by multisubject (i.e., hierarchical) studies.
	(ii) Spatiotemporal Bayesian models for imaging data, in which voxels-specific
	effects are constrained by responses in other voxels. (iii) Bayesian
	estimation of nonlinear models of hemodynamic responses and (iv)
	principled ways of mixing structural and functional priors in \{EEG\}
	source reconstruction. Although diverse, all these estimation problems
	are accommodated by the \{PEB\} framework described in this paper.},
  timestamp = {2016-07-08T11:50:07Z},
  number = {2},
  journal = {NeuroImage},
  author = {Friston, K. J. and Penny, W. and Phillips, C. and Kiebel, S. and Hinton, G. and Ashburner, J.},
  year = {2002},
  keywords = {Bayesian inference,EM algorithm,fMRI,hierarchical models.,PET,Random
	effects,ReML},
  pages = {465 - 483},
  owner = {afdidehf}
}

@article{Fu2014,
  title = {Block-sparse recovery via redundant block OMP},
  volume = {97},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2013.10.030},
  abstract = {Abstract Recently, it has been found that the redundant blocks problem
	existed in many fields, such as face recognition and motion segmentation.
	In this paper, taking the redundant blocks into account, we propose
	some greedy type algorithms that exploit the subspace information
	of the redundant blocks to solve the redundant blocks problem. The
	exact recovery conditions of these algorithms are presented via block
	restricted isometry property (RIP). Numerical experiments demonstrate
	the validity of these algorithms in solving the problems with both
	non-redundant and redundant blocks.},
  timestamp = {2016-07-08T11:42:01Z},
  journal = {Signal Processing},
  author = {Fu, Yuli and Li, Haifeng and Zhang, Qiheng and Zou, Jian},
  year = {2014},
  keywords = {blocks,Block-sparse,Matching,measurement,Multiple,Orthogonal,Pursuit,redundant,Vectors},
  pages = {162 - 171},
  owner = {afdidehf}
}

@article{Fuchs2004,
  title = {More On Sparse Representations In Arbitrary Bases},
  abstract = {The purpose of this contribution is to generalize some recent results
	on sparse representations of signals in redundant bases. The question
	that is considered is the following : let A be a known (n, m) matrix
	with m > n, one observes b = AX where X is known to have p < n nonzero
	components, under which conditions on A and p is it possible to recover
	X by solving a convex optimization problem such as a linear or quadratic
	program? The solution is known when A is the concatenation of two
	unitary matrices, we extend it to arbitrary matrices.},
  timestamp = {2016-07-09T20:15:06Z},
  journal = {IEEE Transactions on Information Theory},
  author = {Fuchs, Jean Jacques},
  year = {2004},
  keywords = {linear programming,quadratic programming..,sparse representations},
  masid = {6132647},
  owner = {afdidehf}
}

@inproceedings{Fuchs2006,
  title = {Recovery Conditions of Sparse Representations in the Presence of 	Noise.},
  volume = {3},
  doi = {10.1109/ICASSP.2006.1660659},
  abstract = {When seeking a representation of a signal on a redundant basis one
	generally replaces the quest for the sparsest model by an l1 minimization
	and solves thus a linear program. In the presence of noise one has
	in addition to replace the exact reconstruction constraint by an
	approximate one. We consider simultaneously several ways to allow
	for reconstruction errors and analyze precisely under which conditions
	exact recovery is possible in the absence of noise. These are then
	also the conditions that allow recovery in presence of noise in case
	of large signal to noise ratio. We illustrate the results on an example
	that shows that the chances of recovery do indeed depend upon the
	criterion},
  timestamp = {2016-07-10T07:42:06Z},
  booktitle = {Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 	2006 IEEE International Conference on},
  author = {Fuchs, J.-J.},
  month = may,
  year = {2006},
  keywords = {Additive noise,Error analysis,l1 minimization,linear program,linear
	programming,minimisation,Noise,reconstruction errors,recovery conditions,Signal
	analysis,signal reconstruction,Signal to noise ratio,Signal to noise
	ratio,sparse representations,sparsest signal model,Vectors},
  pages = {III-III},
  owner = {Fardin}
}

@article{Fuchs2004a,
  title = {On sparse representations in arbitrary redundant bases},
  volume = {50},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.828141},
  abstract = {The purpose of this contribution is to generalize some recent results
	on sparse representations of signals in redundant bases. The question
	that is considered is the following: given a matrix A of dimension
	(n,m) with m>n and a vector b=Ax, find a sufficient condition for
	b to have a unique sparsest representation x as a linear combination
	of columns of A. Answers to this question are known when A is the
	concatenation of two unitary matrices and either an extensive combinatorial
	search is performed or a linear program is solved. We consider arbitrary
	A matrices and give a sufficient condition for the unique sparsest
	solution to be the unique solution to both a linear program or a
	parametrized quadratic program. The proof is elementary and the possibility
	of using a quadratic program opens perspectives to the case where
	b=Ax+e with e a vector of noise or modeling errors.},
  timestamp = {2016-09-29T16:31:06Z},
  number = {6},
  journal = {Information Theory, IEEE Transactions on},
  author = {Fuchs, J.-J.},
  month = jun,
  year = {2004},
  keywords = {Basis pursuit,combinatorial mathematics,combinatorial search,Dictionaries,global matched filter,global
	matched filter,linear program,linear programming,matched
	filters,matrix algebra,modeling error,Noise,parameter estimation,parametrized
	quadratic program,quadratic program,quadratic programming,redundant dictionaries,redundant
	dictionaries,signal representation,sparse
	matrices,sparse representations,sparse signal representation,Sufficient
	conditions,System testing,unitary matrix,Vectors},
  pages = {1341--1344}
}

@book{Fuchs2006a,
  title = {Curry Course: Multi-modal Neuroimaging using Curry},
  timestamp = {2016-07-08T12:08:09Z},
  author = {Fuchs, M. and Ford, M. and Wagner, M.},
  year = {2006},
  owner = {Fardin}
}

@InProceedings{Ganesh2009,
  author    = {Ganesh, A. and Zhou, Z. and Ma, Y.},
  title     = {Separation of a subspace-sparse signal: Algorithms and conditions},
  booktitle = {Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
  year      = {2009},
  pages     = {3141--3144},
  month     = apr,
  abstract  = {In this paper, we show how two classical sparse recovery algorithms,
	orthogonal matching pursuit and basis pursuit, can be naturally extended
	to recover block-sparse solutions for subspace-sparse signals. A
	subspace-sparse signal is sparse with respect to a set of subspaces,
	instead of atoms. By generalizing the notion of mutual incoherence
	to the set of subspaces, we show that all classical sufficient conditions
	remain exactly the same for these algorithms to work for subspace-sparse
	signals, in both noiseless and noisy cases. The sufficient conditions
	provided are easy to verify for large systems. We conduct simulations
	to compare the performance of the proposed algorithms.},
  annote    = {read},
  doi       = {10.1109/ICASSP.2009.4960290},
  keywords  = {Dictionaries,iterative methods,Linear systems,Matching pursuit algorithms,Optimization methods,orthogonal basis pursuit,orthogonal matching pursuit,Pursuit algorithms,signal processing,signal representations,Sparks,sparse matrices,sparse recovery algorithm,subspace base pursuit,subspace incoherence,subspace matching pursuit,subspace sparse,subspace-sparse signal,Sufficient conditions,time-frequency analysis,Vectors},
  owner     = {afdidehf},
  timestamp = {2016-09-30T13:47:52Z},
}

@inproceedings{Gao2015,
  title = {Block-Based Compressive Sensing Coding of Natural Images by Local 	Structural Measurement Matrix},
  doi = {10.1109/DCC.2015.47},
  abstract = {Gaussian random matrix (GRM) has been widely used to generate linear
	measurements in compressive sensing (CS) of natural images. However,
	in practice, there actually exist two problems with GRM. One is that
	GRM is non-sparse and complicated, leading to high computational
	complexity and high difficulty in hardware implementation. The other
	is that regardless of the characteristics of signal the measurements
	generated by GRM are also random, which results in low efficiency
	of compression coding. In this paper, we design a novel local structural
	measurement matrix (LSMM) for block-based CS coding of natural images
	by utilizing the local smooth property of images. The proposed LSMM
	has two main advantages. First, LSMM is a highly sparse matrix, which
	can be easily implemented in hardware, and its reconstruction performance
	is even superior to GRM at low CS sampling sub rate. Second, the
	adjacent measurement elements generated by LSMM have high correlation,
	which can be exploited to greatly improve the coding efficiency.
	Furthermore, this paper presents a new framework with LSMM for block-based
	CS coding of natural images, including measurement generating, measurement
	coding and CS reconstruction. Experimental results show that the
	proposed framework with LSMM for block-based CS coding of natural
	images greatly enhances the existing CS coding performance when compared
	with other state-of-the-art image CS coding schemes.},
  timestamp = {2016-07-08T11:40:55Z},
  booktitle = {Data Compression Conference (DCC), 2015},
  author = {Gao, Xinwei and Zhang, Jian and Che, Wenbin and Fan, Xiaopeng and Zhao, Debin},
  month = apr,
  year = {2015},
  keywords = {adjacent measurement elements,block-based CS coding,compressed sensing,compression
	coding,compressive sensing,Compressive Sensing Coding,Correlation,CS
	reconstruction,Current measurement,encoding,Gaussian processes,Gaussian
	random matrix,GRM,Hardware,image coding,Image reconstruction,linear
	measurements,local smooth property,local structural measurement matrix,Local
	Structural Measurement Matrix,LSMM,matrix algebra,natural images,sparse
	matrices},
  pages = {133-142},
  owner = {afdidehf}
}

@article{Gao2015a,
  title = {On the Null Space Property of $l_q$-Minimization for $0 <q \leq 1$ 	in Compressed Sensing},
  volume = {2015},
  doi = {10.1155/2015/579853},
  abstract = {Thepaper discusses the relationship between the null space property
	(NSP) and the ??-minimization in compressed sensing. Several versions
	of the null space property, that is, the ?? stable NSP, the ?? robustNSP,
	and the ??,? robustNSP for0 < ? ? ? < 1based on the standard ?? NSP,
	are proposed, and their equivalent forms are derived. Consequently,
	reconstruction results for the ??-minimization can be derived easily
	under the NSP condition and its equivalent form. Finally, the ??
	NSP is extended to the ??-synthesis modeling and the mixed ?2/??-minimization,
	which deals with the dictionary-based sparse signals and the block
	sparse signals, respectively.},
  timestamp = {2016-07-10T07:17:01Z},
  journal = {Journal of Function Spaces},
  author = {Gao, Yi and Peng, Jigen and Yue, Shigang and Zhao, Yuan},
  year = {2015},
  keywords = {Block,NSP},
  owner = {afdidehf}
}

@article{Gao2014,
  title = {Block-Sparse RPCA for Salient Motion Detection},
  volume = {36},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2014.2314663},
  abstract = {Recent evaluation [2], [13] of representative background subtraction
	techniques demonstrated that there are still considerable challenges
	facing these methods. Challenges in realistic environment include
	illumination change causing complex intensity variation, background
	motions (trees, waves, etc.) whose magnitude can be greater than
	those of the foreground, poor image quality under low light, camouflage,
	etc. Existing methods often handle only part of these challenges;
	we address all these challenges in a unified framework which makes
	little specific assumption of the background. We regard the observed
	image sequence as being made up of the sum of a low-rank background
	matrix and a sparse outlier matrix and solve the decomposition using
	the Robust Principal Component Analysis method. Our contribution
	lies in dynamically estimating the support of the foreground regions
	via a motion saliency estimation step, so as to impose spatial coherence
	on these regions. Unlike smoothness constraint such as MRF, our method
	is able to obtain crisply defined foreground regions, and in general,
	handles large dynamic background motion much better. Furthermore,
	we also introduce an image alignment step to handle camera jitter.
	Extensive experiments on benchmark and additional challenging data
	sets demonstrate that our method works effectively on a wide range
	of complex scenarios, resulting in best performance that significantly
	outperforms many state-of-the-art approaches.},
  timestamp = {2016-07-08T11:42:09Z},
  number = {10},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  author = {Gao, Zhi and Cheong, Loong-Fah and Wang, Yu-Xiang},
  month = oct,
  year = {2014},
  keywords = {block-sparse RPCA,camera jitter,camera jitter handling,Cameras,camouflage,complex
	intensity variation,crisply defined foreground region,dynamic background,foreground
	regions,IEEE transactions,illumination change,image alignment,image
	motion analysis,image quality,image sequence,image sequences,Jitter,large
	dynamic background motion handling,lighting,low-rank background matrix,Matrix
	decomposition,motion saliency estimation,MRF,principal component
	analysis,representative background subtraction technique,robust principal
	component analysis method,salient motion,salient motion detection,smoothness
	constraint,sparse matrices,sparse outlier matrix,spatial coherence,spatial
	coherence,Tracking,Trajectory},
  pages = {1975-1987},
  owner = {afdidehf}
}

@inproceedings{Garcia-Frias2008,
  title = {Design of near-optimum quantum error-correcting codes based on generator 	and parity-check matrices of LDGM codes},
  doi = {10.1109/CISS.2008.4558588},
  abstract = {We study the design of near-optimum quantum error correcting codes
	based on the use of sparse matrices. The basic idea is to construct
	a Calderbank-Shor-Steane (CSS) code based on the generator and parity-check
	matrices of a classical channel code with low density generator matrix
	(LDGM code), which is designed with a specific structure inspired
	in the parallel concatenation of regular LDGM codes. Then, row operations
	are performed in both matrices to achieve the desired quantum rate.
	Decoding is performed in an iterative manner, by applying message
	passing over the corresponding graphs. The proposed codes allow greater
	flexibility and are easier to design than existing sparse-graph quantum
	codes, while leading to better performance.},
  timestamp = {2016-07-08T12:11:14Z},
  booktitle = {Information Sciences and Systems, 2008. CISS 2008. 42nd Annual Conference 	on},
  author = {Garcia-Frias, J. and Liu, Kejing},
  month = mar,
  year = {2008},
  keywords = {block codes,Calderbank-Shor-Steane code,Cascading style sheets,Computational
	complexity,CSS codes,Decoding,error correction codes,Iterative decoding,LDGM
	codes,low density generator matrix,message passing,near-optimum quantum
	error-correcting code,parity check codes,parity-check matrix,Quantum
	computing,Quantum error correction,sparse matrices,turbo codes,turbo
	codes},
  pages = {562-567},
  owner = {afdidehf}
}

@techreport{Garneroxxxx,
  title = {Localisation de sources en MEG/EEG},
  abstract = {Seule la possibilit? de localiser les sources des signaux MEG et EEG
	permettra de r?pondre ? la question ��quand et o?�� se d?roulent
	les ?tapes du traitement de l�information dans le cerveau [Wood,
	1994]. En effet, il est souvent fr?quent de d?tecter en TEP ou en
	IRMf les m?mes zones d�activation pour deux t?ches cognitives diff?rentes
	[Maquet et al, 1996], le seul moyen alors de diff?rentier le fonctionnement
	du cerveau dans ces deux t?ches est d�?tudier la dynamique du r?seau
	d�activations. De plus, une excellente r?solution temporelle est
	un atout consid?rable, car elle permet d�isoler dans les signaux
	des ?tapes pr?cises du fonctionnement du cerveau et de n�extraire
	que les informations sp?cifiques ? celles ci, ce qui ne peut s�envisager
	en IRMf et en TEP car les donn?es r?sultent d�une int?gration dans
	une grande plage de temps de l�ensemble des activations. Mais la
	localisation des sources en EEG et MEG suscite actuellement d�intenses
	recherches, car elle soul?ve de nombreuses difficult?s. Dans cet
	article nous d?taillerons bri?vement les diff?rentes approches qui
	sont utilis?es pour d?crire math?matiquement les relations qui lient
	les sources aux donn?es, afin de r?soudre le probl?me dit direct,
	ainsi que les diff?rents mod?les de sources neuronales utilis?es
	pour le probl?me inverse, c�est ? dire la localisation des activit?s
	? partir des signaux MEG ou EEG enregistr?s en surface. Nous ne donnerons
	que les principes de ces m?thodes, les techniques math?matiques utilis?es
	et les ?quations du formalisme seront donn?es dans l�article de
	Sylvain Baillet de ce Proccedings. Nous donnerons aussi quelques
	exemples de localisation de sources obtenus dans une exp?rience de
	somesth?sie.},
  timestamp = {2016-07-09T19:57:25Z},
  author = {Garnero, Line},
  year = {xxxx},
  owner = {Fardin}
}

@article{Gavit2001,
  title = {A multiresolution framework to MEG/EEG source imaging},
  volume = {48},
  issn = {0018-9294},
  doi = {10.1109/10.951510},
  abstract = {A new method based on a multiresolution approach for solving the ill-posed
	problem of brain electrical activity reconstruction from EEG/MEG
	signals is proposed in a distributed source model. At each step of
	the algorithm, a regularized solution to the inverse problem is used
	to constrain the source space on the cortical surface to be scanned
	at higher spatial resolution. We present the iterative procedure
	together with an extension of the ST-maximum a posteriori method
	that integrates spatial and temporal a priori information in an estimator
	of the brain electrical activity. Results from EEG in a phantom head
	experiment with a real human skull and from real MEG data on a healthy
	human subject are presented. The performances of the multiresolution
	method combined with a nonquadratic estimator are compared with commonly
	used dipolar methods, and to minimum-norm method with and without
	multiresolution. In all cases, the proposed approach proved to be
	more efficient both in terms of computational load and result quality,
	for the identification of sparse focal patterns of cortical current
	density, than the fixed scale imaging approach},
  timestamp = {2016-07-08T10:28:23Z},
  number = {10},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Gavit, L. and Baillet, S. and Mangin, J.-F. and Pescatore, J. and Garnero, L.},
  month = oct,
  year = {2001},
  keywords = {brain electrical activity reconstruction,Brain modeling,computational
	load,cortical current density,cortical surface,distributed source
	model,EEG source imaging,electroencephalography,high spatial resolution,Humans,ill-posed
	problem,Image reconstruction,Image resolution,inverse problem,Inverse
	problems,Iterative algorithms,iterative methods,iterative procedure,magnetoencephalography,maximum
	likelihood estimation,medical signal processing,MEG source imaging,minimum-norm
	method,multiresolution approach,nonquadratic estimator,phantom head,real
	human skull,regularized solution,signal resolution,sparse focal patterns,sparse
	focal patterns,spatial a priori information,spatial
	a priori information,Spatial resolution,ST-maximum a posteriori method,ST-maximum
	a posteriori method,Surface reconstruction,Surface
	reconstruction,temporal a priori information},
  pages = {1080-1087},
  owner = {afdidehf}
}

@article{Gedalyahu2010,
  title = {Time-Delay Estimation From Low-Rate Samples: A Union of Subspaces 	Approach},
  volume = {58},
  issn = {1053-587X},
  doi = {10.1109/TSP.2010.2044253},
  abstract = {Time-delay estimation arises in many applications in which a multipath
	medium has to be identified from pulses transmitted through the channel.
	Previous methods for time delay recovery either operate on the analog
	received signal, or require sampling at the Nyquist rate of the transmitted
	pulse. In this paper, we develop a unified approach to time delay
	estimation from low-rate samples. This problem can be formulated
	in the broader context of sampling over an infinite union of subspaces.
	Although sampling over unions of subspaces has been receiving growing
	interest, previous results either focus on unions of finite-dimensional
	subspaces, or finite unions. The framework we develop here leads
	to perfect recovery of the multipath delays from samples of the channel
	output at the lowest possible rate, even in the presence of overlapping
	transmitted pulses, and allows for a variety of different sampling
	methods. The sampling rate depends only on the number of multipath
	components and the transmission rate, but not on the bandwidth of
	the probing signal. This result can be viewed as a sampling theorem
	over an infinite union of infinite dimensional subspaces. By properly
	manipulating the low-rate samples, we show that the time delays can
	be recovered using the well-known ESPRIT algorithm. Combining results
	from sampling theory with those obtained in the context of direction
	of arrival estimation, we develop sufficient conditions on the transmitted
	pulse and the sampling functions in order to ensure perfect recovery
	of the channel parameters at the minimal possible rate.},
  timestamp = {2016-09-30T11:27:39Z},
  number = {6},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Gedalyahu, K. and Eldar, Y.C.},
  month = jun,
  year = {2010},
  keywords = {analog received signal,direction-of-arrival estimation,Direction of
	arrival estimation,ESPRIT algorithm,finite-dimensional subspaces,low-rate
	samples,multipath delays,Nyquist rate,Sampling methods,sampling theory,signal
	sampling,sub-Nyquist sampling,time-delay estimation,time delay recovery,time
	delay recovery,union of subspaces,union
	of subspaces},
  pages = {3017--3031},
  owner = {afdidehf}
}

@inproceedings{Gedalyahu2010a,
  title = {Time delay estimation: Compressed sensing over an infinite union 	of subspaces},
  doi = {10.1109/ICASSP.2010.5495810},
  abstract = {Sampling theorems for signals that lie in a union of subspaces have
	been receiving growing interest. A recent model that describes analog
	signals over a union is that of a union of shift-invariant (SI) subspaces.
	Until now, sampling and recovery algorithms have been developed only
	for a finite union of SI subspaces. Here we extend this paradigm
	to a special case of an infinite union, in which the SI subspaces
	are generated by pulses with unknown delays, taken from a continuous
	interval. We develop a unified approach to time delay recovery of
	the pulses, from low rate samples of the signal taken at the lowest
	possible rate. In particular, we derive sufficient conditions on
	the pulses and the sampling filters in order to ensure perfect recovery
	of the signal. We then show that by properly manipulating the low-rate
	samples, the time delays can be recovered using the well-known ESPRIT
	algorithm.},
  timestamp = {2016-07-11T17:06:38Z},
  booktitle = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International 	Conference on},
  author = {Gedalyahu, K. and Eldar, Y.C.},
  month = mar,
  year = {2010},
  keywords = {Analog-digital conversion,compressed sensing,Delay effects,Delay estimation,delays,ESPRIT
	algorithm,Estimation theory,Filters,Frequency conversion,infinite
	union of subspaces,Pulse generation,Sampling,Sampling methods,sampling
	theorems,shift-invariant subspaces,Signal generators,signal processing,Sufficient
	conditions,time delay estimation,union of subspaces},
  pages = {3902-3905},
  owner = {afdidehf}
}

@article{Genovese2011,
  title = {A Comparison of the Lasso and Marginal Regression},
  abstract = {The lasso is an important method for sparse, high-dimensional regression
	problems, with efficient algorithms available, a long history of
	practical success, and a large body of theoretical results supporting
	and explaining its performance. But even with the best available
	algorithms, finding the lasso solutions remains a computationally
	challenging task in cases where the number of covariates vastly exceeds
	the number of data points. Marginal regression, where each dependent
	variable is regressed separately on each covariate, offers a promising
	alternative in this case because the estimates can be computed roughly
	two orders faster than the lasso solutions. The question that remains
	is how the statistical performance of the method compares to that
	of the lasso in these cases. In this paper, we study the relative
	statistical performance of the lasso and marginal regression for
	sparse, high-dimensional regression problems. We consider the problem
	of learning which coefficients are non-zero. Our main results are
	as follows: (i) we compare the conditions under which the lasso and
	marginal regression guarantee exact recovery in the fixed design,
	noise free case; (ii) we establish conditions under which marginal
	regression provides exact recovery with high probability in the fixed
	design, noise free, random coefficients case; and (iii) we derive
	rates of convergence for both procedures, where performance is measured
	by the number of coefficients with incorrect sign, and characterize
	the regions in the parameter space recovery is and is not possible
	under this metric. In light of the computational advantages of marginal
	regression in very high dimensional problems, our theoretical and
	simulations results suggest that the procedure merits further study.},
  timestamp = {2016-07-08T10:08:41Z},
  journal = {Journal of Machine Learning Research},
  author = {Genovese, Christopher R. and Jin, Jiashun and Wasserman, Larry and Yao, Zhigang},
  month = jun,
  year = {2011},
  keywords = {high-dimensional regression,Lasso,phase diagram,regularization},
  pages = {2107?2143},
  owner = {Fardin}
}

@article{Gharibi2012,
  title = {An Improved Lower Bound of The Spark With Application},
  volume = {abs/1210.5083},
  timestamp = {2016-07-08T11:23:56Z},
  journal = {CoRR},
  author = {Gharibi, Wajeb},
  year = {2012},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1210-5083},
  owner = {afdidehf}
}

@inproceedings{Gilbert2003,
  title = {Approximation of functions over redundant dictionaries using coherence},
  doi = {10.1145/644108.644149},
  abstract = {One of the central problems of modern mathematical approximation theory
	is to approximate functions, or signals, concisely, with elements
	from a large candidate set called a dictionary. Formally, we are
	given a signal A ? RN and a dictionary D = {?i}i?I of unit vectors
	that span RN. A representation R of B terms for input A ? RN is a
	linear combination of dictionary elements, R = ?i?A ?i?i, for ?i
	? D and some A, &verbar;A&verbar; ? B. Typically, B ? N, so that
	R is a concise approximation to signal A. The error of the representation
	indicates by how well it approximates A, and is given by ?A - R?2
	= ??t|A[t - R[t]|2. The problem is to find the best B-term representation,
	i.e., find a R that minimizes ?A - R?2. A dictionary may be redundant
	in the sense that there is more than one possible exact representation
	for A, i.e., &verbar;D&verbar; > N = dim(RN). Redundant dictionaries
	are used because, both theoretically and in practice, for important
	classes of signals, as the size of a dictionary increases, the error
	and the conciseness of the approximations improve.We present the
	first known efficient algorithm for finding a provably approximate
	representation for an input signal over redundant dictionaries. We
	identify and focus on redundant dictionaries with small coherence
	(ie., vectors are nearly orthogonal). We present an algorithm that
	preprocesses any such dictionary in time and space polynomial in
	&verbar;D&verbar;, and obtains an 1 + ? approximate representation
	of the given signal in time nearly linear in signal size N and polylogarithmic
	in &verbar;D&verbar;; by contrast, most algorithms in the literature
	require ?(&verbar;D&verbar;)time, and, yet, provide no provable bounds.
	The technical crux of our result is our proof that two commonly used
	local search techniques, when combined appropriately, gives a provably
	near-optimal signal representation over redundant dictionaries with
	small coherence. Our result immediately applies to several specific
	redundant dictionaries considered by the domain experts thus far.
	In addition, we present new redundant dictionaries which have small
	coherence (and therefore are amenable to our algorithms) and yet
	have significantly large sizes, thereby adding to the redundant dictionary
	construction literature.Work with redundant dictionaries forms the
	emerging field of highly nonlinear approximation theory. We have
	presented algorithmic results for some of the most basic problems
	in this area, but other mathematical and algorithmic questions remain
	to be explored.},
  timestamp = {2016-09-29T16:05:47Z},
  booktitle = {ACM-SIAM Symposium on Discrete Algorithms},
  author = {Gilbert, Anna C. and Muthukrishnan, S. and Strauss, Martin J.},
  year = {2003},
  pages = {243--252},
  masid = {572984},
  owner = {Fardin}
}

@inproceedings{Giri2014,
  title = {Block sparse excitation based all-pole modeling of speech},
  doi = {10.1109/ICASSP.2014.6854303},
  abstract = {In this paper, it is shown that an appropriate model for voiced speech
	is an all-pole filter excited by a block sparse excitation sequence.
	The modeling approach is generalized in a novel manner to deal with
	a wide spectrum of speech signal; voiced speech, unvoiced speech
	and mixed excitation speech. In this context, the input sequence
	to the all-pole model is modeled as a suitable weighted linear combination
	of a block sparse signal and white noise. We develop the corresponding
	estimation procedure to reconstruct the generalized input sequence
	and model parameters via sparse Bayesian learning methods employing
	the Expectation-Maximization based procedure. Rigorous experiments
	have been performed to show the efficacy of our proposed model for
	the speech modeling task. By imposing a block sparse structure on
	the input sequence, the problems associated with the commonly used
	Linear Prediction approach is alleviated leading to a more robust
	modeling scheme.},
  timestamp = {2016-07-08T11:41:44Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Giri, R. and Rao, B.D.},
  month = may,
  year = {2014},
  keywords = {all-pole filter modeling,Bayes methods,belief networks,block sparse
	excitation sequence,block sparse signal,Computational modeling,Data
	models,Deconvolution,Distortion measurement,expectation-maximisation
	algorithm,Expectation-Maximization,expectation-maximization based
	procedure,filtering theory,generalized input sequence reconstruction,linear
	prediction approach,mixed excitation speech,model parameter,prediction
	theory,sequences,signal reconstruction,sparse Bayesian learning,sparse
	Bayesian learning method,Speech,speech modeling,Speech processing,speech
	signal spectrum,unvoiced speech,weighted linear combination,White noise,White
	noise},
  pages = {3754-3758},
  owner = {afdidehf}
}

@article{Giryes2015,
  title = {Sampling in the analysis transform domain},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2015.04.004},
  abstract = {Abstract Many signal and image processing applications have benefited
	remarkably from the fact that the underlying signals reside in a
	low dimensional subspace. One of the main models for such a low dimensionality
	is the sparsity one. Within this framework there are two main options
	for the sparse modeling: the synthesis and the analysis ones, where
	the first is considered the standard paradigm for which much more
	research has been dedicated. In it the signals are assumed to have
	a sparse representation under a given dictionary. On the other hand,
	in the analysis approach the sparsity is measured in the coefficients
	of the signal after applying a certain transformation, the analysis
	dictionary, on it. Though several algorithms with some theory have
	been developed for this framework, they are outnumbered by the ones
	proposed for the synthesis methodology. Given that the analysis dictionary
	is either a frame or the two dimensional finite difference operator,
	we propose a new sampling scheme for signals from the analysis model
	that allows recovering them from their samples using any existing
	algorithm from the synthesis model. The advantage of this new sampling
	strategy is that it makes the existing synthesis methods with their
	theory also available for signals from the analysis framework.},
  timestamp = {2016-07-10T08:09:11Z},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Giryes, Raja},
  year = {2015},
  keywords = {analysis,Compressed,domain,representations,sensing,Sparse,Synthesis,transform},
  pages = {-},
  owner = {afdidehf}
}

@article{Giryes2015b,
  title = {Greedy signal space methods for incoherence and beyond},
  volume = {39},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2014.07.004},
  abstract = {Abstract Compressive sampling (CoSa) has provided many methods for
	signal recovery of signals compressible with respect to an orthonormal
	basis. However, modern applications have sparked the emergence of
	approaches for signals not sparse in an orthonormal basis but in
	some arbitrary, perhaps highly overcomplete, dictionary. Recently,
	several “signal-space�? greedy methods have been proposed to
	address signal recovery in this setting. However, such methods inherently
	rely on the existence of fast and accurate projections which allow
	one to identify the most relevant atoms in a dictionary for any given
	signal, up to a very strict accuracy. When the dictionary is highly
	overcomplete, no such projections are currently known; the requirements
	on such projections do not even hold for incoherent or well-behaved
	dictionaries. In this work, we provide an alternate analysis for
	signal space greedy methods which enforce assumptions on these projections
	which hold in several settings including those when the dictionary
	is incoherent or structurally coherent. These results align more
	closely with traditional results in the standard CoSa literature
	and improve upon previous work in the signal space setting.},
  timestamp = {2016-07-08T12:39:40Z},
  number = {1},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Giryes, Raja and Needell, Deanna},
  year = {2015},
  keywords = {Algorithms,Approximation,Coherent,Compressed,CoSaMP,Dictionaries,isometry,methods,modeling,property,Restricted,sensing,Signal,space,Sparse},
  pages = {1 - 20},
  owner = {afdidehf}
}

@inproceedings{Gishkori2014,
  title = {Compressed sensing for block-sparse smooth signals},
  doi = {10.1109/ICASSP.2014.6854386},
  abstract = {We present reconstruction algorithms for smooth signals with block
	sparsity from their compressed measurements. We tackle the issue
	of varying group size via the group-sparse least absolute shrinkage
	selection operator (LASSO) as well as via latent group LASSO regularizations.
	We achieve smoothness in the signal via fusion. We develop low-complexity
	solvers for our proposed formulations through the alternating direction
	method of multipliers.},
  timestamp = {2016-07-08T11:55:48Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Gishkori, S. and Leus, G.},
  month = may,
  year = {2014},
  keywords = {alternating direction method of multipliers,block-sparse smooth signals,block
	sparsity,compressed measurements,compressed sensing,Convergence,group
	size,group-sparse least absolute shrinkage selection operator,latent
	group LASSO regularizations,low-complexity solvers,Optimization,signal
	reconstruction,signal via fusion,smoothing methods,smoothness,smooth
	signal reconstruction algorithms,System-on-chip,Vectors},
  pages = {4166-4170},
  owner = {afdidehf}
}

@inproceedings{Goncalves2014,
  title = {DALM-SVD: Accelerated sparse coding through singular value decomposition 	of the dictionary},
  doi = {10.1109/ICIP.2014.7025994},
  abstract = {Sparse coding techniques have seen an increasing range of applications
	in recent years, especially in the area of image processing. In particular,
	sparse coding using ?1-regularization has been efficiently solved
	with the Augmented Lagrangian (AL) applied to its dual formulation
	(DALM). This paper proposes the decomposition of the dictionary matrix
	in its Singular Value/Vector form in order to simplify and speed-up
	the implementation of the DALM algorithm. Furthermore, we propose
	an update rule for the penalty parameter used in AL methods that
	improves the convergence rate. The SVD of the dictionary matrix is
	done as a pre-processing step prior to the sparse coding, and thus
	the method is better suited for applications where the same dictionary
	is reused for several sparse recovery steps, such as block image
	processing.},
  timestamp = {2016-07-08T12:09:51Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Goncalves, H. and Correia, M. and Li, Xin and Sankaranarayanan, A. and Tavares, V.},
  month = oct,
  year = {2014},
  keywords = {accelerated sparse coding,augmented Lagrangian,block image processing,Convergence,DALM-SVD,Dictionaries,dictionary
	matrix,image coding,Image reconstruction,matrix decomposition,singular
	value decomposition,sparse matrices,sparse recovery steps,Vectors},
  pages = {4907-4911},
  owner = {afdidehf}
}

@inproceedings{Goodman2011,
  title = {Efficient reconstruction of block-sparse signals},
  doi = {10.1109/SSP.2011.5967779},
  abstract = {In many sparse reconstruction problems, M observations are used to
	estimate K components in an N dimensional basis, where N >; M ? K.
	The exact basis vectors, however, are not known a priori and must
	be chosen from an M � N matrix. Such under-determined problems
	can be solved using an ?2 optimization with an ?1 penalty on the
	sparsity of the solution. There are practical applications in which
	multiple measurements can be grouped together, so that K � P data
	must be estimated from M � P observations, where the ?1 sparsity
	penalty is taken with respect to the vector formed using the ?2 norms
	of the rows of the data matrix. In this paper we develop a computationally
	efficient block partitioned homotopy method for reconstructing K
	� P data from M � P observations using a grouped sparsity constraint,
	and compare its performance to other block reconstruction algorithms.},
  timestamp = {2016-07-08T12:19:40Z},
  booktitle = {Statistical Signal Processing Workshop (SSP), 2011 IEEE},
  author = {Goodman, J. and Forsythe, K. and Miller, B.},
  month = jun,
  year = {2011},
  keywords = {?1 sparsity penalty,?2 optimization,Approximation algorithms,Approximation
	methods,Arrays,block partitioned homotopy method,block-sparse signal
	reconstruction,computational complexity,Convex functions,data matrix,grouped
	sparsity constraint,matrix algebra,optimisation,Sensors,signal reconstruction,sparse
	matrices},
  pages = {629-632},
  owner = {afdidehf}
}

@article{Gorodnitsky1997,
  title = {Sparse signal reconstruction from limited data using FOCUSS: a re-weighted 	minimum norm algorithm},
  volume = {45},
  issn = {1053-587X},
  doi = {10.1109/78.558475},
  abstract = {We present a nonparametric algorithm for finding localized energy
	solutions from limited data. The problem we address is underdetermined,
	and no prior knowledge of the shape of the region on which the solution
	is nonzero is assumed. Termed the FOcal Underdetermined System Solver
	(FOCUSS), the algorithm has two integral parts: a low-resolution
	initial estimate of the real signal and the iteration process that
	refines the initial estimate to the final localized energy solution.
	The iterations are based on weighted norm minimization of the dependent
	variable with the weights being a function of the preceding iterative
	solutions. The algorithm is presented as a general estimation tool
	usable across different applications. A detailed analysis laying
	the theoretical foundation for the algorithm is given and includes
	proofs of global and local convergence and a derivation of the rate
	of convergence. A view of the algorithm as a novel optimization method
	which combines desirable characteristics of both classical optimization
	and learning-based algorithms is provided. Mathematical results on
	conditions for uniqueness of sparse solutions are also given. Applications
	of the algorithm are illustrated on problems in direction-of-arrival
	(DOA) estimation and neuromagnetic imaging},
  timestamp = {2016-09-30T13:52:32Z},
  number = {3},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Gorodnitsky, I.F. and Rao, B.D.},
  month = mar,
  year = {1997},
  keywords = {Brain,Convergence,convergence of numerical methods,Cost function,direction-of-arrival estimation,direction of arrival estimation,Direction-of-arrival
	estimation,Direction
	of arrival estimation,DOA estimation,final
	localized energy solution,final localized
	energy solution,focal underdetermined system solver,Focusing,FOCUSS,Image
	reconstruction,iteration process,Iterative algorithms,iterative methods,learning-based
	algorithms,limited data,localized energy solutions,low-resolution
	initial estimate,magnetoencephalography,medical image processing,neuromagnetic
	imaging,nonparametric algorithm,optimisation,optimization method,Optimization
	methods,re-weighted minimum norm algorithm,Sensor arrays,Shape,signal
	processing,Signal processing algorithms,signal reconstruction,signal
	reconstruction,sparse signal reconstruction},
  pages = {600--616},
  owner = {Fardin}
}

@phdthesis{Gorst-Rasmussen2011,
  title = {Some Statistical Models for High Dimensional Data},
  abstract = {The dimension of a mathematical entity can be loosely defined as the
	�number of numbers� needed for its description. With this definition
	in mind, high-dimensional data is essentially just data where each
	observation consists of a large number of numbers. Examples could
	be regular measurements of an Internet data stream; or the genetic
	information of a human. Increasingly larger amounts of high-dimensional
	data are collected in medicine and technology, and the development
	of descriptive and inferential methods for such data is the biggest
	current challenge for research in statistics and probability. The
	research work in this thesis contributes to meeting this challenge
	by investigating a range of different applied statistical and probabilistic
	problems from telecommunications, medicine, and biotechnology. Classically,
	high-dimensional data is often taken to mean data describable via
	a suitable stochastic process. This notion of high dimensionality
	is embraced in the initial three papers of the thesis, which deal
	with problems derived from telecommunications. Stochastic processes
	are convenient models for high-dimensional phenomena because of their
	often rich intrinsic structure. Strong use of such intrinsic structure
	is made in the first and third paper which rely on classical asymptotic
	statistical theory to investigate the sampling properties of estimators
	of functional parameters of an underlying stochastic process. Specifically,
	the first paper deals with regenerative sequences appearing in queueing
	theoretical models whereas the third paper concerns a certain time
	series model used for modelling communication systems. These two
	works have applications in the statistical analysis of tele-queues
	and in performance analysis for wireless communications, respectively.
	The second paper is a critical view on the routine use of a particular
	statistical model: fractional time series, often used as prototypical
	examples of long memory time series in, for example, simulation studies
	are shown to exhibit a rather atypical form of long memory. In recent
	years, high-dimensional data has come to refer to standard regression
	data with the additional complication that we seek to estimate a
	large number of parameters compared to the number of observations.
	In modern genetics, for example, millions of measurements may be
	made on each of only a few hundred individuals. A successful approach
	to dealing statistically with such difficult data is to use standard
	�unstructured� statistical models and impose structure at the
	estimation rather than at the modelling stage. This is known as regularised
	estimation and is one of the most active current research areas in
	statistics. It is also the subject of the last four papers of the
	thesis which contribute to both theoretical, computational, and practical
	aspects of regularised regression for survival data in medicine and
	biotechnology. In the fourth and fifth paper, we introduce a recent
	regularisation method, the treelet transform, to an epidemiological
	audience in the context of dietary pattern analysis and show how
	it may substantially improve over existing methods. The last two
	papers promote the so-called semiparametric additive hazards model
	for analysing survival regression data with high-dimensional explanatory
	variables. This flexible model is particularly well suited for regularisation
	purposes because of its simple analytic form and excellent computational
	properties. We develop in the sixth paper highly efficient coordinate
	descent algorithms and software for fitting the lasso regularised
	additive hazard model. In the seventh and final paper of the thesis,
	we present a method for univariate screening for survival data with
	high-dimensional explanatory variables, loosely based on the additive
	hazards model. We provide a study of the consistency properties of
	the method in the asymptotic regime of ultra-high dimension.},
  timestamp = {2016-07-10T08:22:29Z},
  school = {Department of Mathematical Sciences, Aalborg University},
  author = {Gorst-Rasmussen, Anders},
  year = {2011},
  owner = {afdidehf}
}

@article{Gowreesunker2010,
  title = {Learning Sparse Representation Using Iterative Subspace Identification},
  volume = {58},
  issn = {1053-587X},
  doi = {10.1109/TSP.2010.2044251},
  abstract = {In this paper, we introduce the iterative subspace identification
	(ISI) algorithm for learning subspaces in which the data may live.
	Our subspace identification method differs from currently available
	method in its ability to infer the dimension of the subspaces from
	the data without prior knowledge. The learned subspaces can be combined
	to produce a data driven overcomplete dictionary with good sparseness
	and generalizability qualities, or can be directly exploited in applications
	where block sparseness is needed. We describe the ISI algorithm and
	a complementary optimization method. We demonstrate the ability of
	the proposed method to produce sparse representations comparable
	to those achieved with the K-SVD algorithm, but with less than one
	eighth the training time. Furthermore, the computation savings allows
	us to develop a shift-tolerant training procedure. We also illustrate
	its benefits in underdetermined blind source separation of audio,
	where performance is directly impacted by the sparseness of the representation.},
  timestamp = {2016-07-09T19:55:44Z},
  number = {6},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Gowreesunker, B.V. and Tewfik, A.H.},
  month = jun,
  year = {2010},
  keywords = {blind source separation,Block sparseness,complementary optimization
	method,data driven overcomplete dictionary,dictionary training,ISI
	algorithm,iterative methods,iterative subspace identification,iterative
	subspace identification algorithm,K-SVD algorithm,learning (artificial
	intelligence),overcomplete dictionary design,shift-tolerance,shift-tolerant
	training procedure,signal representation,sparse representation,sparse
	representation learning},
  pages = {3055-3065},
  owner = {afdidehf}
}

@article{Gramfort2013,
  title = {MEG and EEG data analysis with MNE-Python},
  volume = {7},
  abstract = {Magnetoencephalographyandelectroencephalography(M/EEG)measuretheweakelectromagneticsignalsgeneratedbyneuronalactivityinthebrain.Usingthesesignalstocharacterizeandlocateneuralactivationinthebrainisachallengethatrequiresexpertiseinphysics,signalprocessing,statistics,andnumericalmethods.AspartoftheMNEsoftwaresuite,MNE-Pythonisanopen-sourcesoftwarepackagethataddressesthischallengebyprovidingstate-of-the-artalgorithmsimplementedinPythonthatcovermultiplemethodsofdatapreprocessing,sourcelocalization,statisticalanalysis,andestimationoffunctionalconnectivitybetweendistributedbrainregions.Allalgorithmsandutilityfunctionsareimplementedinaconsistentmannerwithwell-documentedinterfaces,enablinguserstocreateM/EEGdataanalysispipelinesbywritingPythonscripts.Moreover,MNE-PythonistightlyintegratedwiththecorePythonlibrariesforscientificcomptutation(NumPy,SciPy)andvisualization(matplotlibandMayavi),aswellasthegreaterneuroimagingecosysteminPythonviatheNibabelpackage.ThecodeisprovidedunderthenewBSDlicenseallowingcodereuse,evenincommercialproducts.AlthoughMNE-Pythonhasonlybeenunderheavydevelopmentforacoupleofyears,ithasrapidlyevolvedwithexpandedanalysiscapabilitiesandpedagogicaltutorialsbecausemultiplelabshavecollaboratedduringcodedevelopmenttohelpsharebestpractices.MNE-Pythonalsogiveseasyaccesstopreprocesseddatasets,helpinguserstogetstartedquicklyandfacilitatingreproducibilityofmethodsbyotherresearchers.Fulldocumentation,includingdozensofexamples,isavailableathttp://martinos.org/mne.},
  timestamp = {2016-07-09T20:10:15Z},
  journal = {Frontiers in neuroscience},
  author = {Gramfort, Alexandre and Luessi, Martin and Larson, Eric and A. Engemann, Denis and Strohmeier, Daniel and Brodbeck, Christian and Goj, Roman and Jas, Mainak and Brooks, Teon and Parkkonen, Lauri and H�m�l�inen, Matti},
  month = dec,
  year = {2013},
  note = {read},
  owner = {Fardin}
}

@article{Gramfort2014,
  title = {MNE software for processing MEG and EEG data},
  volume = {86},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2013.10.027},
  abstract = {Abstract Magnetoencephalography and electroencephalography (M/EEG)
	measure the weak electromagnetic signals originating from neural
	currents in the brain. Using these signals to characterize and locate
	brain activity is a challenging task, as evidenced by several decades
	of methodological contributions. MNE, whose name stems from its capability
	to compute cortically-constrained minimum-norm current estimates
	from M/EEG data, is a software package that provides comprehensive
	analysis tools and workflows including preprocessing, source estimation,
	time–frequency analysis, statistical analysis, and several methods
	to estimate functional connectivity between distributed brain regions.
	The present paper gives detailed information about the \{MNE\} package
	and describes typical use cases while also warning about potential
	caveats in analysis. The \{MNE\} package is a collaborative effort
	of multiple institutes striving to implement and share best methods
	and to facilitate distribution of analysis pipelines to advance reproducibility
	of research. Full documentation is available at http://martinos.org/mne.},
  timestamp = {2016-07-09T20:12:34Z},
  number = {0},
  journal = {NeuroImage},
  author = {Gramfort, Alexandre and Luessi, Martin and Larson, Eric and A. Engemann, Denis and Strohmeier, Daniel and Brodbeck, Christian and Parkkonen, Lauri and H{\"a}m{\"a}l{\"a}inen, Matti S.},
  year = {2014},
  keywords = {magnetoencephalography,(MEG)},
  pages = {446 - 460},
  owner = {Fardin}
}

@article{Gramfort2011a,
  title = {Forward Field Computation with OpenMEEG},
  volume = {2011},
  doi = {10.1155/2011/923703},
  abstract = {To recover the sources giving rise to electro- and magnetoencephalography
	in individual measurements, realistic physiological modeling is required,
	and accurate numerical solutions must be computed. We present OpenMEEG,
	which solves the electromagnetic forward problem in the quasistatic
	regime, for head models with piecewise constant conductivity. The
	core of OpenMEEG consists of the symmetric Boundary ElementMethod,
	which is based on an extended Green Representation theorem. OpenMEEG
	is able to provide lead fields for four different electromagnetic
	forward problems: Electroencephalography (EEG), Magnetoencephalography
	(MEG), Electrical Impedance Tomography (EIT), and intracranial electric
	potentials (IPs).OpenMEEG is open source and multiplatform. It can
	be used from Python and Matlab in conjunction with toolboxes that
	solve the inverse problem; its integration within FieldTrip is operational
	since release 2.0.},
  timestamp = {2016-07-08T12:32:28Z},
  journal = {Computational Intelligence and Neuroscience},
  author = {Gramfort, Alexandre and Papadopoulo, Th�odore and Olivi, Emmanuel and Clerc, Maureen},
  year = {2011},
  owner = {afdidehf}
}

@article{Gramfort2010,
  title = {OpenMEEG: opensource software for quasistatic bioelectromagnetics},
  volume = {9},
  abstract = {Abstract Background: Interpreting and controlling bioelectromagnetic
	phenomena require realistic physiological models and accurate numerical
	solvers. A semi-realistic model often used in practise is the piecewise
	constant conductivity model, for which only the interfaces have to
	be meshed. This simplified model makes it possible to use Boundary
	Element Methods. Unfortunately, most Boundary Element solutions are
	confronted with accuracy issues when the conductivity ratio between
	neighboring tissues is high, as for instance the scalp/skull conductivity
	ratio in electroencephalography. To overcome this difficulty, we
	proposed a new method called the symmetric BEM, which is implemented
	in the OpenMEEG software. The aim of this paper is to present OpenMEEG,
	both from the theoretical and the practical point of view, and to
	compare its performances with other competing software packages.
	Methods: We have run a benchmark study in the field of electro- and
	magnetoencephalography, in order to compare the accuracy of OpenMEEG
	with other freely distributed forward solvers. We considered spherical
	models, for which analytical solutions exist, and we designed randomized
	meshes to assess the variability of the accuracy. Two measures were
	used to characterize the accuracy. the Relative Difference Measure
	and the Magnitude ratio. The comparisons were run, either with a
	constant number of mesh nodes, or a constant number of unknowns across
	methods. Computing times were also compared. Results: We observed
	more pronounced differences in accuracy in electroencephalography
	than in magnetoencephalography. The methods could be classified in
	three categories: the linear collocation methods, that run very fast
	but with low accuracy, the linear collocation methods with isolated
	skull approach for which the accuracy is improved, and OpenMEEG that
	clearly outperforms the others. As far as speed is concerned, OpenMEEG
	is on par with the other methods for a constant number of unknowns,
	and is hence faster for a prescribed accuracy level. Conclusions:
	This study clearly shows that OpenMEEG represents the state of the
	art for forward computations. Moreover, our software development
	strategies have made it handy to use and to integrate with other
	packages. The bioelectromagnetic research community should therefore
	be able to benefit from OpenMEEG with a limited development effort.},
  timestamp = {2016-07-10T07:18:10Z},
  number = {45},
  journal = {BioMedical Engineering OnLine},
  author = {Gramfort, Alexandre and Papadopoulo, Th�odore and Olivi, Emmanuel and Clerc, Maureen},
  year = {2010},
  owner = {afdidehf}
}

@article{Grech2008,
  title = {Review on solving the inverse problem in EEG source analysis},
  volume = {5},
  doi = {10.1186/1743-0003-5-25},
  abstract = {In this primer, we give a review of the inverse problem for EEG source
	localization. This is intended for the researchers new in the field
	to get insight in the state-of-the-art techniques used to find approximate
	solutions of the brain sources giving rise to a scalp potential recording.
	Furthermore, a review of the performance results of the different
	techniques is provided to compare these different inverse solutions.
	The authors also include the results of a Monte-Carlo analysis which
	they performed to compare four non parametric algorithms and hence
	contribute to what is presently recorded in the literature. An extensive
	list of references to the work of other researchers is also provided.
	This paper starts off with a mathematical description of the inverse
	problem and proceeds to discuss the two main categories of methods
	which were developed to solve the EEG inverse problem, mainly the
	non parametric and parametric methods. The main difference between
	the two is to whether a fixed number of dipoles is assumed a priori
	or not. Various techniques falling within these categories are described
	including minimum norm estimates and their generalizations, LORETA,
	sLORETA, VARETA, S-MAP, ST-MAP, Backus-Gilbert, LAURA, Shrinking
	LORETA FOCUSS (SLF), SSLOFO and ALF for non parametric methods and
	beamforming techniques, BESA, subspace techniques such as MUSIC and
	methods derived from it, FINES, simulated annealing and computational
	intelligence algorithms for parametric methods. From a review of
	the performance of these techniques as documented in the literature,
	one could conclude that in most cases the LORETA solution gives satisfactory
	results. In situations involving clusters of dipoles, higher resolution
	algorithms such as MUSIC or FINES are however preferred. Imposing
	reliable biophysical and psychological constraints, as done by LAURA
	has given superior results. The Monte-Carlo analysis performed, comparing
	WMN, LORETA, sLORETA and SLF, for different noise levels and different
	simulated source depths has shown that for single source localization,
	regularized sLORETA gives the best solution in terms of both localization
	error and ghost sources. Furthermore the computationally intensive
	solution given by SLF was not found to give any additional benefits
	under such simulated conditions.},
  language = {English},
  timestamp = {2016-07-10T08:03:57Z},
  number = {1},
  journal = {Journal of NeuroEngineering and Rehabilitation},
  author = {Grech, Roberta and Cassar, Tracey and Muscat, Joseph and Camilleri, KennethP and Fabri, SimonG and Zervakis, Michalis and Xanthopoulos, Petros and Sakkalis, Vangelis and Vanrumste, Bart},
  year = {2008},
  eid = {25},
  owner = {Fardin}
}

@article{Greenshtein2004,
  title = {Persistence in high-dimensional linear predictor selection and the 	virtue of overparametrization},
  volume = {10},
  doi = {10.3150/bj/1106314846},
  timestamp = {2016-07-10T07:32:12Z},
  number = {2004},
  journal = {Bernoulli},
  author = {Greenshtein, Eitan and Ritov, Ya'Acov},
  year = {2004},
  pages = {971-988},
  masid = {10511219},
  owner = {afdidehf}
}

@article{Grey2015,
  title = {Mixed-norm estimates and symmetric geometric means},
  abstract = {The mixed-norm versions of the H�older andMinkowski integral inequalities
	are used to produce new, general estimates involving symmetric geometric
	means of mixed norms. Various existing mixed-norm estimates are shown
	to be simple special cases of these new results. Examples are also
	given of applying mixednorm H�older and Minkowski to other estimates,
	providing much easier proofs. Finally, the effectiveness of this
	technique is demonstrated by deriving a new inequality which combines
	features from two separate previous results.},
  timestamp = {2016-07-09T20:11:57Z},
  journal = {ArXiv e-prints},
  author = {Grey, W.},
  month = jul,
  year = {2015},
  keywords = {26B35,26D15,46A45 (secondary),46E30 (primary),Mathematics - Functional
	Analysis},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150708327G},
  archiveprefix = {arXiv},
  owner = {afdidehf},
  primaryclass = {math.FA}
}

@article{Gribonval2014,
  title = {Sparse dictionary learning in the presence of noise and outliers},
  timestamp = {2016-07-11T16:48:32Z},
  author = {Gribonval, R�mi},
  month = sep,
  year = {2014},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Gribonval2011,
  title = {Inverse problems and sparse representations},
  timestamp = {2016-07-09T19:44:44Z},
  author = {Gribonval, R�mi},
  month = nov,
  year = {2011},
  note = {read},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@inproceedings{Gribonval2005,
  title = {A simple test to check the optimality of sparse signal approximations},
  volume = {5},
  doi = {10.1109/ICASSP.2005.1416404},
  abstract = {Approximating a signal or an image with a sparse linear expansion
	from an overcomplete dictionary of atoms is an extremely useful tool
	to solve many signal processing problems. Finding the sparsest approximation
	of a signal from an arbitrary dictionary is an NP-hard problem. Despite
	this, several algorithms have been proposed that provide sub-optimal
	solutions. However, it is generally difficult to know how close the
	computed solution is to being "optimal", and whether another algorithm
	could provide a better result. In this paper we provide a simple
	test to check whether the output of a sparse approximation algorithm
	is nearly optimal, in the sense that no significantly different linear
	expansion from the dictionary can provide both a smaller approximation
	error and a better sparsity. As a byproduct of our theorems, we obtain
	results on the identifiability of sparse overcomplete models in the
	presence of noise, for a fairly large class of sparse priors.},
  timestamp = {2016-07-08T11:28:48Z},
  booktitle = {Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP 	'05). IEEE International Conference on},
  author = {Gribonval, R. and Figueras i Ventura, R.M. and Vandergheynst, P.},
  month = mar,
  year = {2005},
  keywords = {Approximation algorithms,Approximation error,approximation theory,approximation
	theory,Dictionaries,Distortion measurement,Distortion
	measurement,Hilbert space,Hilbert spaces,Hilbert
	spaces,Matching pursuit algorithms,NP-hard problem,NP-hard
	problem,optimisation,overcomplete atom dictionary,overcomplete
	atom dictionary,signal processing,Signal
	processing algorithms,Signal processing
	algorithms,signal representation,source separation,sparse linear
	expansion,sparse
	linear expansion,sparse signal approximation optimality testing,sparsity
	checking,Testing},
  pages = {v/717-v/720 Vol. 5},
  owner = {Fardin}
}

@article{Gribonval2003a,
  title = {Sparse representations in unions of bases},
  volume = {49},
  issn = {0018-9448},
  doi = {10.1109/TIT.2003.820031},
  abstract = {The purpose of this correspondence is to generalize a result by Donoho
	and Huo and Elad and Bruckstein on sparse representations of signals
	in a union of two orthonormal bases for RN. We consider general (redundant)
	dictionaries for RN, and derive sufficient conditions for having
	unique sparse representations of signals in such dictionaries. The
	special case where the dictionary is given by the union of L?2 orthonormal
	bases for RN is studied in more detail. In particular, it is proved
	that the result of Donoho and Huo, concerning the replacement of
	the ?0 optimization problem with a linear programming problem when
	searching for sparse representations, has an analog for dictionaries
	that may be highly redundant.},
  timestamp = {2016-09-29T16:24:26Z},
  number = {12},
  journal = {IEEE Trans. Inf. Theory},
  author = {Gribonval, R. and Nielsen, M.},
  month = dec,
  year = {2003},
  keywords = {Dictionaries,Grassmannian frame,indeterminancy,linear programming,minimisation,mutually
	incoherent base,nonlinear approximation,NSP,null space property,redundant dictionary,redundant
	dictionary,sparse matrices,sparse
	matrices,sparse representation,Sufficient conditions,Vectors},
  pages = {3320--3325},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  owner = {Fardin}
}

@article{Gribonval2006,
  title = {On the exponential convergence of matching pursuits in quasi-incoherent 	dictionaries},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.860474},
  abstract = {The purpose of this correspondence is to extend results by Villemoes
	and Temlyakov about exponential convergence of Matching Pursuit (MP)
	with some structured dictionaries for "simple" functions in finite
	or infinite dimension. The results are based on an extension of Tropp's
	results about Orthogonal Matching Pursuit (OMP) in finite dimension,
	with the observation that it does not only work for OMP but also
	for MP. The main contribution is a detailed analysis of the approximation
	and stability properties of MP with quasi-incoherent dictionaries,
	and a bound on the number of steps sufficient to reach an error no
	larger than a penalization factor times the best m-term approximation
	error.},
  timestamp = {2016-07-10T07:16:45Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Gribonval, R. and Vandergheynst, P.},
  month = jan,
  year = {2006},
  keywords = {Approximation error,approximation theory,Convergence,Dictionaries,Dictionary,exponential
	convergence,finite dimension,greedy algorithm,Greedy algorithms,Greedy
	algorithms,Hilbert space,Image Processing,Iterative algorithms,iterative
	methods,Matching pursuit algorithms,matching pursuit (MP),nonlinear
	approximation,numerical stability,OMP,orthogonal matching pursuit,Pursuit
	algorithms,quasiincoherent dictionary,signal processing,Signal processing
	algorithms,Signal synthesis,sparse matrices,sparse representation,sparse
	representation,stability property,time-frequency analysis},
  pages = {255-261},
  owner = {afdidehf}
}

@article{Grossi2015,
  title = {High-rate compression of ECG signals by an accuracy-driven sparsity 	model relying on natural basis},
  volume = {45},
  issn = {1051-2004},
  doi = {http://dx.doi.org/10.1016/j.dsp.2015.06.006},
  abstract = {Abstract Long duration recordings of \{ECG\} signals require high
	compression ratios, in particular when storing on portable devices.
	Most of the \{ECG\} compression methods in literature are based on
	wavelet transform while only few of them rely on sparsity promotion
	models. In this paper we propose a novel \{ECG\} signal compression
	framework based on sparse representation using a set of \{ECG\} segments
	as natural basis. This approach exploits the signal regularity, i.e.
	the repetition of common patterns, in order to achieve high compression
	ratio (CR). We apply k-LiMapS as fine-tuned sparsity solver algorithm
	guaranteeing the required signal reconstruction quality \{PRDN\}
	(Normalized Percentage Root-mean-square Difference). Extensive experiments
	have been conducted on all the 48 records of MIT-BIH Arrhythmia Database
	and on some 24 hour records from the Long-Term \{ST\} Database. Direct
	comparisons of our method with several state-of-the-art \{ECG\} compression
	methods (namely ARLE, Rajoub's, SPIHT, TRE) prove its effectiveness.
	Our method achieves average performances that are two-three times
	higher than those obtained by the other assessed methods. In particular
	the compression ratio gap between our method and the others increases
	with growing PRDN.},
  timestamp = {2016-07-08T12:43:29Z},
  journal = {Digital Signal Processing},
  author = {Grossi, Giuliano and Lanzarotti, Raffaella and Lin, Jianyi},
  year = {2015},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(ECG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(ECG\\\\\\\\\\\\\\\\,\\\\\\\\(ECG\\\\\\\\,\\\\(ECG\\\\,arrhythmia,Cardiac,compression,guaranteed,High,PRDN,ratio,representation,Sparse},
  pages = {96 - 106},
  owner = {afdidehf}
}

@article{Grova2008,
  title = {Concordance between distributed EEG source localization and simultaneous 	EEG-fMRI studies of epileptic spikes},
  volume = {39},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2007.08.020},
  abstract = {In order to analyze where epileptic spikes are generated, we assessed
	the level of concordance between \{EEG\} source localization using
	distributed source models and simultaneous EEG-fMRI which measures
	the hemodynamic correlates of \{EEG\} activity. Data to be compared
	were first estimated on the same cortical surface and two comparison
	strategies were used: (1) MEM-concordance: a comparison between \{EEG\}
	sources localized with the Maximum Entropy on the Mean (MEM) method
	and fMRI clusters showing a significant hemodynamic response. Minimal
	geodesic distances between local extrema and overlap measurements
	between spatial extents of \{EEG\} sources and fMRI clusters were
	used to quantify MEM-concordance. (2) fMRI-relevance: estimation
	of the fMRI-relevance index α quantifying if sources located in
	an fMRI cluster could explain some scalp \{EEG\} data, when this
	fMRI cluster was used to constrain the \{EEG\} inverse problem. Combining
	MEM-concordance and fMRI-relevance (α) indexes, each fMRI cluster
	showing a significant hemodynamic response (p &lt; 0.05 corrected)
	was classified according to its concordance with \{EEG\} data. Nine
	patients with focal epilepsy who underwent EEG-fMRI examination followed
	by \{EEG\} recording outside the scanner were selected for this study.
	Among the 62 fMRI clusters analyzed (7 patients), 15 (24%) found
	in 6 patients were highly concordant with \{EEG\} according to both
	MEM-concordance and fMRI-relevance. \{EEG\} concordance was found
	for 5 clusters (8%) according to α only, suggesting sources missed
	by the MEM. No concordance with \{EEG\} was found for 30 clusters
	(48%) and for 10 clusters (16%) α was significantly negative, suggesting
	EEG-fMRI discordance. We proposed two complementary strategies to
	assess and classify EEG-fMRI concordance. We showed that for most
	patients, part of the hemodynamic response to spikes was highly concordant
	with \{EEG\} sources, whereas other fMRI clusters in response to
	the same spikes were found distant or discordant with \{EEG\} sources.},
  timestamp = {2016-07-08T12:01:52Z},
  number = {2},
  journal = {NeuroImage},
  author = {Grova, C. and Daunizeau, J. and Kobayashi, E. and Bagshaw, A. P. and Lina, J.-M. and Dubeau, F. and Gotman, J.},
  year = {2008},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\{EEG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\} source localization,\\\\\\\\\\\\\\\\{EEG\\\\\\\\\\\\\\\\} source localization,\\\\\\\\{EEG\\\\\\\\} source localization,\\\\{EEG\\\\} source localization,Concordance,Interictal epileptic spikes,Interictal
	epileptic spikes,Interictal epileptic
	spikes,Simultaneous EEG-fMRI,Simultaneous
	EEG-fMRI},
  pages = {755 - 774},
  owner = {Fardin}
}

@inproceedings{Gui2014,
  title = {Block Bayesian sparse learning algorithms with application to estimating 	channels in OFDM systems},
  doi = {10.1109/WPMC.2014.7014823},
  abstract = {Cluster-sparse channels often exist in frequency-selective fading
	broadband communication systems. The main reason is received scattered
	waveform exhibits cluster structure which is caused by a few reflectors
	near the receiver. Conventional sparse channel estimation methods
	have been proposed for general sparse channel model which without
	considering the potential cluster-sparse structure information. In
	this paper, we investigate the cluster-sparse channel estimation
	(CS-CE) problems in the state of the art orthogonal frequency-division
	multiplexing (OFDM) systems. Novel Bayesian cluster-sparse channel
	estimation (BCS-CE) methods are proposed to exploit the cluster-sparse
	structure by using block sparse Bayesian learning (BSBL) algorithm.
	The proposed methods take advantage of the cluster correlation in
	training matrix so that they can improve estimation performance.
	In addition, different from our previous method using uniform block
	partition information, the proposed methods can work well when the
	prior block partition information of channels is unknown. Computer
	simulations show that the proposed method has a superior performance
	when compared with the previous methods.},
  timestamp = {2016-07-08T11:41:00Z},
  booktitle = {Wireless Personal Multimedia Communications (WPMC), 2014 International 	Symposium on},
  author = {Gui, Guan and Xu, Li and Shan, Lin},
  month = sep,
  year = {2014},
  keywords = {BCS-CE methods,block Bayesian sparse learning algorithms,block partition
	information,broadband networks,BSBL algorithm,channel estimation,Channel
	models,Clustering algorithms,cluster-sparse structure information,Complexity
	theory,conventional sparse channel estimation methods,fading channels,frequency-selective
	fading broadband communication systems,learning (artificial intelligence),matrix
	algebra,OFDM,OFDM modulation,OFDM systems,orthogonal frequency- division
	multiplexing systems,Signal to noise ratio,telecommunication computing,Wireless
	communication},
  pages = {238-242},
  owner = {afdidehf}
}

@article{Guleryuz2006,
  title = {Nonlinear approximation based image recovery using adaptive sparse 	reconstructions and iterated denoising-part II: adaptive algorithms},
  volume = {15},
  issn = {1057-7149},
  doi = {10.1109/TIP.2005.863055},
  abstract = {We combine the main ideas introduced in Part I with adaptive techniques
	to arrive at a powerful algorithm that estimates missing data in
	nonstationary signals. The proposed approach operates automatically
	based on a chosen linear transform that is expected to provide sparse
	decompositions over missing regions such that a portion of the transform
	coefficients over missing regions are zero or close to zero. Unlike
	prevalent algorithms, our method does not necessitate any complex
	preconditioning, segmentation, or edge detection steps, and it can
	be written as a progression of denoising operations. We show that
	constructing estimates based on nonlinear approximants is fundamentally
	a nonconvex problem and we propose a progressive algorithm that is
	designed to deal with this issue directly. The algorithm is applied
	to images through an extensive set of simulation examples, primarily
	on missing regions containing textures, edges, and other image features
	that are not readily handled by established estimation and recovery
	methods. We discuss the properties required of good transforms, and
	in conjunction, show the types of regions over which well-known transforms
	provide good predictors. We further discuss extensions of the algorithm
	where the utilized transforms are also chosen adaptively, where unpredictable
	signal components in the progressions are identified and not predicted,
	and where the prediction scenario is more general.},
  timestamp = {2016-07-10T06:56:55Z},
  number = {3},
  journal = {Image Processing, IEEE Transactions on},
  author = {Guleryuz, O.G.},
  month = mar,
  year = {2006},
  keywords = {Adaptive algorithm,adaptive signal processing,adaptive sparse reconstructions,Algorithm
	design and analysis,Algorithms,Approximation algorithms,Artificial
	Intelligence,Automated,Computer-Assisted,computer graphics,Computer
	Simulation,Data models,Engineering profession,Error concealment,image
	denoising,Image edge detection,Image Enhancement,Image Interpretation,Image reconstruction,Image
	reconstruction,image recovery,Image resolution,Image
	segmentation,image texture,imaging,Information Storage and Retrieval,inpainting,iterated
	denoising,iterated methods,iterative methods,linear transform,mean
	square error methods,Models,Noise reduction,nonlinear approximation,nonlinear
	approximation,Nonlinear Dynamics,nonstationary signals,Pattern Recognition,progressive
	algorithm,Reproducibility of Results,Sensitivity and Specificity,signal
	processing,sparse recovery,sparse representations,Statistical,Three-Dimensional,transforms},
  pages = {555-571},
  owner = {afdidehf}
}

@article{Guleryuz2006a,
  title = {Nonlinear approximation based image recovery using adaptive sparse 	reconstructions and iterated denoising-part I: theory},
  volume = {15},
  issn = {1057-7149},
  doi = {10.1109/TIP.2005.863057},
  abstract = {We study the robust estimation of missing regions in images and video
	using adaptive, sparse reconstructions. Our primary application is
	on missing regions of pixels containing textures, edges, and other
	image features that are not readily handled by prevalent estimation
	and recovery algorithms. We assume that we are given a linear transform
	that is expected to provide sparse decompositions over missing regions
	such that a portion of the transform coefficients over missing regions
	are zero or close to zero. We adaptively determine these small magnitude
	coefficients through thresholding, establish sparsity constraints,
	and estimate missing regions in images using information surrounding
	these regions. Unlike prevalent algorithms, our approach does not
	necessitate any complex preconditioning, segmentation, or edge detection
	steps, and it can be written as a sequence of denoising operations.
	We show that the region types we can effectively estimate in a mean-squared
	error sense are those for which the given transform provides a close
	approximation using sparse nonlinear approximants. We show the nature
	of the constructed estimators and how these estimators relate to
	the utilized transform and its sparsity over regions of interest.
	The developed estimation framework is general, and can readily be
	applied to other nonstationary signals with a suitable choice of
	linear transforms. Part I discusses fundamental issues, and Part
	II is devoted to adaptive algorithms with extensive simulation examples
	that demonstrate the power of the proposed techniques.},
  timestamp = {2016-07-10T06:56:59Z},
  number = {3},
  journal = {Image Processing, IEEE Transactions on},
  author = {Guleryuz, O.G.},
  month = mar,
  year = {2006},
  keywords = {Adaptive algorithm,adaptive signal processing,adaptive sparse reconstructions,Algorithms,Artificial
	Intelligence,Automated,Computer-Assisted,computer graphics,Computer
	Simulation,Error concealment,image denoising,Image edge detection,Image
	Enhancement,Image Interpretation,Image reconstruction,image recovery,Image
	resolution,Image segmentation,image sequences,Image storage,image
	texture,imaging,Information Storage and Retrieval,inpainting,iterated denoising,iterated
	denoising,Iterative decoding,iterative methods,linear
	transform,mean-squared error method,Models,Noise reduction,nonlinear
	approximation,Nonlinear Dynamics,Pattern Recognition,Pixel,Reproducibility
	of Results,Robustness,Sensitivity and Specificity,signal processing,sparse
	recovery,sparse representations,Statistical,Three-Dimensional,transforms,Video
	compression},
  pages = {539-554},
  owner = {afdidehf}
}

@inproceedings{Guo2011,
  title = {A tree based recovery algorithm for block sparse signals},
  abstract = {The structure of block sparsity in multi-band signals is prevalent.
	Performance of recovery algorithms that taking advantage of the block
	sparsity structure is promising in the compressed sensing framework.
	In this paper, we propose a binary tree based recovery algorithm
	for block-sparse signals, where we exploit the fact that each block
	may have zero and nonzero elements both. The proposed algorithm improves
	the current algorithms through iteratively separating the recovered
	blocks of signals into two smaller blocks. Therefore, greedy searching
	based algorithm is possible to obtain more accurate basis for signal
	recovery. Simulations are performed and the results show the improvements
	over current block-based recovery algorithms.},
  timestamp = {2016-07-08T11:33:47Z},
  booktitle = {Cognitive Radio Oriented Wireless Networks and Communications (CROWNCOM), 	2011 Sixth International ICST Conference on},
  author = {Guo, Wenbin and Wang, Xing and Lu, Yang and Wang, Wenbo},
  month = jun,
  year = {2011},
  keywords = {binary tree based recovery algorithm,Binary trees,block sparse signals,compressed
	sensing framework,Matching pursuit algorithms,multiband signals,Noise,Robustness,Signal
	processing algorithms,signal reconstruction,signal recovery,sparse
	matrices,trees (mathematics),Vectors},
  pages = {91-95},
  owner = {afdidehf}
}

@article{Gurevich2008,
  title = {The Finite Harmonic Oscillator And Its Associated Sequences},
  volume = {105},
  abstract = {A system of functions (signals) on the finite line, called the oscillator
	system, is described and studied. Applications of this system for
	discrete radar and digital communication theory are explained.},
  timestamp = {2016-07-11T17:01:05Z},
  number = {29},
  journal = {Proc. Natl. Acad. Sci. (PNAS) USA},
  author = {Gurevich, Shamgar and Hadani, Ronny and Sochen, Nir},
  month = jul,
  year = {2008},
  keywords = {commutative subgroups,deterministic construction,eigenfunctions,random
	behavior,Weil representation},
  pages = {9869�9873},
  owner = {Fardin}
}

@article{Gurevich2008a,
  title = {On Some Deterministic Dictionaries Supporting Sparsity},
  volume = {14},
  abstract = {We describe a new construction of an incoherent dictionary, referred
	to as the oscillator dictionary, which is based on considerations
	in the representation theory of finite groups. The oscillator dictionary
	consists of order of p^5 unit vectors in a Hilbert space of dimension
	p, where p is an odd prime, whose pairwise inner products have magnitude
	of at most 4/sqrt(p). An explicit algorithm to construct a large
	portion of the oscillator dictionary is presented.},
  timestamp = {2016-07-10T07:13:39Z},
  number = {5-6},
  journal = {pecial issue on sparsity, the Journal of Fourier Analysis and Applications},
  author = {Gurevich, Shamgar and Hadani, Ronny and Sochen, Nir},
  month = dec,
  year = {2008},
  pages = {859-876},
  owner = {Fardin}
}

@inproceedings{Guyon2012,
  title = {Foreground detection based on low-rank and block-sparse matrix decomposition},
  doi = {10.1109/ICIP.2012.6467087},
  abstract = {Foreground detection is the first step in video surveillance system
	to detect moving objects. Principal Components Analysis (PCA) shows
	a nice framework to separate moving objects from the background but
	without a mechanism of robust analysis, the moving objects may be
	absorbed into the background model. This drawback can be solved by
	recent researches on Robust Principal Component Analysis (RPCA).
	The background sequence is then modeled by a low rank subspace that
	can gradually change over time, while the moving foreground objects
	constitute the correlated sparse outliers. In this paper, we propose
	to use a RPCA method based on low-rank and block-sparse matrix decomposition
	to achieve foreground detection. This decomposition enforces the
	low-rankness of the background and the block-sparsity aspect of the
	foreground. Experimental results on different datasets show the pertinence
	of the proposed approach.},
  timestamp = {2016-07-08T12:31:41Z},
  booktitle = {Image Processing (ICIP), 2012 19th IEEE International Conference 	on},
  author = {Guyon, C. and Bouwmans, T. and Zahzah, E.-H.},
  month = sep,
  year = {2012},
  keywords = {background sequence,block-sparse matrix decomposition,foreground detection,low-rank
	matrix decomposition,low rank subspace,matrix decomposition,Matrix
	decomposition,moving foreground objects,Noise,object detection,PCA,Principal
	component analysis,principal components analysis,Robustness,robust principal component analysis,Robust
	Principal Component Analysis,RPCA,sparse
	matrices,Training,video surveillance,video surveillance system},
  pages = {1225-1228},
  owner = {afdidehf}
}

@inproceedings{Haifeng2015,
  title = {Block MMV for the reconstruction of multiband signals},
  doi = {10.1109/ChiCC.2015.7260339},
  abstract = {In this paper, we present the exact recovery condition of block multiple
	measurements vectors (BMMV) algorithm using block-coherence that
	is easy to compute. We also use the BMMV algorithm to reconstruct
	multiband signals even there is not the knowledge of the number of
	bands and the bandwidth. The numerical experiments show that BMMV
	is validity when the sampling rate is small.},
  timestamp = {2016-07-08T11:41:30Z},
  booktitle = {Control Conference (CCC), 2015 34th Chinese},
  author = {Haifeng, Li and Rui, Li and Bei, Li},
  month = jul,
  year = {2015},
  keywords = {Bandwidth,block-coherence,block MMV algorithm,block multiple measurements
	vectors algorithm,block sparse,Coherence,compressed sensing,compressed
	sensing,Electronic mail,exact recovery condition,Fourier transforms,Joints,Linear
	matrix inequalities,multiband signal reconstruction,multiband signals,multiple
	measurements vectors,sampling rate,signal reconstruction,signal sampling,signal
	sampling,sparse matrices,sparse
	matrices,Vectors},
  pages = {4523-4528},
  owner = {afdidehf}
}

@incollection{Halchenko2005,
  title = {Multimodal Integration: fMRI, MRI, EEG, MEG},
  abstract = {This chapter provides a comprehensive survey of the motivations, assumptions
	and pitfalls associated with combining signals such as fMRI with
	EEG or MEG. Our initial focus in the chapter concerns mathematical
	approaches for solving the localization problem in EEG and MEG. Next
	we document the most recent and promising ways in which these signals
	can be combined with fMRI. Specically, we look at correlative analysis,
	decomposition techniques, equivalent dipole tting, distributed sources
	modeling, beamforming, and Bayesian methods. Due to difculties in
	assessing ground truth of a combined signal in any realistic experiment?a
	difculty further confounded by lack of accurate biophysical models
	of BOLD signal?we are cautious to be optimistic about multimodal
	integration. Nonetheless, as we highlight and explore the technical
	and methodological difculties of fusing heterogeneous signals, it
	seems likely that correct fusion of multimodal data will allow previously
	inaccessible spatiotemporal structures to be visualized and formalized
	and thus eventually become a useful tool in brain imaging research.},
  timestamp = {2016-10-21T13:35:35Z},
  author = {Halchenko, Yaroslav O. and Hanson, Stephen Jose and Pearlmutter, Barak A.},
  year = {2005},
  pages = {223--265},
  owner = {Fardin}
}

@article{Hallez2007,
  title = {Review on solving the forward problem in EEG source analysis},
  volume = {4},
  doi = {10.1186/1743-0003-4-46},
  abstract = {Background: The aim of electroencephalogram (EEG) source localization
	is to find the brain areas responsible for EEG waves of interest.
	It consists of solving forward and inverse problems. The forward
	problem is solved by starting from a given electrical source and
	calculating the potentials at the electrodes. These evaluations are
	necessary to solve the inverse problem which is defined as finding
	brain sources which are responsible for the measured potentials at
	the EEG electrodes. Methods: While other reviews give an extensive
	summary of the both forward and inverse problem, this review article
	focuses on different aspects of solving the forward problem and it
	is intended for newcomers in this research field. Results: It starts
	with focusing on the generators of the EEG: the post-synaptic potentials
	in the apical dendrites of pyramidal neurons. These cells generate
	an extracellular current which can be modeled by Poisson's differential
	equation, and Neumann and Dirichlet boundary conditions. The compartments
	in which these currents flow can be anisotropic (e.g. skull and white
	matter). In a three-shell spherical head model an analytical expression
	exists to solve the forward problem. During the last two decades
	researchers have tried to solve Poisson's equation in a realistically
	shaped head model obtained from 3D medical images, which requires
	numerical methods. The following methods are compared with each other:
	the boundary element method (BEM), the finite element method (FEM)
	and the finite difference method (FDM). In the last two methods anisotropic
	conducting compartments can conveniently be introduced. Then the
	focus will be set on the use of reciprocity in EEG source localization.
	It is introduced to speed up the forward calculations which are here
	performed for each electrode position rather than for each dipole
	position. Solving Poisson's equation utilizing FEM and FDM corresponds
	to solving a large sparse linear system. Iterative methods are required
	to solve these sparse linear systems. The following iterative methods
	are discussed: successive overrelaxation, conjugate gradients method
	and algebraic multigrid method. Conclusion: Solving the forward problem
	has been well documented in the past decades. In the past simplified
	spherical head models are used, whereas nowadays a combination of
	imaging modalities are used to accurately describe the geometry of
	the head model. Efforts have been done on realistically describing
	the shape of the head model, as well as the heterogenity of the tissue
	types and realistically determining the conductivity. However, the
	determination and validation of the in vivo conductivity values is
	still an important topic in this field. In addition, more studies
	have to be done on the influence of all the parameters of the head
	model and of the numerical techniques on the solution of the forward
	problem.},
  timestamp = {2016-07-10T08:03:54Z},
  number = {46},
  journal = {Journal of NeuroEngineering and Rehabilitation},
  author = {Hallez, Hans and Vanrumste, Bart and Grech, Roberta and Muscat, Joseph and Clercq, Wim De and Vergult, Anneleen and D'Asseler, Yves and Camilleri, Kenneth P. and Fabri, Simon G. and Huffel, Sabine Van and Lemahieu, Ignace},
  year = {2007},
  owner = {afdidehf}
}

@inproceedings{Haltmeier2013,
  title = {Block-sparse analysis regularization of ill-posed problems via $\ell^{2,1}$-minimization},
  doi = {10.1109/MMAR.2013.6669964},
  abstract = {Recovering an infinite dimensional parameter from incomplete and noisy
	observations is a fundamental task in many branches of mathematical
	and engineering science. Reasonable solution approaches require the
	use of regularization techniques, which incorporate a-priori knowledge
	about the desired unknown. For that purpose a frequently used property
	is the (block) sparsity of the coefficients with respect to some
	sparsifying transformation. In this paper we review regularization
	methods for sparse inverse problems and derive linear stability estimates
	for block-sparse analysis regularization implemented via ?2,1-minimization.},
  timestamp = {2016-07-08T11:41:35Z},
  booktitle = {Methods and Models in Automation and Robotics (MMAR), 2013 18th International 	Conference on},
  author = {Haltmeier, M.},
  month = aug,
  year = {2013},
  keywords = {?2,1-minimization,analysis prior,block-sparse analysis regularization
	technique,Block-sparsity,compressed sensing,Convergence,Hilbert space,ill-posed
	problems,incomplete observations,infinite dimensional parameter recovery,inverse problems,Inverse
	problems,linear stability estimates,minimisation,Minimization,Noise,noisy
	observations,set theory,sparse inverse problems,sparsifying transformation,Stability,Stability
	analysis,transforms},
  pages = {520-523},
  owner = {afdidehf}
}

@inproceedings{Hamid2013,
  title = {MEG-EEG Fusion By Kalman Filtering Within A Source Analysis Framework},
  doi = {10.1109/EMBC.2013.6610626},
  abstract = {The fusion of data from multiple neuroimaging modalities may improve
	the temporal and spatial resolution of non-invasive brain imaging.
	In this paper, we present a novel method for the fusion of simultaneously
	recorded electroencephalograms (EEG) and magnetoencephalograms (MEG)
	within the framework of source analysis. This method represents an
	extension of a previously published spatio-temporal inverse solution
	method to the case of MEG or combined MEG-EEG signals. Moreover,
	we use a state-of-the-art realistic finite element (FE) head model
	especially calibrated for the MEG-EEG fusion problem. Using a real
	data set containing an epileptic spike, we validate the source analysis
	results of the spatio-temporal inverse solution using the results
	of the LORETA method and the findings from other structural and functional
	modalities. We show that the proposed fusion method, despite the
	low signal-to-noise ratio (SNR) of single spikes, points to the same
	brain area that was found by the other modalities. Furthermore, it
	correctly identifies the same source as the main generator for the
	MEG and EEG spikes.},
  timestamp = {2016-10-21T13:55:29Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 35th Annual 	International Conference of the IEEE},
  author = {Hamid, L. and Aydin, U. and Wolters, C. and Stephani, U. and Siniatchkin, M. and Galka, A.},
  month = jul,
  year = {2013},
  keywords = {bioelectric phenomena,Brain,brain area,Brain modeling,calibration,EEG
	spikes,electroencephalograms,electroencephalography,epileptic spike,Equations,finite
	element analysis,finite element head model,functional modalities,Head,Kalman
	filtering,Kalman filters,LORETA method,Magnetic heads,magnetoencephalograms,magnetoencephalography,Mathematical
	model,medical signal processing,MEG-EEG fusion,MEG-EEG signals,MEG
	spikes,neuroimaging modalities,noninvasive brain imaging,signal-to-noise
	ratio,source analysis,spatiotemporal inverse solution,spatiotemporal
	phenomena,structural modalities},
  pages = {4819--4822},
  annote = {read},
  owner = {Fardin}
}

@inproceedings{Han2004,
  title = {Regularized FOCUSS algorithm for EEG/MEG source imaging},
  volume = {1},
  doi = {10.1109/IEMBS.2004.1403106},
  abstract = {We derived a generalized version of the regularized FOCUSS algorithm
	which was derived in a paper by Phillips, JW et al., (1997). It allows
	general forms of noise covariance and reduces depth effect when imaging
	focal neural sources from electroencephalography (EEG) / magnetoencephalography
	(MEG) data. We compared a depth-weighted regularized algorithm with
	FOCUSS and a regularized FOCUSS through simulation study. The suggested
	algorithm gave sparser and less spurious solutions than the others.},
  timestamp = {2016-07-10T07:50:45Z},
  booktitle = {Engineering in Medicine and Biology Society, 2004. IEMBS '04. 26th 	Annual International Conference of the IEEE},
  author = {Han, Jooman and Park, Kwang Suk},
  month = sep,
  year = {2004},
  keywords = {Bayesian methods,biomedical engineering,Brain modeling,Current measurement,depth-weighted
	regularized algorithm,Dipole,electroencephalography,focal neural
	source imaging,Focusing,inverse problem,inverse problems,Iterative
	algorithms,magnetic field measurement,Magnetic heads,magnetoencephalography,medical
	signal processing,neurophysiology,noise covariance,regularization,Sparse},
  pages = {122-124},
  owner = {afdidehf}
}

@inproceedings{Hansen2014,
  title = {EEG source reconstruction using sparse basis function representations},
  doi = {10.1109/PRNI.2014.6858521},
  abstract = {State of the art performance of 3D EEG imaging is based on reconstruction
	using spatial basis function repre-sentations. In this work we augment
	the Variational Garrote (VG) approach for sparse approximation to
	incorporate spatial basis functions. As VG handles the bias variance
	trade-off with cross-validation this approach is more automated than
	competing approaches such as Multiple Sparse Priors (Friston et al.,
	2008) or Champagne (Wipf et al., 2010) that require manual selection
	of noise level and auxiliary signal free data, respectively. Finally,
	we propose an unbiased estimator of the reproducibility of the reconstructed
	activation time course based on a split-half resampling protocol.},
  timestamp = {2016-07-08T12:18:08Z},
  booktitle = {Pattern Recognition in Neuroimaging, 2014 International Workshop 	on},
  author = {Hansen, S.T. and Hansen, L.K.},
  month = jun,
  year = {2014},
  keywords = {3D EEG imaging,auxiliary signal free data,Bayes methods,bias variance
	trade-off,Brain modeling,EEG source reconstruction,electroencephalography,Face,feature
	selection,Image reconstruction,manual selection,medical signal processing,multiple
	sparse priors,Noise level,reconstructed activation time course,signal
	denoising,signal reconstruction,signal sampling,Signal to noise ratio,sparse
	approximation,sparse basis function representations,spatial basis
	function representations,split-half resampling protocol,unbiased
	estimator,variational Garrote approach},
  pages = {1-4},
  owner = {afdidehf}
}

@article{Haufe2014,
  title = {Solving the EEG inverse problem},
  timestamp = {2016-07-10T08:21:18Z},
  author = {Haufe, Stefan},
  year = {2014},
  howpublished = {BBCI Winter School 2014, Berlin},
  owner = {Fardin}
}

@article{Haufe2013,
  title = {A critical assessment of connectivity measures for EEG data: A simulation 	study},
  volume = {64},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2012.09.036},
  abstract = {Information flow between brain areas is difficult to estimate from
	\{EEG\} measurements due to the presence of noise as well as due
	to volume conduction. We here test the ability of popular measures
	of effective connectivity to detect an underlying neuronal interaction
	from simulated \{EEG\} data, as well as the ability of commonly used
	inverse source reconstruction techniques to improve the connectivity
	estimation. We find that volume conduction severely limits the neurophysiological
	interpretability of sensor-space connectivity analyses. Moreover,
	it may generally lead to conflicting results depending on the connectivity
	measure and statistical testing approach used. In particular, we
	note that the application of Granger-causal (GC) measures combined
	with standard significance testing leads to the detection of spurious
	connectivity regardless of whether the analysis is performed on sensor-space
	data or on sources estimated using three different established inverse
	methods. This empirical result follows from the definition of GC.
	The phase-slope index (PSI) does not suffer from this theoretical
	limitation and therefore performs well on our simulated data. We
	develop a theoretical framework to characterize artifacts of volume
	conduction, which may still be present even in reconstructed source
	time series as zero-lag correlations, and to distinguish their time-delayed
	brain interaction. Based on this theory we derive a procedure which
	suppresses the influence of volume conduction, but preserves effects
	related to time-lagged brain interaction in connectivity estimates.
	This is achieved by using time-reversed data as surrogates for statistical
	testing. We demonstrate that this robustification makes Granger-causal
	connectivity measures applicable to \{EEG\} data, achieving similar
	results as PSI. Integrating the insights of our study, we provide
	a guidance for measuring brain interaction from \{EEG\} data. Software
	for generating benchmark data is made available.},
  timestamp = {2016-07-08T10:08:50Z},
  journal = {NeuroImage},
  author = {Haufe, Stefan and Nikulin, Vadim V. and M{\"u}ller, Klaus-Robert and Nolte, Guido},
  year = {2013},
  keywords = {EEG},
  pages = {120 - 133},
  owner = {afdidehf}
}

@article{Haufe2008,
  title = {Focal And Rotationally Invariant Source Reconstruction From EEG/MEG},
  abstract = {Simultaneous localization of close neuronal sources from EEG/MEG measurements
	is a challenging problem. To solve it, we here propose Focal Vector
	Field Reconstruction (FVR) as a new source imaging technique. The
	FVR objective facilitates at the same time sparsity and smoothness
	of the solutions while it is invariant with respect to rotations
	of the coordinate system. In the simultaneous localization of left
	and right somatosensory N20 generators from real EEG recordings,
	FVR outperforms state-of-the-art methods (such as LORETA and Minimum
	Current Estimate), as it is the only method that delivers correct
	location of the source in the somatosensory area of each hemisphere;
	in accordance with neurophysiological prior knowledge.},
  timestamp = {2016-07-08T12:31:37Z},
  author = {Haufe, S. and Nikulin, V. V. and Ziehe, A. and M�ller, K.-R. and Nolte, G.},
  year = {2008},
  keywords = {EEG/MEG,inverse problem,Rotational Invariance,smoothness,Sparsity},
  owner = {afdidehf}
}

@article{Haufe2008a,
  title = {Combining sparsity and rotational invariance in EEG/MEG source reconstruction},
  volume = {42},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2008.04.246},
  abstract = {We introduce Focal Vector Field Reconstruction (FVR), a novel technique
	for the inverse imaging of vector fields. The method was designed
	to simultaneously achieve two goals: a) invariance with respect to
	the orientation of the coordinate system, and b) a preference for
	sparsity of the solutions and their spatial derivatives. This was
	achieved by defining the regulating penalty function, which renders
	the solutions unique, as a global l1-norm of local l2-norms. We show
	that the method can be successfully used for solving the \{EEG\}
	inverse problem. In the joint localization of 2–3 simulated dipoles,
	\{FVR\} always reliably recovers the true sources. The competing
	methods have limitations in distinguishing close sources because
	their estimates are either too smooth (LORETA, Minimum l1-norm) or
	too scattered (Minimum l2-norm). In both noiseless and noisy simulations,
	\{FVR\} has the smallest localization error according to the Earth
	Mover's Distance (EMD), which is introduced here as a meaningful
	measure to compare arbitrary source distributions. We also apply
	the method to the simultaneous localization of left and right somatosensory
	\{N20\} generators from real \{EEG\} recordings. Compared to its
	peers \{FVR\} was the only method that delivered correct location
	of the source in the somatosensory area of each hemisphere in accordance
	with neurophysiological prior knowledge.},
  timestamp = {2016-07-08T11:53:40Z},
  number = {2},
  journal = {NeuroImage},
  author = {Haufe, Stefan and Nikulin, Vadim V. and Ziehe, Andreas and M�ller, Klaus-Robert and Nolte, Guido},
  month = aug,
  year = {2008},
  keywords = {EEG/MEG},
  pages = {726 - 738},
  owner = {afdidehf}
}
{Haufe2011,
  title = {Large-scale EEG/MEG source localization with spatial flexibility},
  volume = {54},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.09.003},
  abstract = {We propose a novel approach to solving the electro-/magnetoencephalographic
	(EEG/MEG) inverse problem which is based upon a decomposition of
	the current density into a small number of spatial basis fields.
	It is designed to recover multiple sources of possibly different
	extent and depth, while being invariant with respect to phase angles
	and rotations of the coordinate system. We demonstrate the method's
	ability to reconstruct simulated sources of random shape and show
	that the accuracy of the recovered sources can be increased, when
	interrelated field patterns are co-localized. Technically, this leads
	to large-scale mathematical problems, which are solved using recent
	advances in convex optimization. We apply our method for localizing
	brain areas involved in different types of motor imagery using real
	data from Brain–Computer Interface (BCI) sessions. Our approach
	based on single-trial localization of complex Fourier coefficients
	yields class-specific focal sources in the sensorimotor cortices.},
  timestamp = {2016-07-09T19:52:56Z},
  number = {2},
  journal = {NeuroImage},
  author = {Haufe, Stefan and Tomioka, Ryota and Dickhaus, Thorsten and Sannelli, Claudia and Blankertz, Benjamin and Nolte, Guido and M�ller, Klaus-Robert},
  year = {2011},
  note = {read},
  keywords = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\,\\\\\\\\(EEG\\\\\\\\,\\\\(EEG\\\\},
  pages = {851 - 859},
  owner = {afdidehf}
}

@article{Haupt2006,
  title = {Signal Reconstruction From Noisy Random Projections},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2006.880031},
  abstract = {Recent results show that a relatively small number of random projections
	of a signal can contain most of its salient information. It follows
	that if a signal is compressible in some orthonormal basis, then
	a very accurate reconstruction can be obtained from random projections.
	This "compressive sampling" approach is extended here to show that
	signals can be accurately recovered from random projections contaminated
	with noise. A practical iterative algorithm for signal reconstruction
	is proposed, and potential applications to coding, analog-digital
	(A/D) conversion, and remote wireless sensing are discussed},
  timestamp = {2016-09-29T16:14:17Z},
  number = {9},
  journal = {IEEE Trans. Inf. Theory},
  author = {Haupt, J. and Nowak, R.},
  month = sep,
  year = {2006},
  keywords = {Chaos,Complexity regularization,compressive sampling approach,Conferences,Data
	Compression,denoising,Distortion,iterative algorithm,Iterative algorithms,iterative
	methods,Noise reduction,noisy random projection,Rademacher chaos,random
	projections,Random variables,Sampling,Sampling methods,signal reconstruction,signal
	reconstruction,signal sampling,Wireless sensor networks,wireless
	sensor networks},
  pages = {4036--4048},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on}
}

@article{He2004,
  title = {Methods of canonical analysis for functional data},
  volume = {122},
  doi = {10.1016/j.jspi.2003.06.003},
  abstract = {We consider estimates for functional canonical correlations and canonical
	weight functions. Four computational methods for the estimation of
	functional canonical correlation and canonical weight functions are
	proposed and compared, including one which is a slight variation
	of the spline method proposed by Leurgans, Moyeed and Silverman (1993).
	We propose dimension reduction and dimension augmentation procedures
	to address the dimensionality problems of functional canonical analysis
	(FCA) that are associated with computational break-down. Cross-validation
	is used for the automatic selection of tuning parameters, based on
	the minimax property of FCA. This allows to estimate several canonical
	correlations and canonical weight functions simultaneously and reasonably
	well as we show in simulations. The proposed estimation methods are
	compared and their use is demonstrated with medfly mortality data.},
  timestamp = {2016-07-09T20:11:48Z},
  number = {1},
  journal = {Journal of Statistical Planning and Inference},
  author = {He, Guozhong and M�ller, Hans-Georg and Wang, Jane-Ling},
  year = {2004},
  keywords = {Canonical correlation,canonical weight function,covariance operator,cross-validation,functional
	data analysis,functional least squares,L2-process,stochastic process},
  pages = {141-159},
  masid = {16192535},
  owner = {Fardin}
}

@article{He2015,
  title = {Small infrared target detection based on low-rank and sparse representation},
  volume = {68},
  issn = {1350-4495},
  doi = {http://dx.doi.org/10.1016/j.infrared.2014.10.022},
  abstract = {Abstract The method by which to obtain the correct detection result
	for infrared small targets is an important and challenging issue
	in infrared applications. In this paper, a low-rank and sparse representation
	(LRSR) model is proposed. This model can describe the specific structure
	of noise data effectively by utilizing sparse representation theory
	on the basis of low-rank matrix representation. In addition, \{LRSR\}
	based infrared small target detection algorithm is presented. First,
	a two-dimensional Gaussian model is used to produce the atoms that
	construct over-complete target dictionary. Then, the reset image
	data matrix is decomposed by the \{LRSR\} model to obtain the background,
	noise and target components of the image. Finally, the target position
	can be determined by threshold processing for the target component
	data. The experimental results in single objective frame, multi-objective
	image sequences, and strong noise background conditions demonstrate
	that the proposed method not only has high detection performance
	in effectively reducing the false alarm rate but also has strong
	robustness against noise interference.},
  timestamp = {2016-07-10T08:19:11Z},
  journal = {Infrared Physics \& Technology},
  author = {He, YuJie and Li, Min and Zhang, JinLi and An, Qi},
  year = {2015},
  keywords = {and,detection,Infrared,Low,rank,representation,small,Sparse,target},
  pages = {98 - 109},
  owner = {afdidehf}
}

@article{He2015a,
  title = {Optimization of learned dictionary for sparse coding in speech processing},
  issn = {0925-2312},
  doi = {http://dx.doi.org/10.1016/j.neucom.2015.03.061},
  abstract = {Abstract As a promising technique, sparse coding has been widely used
	for the analysis, representation, compression, denoising and separation
	of speech. This technique needs a good dictionary which contains
	atoms to represent speech signals. Although many methods have been
	proposed to learn such a dictionary, there are still two problems.
	First, unimportant atoms bring a heavy computational load to sparse
	decomposition and reconstruction, which prevents sparse coding from
	real-time application. Second, in speech denoising and separation,
	harmful atoms have no or ignorable contributions to reducing the
	sparsity degree but increase the source confusion, resulting in severe
	distortions. To solve these two problems, we first analyze the inherent
	assumptions of sparse coding and show that distortion can be caused
	if the assumptions do not hold true. Next, we propose two methods
	to optimize a given dictionary by removing unimportant atoms and
	harmful atoms, respectively. Experiments show that the proposed methods
	can further improve the performance of dictionaries.},
  timestamp = {2016-07-10T07:20:18Z},
  journal = {Neurocomputing},
  author = {He, Yongjun and Sun, Guanglu and Han, Jiqing},
  year = {2015},
  keywords = {dictionary optimization,Sparse Coding,speech denoising,speech recognition},
  pages = {-},
  owner = {afdidehf}
}

@article{Heers2015,
  title = {Localization Accuracy of Distributed Inverse Solutions for Electric 	and Magnetic Source Imaging of Interictal Epileptic Discharges in 	Patients with Focal Epilepsy},
  issn = {0896-0267},
  doi = {10.1007/s10548-014-0423-1},
  abstract = {Distributed inverse solutions aim to realistically reconstruct the
	origin of interictal epileptic discharges (IEDs) from noninvasively
	recorded electroencephalography (EEG) and magnetoencephalography
	(MEG) signals. Our aim was to compare the performance of different
	distributed inverse solutions in localizing IEDs: coherent maximum
	entropy on the mean (cMEM), hierarchical Bayesian implementations
	of independent identically distributed sources (IID, minimum norm
	prior) and spatially coherent sources (COH, spatial smoothness prior).
	Source maxima (i.e., the vertex with the maximum source amplitude)
	of IEDs in 14 EEG and 19 MEG studies from 15 patients with focal
	epilepsy were analyzed. We visually compared their concordance with
	intracranial EEG (iEEG) based on 17 cortical regions of interest
	and their spatial dispersion around source maxima. Magnetic source
	imaging (MSI) maxima from cMEM were most often confirmed by iEEG
	(cMEM: 14/19, COH: 9/19, IID: 8/19 studies). COH electric source
	imaging (ESI) maxima co-localized best with iEEG (cMEM: 8/14, COH:
	11/14, IID: 10/14 studies). In addition, cMEM was less spatially
	spread than COH and IID for ESI and MSI (p < 0.001 Bonferroni-corrected
	post hoc t test). Highest positive predictive values for cortical
	regions with IEDs in iEEG could be obtained with cMEM for MSI and
	with COH for ESI. Additional realistic EEG/MEG simulations confirmed
	our findings. Accurate spatially extended sources, as found in cMEM
	(ESI and MSI) and COH (ESI) are desirable for source imaging of IEDs
	because this might influence surgical decision. Our simulations suggest
	that COH and IID overestimate the spatial extent of the generators
	compared to cMEM.},
  language = {English},
  timestamp = {2016-10-21T14:02:33Z},
  journal = {Brain Topography},
  author = {Heers, Marcel and Chowdhury, RashedaA. and Hedrich, Tanguy and Dubeau, François and Hall, JefferyA. and Lina, Jean-Marc and Grova, Christophe and Kobayashi, Eliane},
  year = {2015},
  keywords = {Electric source imaging (ESI),Electroencephalography (EEG),Focal epilepsy,Intracranial
	EEG,Magnetic source imaging (MSI),Magnetoencephalography (MEG)},
  pages = {1--20},
  owner = {afdidehf}
}

@book{Heisenberg1930,
  title = {The Physical Principles of the Quantum Theory},
  isbn = {0-486-60113-7 978-0-486-60113-7},
  timestamp = {2016-07-11T17:02:47Z},
  publisher = {{\{Dover Publications\}}},
  author = {Heisenberg, Werner},
  year = {1930},
  owner = {Fardin}
}

@inproceedings{Herrity2006,
  title = {Sparse Approximation Via Iterative Thresholding},
  volume = {3},
  doi = {10.1109/ICASSP.2006.1660731},
  abstract = {The well-known shrinkage technique is still relevant for contemporary
	signal processing problems over redundant dictionaries. We present
	theoretical and empirical analyses for two iterative algorithms for
	sparse approximation that use shrinkage. The general IT algorithm
	amounts to a Landweber iteration with nonlinear shrinkage at each
	iteration step. The block IT algorithm arises in morphological components
	analysis. A sufficient condition for which general IT exactly recovers
	a sparse signal is presented, in which the cumulative coherence function
	naturally arises. This analysis extends previous results concerning
	the orthogonal matching pursuit (OMP) and basis pursuit (BP) algorithms
	to IT algorithms},
  timestamp = {2016-07-11T16:40:00Z},
  booktitle = {Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 	2006 IEEE International Conference on},
  author = {Herrity, K.K. and Gilbert, A.C. and Tropp, J.A.},
  month = may,
  year = {2006},
  keywords = {Algorithm design and analysis,basis pursuit algorithms,cumulative
	coherence function,Dictionaries,Iterative algorithms,iterative methods,iterative
	methods,iterative thresholding,iterative
	thresholding,landweber iteration,Machine learning algorithms,Machine learning
	algorithms,Matching pursuit algorithms,Matching
	pursuit algorithms,morphological components analysis,Noise reduction,Noise
	reduction,orthogonal matching pursuit,orthogonal
	matching pursuit,Pursuit algorithms,redundant dictionaries,redundant
	dictionaries,shrinkage technique,shrinkage
	technique,Signal analysis,signal processing,Signal processing algorithms,Signal
	processing algorithms,signal processing problems,signal
	processing problems,sparse approximation,sparse signal,sparse
	signal,statistical analysis,statistical
	analysis,Sufficient conditions},
  pages = {III-III},
  owner = {afdidehf}
}

@article{Herzet2013,
  title = {Exact Recovery Conditions for Sparse Representations With Partial 	Support Information},
  volume = {59},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2278179},
  abstract = {We address the exact recovery of a k-sparse vector in the noiseless
	setting when some partial information on the support is available.
	This partial information takes the form of either a subset of the
	true support or an approximate subset including wrong atoms as well.
	We derive a new sufficient and worst-case necessary (in some sense)
	condition for the success of some procedures based on ?p-relaxation,
	orthogonal matching pursuit (OMP), and orthogonal least squares (OLS).
	Our result is based on the coherence ? of the dictionary and relaxes
	the well-known condition ? <; 1/2k - 1) ensuring the recovery of
	any k-sparse vector in the noninformed setup. It reads ? <; 1/(2k
	- g + b - 1) when the informed support is composed of g good atoms
	and b wrong atoms. We emphasize that our condition is complementary
	to some restricted-isometry-based conditions by showing that none
	of them implies the other. Because this mutual coherence condition
	is common to all procedures, we carry out a finer analysis based
	on the null space property (NSP) and the exact recovery condition
	(ERC). Connections are established regarding the characterization
	of ?p-relaxation procedures and OMP in the informed setup. First,
	we emphasize that the truncated NSP enjoys an ordering property when
	p is decreased. Second, the partial ERC for OMP (ERC-OMP) implies
	in turn the truncated NSP for the informed ?1 problem, and the truncated
	NSP for p <; 1 .},
  timestamp = {2016-07-08T12:26:44Z},
  number = {11},
  journal = {Information Theory, IEEE Transactions on},
  author = {Herzet, C. and Soussen, C. and Idier, J. and Gribonval, R.},
  month = nov,
  year = {2013},
  keywords = {$ell _{p}$ relaxation,$k$-step analysis,Algorithm design and analysis,Coherence,Context,Dictionaries,ERC-OMP,exact
	recovery condition,exact recovery conditions,exact support recovery,iterative
	methods,K-sparse vector,least squares approximations,lp-relaxation-based
	procedures,Matching pursuit algorithms,mutual coherence,mutual coherence
	condition,NSP truncation,null space property,OLS,OMP,orthogonal least
	squares,orthogonal matching pursuit,partial support information,partial
	support information,restricted-isometry-based conditions,restricted-isometry-based
	conditions,signal representation,sparse representations,sparse
	representations,Standards,Vectors},
  pages = {7509-7524},
  owner = {Fardin}
}

@inproceedings{Hirayama2014,
  title = {Simultaneous blind separation and clustering of coactivated EEG/MEG 	sources for analyzing spontaneous brain activity},
  doi = {10.1109/EMBC.2014.6944730},
  abstract = {Analysis of the dynamics (non-stationarity) of functional connectivity
	patterns has recently received a lot of attention in the neuroimaging
	community. Most analysis has been using functional magnetic resonance
	imaging (fMRI), partly due to the inherent technical complexity of
	the electro- or magnetoencephalography (EEG/MEG) signals, but EEG/MEG
	holds great promise in analyzing fast changes in connectivity. Here,
	we propose a method for dynamic connectivity analysis of EEG/MEG,
	combining blind source separation with dynamic connectivity analysis
	in a single probabilistic model. Blind source separation is extremely
	useful for interpretation of the connectivity changes, and also enables
	rejection of artifacts. Dynamic connectivity analysis is performed
	by clustering the coactivation patterns of separated sources by modeling
	their variances. Experiments on resting-state EEG show that the obtained
	clusters correlate with physiologically meaningful quantities.},
  timestamp = {2016-07-10T08:18:18Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual 	International Conference of the IEEE},
  author = {Hirayama, J.-I. and Ogawa, T. and Hyvarinen, A.},
  month = aug,
  year = {2014},
  keywords = {biomedical MRI,blind source separation,Brain modeling,coactivation
	patterns,Computational modeling,dynamic connectivity analysis,EEG
	signals,EEG sources,electroencephalography,electroencephalography
	signals,fMRI,functional connectivity patterns,functional magnetic
	resonance imaging,magnetoencephalography,magnetoencephalography signals,MEG
	signals,MEG sources,neuroimaging community,neurophysiology,probabilistic
	model,spontaneous brain activity,Surfaces},
  pages = {4932-4935},
  owner = {afdidehf}
}

@inproceedings{Hodzic1983,
  title = {A Hierarchical Estimator for Sparse Systems},
  abstract = {The objective of this paper is to present a "piece-by-piece" design
	of hierarchtical Kalman estimator for sparse systems. The proposed
	procedure is applied to a block-triangular representation of the
	system obtained via a graph-theoretic decomposition algorithm. The
	estimator is built as a union of low-order optimal estimators, and
	offers a reduction of computational requirements for systems of large
	dimensions.},
  timestamp = {2016-07-08T10:17:44Z},
  booktitle = {American Control Conference, 1983},
  author = {Hodzic, M. and Siljak, D.D.},
  month = jun,
  year = {1983},
  keywords = {Buildings,Control systems,Design engineering,Equations,Kalman filters,Matrix
	decomposition,Noise measurement,Noise reduction,sparse matrices,State
	estimation},
  pages = {301-304},
  owner = {afdidehf}
}

@article{Hong2013,
  title = {Localization of coherent sources by simultaneous MEG and EEG beamformer},
  volume = {51},
  issn = {0140-0118},
  doi = {10.1007/s11517-013-1092-z},
  abstract = {Simultaneous magnetoencephalography (MEG) and electroencephalography
	(EEG) analysis is known generally to yield better localization performance
	than a single modality only. For simultaneous analysis, MEG and EEG
	data should be combined to maximize synergistic effects. Recently,
	beamformer for simultaneous MEG/EEG analysis was proposed to localize
	both radial and tangential components well, while single modality
	analyses could not detect them, or had relatively higher location
	bias. In practice, most interesting brain sources are likely to be
	activated coherently; however, conventional beamformer may not work
	properly for such coherent sources. To overcome this difficulty,
	a linearly constrained minimum variance (LCMV) beamformer may be
	used with a source suppression strategy. In this work, simultaneous
	MEG/EEG LCMV beamformer using source suppression was formulated firstly
	to investigate its capability over various suppression strategies.
	The localization performance of our proposed approach was examined
	mainly for coherent sources and compared thoroughly with the conventional
	simultaneous and single modality approaches, over various suppression
	strategies. For this purpose, we used numerous simulated data, as
	well as empirical auditory stimulation data. In addition, some strategic
	issues of simultaneous MEG/EEG analysis were discussed. Overall,
	we found that our simultaneous MEG/EEG LCMV beamformer using a source
	suppression strategy is greatly beneficial in localizing coherent
	sources.},
  language = {English},
  timestamp = {2016-10-21T13:57:01Z},
  number = {10},
  journal = {Medical and Biological Engineering and Computing},
  author = {Hong, JunHee and Ahn, Minkyu and Kim, Kiwoong and Jun, SungChan},
  year = {2013},
  keywords = {Beamformer,Coherent sources,EEG,MEG,Simultaneous analysis,Source,Suppression},
  pages = {1121--1135},
  owner = {afdidehf}
}

@article{Hoyer2004,
  title = {Non-negative Matrix Factorization with Sparseness Constraints},
  volume = {5},
  abstract = {Non-negative matrix factorization (NMF) is a recently developed technique
	for finding parts-based, linear representations of non-negative data.
	Although it has successfully been applied in several applications,
	it does not always result in parts-based representations. In this
	paper, we show how explicitly incorporating the notion of �sparseness�
	improves the found decompositions. Additionally, we provide complete
	MATLAB code both for standard NMF and for our extension. Our hope
	is that this will further the application of these methods to solving
	novel data-analysis problems.},
  timestamp = {2016-07-10T07:00:20Z},
  journal = {Journal of Machine Learning Research},
  author = {Hoyer, Patrik O.},
  year = {2004},
  keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
  pages = {1457�1469},
  owner = {Fardin}
}

@article{Huang2015,
  title = {Recovery of block sparse signals by a block version of StOMP},
  volume = {106},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.07.023},
  abstract = {Abstract It is in general NP-hard to pursue the sparsest solution
	of an underdetermined system of linear equations. The Stagewise Orthogonal
	Matching Pursuit (StOMP) algorithm has been proposed in Donoho et
	al. [1] to recover sparse signals from compressed measurements, which
	is a greedy algorithm with low computational complexity and has some
	particularly interesting theoretical properties. On the other hand,
	problems of block sparse signal recovery have arisen in fields like
	signal processing and systems biology, under which a block sparse
	signal recovery algorithm with low computational complexity and theoretical
	guarantees is needed. In this paper, a block version of StOMP is
	proposed, termed Block Stagewise Orthogonal Matching Pursuit (Block-StOMP).
	Block-StOMP combines advantages of StOMP and block structure of signals
	for a high-efficiency recovery of the block sparse signal. Moreover,
	a rigorous theoretical analysis for Block-StOMP is given in this
	paper. Compared with StOMP and other algorithms, Block-StOMP has
	excellent recovery performance when the measurement noise level is
	moderate.},
  timestamp = {2016-07-10T07:42:26Z},
  journal = {Signal Processing},
  author = {Huang, B. X. and Zhou, T.},
  year = {2015},
  keywords = {Block,Compressed,Equations,Phase,sensing,Sparse,transition,underdetermined},
  pages = {231 - 244},
  owner = {afdidehf}
}

@inproceedings{Huang2009,
  title = {Learning with dynamic group sparsity},
  doi = {10.1109/ICCV.2009.5459202},
  abstract = {This paper investigates a new learning formulation called dynamic
	group sparsity. It is a natural extension of the standard sparsity
	concept in compressive sensing, and is motivated by the observation
	that in some practical sparse data the nonzero coefficients are often
	not random but tend to be clustered. Intuitively, better results
	can be achieved in these cases by reasonably utilizing both clustering
	and sparsity priors. Motivated by this idea, we have developed a
	new greedy sparse recovery algorithm, which prunes data residues
	in the iterative process according to both sparsity and group clustering
	priors rather than only sparsity as in previous methods. The proposed
	algorithm can recover stably sparse data with clustering trends using
	far fewer measurements and computations than current state-of-the-art
	algorithms with provable guarantees. Moreover, our algorithm can
	adaptively learn the dynamic group structure and the sparsity number
	if they are not available in the practical applications. We have
	applied the algorithm to sparse recovery and background subtraction
	in videos. Numerous experiments with improved performance over previous
	methods further validate our theoretical proofs and the effectiveness
	of the proposed algorithm.},
  timestamp = {2016-07-09T19:55:58Z},
  booktitle = {Computer Vision, 2009 IEEE 12th International Conference on},
  author = {Huang, Junzhou and Huang, Xiaolei and Metaxas, D.},
  month = sep,
  year = {2009},
  keywords = {background subtraction,Clustering algorithms,compressive sensing,Computer
	vision,Current measurement,dynamic group sparsity,Greedy algorithms,greedy
	sparse recovery algorithm,group clustering,Iterative algorithms,iterative methods,iterative
	methods,iterative process,learning (artificial
	intelligence),learning formulation,Matching pursuit algorithms,Minimization
	methods,Noise measurement,pattern clustering,Pursuit algorithms,sparse
	matrices,standard sparsity concept,Videos},
  pages = {64-71},
  owner = {afdidehf}
}

@article{Huang2009a,
  title = {The Benefit of Group Sparsity},
  abstract = {This paper develops a theory for group Lasso using a concept called
	strong group sparsity. Our result shows that group Lasso is superior
	to standard Lasso for strongly group-sparse signals. This provides
	a convincing theoretical justification for using group sparse regularization
	when the underlying group structure is consistent with the data.
	Moreover, the theory predicts some limitations of the group Lasso
	formulation that are confirmed by simulation studies.},
  timestamp = {2016-07-11T16:59:52Z},
  journal = {ArXiv e-prints},
  author = {Huang, Junzhou and Zhang, Tong},
  year = {2009},
  keywords = {Mathematics - Statistics,Statistics - Machine Learning},
  owner = {afdidehf}
}

@inproceedings{Huang2013,
  title = {SAR Target Recognition using block-sparse representation},
  doi = {10.1109/MEC.2013.6885274},
  abstract = {We propose a block-sparse representation approach with wavelet approximate
	coefficients features for synthetic aperture radar (SAR) target recognition.
	Inspired by sparse representation-based classification (SRC), we
	take the block structure of the dictionary into account, and introduce
	block-sparse representation to find one or few blocks in one class
	to represent the test sample. We use block orthogonal matching pursuit
	(BOMP) to obtain the linear representation coefficients associated
	to atoms from one class. Experiments are carried out on Moving and
	Stationary Target Acquisition and Recognition (MSTAR) public database.
	We compare our method with SRC. Numerical results demonstrate that
	the proposed method can improve classification accuracy of SRC and
	approximate coefficients features perform better than amplitude values
	features.},
  timestamp = {2016-07-10T08:10:05Z},
  booktitle = {Mechatronic Sciences, Electric Engineering and Computer (MEC), Proceedings 	2013 International Conference on},
  author = {Huang, Xiayuan and Wang, Peng and Zhang, Bo and Qiao, Hong},
  month = dec,
  year = {2013},
  keywords = {Accuracy,approximate coefficients,block orthogonal matching pursuit,block-sparse
	representation,BOMP,compressed sensing,Dictionaries,Dictionary,iterative
	methods,linear representation coefficients,moving and stationary
	target acquisition and recognition,MSTAR,radar target recognition,SAR
	images,SAR target recognition,sparse representation-based classification,SRC,synthetic
	aperture radar,synthetic aperture radar target recognition,Target recognition,target
	recognition,time-frequency analysis,Training,Vectors,wavelet
	approximate coefficients,wavelet transform,wavelet transforms},
  pages = {1332-1336},
  owner = {afdidehf}
}

@inproceedings{Hundet2014,
  title = {Block based compressive sensing algorithm using Eigen vectors for 	image compression},
  doi = {10.1109/ICAETR.2014.7012884},
  abstract = {The image compression is widely used throughout the multimedia applications
	and presently many standard techniques are already available, however
	the in many situation (like after encryption, highly textured etc.)
	the data compression with the stated techniques are not sufficient,
	for such cases that relatively new approach called Compressive Sensing
	can provide a better results as recent research shows. The Compressive
	Sensing is a concepts primarily used for reduction in reduction in
	number of observation required for reconstructing the data from linear
	acquisition system. It fundamentally states that a linear system
	with N number of equations can be approximated by M equation (M <;
	N), if system follows sparsely condition. The paper utilizes the
	same concept for image compression, however the reduction in approximated
	system equation is performed by calculating the Eigen vectors. The
	application of Eigen value and vector not only simplifies the process
	but also provides efficient reconstruction with high compression.
	The simulation results also verifies the superiority proposed algorithm
	over previous algorithms by considerable margin.},
  timestamp = {2016-07-08T11:40:51Z},
  booktitle = {Advances in Engineering and Technology Research (ICAETR), 2014 International 	Conference on},
  author = {Hundet, A. and Jain, R.C. and Sharma, V.},
  month = aug,
  year = {2014},
  keywords = {approximated system equation,biomedical imaging,block based compressive
	sensing algorithm,compressed sensing,compressive sensing,cryptography,Data
	Compression,data reconstruction,dimension reduction,eigenvalue and
	eigenvector,Eigenvalues and eigenfunctions,Equations,Hyperspectral
	imaging,image coding,image compression,Image reconstruction,Image
	resolution,linear acquisition system,Linear systems,multimedia applications,Sensors,sparse
	matrices},
  pages = {1-5},
  owner = {afdidehf}
}

@article{Hunter2005,
  title = {Variable Selection Using MM Algorithms},
  volume = {33},
  doi = {10.1214/009053605000000200},
  abstract = {Variable selection is fundamental to high-dimensional statistical
	modeling. Many variable selection techniques may be implemented by
	maximum penalized likelihood using various penalty functions. Optimizing
	the penalized likelihood function is often challenging because it
	may be nondifferentiable and/or nonconcave. This article proposes
	a new class of algorithms for finding a maximizer of the penalized
	likelihood for a broad class of penalty functions. These algorithms
	operate by perturbing the penalty function slightly to render it
	differentiable, then optimizing this differentiable function using
	a minorize�maximize (MM) algorithm. MM algorithms are useful extensions
	of the well-known class of EM algorithms, a fact that allows us to
	analyze the local and global convergence of the proposed algorithm
	using some of the techniques employed for EM algorithms. In particular,
	we prove that when our MM algorithms converge, they must converge
	to a desirable point; we also discuss conditions under which this
	convergence may be guaranteed. We exploit the Newton�Raphson-like
	aspect of these algorithms to propose a sandwich estimator for the
	standard errors of the estimators. Our method performs well in numerical
	tests.},
  timestamp = {2016-07-11T17:10:23Z},
  number = {4},
  journal = {The Annals of Statistics},
  author = {Hunter, David R. and Li, Runze},
  year = {2005},
  keywords = {AIC,BIC,EM algorithm,Lasso,MM algorithm,oracle estimator,Penalized
	likelihood,SCAD},
  pages = {1617�1642},
  owner = {afdidehf}
}

@phdthesis{Huo1999,
  title = {Sparse Image Representation Via Combined Transforms},
  abstract = {We consider sparse image decomposition, in the hope that a sparser
	decomposition of an image may lead to a more efficient method of
	image coding or compression. Recently, many transforms have been
	proposed. Typically, each of them is good at processing one class
	of features in an image but not other features. For example, the
	2-D wavelet transform is good at processing point singularities and
	patches in an image but not linear singularities, while a recently
	proposed method the edgelet-like transform is good for linear
	singularities, but not for points. Intuitively, a combined scheme
	may lead to a sparser decomposition than a scheme using only a single
	transform. Combining several transforms, we get an overcomplete system
	or dictionary. For a given image, there are infinitely many ways
	to decompose it. How to find the one with the sparsest coefficients?
	We follow the idea of Basis Pursuit finding a minimum 1 norm solution.
	Some intuitive discussion and theoretical results show that this
	method is optimal in many cases. A big challenge in solving a minimum
	1 norm problem is the computational complexity. In many cases, due
	to the intrinsic nature of the high-dimensionality of images, finding
	the minimum 1 norm solution is nearly impossible. We take advantage
	of the recent advances in convex optimization and iterative methods.
	Our approach is mainly based on two facts: first, we have fast algorithms
	for each transform; second, we have efficient iterative algorithms
	to solve for the Newton direction. The numerical results (to some
	extent) verify our intuitions, in the sense that: [1] the combined
	scheme does give sparser representations than a scheme applying only
	a single transform; [2] each transform in the combined scheme captures
	the features that this transform is good at processing. (Actually,
	[2] is an extension of [1].) With improved efficiency in numerical
	algorithms, this approach has the promise of producing more compact
	image coding and compression schemes than existing ones.},
  timestamp = {2017-06-23T13:12:52Z},
  school = {The Department Of Statistics Of Stanford University},
  author = {Huo, Xiaoming},
  month = aug,
  year = {1999},
  owner = {afdidehf}
}

@article{Hurley2009,
  title = {Comparing Measures of Sparsity},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2027527},
  abstract = {Sparsity of representations of signals has been shown to be a key
	concept of fundamental importance in fields such as blind source
	separation, compression, sampling and signal analysis. The aim of
	this paper is to compare several commonly-used sparsity measures
	based on intuitive attributes. Intuitively, a sparse representation
	is one in which a small number of coefficients contain a large proportion
	of the energy. In this paper, six properties are discussed: (Robin
	Hood, Scaling, Rising Tide, Cloning, Bill Gates, and Babies), each
	of which a sparsity measure should have. The main contributions of
	this paper are the proofs and the associated summary table which
	classify commonly-used sparsity measures based on whether or not
	they satisfy these six propositions. Only two of these measures satisfy
	all six: the pq-mean with p les 1, q > 1 and the Gini index.},
  timestamp = {2016-09-30T10:42:08Z},
  number = {10},
  journal = {IEEE Trans. Inf. Theory},
  author = {Hurley, N. and Rickard, Scott},
  month = oct,
  year = {2009},
  keywords = {adaptive signal processing,blind source separation,Cloning,compression
	analysis,Gini index,image coding,information theory,Machine learning,Measures
	of sparsity,measuring sparsity,sampling analysis,Sampling methods,Sea
	measurements,Signal analysis,source separation,sparse distribution,sparse
	representation,Sparsity,sparsity measures,Tides},
  pages = {4723--4741},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  annote = {read Information Theory, IEEE Transactions on},
  owner = {Fardin}
}

@inproceedings{Hurmalainen2015,
  title = {Similarity induced group sparsity for non-negative matrix factorisation},
  doi = {10.1109/ICASSP.2015.7178807},
  abstract = {Non-negative matrix factorisations are used in several branches of
	signal processing and data analysis for separation and classification.
	Sparsity constraints are commonly set on the model to promote discovery
	of a small number of dominant patterns. In group sparse models, atoms
	considered to belong to a consistent group are permitted to activate
	together, while activations across groups are suppressed, reducing
	the number of simultaneously active sources or other structures.
	Whereas most group sparse models require explicit division of atoms
	into separate groups without addressing their mutual relations, we
	propose a constraint that permits dynamic relationships between atoms
	or groups, based on any defined distance measure. The resulting solutions
	promote approximation with components considered similar to each
	other. Evaluation results are shown for speech enhancement and noise
	robust speech and speaker recognition.},
  timestamp = {2016-07-10T08:18:15Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International 	Conference on},
  author = {Hurmalainen, A. and Saeidi, R. and Virtanen, T.},
  month = apr,
  year = {2015},
  keywords = {Atomic measurements,Cost function,data analysis,group sparsity,group
	sparsity constraint,matrix decomposition,Noise,Noise measurement,noise
	robust speaker recognition,noise robust speech recognition,nonnegative
	matrix factorisation,non-negative matrix factorization,signal classification,signal
	processing,signal separation,source separation,sparse matrices,sparse
	representations,speaker recognition,Speech,speech enhancement,Speech
	enhancement,speech recognition,speech
	recognition},
  pages = {4425-4429},
  owner = {afdidehf}
}

@inproceedings{Ince2010,
  title = {Block sparse signal recovery from sparsely corrupted measurements},
  abstract = {In this study two algorithms are compared in order to reconstruct
	a block sparse signal from sparsely corrupted measurements using
	Compressive Sampling/Compressed Sensing (CS). Compressive sampling/Compressed
	sensing (CS) is a new area of signal processing and attracts too
	much attention in recent years. Compressive sampling states that
	if a signal having length N has a sparse representation on an orthonormal
	basis, then it is possible to recover this signal exactly from M
	? N measurements. Two algorithms are studied to reconstruct the original
	signal one is based on optimization and the other is based on greedy
	algorithms.},
  timestamp = {2016-07-08T11:47:04Z},
  booktitle = {Electrical, Electronics and Computer Engineering (ELECO), 2010 National 	Conference on},
  author = {\.{I}nce, T. and Nacaro\u{g}lu, A. and Watsuji, N.},
  month = dec,
  year = {2010},
  keywords = {block sparse signal recovery,compressed sensing,compressive sampling/compressed
	sensing,Greedy algorithms,Length measurement,Matching pursuit algorithms,Noise
	reduction,orthonormal basis,Robustness,signal processing,Signal
	processing algorithms,Signal processing
	algorithms,sparsely corrupted measurements,Uncertainty},
  pages = {622-625},
  owner = {afdidehf}
}

@article{Im2008,
  title = {A New Neuronal Electrical Source Model Considering Electrophysiology 	to Simulate Realistic Electroencephalography (EEG) Forward Signals},
  volume = {44},
  issn = {0018-9464},
  doi = {10.1109/TMAG.2007.916233},
  abstract = {The present study proposes a new neuronal electrical source model
	to simulate electroencephalography (EEG) forward signals more effectively.
	Contrary to the conventional methods which used a presumed activation
	function at cortical patches or point dipolar sources, the proposed
	model reflects electrophysiological facts and introduces probability
	concept, resulting in more plausible and readily controllable artificial
	EEG waveforms.},
  timestamp = {2016-07-08T11:23:34Z},
  number = {6},
  journal = {Magnetics, IEEE Transactions on},
  author = {Im, Chang-Hwan and Lee, Chany and Jung, Hyun-Kyo and Lee, Soo Yeol},
  month = jun,
  year = {2008},
  keywords = {Artificial electroencephalography (EEG) waveform,bioelectric phenomena,controllable
	artificial EEG waveforms,cortical patches,EEG,electroencephalography,electroencephalography
	forward signals,Electrophysiology,neuronal electrical source model,neurophysiology,point
	dipolar sources,presumed activation function},
  pages = {1434-1437},
  owner = {Fardin}
}

@inproceedings{Inoue2012,
  title = {Local intensity compensation using sparse representation},
  abstract = {This paper proposes a motion compensation method using sparse representation
	for video coding technique. A new generation video compression technology
	known as the HEVC (High Efficiency Video Coding) suggests a novel
	motion compensation called local intensity compensation. Local intensity
	compensation represents a current block by a linear combination of
	some reference blocks. In this study, the weight coefficients of
	the linear combination are calculated by sparse representation. Sparse
	representation is able to obtain many zero coefficients, thus the
	current block can be represented by a few and meaningful reference
	blocks. The experimental observations demonstrate that the quality
	of local intensity compensation increases by sparse representation.},
  timestamp = {2016-07-09T19:57:22Z},
  booktitle = {Pattern Recognition (ICPR), 2012 21st International Conference on},
  author = {Inoue, K. and Saito, H. and Kuroki, Y.},
  month = nov,
  year = {2012},
  keywords = {HEVC,high efficiency video coding technique,image representation,linear
	combination,local intensity compensation,Mobile communication,motion
	compensation,new generation video compression technology,novel motion
	compensation method,reference blocks,sparse matrices,sparse representation,Standards,transform
	coding,Vectors,video coding,Video compression,weight coefficients,zero
	coefficients},
  pages = {951-954},
  owner = {afdidehf}
}

@article{Jabbari2014,
  title = {Eigenvalue, singular values, pseudo-inverse},
  timestamp = {2016-07-08T12:20:11Z},
  author = {Jabbari, Faryar},
  year = {2014},
  howpublished = {University of California, Irvine},
  owner = {afdidehf}
}

@article{Jagannath2013,
  title = {Block Sparse Estimator for Grid Matching in Single Snapshot DoA Estimation},
  volume = {20},
  issn = {1070-9908},
  doi = {10.1109/LSP.2013.2279124},
  abstract = {In this work, the grid mismatch problem for a single snapshot direction
	of arrival estimation problem is studied. We derive a Bayesian Cramer-Rao
	bound for the grid mismatch problem with the errors in variables
	model and propose a block sparse estimator for grid matching and
	sparse recovery.},
  timestamp = {2016-07-08T11:41:40Z},
  number = {11},
  journal = {Signal Processing Letters, IEEE},
  author = {Jagannath, R. and Hari, K.V.S.},
  month = nov,
  year = {2013},
  keywords = {Arrays,Bayesian Cramer�Rao bound,Bayesian Cramer-Rao bound,Bayes
	methods,block sparse estimator,Cramer-Rao bounds,Cramer-Rao
	bounds,direction-of-arrival estimation,direction of arrival estimation,Direction-of-arrival
	estimation,grid matching,grid
	matching,grid mismatch problem,mathematical model,Mathematical
	model,single snapshot DOA estimation,single snapshot
	DOA estimation,sparse recovery,Vectors},
  pages = {1038-1041},
  owner = {afdidehf}
}

@article{Jalali2013,
  title = {A Dirty Model for Multiple Sparse Regression},
  volume = {59},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2280272},
  abstract = {The task of sparse linear regression consists of finding an unknown
	sparse vector from linear measurements. Solving this task even under
	�high-dimensional� settings, where the number of samples is fewer
	than the number of variables, is now known to be possible via methods
	such as the LASSO. We consider the multiple sparse linear regression
	problem, where the task consists of recovering several related sparse
	vectors at once. A simple approach to this task would involve solving
	independent sparse linear regression problems, but a natural question
	is whether one can reduce the overall number of samples required
	by leveraging partial sharing of the support sets, or nonzero patterns,
	of the signal vectors. A line of recent research has studied the
	use of ?1/?q norm block-regularizations with q > 1 for such problems.
	However, depending on the level of sharing, these could actually
	perform worse in sample complexity when compared to solving each
	problem independently. We present a new �adaptive� method for
	multiple sparse linear regression that can leverage support and parameter
	overlap when it exists, but not pay a penalty when it does not. We
	show how to achieve this using a very simple idea: decompose the
	parameters into two components and regularize these differently.
	We show, theoretically and empirically, that our method strictly
	and noticeably outperforms both ?1 or ?1/?q methods, over the entire
	range of possible overlaps (except at boundary cases, where we match
	the best method), even under high-dimensional scaling.},
  timestamp = {2016-07-08T10:12:40Z},
  number = {12},
  journal = {Information Theory, IEEE Transactions on},
  author = {Jalali, A. and Ravikumar, P. and Sanghavi, S.},
  month = dec,
  year = {2013},
  keywords = {covariance matrices,Estimation,graphical models,high-dimensional statistics,Lasso,linear
	measurements,Linear regression,multiple regression,multiple sparse
	linear regression problem,multi-task learning,regression analysis,signal
	vectors,sparse matrices,sparse vector,sparse vectors,Standards,Vectors},
  pages = {7947-7968},
  owner = {afdidehf}
}

@article{Jalali2010,
  title = {A Dirty Model for Multi-task Learning},
  abstract = {We consider multi-task learning in the setting of multiple linear
	regression, and where some relevant features could be shared across
	the tasks. Recent research has studied the use of ?1/?q norm block-regularizations
	with q > 1 for such blocksparse structured problems, establishing
	strong guarantees on recovery even under high-dimensional scaling
	where the number of features scale with the number of observations.
	However, these papers also caution that the performance of such block-regularized
	methods are very dependent on the extent to which the features are
	shared across tasks. Indeed they show [8] that if the extent of overlap
	is less than a threshold, or even if parameter values in the shared
	features are highly uneven, then block ?1/?q regularization could
	actually perform worse than simple separate elementwise ?1 regularization.
	Since these caveats depend on the unknown true parameters, we might
	not know when and which method to apply. Even otherwise, we are far
	away from a realistic multi-task setting: not only do the set of
	relevant features have to be exactly the same across tasks, but their
	values have to as well. Here, we ask the question: can we leverage
	parameter overlap when it exists, but not pay a penalty when it does
	not ? Indeed, this falls under a more general question of whether
	we can model such dirty data which may not fall into a single neat
	structural bracket (all block-sparse, or all low-rank and so on).
	With the explosion of such dirty high-dimensional data in modern
	settings, it is vital to develop tools � dirty models � to perform
	biased statistical estimation tailored to such data. Here, we take
	a first step, focusing on developing a dirty model for the multiple
	regression problem. Our method uses a very simple idea: we estimate
	a superposition of two sets of parameters and regularize them differently.
	We show both theoretically and empirically, our method strictly and
	noticeably outperforms both ?1 or ?1/?q methods, under high-dimensional
	scaling and over the entire range of possible overlaps (except at
	boundary cases, where we match the best method).},
  timestamp = {2016-07-08T10:12:45Z},
  journal = {NIPS},
  author = {Jalali, Ali and Ravikumar, Pradeep and Sanghavi, Sujay and Ruan, Chao},
  year = {2010},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@incollection{Jelfs2008,
  title = {Collaborative Adaptive Filters for Online Knowledge Extraction and 	Information Fusion},
  isbn = {978-0-387-74366-0},
  abstract = {We present a method for extracting information (or knowledge) about
	the nature of a signal, this is achieved by employing recent developments
	in signal characterisation for online analysis of the changes in
	signal modality. We show that it is possible to use the fusion of
	the outputs of adaptive filters to produce a single collaborative
	hybrid filter and that by tracking the dynamics of the mixing parameter
	of this filter rather than the actual filter performance, a clear
	indication as to the nature of the signal is given. Implementations
	of the proposed hybrid filter in both the real R and complex C domains
	are analysed and the potential of such a scheme for tracking signal
	nonlinearity in both domains is highlighted. Simulations on linear
	and nonlinear signals in a prediction configuration support the analysis;
	real world applications of the approach have been illustrated on
	electroencephalogram (EEG), radar and wind data.},
  language = {English},
  timestamp = {2016-07-08T11:51:38Z},
  booktitle = {Signal Processing Techniques for Knowledge Extraction and Information 	Fusion},
  publisher = {{\{Springer US\}}},
  author = {Jelfs, Beth and Vayanos, Phebe and Javidi, Soroush and Goh, VanessaSuLee and Mandic, Danilo},
  editor = {Mandic, Danilo and Golz, Martin and Kuh, Anthony and Obradovic, Dragan and Tanaka, Toshihisa},
  year = {2008},
  pages = {3-21},
  owner = {Fardin}
}

@article{jemdoc2014,
  title = {Block sparsity: An example from bio-chemical reaction},
  timestamp = {2016-07-08T11:48:08Z},
  author = {{{jemdoc}}},
  year = {2014},
  owner = {Fardin}
}

@article{Ji2015,
  title = {Image recovery via geometrically structured approximation},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2015.08.012},
  abstract = {Abstract In recent years, the l 1 norm based regularization has been
	one promising technique for solving many ill-posed inverse problems
	in image recovery. As the performance gain of these methods over
	linear methods comes from the separate process for smooth image regions
	and image discontinuities, their performance largely depends on how
	accurate such separation is. However, there is a lot of ambiguities
	between smooth image regions and image discontinuities when only
	degraded images are available. This paper aims at developing new
	wavelet frame based image regularization to resolve such ambiguities
	by exploiting geometrical regularities of image discontinuities.
	Based on the geometrical connectivity constraint on image discontinuities,
	an alternating iteration scheme is proposed which is simple in implementation
	and efficient in computation. The experiments show that the results
	from the proposed regularization method are compared favorably against
	that from several existing l 1 norm or l 0 norm based image regularizations.},
  timestamp = {2016-07-08T12:46:49Z},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Ji, Hui and Luo, Yu and Shen, Zuowei},
  year = {2015},
  keywords = {image recovery,Wavelet tight frame sparse approximation},
  pages = {-},
  owner = {afdidehf}
}

@inproceedings{Ji2014,
  title = {An Implementation of Block Conjugate Gradient Algorithm on CPU-GPU 	Processors},
  doi = {10.1109/Co-HPC.2014.10},
  abstract = {In this paper, we investigate the implementation of the Block Conjugate
	Gradient (BCG) algorithm on CPU-GPU processors. By analyzing the
	performance of various matrix operations in BCG, we identify the
	main performance bottleneck in constructing new search direction
	matrices. Replacing the QR decomposition by eigendecomposition of
	a small matrix remedies the problem by reducing the computational
	cost of generating orthogonal search directions. Moreover, a hybrid
	(offload) computing scheme is designed to enables the BCG implementation
	to handle linear systems with large, sparse coefficient matrices
	that cannot fit in the GPU memory. The hybrid scheme offloads matrix
	operations to GPU processors while helps hide the CPU-GPU memory
	transaction overhead. We compare the performance of our BCG implementation
	with the one on CPU with Intel Xeon Phi coprocessors using the automatic
	offload mode. With sufficient number of right hand sides, the CPU-GPU
	implementation of BCG can reach speedup of 2.61 over the CPU-only
	implementation, which is significantly higher than that of the CPU-Intel
	Xeon Phi implementation.},
  timestamp = {2016-07-08T11:23:53Z},
  booktitle = {Hardware-Software Co-Design for High Performance Computing (Co-HPC), 	2014},
  author = {Ji, Hao and Sosonkina, M. and Li, Yaohang},
  month = nov,
  year = {2014},
  keywords = {automatic offload mode,BCG algorithm,Block Conjugate Gradient,block
	conjugate gradient algorithm,Clocks,conjugate gradient methods,Coprocessors,CPU-GPU
	memory transaction,CPU-GPU processors,CPU-Intel Xeon Phi coprocessors,Data
	transfer,Graphics Processing Unit,graphics processing units,hybrid
	computing scheme,Intel Xeon Phi,Linear systems,matrix decomposition,Multi-core
	CPU,multiprocessing systems,offload computing scheme,performance
	evaluation,QR decomposition,sparse coefficient matrices,sparse matrices},
  pages = {72-77},
  owner = {afdidehf}
}

@book{Ji2013,
  edition = {1},
  series = {Lecture Notes in Computer Science 8189},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, 	Part II},
  isbn = {978-3-642-40990-5 978-3-642-40991-2},
  timestamp = {2016-10-24T16:41:18Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Ji, Tengfei and Yang, Dongqing and Gao, Jun},
  editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and �elezn�, Filip},
  year = {2013},
  owner = {Fardin}
}

@inproceedings{Jiang2011,
  title = {Color Image Enhancement Based on Sparse Representation},
  doi = {10.1109/ICCASE.2011.5997521},
  abstract = {Image enhancement is used to improve quality of images for further
	analysis. In this paper, we proposed a color image enhancement method
	with the use of sparse representation of orthogonal transform based
	on the characteristics of discrete cosine transform (DCT) coefficients:
	fewer nonzero coefficients exist in smooth domains, the amplitudes
	of DCT change slightly in texture contents but significantly in edge.
	Nonlinearly reduces the impact of textures with the use of the threshold
	on DCT coefficients, and then enhances the texture and edge regions
	by applying the a-power computation on nonzero coefficients in medium
	and high frequency domains. To remove blocky effects, Kaiser window
	function is used on each processed image block. Compared with traditional
	method, this image enhancement algorithm achieves better visual effects,
	while the noise in texture region is restrained at the same time.},
  timestamp = {2016-07-08T11:51:56Z},
  booktitle = {Control, Automation and Systems Engineering (CASE), 2011 International 	Conference on},
  author = {Jiang, Fengjiao and He, Kun},
  month = jul,
  year = {2011},
  keywords = {Algorithm design and analysis,Color,color image enhancement method,DCT
	coefficients,discrete cosine transform,discrete cosine transforms,Frequency
	domain analysis,image colour analysis,Image edge detection,Image
	Enhancement,image representation,image texture,Kaiser window function,Noise,nonzero
	coefficients,orthogonal transform,sparse representation,visual effects},
  pages = {1-4},
  owner = {afdidehf}
}

@inproceedings{Jiang2010,
  title = {Random noise SAR based on compressed sensing},
  doi = {10.1109/IGARSS.2010.5651241},
  abstract = {Recent theory of compressed sensing (CS) suggested that exact recovery
	of an unknown sparse signal can be achieved from few measurements
	with overwhelming probability. In this paper, we combine CS technology
	with a random noise SAR and proposed the concept of random noise
	SAR based on CS. The block diagram of the radar system and the collected
	data processing procedure was presented. Theoretic analysis show
	that the sensing matrix of the random noise SAR exhibits good restricted
	isometry property (RIP).When the target scene is sparse or sparse
	in any basis, the random noise radar based on CS can get high accuracy
	image by collecting far less amount of echo data than traditional
	noise radar does. The conclusions are all demonstrated by simulation
	experiments.},
  timestamp = {2016-07-10T07:40:48Z},
  booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2010 IEEE International},
  author = {Jiang, Hai and Zhang, BingChen and Lin, Yueguan and Hong, Wen and Wu, YiRong and Zhan, Jin},
  month = jul,
  year = {2010},
  keywords = {compressed sensing,data processing,echo,echo data collection,echo data
	collection,Image reconstruction,Image
	reconstruction,Noise,probability,radar imaging,random noise,random
	noise,random noise radar,random
	noise radar,random noise SAR,restricted isometry property,restricted
	isometry property,RIP,sensing matrix,signal representation,sparse matrices,sparse
	matrices,sparse recovery,sparse signal,synthetic aperture radar,synthetic
	aperture radar,target scene},
  pages = {4624-4627},
  owner = {afdidehf}
}

@article{Jiang2015,
  title = {Block-Sparsity-Induced Adaptive Filter for Multi-Clustering System 	Identification},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2453133},
  abstract = {In order to improve the performance of least mean square (LMS)-based
	adaptive filtering for identifying block-sparse systems, a new adaptive
	algorithm called block-sparse LMS (BS-LMS) is proposed in this paper.
	The basis of the proposed algorithm is to insert a penalty of block-sparsity,
	which is a mixed l2, 0 norm of adaptive tap-weights with equal group
	partition sizes, into the cost function of traditional LMS algorithm.
	To describe a block-sparse system response, we first propose a Markov-Gaussian
	model, which can generate a kind of system responses of arbitrary
	average sparsity and arbitrary average block length using given parameters.
	Then we present theoretical expressions of the steady-state misadjustment
	and transient convergence behavior of BS-LMS with an appropriate
	group partition size for white Gaussian input data. Based on the
	above results, we theoretically demonstrate that BS-LMS has much
	better convergence behavior than l0-LMS with the same small level
	of misadjustment. Finally, numerical experiments verify that all
	of the theoretical analysis agrees well with simulation results in
	a large range of parameters.},
  timestamp = {2016-07-08T11:48:15Z},
  number = {20},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Jiang, Shuyang and Gu, Yuantao},
  month = oct,
  year = {2015},
  keywords = {Adaptive algorithms,Adaptive filtering,adaptive filters,Adaptive systems,adaptive
	tap weight,arbitrary average block length,arbitrary average sparsity,block
	sparse LMS,block sparse system identification,block-sparse system
	identification,block sparsity-induced adaptive filter,BS-LMS,Convergence,convergence
	behavior,convergence of numerical methods,Gaussian processes,group
	partition size,Heuristic algorithms,least mean square (LMS),least
	mean square method,least mean squares methods,least squares approximations,Markov-Gaussian model,Markov-Gaussian
	model,Markov processes,multiclustering system
	identification,Partitioning algorithms,pattern clustering,Performance
	analysis,Signal processing algorithms,transient convergence},
  pages = {5318-5330},
  owner = {afdidehf}
}

@article{Jin2011a,
  title = {Limits on Support Recovery of Sparse Signals via Multiple-Access 	Communication Techniques},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2170116},
  abstract = {In this paper, we consider the problem of exact support recovery of
	sparse signals via noisy linear measurements. The main focus is finding
	the sufficient and necessary condition on the number of measurements
	for support recovery to be reliable. By drawing an analogy between
	the problem of support recovery and the problem of channel coding
	over the Gaussian multiple-access channel (MAC), and exploiting mathematical
	tools developed for the latter problem, we obtain an information-theoretic
	framework for analyzing the performance limits of support recovery.
	Specifically, when the number of nonzero entries of the sparse signal
	is held fixed, the exact asymptotics on the number of measurements
	sufficient and necessary for support recovery is characterized. In
	addition, we show that the proposed methodology can deal with a variety
	of models of sparse signal recovery, hence demonstrating its potential
	as an effective analytical tool.},
  timestamp = {2016-07-09T19:57:15Z},
  number = {12},
  journal = {Information Theory, IEEE Transactions on},
  author = {Jin, Yuzhe and Kim, Young-Han and Rao, B.D.},
  month = dec,
  year = {2011},
  keywords = {channel coding,compressed sensing,Decoding,Gaussian channels,Gaussian
	multiple-access channel,Gaussian multiple-access channel (MAC),information-theoretic
	framework,MAC,multi-access systems,multiple-access communication
	techniques,Noise measurement,noisy linear measurement,noisy linear
	measurements,performance tradeoff,Receivers,signal processing,sparse
	matrices,sparse signal,sparse signal support recovery,support recovery},
  pages = {7877-7892},
  owner = {Fardin}
}

@article{Jokar2008,
  title = {Exact and Approximate Sparse Solutions of Underdetermined Linear 	Equations},
  volume = {31},
  doi = {10.1137/070686676},
  abstract = {In this paper, we empirically investigate the NP-hard prob- lem of
	finding sparsest solutions to linear equation systems, i.e., solutions
	with as few nonzeros as possible. This problem has received considerable
	interest in the sparse approximation and signal processing literature,
	re- cently. We use a branch-and-cut approach via the maximum feasible
	subsystem problem to compute optimal solutions for small instances
	and investigate the uniqueness of the optimal solutions. We furthermore
	discuss five (modifications of) heuristics for this problem that
	appear in different parts of the literature. For small instances,
	the exact optimal solutions allow us to evaluate the quality of the
	heuristics, while for larger instances we compare their relative
	performance. One outcome is that the so-called basis pursuit heuristic
	performs worse, compared to the other methods. Among the best heuristics
	are a method due to Mangasarian and a bilinear approach.},
  timestamp = {2016-07-08T12:26:37Z},
  number = {1},
  journal = {Siam Journal on Scientific Computing},
  author = {Jokar, Sadegh and Pfetsch, Marc E.},
  year = {2008},
  pages = {23-44},
  masid = {5689096},
  owner = {Fardin}
}

@inproceedings{Joshi2014,
  title = {On recovery of block sparse signals from multiple measurements},
  doi = {10.1109/ICASSP.2014.6854990},
  abstract = {We consider the problem of recovering block sparse signals which share
	the same sparsity pattern given multiple measurements. We consider
	two different noisy measurement models. In the first model, the sensing
	matrix remains the same for all the measurements. In the second model,
	we employ different sensing matrices for different measurements.
	For both these models, we present greedy algorithms for block sparse
	signal recovery and theoretically establish the recovery guarantees
	of the proposed algorithms. Using numerical simulations, we study
	the performance of the proposed algorithms and some existing algorithms.
	Our results present insights on how the correlation between block
	sparse signals plays a role on the recovery performance.},
  timestamp = {2016-07-10T07:13:16Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Joshi, A. and Kannu, A.P.},
  month = may,
  year = {2014},
  keywords = {block sparse signal,block sparse signals recovery,generalized multiple
	measurement vectors,Greedy algorithms,Joints,Matching pursuit algorithms,multiple
	measurement vectors,Noise measurement,noisy measurement models,Numerical
	Analysis,Numerical models,numerical simulations,sensing matrix,Sensors,signal
	reconstruction,sparse matrices,subspace matching pursuit,Vectors},
  pages = {7163-7167},
  owner = {afdidehf}
}

@article{Juditsky2011,
  title = {On Verifiable Sufficient Conditions for Sparse Signal Recovery via 	$\ell_1$ Minimization},
  volume = {127},
  doi = {10.1007/s10107-010-0417-z},
  abstract = {We discuss necessary and sufficient conditions for a sensing matrix
	to be �s-good� � to allow for exact ?1-recovery of sparse signals
	with s nonzero entries when no measurement noise is present. Then
	we express the error bounds for imperfect ?1-recovery (nonzero measurement
	noise, nearly s-sparse signal, near-optimal solution of the optimization
	problem yielding the ?1-recovery) in terms of the characteristics
	underlying these conditions. Further, we demonstrate (and this is
	the principal result of the paper) that these characteristics, although
	difficult to evaluate, lead to verifiable sufficient conditions for
	exact sparse ?1-recovery and to efficiently computable upper bounds
	on those s for which a given sensing matrix is sgood. We establish
	also instructive links between our approach and the basic concepts
	of the Compressed Sensing theory, like Restricted Isometry or Restricted
	Eigenvalue properties.},
  language = {English},
  timestamp = {2016-07-10T07:18:07Z},
  number = {1},
  journal = {Mathematical Programming B, Springer},
  author = {Juditsky, Anatoli and Nemirovski, Arkadii S.},
  month = apr,
  year = {2011},
  note = {hal-00321775v2},
  keywords = {compressed sensing,l1-recovery,sparse recovery},
  pages = {57-88},
  owner = {Fardin}
}

@article{Jun2015,
  title = {Complex-valued sparse recovery via double-threshold sigmoid penalty},
  volume = {114},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2015.03.001},
  abstract = {Abstract The thresholding methods based on the generalized iteratively
	reweighted least squares (IRLS) iteration are discussed under the
	complex-valued condition in this paper. A new thresholding function
	(Double-Threshold Sigmoid (DTHS) function) and two associated algorithms
	(DTHS-1 and DTHS-2) are proposed herein, and their convergence performances
	are discussed in detail. It is shown that the generalized \{IRLS\}
	algorithm is unbiased if the thresholding penalty can eliminate the
	undesired perturbation term added on the correlation matrix of the
	measurement matrix. Compared with the others, the new algorithms
	are endowed with stability and insensitivity with respect to the
	regularization parameter by selecting some sound upper thresholds
	and dividing the iteration procedures into the degraded stage and
	\{DTHS\} stage respectively. Further analyses show that the DTHS-1
	algorithm is suitable to deal with the sparse and continuous problems
	for both of the i.i.d. random matrix and under-resolution \{PSF\}
	matrix. The noise performance of the DTHS-1 algorithm is always superior
	to that of the \{IRLS\} algorithm, especially in the face of the
	under-resolution \{PSF\} matrix.},
  timestamp = {2016-07-08T11:55:02Z},
  journal = {Signal Processing},
  author = {Jun, Shi and Renhuan, Ding and Gao, Xiang and Xiaoling, Zhang},
  year = {2015},
  keywords = {analysis,Convergence,Double-threshold,Generalized,iteratively,least,penalty,recovery,reweighted,sigmoid,Sparse,squares},
  pages = {231 - 244},
  owner = {afdidehf}
}

@inproceedings{Jun2010,
  title = {MEG and EEG fusion in Bayesian frame},
  volume = {2},
  doi = {10.1109/ICEIE.2010.5559785},
  abstract = {In biomedical brain imaging, several distinctive brain imaging modalities
	have been developed with each demonstrating particular strengths
	and weaknesses. Despite such recent developments in biomedical brain
	imaging, an essential question persists: How can multi-modalities
	be effectively integrated so that they complement each other without
	compromising their inherently beneficial qualities? Toward such an
	end, Bayesian frame represents a reasonable solution for even the
	most complicated problems since corresponding fusion is particularly
	straightforward. Accordingly, a Bayesian integrative strategy for
	MEG and EEG brain imaging modalities is proposed in this work. The
	corresponding effects of synergy as well as overall feasibility are
	examined through numerical simulations. In addition, spatiotemporal
	noise covariance incorporated into the fusion frame is discussed.},
  timestamp = {2016-10-21T13:46:11Z},
  booktitle = {Electronics and Information Engineering (ICEIE), International Conference On},
  author = {Jun, Sung Chan},
  month = aug,
  year = {2010},
  keywords = {Analytical models,Bayesian frame,Bayesian integrative strategy,Bayesian
	methods,belief networks,biomedical brain imaging,Brain modeling,EEG,EEG
	fusion,electroencephalography,Fusion of Brain imaging data,image
	fusion,magnetoencephalography,medical image processing,MEG,MEG fusion,Noise,noise
	covariance,Simultaneous analysis,spatiotemporal phenomena},
  pages = {295--299},
  owner = {Fardin}
}

@article{Kahan2013,
  title = {A Tutorial Overview of Vector and Matrix Norms},
  timestamp = {2016-07-08T11:34:25Z},
  author = {Kahan, W.},
  month = jan,
  year = {2013},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Mathematics Dept., \& Computer Science Dept. University of California},
  owner = {afdidehf}
}

@inproceedings{Karakasis2009,
  title = {Perfomance Models for Blocked Sparse Matrix-Vector Multiplication 	Kernels},
  doi = {10.1109/ICPP.2009.21},
  abstract = {Sparse matrix-vector multiplication (SpMV) is a very challenging computational
	kernel, since its performance depends greatly on both the input matrix
	and the underlying architecture. The main problem of SpMV is its
	high demands on memory bandwidth, which cannot yet be abudantly offered
	from modern commodity architectures. One of the most promising optimization
	techniques for SpMV is blocking, which can reduce the indexing structures
	for storing a sparse matrix, and therefore alleviate the pressure
	to the memory subsystem. However, blocking methods can severely degrade
	performance if not used properly. In this paper, we study and evaluate
	a number of representative blocking storage formats and present a
	performance model that can accurately select the most suitable blocking
	storage format and the corresponding block shape and size for a specific
	sparse matrix. Our model considers both the memory and computational
	part of the kernel, which can be non-negligible when applying blocking,
	and also assumes an overlapping of memory accesses and computations
	that modern commodity architectures can offer through hardware prefetching
	mechanisms.},
  timestamp = {2016-07-10T07:32:01Z},
  booktitle = {Parallel Processing, 2009. ICPP '09. International Conference on},
  author = {Karakasis, V. and Goumas, G. and Koziris, N.},
  month = sep,
  year = {2009},
  keywords = {Bandwidth,blocking,blocking storage format,Computer architecture,Concurrent
	computing,Degradation,hardware prefetching mechanism,Indexing,Kernel,Matrix
	decomposition,matrix multiplication,memory bandwidth,optimisation,optimization
	technique,Parallel processing,performance models,Shape,sparse matrices,sparse
	matrix-vector multiplication,storage management},
  pages = {356-364},
  owner = {afdidehf}
}

@article{Karlovitz1970,
  title = {Construction of nearest points in the $L^p$, $p$ even, and $L^\infty$ 	norms},
  volume = {3},
  doi = {10.1016/0021-9045(70)90019-5},
  timestamp = {2016-07-08T12:03:13Z},
  number = {2},
  journal = {Journal of Approximation Theory},
  author = {Karlovitz, L. A.},
  year = {1970},
  pages = {123-127},
  owner = {Fardin}
}

@inproceedings{Karvanen2003,
  address = {Nara, Japan},
  title = {Measuring Sparseness Of Noisy Signals},
  abstract = {In this paper sparseness measures are reviewed, extended and compared.
	Special attention is paid on measuring sparseness of noisy data.
	We review and extend several definitions and measures for sparseness,
	including the 0, p and  norms. A measure based on order statistics
	is also proposed. The concept of sparseness is extended to the case
	where a signal has a dominant value other than zero. The sparseness
	measures can be easily modified to correspond to this new definition.
	Eight different measures are compared in three examples. It turns
	out that different measures may give complete opposite results if
	the distribution does not have a unique mode at zero. As conclusion,
	we suggest that the kurtosis should be avoided as a sparseness measure
	and recommend tanh-functions for measuring noisy sparseness.},
  timestamp = {2016-07-09T20:09:57Z},
  booktitle = {4th International Symposium on Independent Component Analysis and 	Blind Signal Separation (ICA2003)},
  author = {Karvanen, Juha and Cichocki, Andrzej},
  month = apr,
  year = {2003},
  owner = {Fardin}
}

@book{Kasneci2010,
  edition = {1},
  series = {Lecture Notes in Computer Science 6322 : Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, 	Part II},
  isbn = {3-642-15882-X 978-3-642-15882-7},
  timestamp = {2016-10-24T16:39:56Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Kasneci, Gjergji and Gael, Jurgen Van and Herbrich, Ralf and Graepel, Thore},
  editor = {Balc�zar, Jos� Luis and Bonchi, Francesco and Gionis, Aristides and Sebag, Mich�le},
  year = {2010},
  owner = {Fardin}
}

@inproceedings{Kekatos2010,
  title = {Robust layered sensing: From sparse signals to sparse residuals},
  doi = {10.1109/ACSSC.2010.5757676},
  abstract = {One of the key challenges in sensing networks is the extraction of
	information by fusing data from a multitude of possibly unreliable
	sensors. Robust sensing, viewed here as the simultaneous recovery
	of the wanted information-bearing signal vector together with the
	subset of (un)reliable sensors, is a problem whose optimum solution
	incurs combinatorial complexity. The present paper relaxes this problem
	to its closest convex approximation that turns out to yield a vector-generalization
	of Huber's scalar criterion for robust linear regression. The novel
	generalization is shown equivalent to a second-order cone program
	(SOCP), and exploits the block-sparsity inherent to a suitable model
	of the residuals. A computationally efficient solver is developed
	using a block-coordinate descent algorithm, and is tested with simulations.},
  timestamp = {2016-07-10T08:07:35Z},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2010 Conference Record 	of the Forty Fourth Asilomar Conference on},
  author = {Kekatos, V. and Giannakis, G.B.},
  month = nov,
  year = {2010},
  keywords = {Approximation methods,block-coordinate descent algorithm,Block-sparsity,combinatorial
	complexity,convex programming,feature extraction,Huber scalar,information-bearing
	signal vector,Linear regression,Noise,Optimization,regression analysis,robust
	layered sensing,robust linear regression,Robustness,second-order
	cone program,Sensors,signal processing,SOCP,sparse residual,sparse
	signal,unreliable sensor,vector-generalization,Vectors},
  pages = {803-807},
  owner = {afdidehf}
}

@article{Khajehnejad2011,
  title = {Explicit matrices for sparse approximation},
  issn = {2157-8095},
  doi = {10.1109/ISIT.2011.6034170},
  abstract = {We show that girth can be used to certify that sparse compressed sensing
	matrices have good sparse approximation guarantees. This allows us
	to present the first deterministic measurement matrix constructions
	that have an optimal number of measurements for ?1/?1 approximation.
	Our techniques are coding theoretic and rely on a recent connection
	of compressed sensing to LP relaxations for channel decoding.},
  timestamp = {2016-07-08T12:27:14Z},
  journal = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium 	on},
  author = {Khajehnejad, A. and Tehrani, A.S. and Dimakis, A.G. and Hassibi, B.},
  month = jul,
  year = {2011},
  keywords = {Approximation methods,approximation theory,channel coding,channel
	decoding,compressed sensing,Decoding,explicit matrices,first deterministic
	measurement matrix constructions,LP relaxations,parity check codes,sparse
	approximation,sparse compressed sensing matrices,sparse matrices,Symmetric
	matrices,USA Councils},
  pages = {469-473},
  owner = {Fardin}
}

@article{Kidron2007,
  title = {Cross-Modal Localization via Sparsity},
  volume = {55},
  issn = {1053-587X},
  doi = {10.1109/TSP.2006.888095},
  abstract = {Cross-modal analysis is a natural progression beyond processing of
	single-source signals. Simultaneous processing of two sources can
	reveal information that is unavailable when handling the sources
	separately. Indeed, human and animal perception, computer vision,
	weather forecasting, and various other scientific and technological
	fields can benefit from such a paradigm. A particular cross-modal
	problem is localization: out of the entire data array originating
	from one source, localize the components that best correlate with
	the other. For example, auditory and visual data sampled from a scene
	can be used to localize visual events associated with the sound track.
	In this paper we present a rigorous analysis of fundamental problems
	associated with the localization task. We then develop an approach
	that leads efficiently to a unique, high definition localization
	outcome. Our method is based on canonical correlation analysis (CCA),
	where inherent ill-posedness is removed by exploiting sparsity of
	cross-modal events. We apply our approach to localization of audio-visual
	events. The proposed algorithm grasps such dynamic audio-visual events
	with high spatial resolution. The algorithm effectively detects the
	pixels that are associated with sound, while filtering out other
	dynamic pixels, overcoming substantial visual distractions and audio
	noise. The algorithm is simple and efficient thanks to its reliance
	on linear programming, while being free of user-defined parameters},
  timestamp = {2016-07-08T12:06:33Z},
  number = {4},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Kidron, E. and Schechner, Y.Y. and Elad, M.},
  month = apr,
  year = {2007},
  keywords = {Animals,audio noise,audio signal processing,audio-visual systems,canonical
	correlation analysis,computer vision,cross-modal localization,cross-sensor
	fusion,data array,dynamic audio-visual events,dynamic pixels,Filtering
	algorithms,Heuristic algorithms,high spatial resolution,Humans,Image
	resolution,Layout,linear programming,multimedia,multimodal analysis,multimodal
	analysis,multisensor fusion,multisensor
	fusion,overfitting,regularization,Signal analysis,signal processing,signal
	processing,single-source signals processing,single-source
	signals processing,Spatial resolution,stochastic analysis,stochastic
	analysis,substantial visual distractions,substantial
	visual distractions,Weather forecasting},
  pages = {1390-1404},
  owner = {afdidehf}
}

@article{Kindermann2011,
  title = {Inverse Problems and Regularization - An Introduction},
  timestamp = {2016-07-09T19:44:22Z},
  author = {Kindermann, Stefan},
  year = {2011},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@book{Klein2013,
  edition = {1},
  series = {Lecture Notes in Computer Science 8188},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, 	Part I},
  isbn = {978-3-642-40987-5 978-3-642-40988-2},
  timestamp = {2016-10-24T16:41:14Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Klein, Edouard and Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},
  editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and �elezn�, Filip},
  year = {2013},
  owner = {Fardin}
}

@inproceedings{Koh2007,
  title = {An Efficient Method for Large-Scale l1-Regularized Convex Loss Minimization},
  doi = {10.1109/ITA.2007.4357584},
  abstract = {Convex loss minimization with lscr1 regularization has been proposed
	as a promising method for feature selection in classification (e.g.,
	lscr1-regularized logistic regression) and regression (e.g., lscr1-regularized
	least squares). In this paper we describe an efficient interior-point
	method for solving large-scale lscr1-regularized convex loss minimization
	problems that uses a preconditioned conjugate gradient method to
	compute the search step. The method can solve very large problems.
	For example, the method can solve an lscr1-regularized logistic regression
	problem with a million features and examples (e.g., the 20 Newsgroups
	data set), in a few minutes, on a PC.},
  timestamp = {2016-09-30T10:51:04Z},
  booktitle = {Information Theory and Applications Workshop, 2007},
  author = {Koh, Kwangmoo and Kim, Seung-Jean and Boyd, S.},
  month = jan,
  year = {2007},
  keywords = {compressed sensing,conjugate gradient method,conjugate gradient methods,convex
	loss minimization method,gradient methods,interior-point method,large-scale
	lscr1-regularization,Large-scale systems,Least squares methods,Logistics,lscr1-regularized
	least squares,lscr1-regularized logistic regression,minimisation,Minimization
	methods,Optimization methods,Predictive models,regression analysis,signal
	processing,Vectors},
  pages = {223--230},
  owner = {afdidehf}
}

@inproceedings{Korki2015,
  title = {An iterative bayesian algorithm for block-sparse signal reconstruction},
  doi = {10.1109/ICASSP.2015.7178356},
  abstract = {This paper presents a novel iterative Bayesian algorithm, Block Iterative
	Bayesian Algorithm (Block-IBA), for reconstructing block-sparse signals
	with unknown block structures. Unlike the other existing algorithms
	for block sparse signal recovery which assume the cluster structure
	of the non-zero elements of the unknown signal to be independent
	and identically distributed (i.i.d.), we use a more realistic Bernoulli-Gaussian
	hidden Markov model (BGHMM) to capture the burstiness (block structure)
	of the impulsive noise in practical applications such as Power Line
	Communication (PLC). The Block-IBA iteratively estimates the amplitudes
	and positions of the block-sparse signal based on Expectation-Maximization
	(EM) algorithm which is also optimized with the steepest-ascent method.
	Simulation results show the effectiveness of our algorithm for block-sparse
	signal recovery.},
  timestamp = {2016-07-08T11:24:05Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International 	Conference on},
  author = {Korki, M. and Zhangy, J. and Zhang, C. and Zayyani, H.},
  month = apr,
  year = {2015},
  keywords = {amplitude estimation,Bayes methods,Bernoulli-Gaussian
	hidden Markov model,Bernoulli-Gaussian hidden Markov
	model,Block-IBA,block-iterative Bayesian algorithm,Block-sparse,block
	sparse signal reconstruction,block sparse signal recovery,Clustering
	algorithms,cluster structure,compressed sensing,expectation-maximisation
	algorithm,expectation-maximization algorithm,Gaussian processes,gradient
	methods,hidden Markov models,impulse noise,impulsive noise,iterative
	Bayesian algorithm,mathematical model,Noise,non-zero elements,position
	estimation,signal reconstruction,steepest-ascent,steepest-ascent
	method},
  pages = {2174-2178},
  owner = {afdidehf}
}

@article{Kousta2007,
  title = {Source Localization for EEG/MEG},
  timestamp = {2016-07-10T08:23:26Z},
  author = {Kousta, Stavroula and Chadwick, Martin},
  year = {2007},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Methods for Dummies},
  owner = {Fardin}
}

@book{Kovacevic2013,
  title = {Fourier and Wavelet Signal Processing},
  timestamp = {2016-07-08T12:33:13Z},
  author = {Kovacevic, Jelena and Goyal, Vivek K. and Vetterli, Martin},
  month = jan,
  year = {2013},
  owner = {Fardin}
}

@article{Kowalski2009,
  title = {Sparse regression using mixed norms},
  volume = {27},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2009.05.006},
  abstract = {Mixed norms are used to exploit in an easy way, both structure and
	sparsity in the framework of regression problems, and introduce implicitly
	couplings between regression coefficients. Regression is done through
	optimization problems, and corresponding algorithms are described
	and analyzed. Beside the classical sparse regression problem, multi-layered
	expansion on unions of dictionaries of signals are also considered.
	These sparse structured expansions are done subject to an exact reconstruction
	constraint, using a modified \{FOCUSS\} algorithm. When the mixed
	norms are used in the framework of regularized inverse problem, a
	thresholded Landweber iteration is used to minimize the corresponding
	variational problem.},
  timestamp = {2016-07-11T16:51:23Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Kowalski, Matthieu},
  year = {2009},
  keywords = {FOCUSS,iterations,Landweber,mixed,norms,regression,Sparse,structured,Thresholded},
  pages = {303 - 324},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {afdidehf}
}

@inproceedings{Kreutz-Delgado1998,
  title = {Measures and algorithms for best basis selection},
  volume = {3},
  doi = {10.1109/ICASSP.1998.681831},
  abstract = {A general framework based on majorization, Schur-concavity, and concavity
	is given that facilitates the analysis of algorithm performance and
	clarifies the relationships between existing proposed diversity measures
	useful for best basis selection. Admissible sparsity measures are
	given by the Schur-concave functions, which are the class of functions
	consistent with the partial ordering on vectors known as majorization.
	Concave functions form an important subclass of the Schur-concave
	functions which attain their minima at sparse solutions to the basis
	selection problem. Based on a particular functional factorization
	of the gradient, we give a general affine scaling optimization algorithm
	that converges to a sparse solution for measures chosen from within
	this subclass},
  timestamp = {2016-07-09T20:09:54Z},
  booktitle = {Acoustics, Speech and Signal Processing, 1998. Proceedings of the 	1998 IEEE International Conference on},
  author = {Kreutz-Delgado, K. and Rao, B.D.},
  month = may,
  year = {1998},
  keywords = {Algorithm design and analysis,algorithm performance,best basis selection,Convergence,convergence
	of numerical methods,Dictionaries,diversity measures,Electric variables
	measurement,Entropy,Equations,functional analysis,functional factorization,general
	affine scaling optimization algorithm,gradient,Length measurement,majorization,minima,optimisation,partial
	ordering,Particle measurements,scaling matrix,Schur-concave functions,Schur-concavity,signal
	representation,signal representations,sparse matrices,sparse signal
	representation,sparse solutions,Vectors},
  pages = {1881-1884 vol.3},
  owner = {Fardin}
}

@inproceedings{Krishnan2013,
  title = {Advanced MEG Source Analysis for Epileptogenic Focus Localization 	in Patients with Non-Lesional MRI},
  doi = {10.1109/SBEC.2013.37},
  abstract = {Accurate determination of the epileptogenic focus can be challenging,
	especially when electroencephalography (EEG) and/or imaging studies
	(e.g. MRI) are inconclusive. A significant number of patients who
	undergo presurgical MEG testing have neocortical epilepsy with normal
	(non-lesional) MRI. The methodology we developed first identifies
	potential current sources in the brain that could account for the
	spontaneous MEG signals that are recorded in the sensor space, and
	subsequently measures directional information flow in the space of
	the identified current dipole sources. Applying this methodology
	to the interictal MEG recordings from two patients with neocortical
	epilepsy and non-lesional MRIs, who were seizure-free at least 6
	months after surgery and resection of their epileptogenic focus,
	we were able to correctly localize the focus irrespectively of the
	presence or absence of interictal epileptic spikes in the data. It
	is anticipated that the promise of the proposed methodology to noninvasively
	identify the location of obscure neocortical epileptogenic foci,
	a yet unfulfilled goal for many patients with focal epilepsy and
	normal MRIs, could lead to a paradigm shift in the diagnosis and
	treatment of epilepsy by decreasing the need for prolonged hospitalization,
	improving selection of surgical candidates, and guiding the placement
	of intracranial electrodes when needed.},
  timestamp = {2016-07-08T10:14:17Z},
  booktitle = {Biomedical Engineering Conference (SBEC), 2013 29th Southern},
  author = {Krishnan, B. and Vlachos, I. and Wang, Z.I. and Mosher, J. and Iasemidis, L. and Burgess, R. and Alexopoulos, A.V.},
  month = may,
  year = {2013},
  keywords = {advanced MEG source analysis,biomedical electrodes,biomedical MRI,Brain
	modeling,electroencephalography,Epilepsy,epileptogenic focus localization,Head,Interictal
	epileptic spikes,intracranial electrodes,Magnetic resonance imaging,magnetoencephalography,medical
	disorders,neocortical epilepsy,nonlesional MRI,presurgical MEG testing,Principal
	component analysis,Surgery},
  pages = {57-58},
  owner = {afdidehf}
}

@inproceedings{Kronvall2014a,
  title = {Joint DOA and multi-pitch estimation using block sparsity},
  doi = {10.1109/ICASSP.2014.6854344},
  abstract = {In this paper, we propose a novel method to estimate the fundamental
	frequencies and directions-of-arrival (DOA) of multi-pitch signals
	impinging on a sensor array. Formulating the estimation as a group
	sparse convex optimization problem, we use the alternating direction
	of multipliers method (ADMM) to estimate both temporal and spatial
	correlation of the array signal. By first jointly estimating both
	fundamental frequencies and time-of-arrivals (TOAs) for each sensor
	and sound source, we then form a non-linear least squares estimate
	to obtain the DOAs. Numerical simulations indicate the preferable
	performance of the proposed estimator as compared to current state-of-the-art
	methods.},
  timestamp = {2016-07-09T19:48:16Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Kronvall, T. and Adalbjornsson, S.I. and Jakobsson, A.},
  month = may,
  year = {2014},
  keywords = {ADMM,alternating direction of multipliers method,Arrays,array signal
	processing,block sparsity,convex optimization,convex programming,direction-of-arrival,direction-of-arrival estimation,Direction-of-arrival
	estimation,Estimation,frequency estimation,Frequency
	estimation,fundamental frequency estimation,group sparse convex optimization
	problem,group sparsity,Harmonic analysis,joint DOA,Joints,least squares
	approximations,multi-pitch estimation,multipitch estimation,multipitch
	signal,nonlinear least square estimate,numerical simulations,Optimization,sensor
	array signal,sound source,spatial correlation,temporal correlation,time-of-arrival,time-of-arrival
	estimation},
  pages = {3958-3962},
  owner = {afdidehf}
}

@inproceedings{Kudekar2010,
  title = {The effect of spatial coupling on compressive sensing},
  doi = {10.1109/ALLERTON.2010.5706927},
  abstract = {Recently, it was observed that spatially-coupled LDPC code ensembles
	approach the Shannon capacity for a class of binary-input memoryless
	symmetric (BMS) channels. The fundamental reason for this was attributed
	to a threshold saturation phenomena derived in. In particular, it
	was shown that the belief propagation (BP) threshold of the spatially
	coupled codes is equal to the maximum a posteriori (MAP) decoding
	threshold of the underlying constituent codes. In this sense, the
	BP threshold is saturated to its maximum value. Moreover, it has
	been empirically observed that the same phenomena also occurs when
	transmitting over more general classes of BMS channels. In this paper,
	we show that the effect of spatial coupling is not restricted to
	the realm of channel coding. The effect of coupling also manifests
	itself in compressed sensing. Specifically, we show that spatially-coupled
	measurement matrices have an improved sparsity to sampling threshold
	for reconstruction algorithms based on verification decoding. For
	BP-based reconstruction algorithms, this phenomenon is also tested
	empirically via simulation. At the block lengths accessible via simulation,
	the effect is rather small but, based on the threshold analysis,
	we believe this warrants further study.},
  timestamp = {2016-07-11T17:00:38Z},
  booktitle = {Communication, Control, and Computing (Allerton), 2010 48th Annual 	Allerton Conference on},
  author = {Kudekar, S. and Pfister, H.D.},
  month = sep,
  year = {2010},
  keywords = {belief propagation threshold,binary codes,binary-input memoryless
	symmetric channels,BMS channels,BP-based reconstruction algorithms,BP
	threshold analysis,channel coding,compressed sensing,Compressive
	sensing,Couplings,Decoding,Equations,matrix algebra,maximum a posteriori
	decoding threshold,maximum likelihood decoding,Noise measurement,parity
	check codes,sampling threshold analysis,Shannon capacity,signal reconstruction,signal
	reconstruction algorithms,signal sampling,sparse matrices,spatial-coupled
	LDPC code ensemble approach,spatial-coupled measurement matrices,spatial
	coupling effect,threshold saturation phenomena,verification decoding},
  pages = {347-353},
  owner = {afdidehf}
}

@inproceedings{Kung2011,
  title = {Partitioned compressive sensing with neighbor-weighted decoding},
  doi = {10.1109/MILCOM.2011.6127519},
  abstract = {Compressive sensing has gained momentum in recent years as an exciting
	new theory in signal processing with several useful applications.
	It states that signals known to have a sparse representation may
	be encoded and later reconstructed using a small number of measurements,
	approximately proportional to the signal's sparsity rather than its
	size. This paper addresses a critical problem that arises when scaling
	compressive sensing to signals of large length: that the time required
	for decoding becomes prohibitively long, and that decoding is not
	easily parallelized. We describe a method for partitioned compressive
	sensing, by which we divide a large signal into smaller blocks that
	may be decoded in parallel. However, since this process requires
	a significant increase in the number of measurements needed for exact
	signal reconstruction, we focus on mitigating artifacts that arise
	due to partitioning in approximately reconstructed signals. Given
	an error-prone partitioned decoding, we use large magnitude components
	that are detected with highest accuracy to influence the decoding
	of neighboring blocks, and call this approach neighbor-weighted decoding.
	We show that, for applications with a predefined error threshold,
	our method can be used in conjunction with partitioned compressive
	sensing to improve decoding speed, requiring fewer additional measurements
	than unweighted or locally-weighted decoding.},
  timestamp = {2016-07-10T07:25:32Z},
  booktitle = {MILITARY COMMUNICATIONS CONFERENCE, 2011 - MILCOM 2011},
  author = {Kung, H.T. and Tarsa, S.J.},
  month = nov,
  year = {2011},
  keywords = {Bismuth,compressed sensing,Decoding,decoding speed,error-prone partitioned
	decoding,exact signal reconstruction,Finite wordlength effects,Frequency
	measurement,gained momentum,locally-weighted decoding,Matching pursuit
	algorithms,neighboring blocks,neighbor-weighted decoding,partitioned
	compressive sensing,scaling compressive sensing,signal processing,signal
	reconstruction,Silicon,sparse representation},
  pages = {149-156},
  owner = {afdidehf}
}

@article{Kutyniok2014,
  title = {Sparse Representations, Numerical Linear Algebra, and Optimization},
  timestamp = {2016-07-11T16:52:59Z},
  journal = {Sparse Representations, Numerical Linear Algebra, and Optimization},
  author = {Kutyniok, Gitta and Saunders, Michael and Wright, Stephen and Yilmaz, Ozgur},
  year = {2014},
  owner = {afdidehf}
}

@InProceedings{Kwon2012,
  author    = {Kwon, H. and Rao, B.D.},
  title     = {On the benefits of the block-sparsity structure in sparse signal recovery},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on},
  year      = {2012},
  pages     = {3685-3688},
  month     = mar,
  abstract  = {We study the problem of support recovery of block-sparse signals,
	where nonzero entries occur in clusters, via random noisy measurements.
	By drawing analogy between the problem of block-sparse signal recovery
	and the problem of communication over Gaussian multi-input and single-output
	multiple access channel, we derive the sufficient and necessary condition
	under which exact support recovery is possible. Based on the results,
	we show that block-sparse signals can reduce the number of measurements
	required for exact support recovery, by at least `1/(block size)',
	compared to conventional or scalar-sparse signals. The minimum gain
	is guaranteed by increased signal to noise power ratio (SNR) and
	reduced effective number of entries (i.e., not individual elements
	but blocks) that are dominant at low SNR and at high SNR, respectively.
	When the correlation between the elements of each nonzero block is
	low, a larger gain than `1/(block size)' is expected due to, so called,
	diversity effect, especially in the moderate and low SNR regime.},
  annote    = {read},
  doi       = {10.1109/ICASSP.2012.6288716},
  keywords  = {Block-sparse signal recovery,block-sparse signals,block-sparsity structure,Brain modeling,channel capacity,Gaussian channels,Gaussian multiinput and single-output multiple access channel,MIMO communication,MISO-MAC channel capacity,MISOMAC channel capacity,MISOMAC channel capacity,MISO-MAC channel capacity,multi-access systems,Noise measurement,random noisy measurements,Receivers,scalar-sparse signals,signal reconstruction,signal to noise power ratio,Signal to noise ratio,Size measurement,SNR,support recovery,Vectors},
  owner     = {afdidehf},
  timestamp = {2016-07-10T07:15:35Z},
}

@inproceedings{Lahat2014,
  title = {Challenges in multimodal data fusion},
  abstract = {In various disciplines, information about the same phenomenon can
	be acquired from different types of detectors, at different conditions,
	different observations times, in multiple experiments or subjects,
	etc. We use the term �modality� to denote each such type of acquisition
	framework. Due to the rich characteristics of natural phenomena,
	as well as of the environments in which they occur, it is rare that
	a single modality can provide complete knowledge of the phenomenon
	of interest. The increasing availability of several modalities at
	once introduces new degrees of freedom, which raise questions beyond
	those related to exploiting each modality separately. It is the aim
	of this paper to evoke and promote various challenges in multimodal
	data fusion at the conceptual level, without focusing on any specific
	model, method or application.},
  timestamp = {2016-07-08T11:49:56Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd 	European},
  author = {Lahat, D. and Adali, T. and Jutten, C.},
  month = sep,
  year = {2014},
  keywords = {acquisition framework,Analytical models,Brain modeling,data fusion,data
	integration,Data models,degrees of freedom,electroencephalography,imaging,multimodal
	data fusion,multimodality,sensor fusion,Spatial resolution},
  pages = {101-105},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@article{Lall2001,
  title = {The projection theorem},
  timestamp = {2016-07-11T17:02:56Z},
  author = {Lall, Sanjay},
  year = {2001},
  howpublished = {Electrical Engineering, Stanford University},
  owner = {afdidehf}
}

@article{Lalos2014,
  title = {Model based compressed sensing reconstruction algorithms for ECG 	telemonitoring in WBANs},
  volume = {35},
  issn = {1051-2004},
  doi = {http://dx.doi.org/10.1016/j.dsp.2014.08.007},
  abstract = {Abstract Wireless Body area networks (WBANs) consist of sensors that
	continuously monitor and transmit real time vital signals to a nearby
	coordinator and then to a remote terminal via the Internet. One of
	the most important signals for monitoring in \{WBANs\} is the electrocardiography
	(ECG) signal. The design of an accurate and energy efficient \{ECG\}
	telemonitoring system can be achieved by: i) reducing the amount
	of data that should be transmitted ii) minimizing the computational
	operations executed at any transmitter/receiver in a WBAN. To this
	end, compressed sensing (CS) approaches can offer a viable solution.
	In this paper, we propose two novel \{CS\} based \{ECG\} reconstruction
	algorithms that minimize the samples that are required to be transmitted
	for an accurate reconstruction, by exploiting the block structure
	of the \{ECG\} in the time domain (TD) and in an uncorrelated domain
	(UD). The proposed schemes require the solutions of second-order
	cone programming (SOCP) problems that are usually tackled by computational
	demanding interior point (IP) methods. To solve these problems efficiently,
	we develop a path-wise coordinate descent based scheme. The reconstruction
	accuracy is evaluated by the percentage root-mean-square difference
	(PRD) metric. A reconstructed signal is acceptable if and only if
	\{PRD\} &lt; 9 % . Simulation studies carried out with real electrocardiographic
	(ECG) data, show that the proposed schemes, operating in both the
	\{TD\} and in the \{UD\} as compared to the conventional \{CS\} techniques,
	reduce the Compression Ratio (CR) by 20 % and 44 % respectively,
	offering at the same time significantly low computational complexity.},
  timestamp = {2016-07-09T20:13:46Z},
  journal = {Digital Signal Processing},
  author = {Lalos, Aris S. and Alonso, Luis and Verikoukis, Christos},
  year = {2014},
  keywords = {area,Block,body,Compressed,ECG,monitoring,networks,Real-time,sensing,Sparsity,Wireless},
  pages = {105 - 116},
  owner = {afdidehf}
}

@article{Landau1967,
  title = {Necessary density conditions for sampling and interpolation of certain 	entire functions},
  volume = {117},
  issn = {0001-5962},
  doi = {10.1007/BF02395039},
  language = {English},
  timestamp = {2016-09-30T11:13:59Z},
  number = {1},
  journal = {Acta Mathematica},
  author = {Landau, H.J.},
  year = {1967},
  pages = {37--52},
  annote = {http://dx.doi.org/10.1007/BF02395039},
  annote = {http://dx.doi.org/10.1007/BF02395039},
  annote = {http://dx.doi.org/10.1007/BF02395039},
  annote = {http://dx.doi.org/10.1007/BF02395039},
  annote = {http://dx.doi.org/10.1007/BF02395039},
  owner = {afdidehf}
}

@article{Lapalme2006,
  title = {Data-driven parceling and entropic inference in MEG},
  volume = {30},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.08.067},
  abstract = {In Amblard et al. [Amblard, C., Lapalme, E., Lina, J.M. 2004. Biomagnetic
	source detection by maximum entropy and graphical models. \{IEEE\}
	Trans. Biomed. Eng. 55 (3) 427–442], the authors introduced the
	maximum entropy on the mean (MEM) as a methodological framework for
	solving the magnetoencephalography (MEG) inverse problem. The main
	component of the \{MEM\} is a reference probability density that
	enables one to include all kind of prior information on the source
	intensity distribution to be estimated. This reference law also encompasses
	the definition of a model. We consider a distributed source model
	together with a clustering hypothesis that assumes functionally coherent
	dipoles. The reference probability distribution is defined as a prior
	parceling of the cortical surface. In this paper, we present a data-driven
	approach for parceling out the cortex into regions that are functionally
	coherent. Based on the recently developed multivariate source prelocalization
	(MSP) principle [Mattout, J., Pelegrini-Issac, M., Garnero, L., Benali,
	H. 2005. Multivariate source prelocalization (MSP): Use of functionally
	informed basis functions for better conditioning the \{MEG\} inverse
	problem. NeuroImage 26 (2) 356–373], the data-driven clustering
	(DDC) of the dipoles provides an efficient parceling of the sources
	as well as an estimate of parameters of the initial reference probability
	distribution. On \{MEG\} simulated data, the \{DDC\} is shown to
	further improve the \{MEM\} inverse approach, as evaluated considering
	two different iterative algorithms and using classical error metrics
	as well as \{ROC\} (receiver operating characteristic) curve analysis.
	The \{MEM\} solution is also compared to a LORETA-like inverse approach.
	The data-driven clustering allows to take most advantage of the \{MEM\}
	formalism. Its main trumps lie in the flexible probabilistic way
	of introducing priors and in the notion of spatial coherent regions
	of activation. The latter reduces the dimensionality of the problem.
	In so doing, it narrows down the gap between the two types of inverse
	methods, the popular dipolar approaches and the distributed ones.},
  timestamp = {2016-07-08T12:09:55Z},
  number = {1},
  journal = {NeuroImage},
  author = {Lapalme, Ervig and Lina, Jean-Marc and Mattout, J�r�mie},
  year = {2006},
  keywords = {Entropy,Maximum,mean,on,the},
  pages = {160 - 171},
  owner = {afdidehf}
}

@article{Lazzaro2015,
  title = {Blind cluster structured sparse signal recovery: A nonconvex approach},
  volume = {109},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.11.002},
  abstract = {Abstract We consider the problem of recovering a sparse signal when
	its nonzero coefficients tend to cluster into blocks, whose number,
	dimension and position are unknown. We refer to this problem as blind
	cluster structured sparse recovery. For its solution, differently
	from the existing methods that consider the problem in a statistical
	context, we propose a deterministic neighborhood based approach characterized
	by the use both of a nonconvex, nonseparable sparsity inducing function
	and of a penalized version of the iterative l1 reweighted method.
	Despite the high nonconvexity of the approach, a suitable integration
	of these building elements led to the development of MB-NFCS (Model
	Based Nonlinear Filtering for Compressed Sensing), an iterative fast,
	self-adaptive, and efficient algorithm that, without requiring any
	information on the sparsity pattern, adjusts at each iteration the
	action of the sparsity inducing function in order to strongly encourage
	the emerging cluster structure. The effectiveness of the proposed
	approach is demonstrated by a large set of numerical experiments
	that show the superior performance of MB-NFCS to the state-of-the-art
	algorithms.},
  timestamp = {2016-07-08T11:38:52Z},
  journal = {Signal Processing},
  author = {Lazzaro, Damiana and Montefusco, Laura B. and Papi, Serena},
  year = {2015},
  keywords = {based,Cluster,Compressed,method,methods,Minimization,Neighborhood,Nonconvex,Penalization,reweighted,sensing,Sparsity,structured},
  pages = {212 - 225},
  owner = {afdidehf}
}

@article{LePennec2005,
  title = {Sparse geometric image representations with bandelets},
  volume = {14},
  issn = {1057-7149},
  doi = {10.1109/TIP.2005.843753},
  abstract = {This paper introduces a new class of bases, called bandelet bases,
	which decompose the image along multiscale vectors that are elongated
	in the direction of a geometric flow. This geometric flow indicates
	directions in which the image gray levels have regular variations.
	The image decomposition in a bandelet basis is implemented with a
	fast subband-filtering algorithm. Bandelet bases lead to optimal
	approximation rates for geometrically regular images. For image compression
	and noise removal applications, the geometric flow is optimized with
	fast algorithms so that the resulting bandelet basis produces minimum
	distortion. Comparisons are made with wavelet image compression and
	noise-removal algorithms.},
  timestamp = {2016-07-11T16:48:57Z},
  number = {4},
  journal = {Image Processing, IEEE Transactions on},
  author = {Le Pennec, E. and Mallat, S.},
  month = apr,
  year = {2005},
  keywords = {Algorithms,Approximation error,Artificial Intelligence,Automated,bandelet
	bases,Computer-Assisted,computer graphics,Costs,data compression,Data
	Compression,Distortion,fast subband-filtering algorithm,Filtering,filtering
	theory,image coding,Image decomposition,image denoising,image
	denoising,Image Enhancement,image gray level,image
	gray level,Image Interpretation,image representation,image
	representation,Image resolution,Image
	resolution,inverse problems,Inverse
	problems,multimedia,multiscale vector,multiscale
	vector,Noise reduction,Nonlinear filtering
	and enhancement (2-NFLT),Nonlinear filtering and enhancement (2-NFLT),Nonlinear
	filtering and enhancement (2-NFLT),numerical analysis,Numerical
	Analysis,optimal approximation,optimisation,Pattern Recognition,Pattern
	Recognition,rate distortion theory,rate
	distortion theory,Reproducibility of Results,Sensitivity and Specificity,Sensitivity
	and Specificity,signal processing,signal
	processing,sparse geometric image representation,still image coding
	(1-STIL),still
	image coding (1-STIL),wavelet image compression,wavelets and multiresolution
	processing (2-WAVP),wavelets and multiresolution processing
	(2-WAVP),wavelet transforms},
  pages = {423-438},
  owner = {Fardin}
}

@inproceedings{Lecumberri2002,
  title = {Estimation and localization of electrical dipoles in somatosensory 	evoked potentials},
  volume = {1},
  doi = {10.1109/IEMBS.2002.1134490},
  abstract = {Neuronal currents in the brain produce external magnetic fields and
	scalp surface potentials that can be measured using magnetoencephalography
	(MEG) and electroencephalography (EEG), respectively. In the context
	of the localization of neuronal sources, the forward problem is to
	determinate the potentials and magnetic fields that result from primary
	current sources. The inverse problem is to estimate the location
	of these primary current sources. Several algorithms have been applied
	to solve it. There are two kinds of such algorithms: Imaging methods,
	e.g. see R. D. Pascual-Marqui (1994), and parametric approaches.
	Here we present a revision of the MUSIC (J. C. Mosher et al., 1992)
	parametric algorithm and we analyze the effectiveness of this method
	with real EEG signals.},
  timestamp = {2016-07-08T12:24:26Z},
  booktitle = {Engineering in Medicine and Biology, 2002. 24th Annual Conference 	and the Annual Fall Meeting of the Biomedical Engineering Society 	EMBS/BMES Conference, 2002. Proceedings of the Second Joint},
  author = {Lecumberri, P. and Gomez, M. and Malanda, A. and Artieda, J. and Alegre, M. and de Gurtubay, I.G. and Valencia, M. and Colino, M.},
  year = {2002},
  keywords = {bioelectric phenomena,Brain modeling,brain neuronal currents,EEG signals,electrical
	dipoles estimation,Electrodes,electroencephalography,forward problem,Inverse
	problems,Magnetic fields,Magnetic heads,magnetoencephalography,median
	nerve stimulation,medical signal processing,Multiple signal classification,MUSIC
	parametric algorithm,neuronal sources localization,primary current
	sources,Radar antennas,Radar detection,scalp surface potentials,signal
	detection,somatosensory phenomena},
  pages = {275-276 vol.1},
  owner = {afdidehf}
}

@article{Lee2013,
  title = {Dipole Source Localization of Mouse Electroencephalogram Using the 	Fieldtrip Toolbox},
  volume = {8},
  doi = {10.1371/journal.pone.0079442},
  abstract = {The mouse model is an important research tool in neurosciences to
	examine brain function and diseases with genetic perturbation in
	different brain regions. However, the limited techniques to map activated
	brain regions under specific experimental manipulations has been
	a drawback of the mouse model compared to human functional brain
	mapping. Here, we present a functional brain mapping method for fast
	and robust in vivo brain mapping of the mouse brain. The method is
	based on the acquisition of high density electroencephalography (EEG)
	with a microarray and EEG source estimation to localize the electrophysiological
	origins. We adapted the Fieldtrip toolbox for the source estimation,
	taking advantage of its software openness and flexibility in modeling
	the EEG volume conduction. Three source estimation techniques were
	compared: Distribution source modeling with minimum-norm estimation
	(MNE), scanning with multiple signal classification (MUSIC), and
	single-dipole fitting. Known sources to evaluate the performance
	of the localization methods were provided using optogenetic tools.
	The accuracy was quantified based on the receiver operating characteristic
	(ROC) analysis. The mean detection accuracy was high, with a false
	positive rate less than 1.3% and 7% at the sensitivity of 90% plotted
	with the MNE and MUSIC algorithms, respectively. The mean center-to-center
	distance was less than 1.2 mm in single dipole fitting algorithm.
	Mouse microarray EEG source localization using microarray allows
	a reliable method for functional brain mapping in awake mouse opening
	an access to cross-species study with human brain.},
  timestamp = {2016-07-08T12:11:41Z},
  number = {11},
  journal = {PLoS One},
  author = {Lee, Chungki and Oostenveld, Robert and Lee, Soo Hyun and Kim, Lae
	Hyun and Sung, Hokun and Choi, Jee Hyun},
  year = {2013},
  note = {read},
  owner = {Fardin}
}

@inproceedings{Lee2011,
  title = {A new block compressive sensingto control the number of measurements},
  doi = {10.1109/ICIP.2011.6116229},
  abstract = {Compressive Sensing (CS) aims to recover a sparse signal from a small
	number of projections onto random vectors. Because of its great practical
	possibility, both academia and industries have made efforts to develop
	the CS's reconstruction performance, but most of existing works remain
	at the theoretical study. In this paper, we propose a new Block Compres-sive
	Sensing (nBCS), which has several benefits compared to the general
	CS methods. In particular, the nBCS can be dynamically adaptive to
	varying channel capacity because it conveys the good inheritance
	of the wavelet transform.},
  timestamp = {2016-07-08T10:31:29Z},
  booktitle = {Image Processing (ICIP), 2011 18th IEEE International Conference 	on},
  author = {Lee, Hyungkeuk and Oh, Heeseok and Lee, Sanghoon},
  month = sep,
  year = {2011},
  keywords = {block compressive sensing,channel capacity variation,channel capacity
	variation,compressed sensing,compressive sensing,Compressive
	sensing,data compression,Distortion measurement,Distortion
	measurement,image coding,Image reconstruction,Image
	reconstruction,PSNR,random processes,random vectors,random
	vectors,signal representation,signal restoration,signal
	restoration,sparse signal recovery,transforms,Vectors,wavelet transform,wavelet
	transform,wavelet transforms,wavelet
	transforms},
  pages = {2713-2716},
  owner = {afdidehf}
}

@article{Lee2013a,
  title = {Visually Weighted Compressive Sensing: Measurement and Reconstruction},
  volume = {22},
  issn = {1057-7149},
  doi = {10.1109/TIP.2012.2231688},
  abstract = {Compressive sensing (CS) makes it possible to more naturally create
	compact representations of data with respect to a desired data rate.
	Through wavelet decomposition, smooth and piecewise smooth signals
	can be represented as sparse and compressible coefficients. These
	coefficients can then be effectively compressed via the CS. Since
	a wavelet transform divides image information into layered blockwise
	wavelet coefficients over spatial and frequency domains, visual improvement
	can be attained by an appropriate perceptually weighted CS scheme.
	We introduce such a method in this paper and compare it with the
	conventional CS. The resulting visual CS model is shown to deliver
	improved visual reconstructions.},
  timestamp = {2016-07-11T17:11:35Z},
  number = {4},
  journal = {Image Processing, IEEE Transactions on},
  author = {Lee, Hyungkeuk and Oh, Heeseok and Lee, Sanghoon and Bovik, A.C.},
  month = apr,
  year = {2013},
  keywords = {compressed sensing,Compressive sensing (CS),frequency-domain analysis,frequency
	domains,hidden Markov models,image information,Image reconstruction,Indexes,layered
	blockwise wavelet coefficients,modified block compressive sensing,spatial
	domains,visual compressive sensing,visual improvement,Visualization,visually
	weighted compressive sensing,visual reconstructions,wavelet decomposition,Wavelet
	domain,wavelet transform,wavelet transforms,weighted
	compressive sampling matching pursuit,weighted compressive
	sampling matching pursuit,Weight measurement},
  pages = {1444-1455},
  owner = {afdidehf}
}

@inproceedings{Lee2014,
  title = {Big Data 'Fork': Tensor Product for DCT-II/DST-II/ DFT/HWT},
  doi = {10.1109/BMSB.2014.6873510},
  abstract = {The tensor product, denoted by ? (Kronecker Product), may be applied
	in different contexts to vectors, matrices, tensors, vector spaces,
	algebras etc. The element (block)-wise inverse Jacket (BIJM) is simply
	calculated for big data which we called big data `Fork' matrix. We
	use matrix decomposition method to compress or minimize the big matrix
	to smaller matrix. In this paper, based on the BIJM, a unified fast
	hybrid diagonal block-wise transform (FHDBT) algorithm is proposed.
	A new fast diagonal block matrix decomposition is made by the matrix
	product of successively lower order diagonal Jacket matrix and Hadamard
	matrix. The FHDBT, which is able to convert a newly developed discrete
	cosine transform (DCT)-II, discrete sine transform (DST)-II, discrete
	Fourier transform (DFT), and Haar-based wavelet transform (HWT).
	Comparing with pre-existing DCT-II, DST-II, DFT, and HWT, it is shown
	that the proposed FHDBT exhibits less the complexity as its matrix
	size gets larger. From the numerical experiments, it is shown that
	a better performance can be achieved by the use of DCT/DST-II compression
	scheme compared with the DCT-II only compression method.},
  timestamp = {2016-07-08T11:38:07Z},
  booktitle = {Broadband Multimedia Systems and Broadcasting (BMSB), 2014 IEEE International 	Symposium on},
  author = {Lee, Moon Ho and Khan, M.H.A.},
  month = jun,
  year = {2014},
  keywords = {algebras,Big data,big data fork,block-wise inverse Jacket,DCT-II,DFT,Diagonal
	block (element)-wise inverse Jacket matrix (BIJM),discrete cosine
	transform,discrete cosine transforms,discrete Fourier transform,Discrete
	Fourier transforms,discrete sine transform,DST-II,fast diagonal block
	matrix decomposition,fast hybrid diagonal block-wise transform,Haar-based
	wavelet transform,Haar transforms,Hadamard matrices,Hadamard matrix,Hadamard
	matrix,HWT,image coding,Jacket matrix,Kronecker product,matrices,matrix decomposition,Matrix
	decomposition,sparse matrices,tensors,vector
	spaces},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Lee2013b,
  title = {Many-view under-sampling (MVUS) technique for low-dose CT},
  doi = {10.1109/NSSMIC.2013.6829301},
  abstract = {In computed tomography (CT) imaging, radiation dose delivered to the
	patient is one of the major concerns. Among many technical solutions
	to lowering radiation dose while preserving clinical utilities of
	the images, sparse-view CT is promising technique. However, a fast
	power switching of an X-ray tube, which is needed for the sparse-view
	sampling, can be challenging in many CT systems. We have recently
	proposed a novel alternative approach to sparse-view circular CT
	that can be readily incorporated on the existing CT systems, and
	have successfully shown its feasibility. Instead of switching the
	X-ray tube power, one can place an oscillating multi-slit collimator
	between the X-ray tube and the patient to partially block the X-ray
	beam thereby reducing the radiation. In this study, an experimental
	study was performed to evaluate the performance of the proposed XT
	scan scheme. Industrial CT projection data of a CatPhan� 600 phantom
	was acquired by use of the oscillating multi-slit collimator. We
	used a sinusoidal motion of the collimator to the perpendicular direction
	of the rotation axis for the purpose of obtaining more uniform spatial
	sampling of the image. For image reconstruction, we used a total-variation
	minimization (TV) algorithm which has shown its out-performance in
	many sparse-view CT applications.},
  timestamp = {2016-07-09T20:07:13Z},
  booktitle = {Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC), 	2013 IEEE},
  author = {Lee, Taewon and Abbas, S. and Cho, Byungchul and Kim, Insoo and Han, Bumsoo and Cho, Seungryong},
  month = oct,
  year = {2013},
  keywords = {CatPhan 600 phantom,collimators,Computed tomography,computerised tomography,dosimetry,Electron
	tubes,fast power switching,image quality,Image reconstruction,image
	sampling,industrial CT projection data,low-dose computerised tomography
	imaging,many-view under-sampling technique,medical image processing,minimisation,Minimization,oscillating
	multislit collimator,phantoms,radiation dose delivery,rotation axis,sinusoidal
	motion,sparse-view circular computerised tomography,sparse-view sampling,total-variation
	minimization algorithm,uniform spatial sampling,X-ray beam,X-ray
	imaging,X-ray tube power switching,X-ray tubes,XT scan scheme},
  pages = {1-3},
  owner = {afdidehf}
}

@inproceedings{Lefevre2007,
  title = {Mapping and Tracking the Flow of Brain Activations using MEG/EEG: 	Hypothesis and Methods},
  doi = {10.1109/NFSI-ICFBI.2007.4387675},
  abstract = {We introduce a mathematical tool for the exploration of spatiotemporal
	dynamics of brain activations as revealed by time-resolved brain
	mapping techniques. In that respect, Magneto (MEG) and Electroencephalography
	(EEG) source imaging has been considerably maturing in terms of leading
	access to the dynamics of brain activity. Here we suggest that the
	local analysis in space and time of the resulting source measures
	might be performed through the computation of a cortical displacement
	field within the optical flow framework. The theoretical principles
	of the method are briefly introduced and are subsequently illustrated
	by simulations. Finally, this technique was applied on brain image
	sequences from a ball-catching paradigm, in which significant structures
	of the resulting optical flow during the early brain response that
	accompanied the fall of the ball could be revealed.},
  timestamp = {2016-07-09T20:07:16Z},
  booktitle = {Noninvasive Functional Source Imaging of the Brain and Heart and 	the International Conference on Functional Biomedical Imaging, 2007. 	NFSI-ICFBI 2007. Joint Meeting of the 6th International Symposium 	on},
  author = {Lefevre, J. and Baillet, S.},
  month = oct,
  year = {2007},
  keywords = {ball-catching paradigm,brain activations,brain image sequences,Brain
	Mapping,EEG,electroencephalography,Extraterrestrial measurements,flow
	mapping,flow tracking,Fluid flow measurement,image motion analysis,image
	sequences,Magnetic analysis,magnetic field measurement,magnetoencephalography,medical
	image processing,MEG,neurophysiology,optical flow framework,Optical
	imaging,Performance analysis,spatiotemporal dynamics,spatiotemporal phenomena,spatiotemporal
	phenomena,time-resolved brain mapping techniques},
  pages = {14-17},
  owner = {afdidehf}
}

@article{Lemmens1973,
  title = {Equiangular lines},
  volume = {24},
  issn = {0021-8693},
  doi = {http://dx.doi.org/10.1016/0021-8693(73)90123-3},
  timestamp = {2016-07-08T12:21:21Z},
  number = {3},
  journal = {Journal of Algebra},
  author = {Lemmens, P. W. H. and Seidel, J. J.},
  year = {1973},
  pages = {494 - 512},
  owner = {Fardin}
}

@inproceedings{Lesage2005,
  title = {Learning unions of orthonormal bases with thresholded singular value 	decomposition},
  volume = {5},
  doi = {10.1109/ICASSP.2005.1416298},
  abstract = {We propose a new method to learn overcomplete dictionaries for sparse
	coding structured as unions of orthonormal bases. The interest of
	such a structure is manifold. Indeed, it seems that many signals
	or images can be modeled as the superimposition of several layers
	with sparse decompositions in as many bases. Moreover, in such dictionaries,
	the efficient block coordinate relaxation (BCR) algorithm can be
	used to compute sparse decompositions. We show that it is possible
	to design an iterative learning algorithm that produces a dictionary
	with the required structure. Each step is based on the coefficients
	estimation, using a variant of BCR, followed by the update of one
	chosen basis, using singular value decomposition. We assess experimentally
	how well the learning algorithm recovers dictionaries that may or
	may not have the required structure, and to what extent the noise
	level is a disturbing factor.},
  timestamp = {2016-07-09T19:55:55Z},
  booktitle = {Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP 	'05). IEEE International Conference on},
  author = {Lesage, S. and Gribonval, R. and Bimbot, F. and Benaroya, L.},
  month = mar,
  year = {2005},
  keywords = {Algorithm design and analysis,BCR variant,block coordinate relaxation
	algorithm,coefficients estimation,Constraint optimization,Dictionaries,dictionary
	recovery,dictionary structure,encoding,image analysis,image coding,Image
	coding,image model,Image reconstruction,Iterative algorithms,iterative
	learning algorithm,iterative methods,learning,learning algorithm,learning
	(artificial intelligence),Matching pursuit algorithms,multilayer
	superimposition,Noise level,orthonormal base unions,overcomplete
	dictionaries,parameter estimation,signal model,signal reconstruction,singular
	value decomposition,source separation,Sparse Coding,sparse decompositions,sparse
	matrices,thresholded singular value decomposition},
  pages = {v/293-v/296 Vol. 5},
  owner = {afdidehf}
}

@article{Levine2014,
  title = {Block-sparse reconstruction and imaging for lamb wave structural 	health monitoring},
  volume = {61},
  issn = {0885-3010},
  doi = {10.1109/TUFFC.2014.2996},
  abstract = {A frequently investigated paradigm for monitoring the integrity of
	plate-like structures is a spatially-distributed array of piezoelectric
	transducers, with each array element capable of both transmitting
	and receiving ultrasonic guided waves. This configuration is relatively
	inexpensive and allows interrogation of defects from multiple directions
	over a relatively large area. Typically, full sets of pairwise transducer
	signals are acquired by exciting one transducer at a time in a round-robin
	fashion. Many algorithms that operate on such data use differential
	signals that are created by subtracting prerecorded baseline signals,
	leaving only signal differences introduced by scatterers. Analysis
	methods such as delay-and-sum imaging operate on these signals to
	detect and locate point-like defects, but such algorithms have limited
	performance and suffer when potential scatterers have high directionality
	or unknown phase-shifting behavior. Signal envelopes are commonly
	used to mitigate the effects of unknown phase shifts, but this further
	reduces performance. The block-sparse technique presented here uses
	a different principle to locate damage: each pixel is assumed to
	have a corresponding multidimensional linear scattering model, allowing
	any possible amplitude and phase shift for each transducer pair should
	a scatterer be present. By assuming that the differential signals
	are linear combinations of a sparse subset of these models, it is
	possible to split such signals into location-based components. Results
	are presented here for three experiments using aluminum and composite
	plates, each with a different type of scatterer. The scatterers in
	these images have smaller spot sizes than delay-and-sum imaging,
	and the images themselves have fewer artifacts. Although a propagation
	model is required, block-sparse imaging performs well even with a
	small number of transducers or without access to dispersion curves.},
  timestamp = {2016-07-08T11:41:53Z},
  number = {6},
  journal = {Ultrasonics, Ferroelectrics, and Frequency Control, IEEE Transactions 	on},
  author = {Levine, R. and Michaels, J.E.},
  month = jun,
  year = {2014},
  keywords = {acoustic imaging,acoustic wave scattering,aluminum,amplitude shift,block-sparse
	imaging,block-sparse reconstruction,composite plates,condition monitoring,damage
	location,Dictionaries,differential signals,Dispersion,flaw detection,Image
	reconstruction,imaging,lamb wave structural health monitoring,location-based
	components,mathematical model,multidimensional linear scattering
	model,phase shift,plate-like structures integrity monitoring,Scattering,structural
	engineering,surface acoustic waves,Transducers,Vectors},
  pages = {1006-1015},
  owner = {afdidehf}
}

@inproceedings{Li2014,
  title = {Performance guarantees for distributed MIMO radar based on sparse 	sensing},
  doi = {10.1109/RADAR.2014.6875813},
  abstract = {Sparse sensing-based distributed MIMO radars exploit the sparsity
	of the targets in the discretized target space to achieve the good
	target estimation performance of MIMO radars but with fewer measurements.
	Based on sparse sensing, the problem of target estimation is formulated
	as a sparse signal recovery problem, where the signal to be recovered
	is block sparse, or equivalently, the sensing matrix is block-diagonal
	and the signal to be recovered consists of equal size blocks that
	have the same sparsity profile. This paper develops the theoretical
	requirements and performance guarantees for the application of sparse
	recovery techniques to this problem. The obtained theoretical results
	confirm that exploiting the block sparsity of the target in the target
	space can reduce the number of measurements needed for target estimation,
	or can result in improved target estimation for the same number of
	samples.},
  timestamp = {2016-07-10T07:32:05Z},
  booktitle = {Radar Conference, 2014 IEEE},
  author = {Li, Bo and Petropulu, A.},
  month = may,
  year = {2014},
  keywords = {block diagonal matrices,block sparsity,discretized target space,discretized
	target space,distributed MIMO radar,distributed
	MIMO radar,Estimation,MIMO radar,MIMO
	radar,Radar antennas,Radar detection,Radar signal processing,radar
	signal processing,Receiving antennas,Receiving
	antennas,restricted isometry property,Sensors,sparse matrices,sparse
	matrices,sparse sensing,sparse signal recovery problem,target estimation
	performance,target
	estimation performance,Vectors},
  pages = {1369-1372},
  owner = {afdidehf}
}

@inproceedings{Li2014a,
  title = {Efficient target estimation in distributed MIMO radar via the ADMM},
  doi = {10.1109/CISS.2014.6814116},
  abstract = {We consider the problem of target estimation in distributed MIMO radars
	that employ compressive sensing. The problem is formulated as a sparse
	signal recovery problem with magnitude constraints on the target
	reflection coefficients, where the signal to be recovered consist
	of equal size blocks that have the same sparsity profile. A solution
	is proposed based on the alternating direction method of multipliers
	(ADMM), which significantly lowers the computational complexity of
	sparse recovery and improves the estimation accuracy. Due to the
	block diagonal structure of the sensing matrix, the iterations of
	all ADMM subproblems are amenable to parallel implementation, which
	can reduce the running time. A semi-distributed implementation, which
	relaxes the need of a powerful fusion center is also discussed.},
  timestamp = {2016-07-08T12:19:55Z},
  booktitle = {Information Sciences and Systems (CISS), 2014 48th Annual Conference 	on},
  author = {Li, Bo and Petropulu, A.},
  month = mar,
  year = {2014},
  keywords = {Accuracy,ADMM,alternating direction method of multipliers,block diagonal
	structure,compressed sensing,compressive sensing,distributed algorithms,distributed
	MIMO radar,distributed MIMO radars,Estimation,estimation accuracy,Estimation
	theory,magnitude constraints,MIMO radar,Optimization,parallel implementation,parallel
	implementation,Peer-to-peer computing,Peer-to-peer
	computing,Radar antennas,semidistributed implementation,semidistributed
	implementation,sensing matrix,signal restoration,signal
	restoration,sparse sensing,sparse signal recovery problem,sparse
	signal recovery problem,target estimation,target
	estimation,target reflection coefficients,Vectors},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Li2013,
  title = {Structured sampling of structured signals},
  doi = {10.1109/GlobalSIP.2013.6737064},
  abstract = {The paper considers structured sampling of structured signals, more
	specifically, using block diagonal (BD) measurement matrices to sense
	signals with uniform partitions that share the same sparsity profile.
	This model arises in distributed compressive sensing systems. In
	general, the fact that the number of nonzero entries in the measurement
	matrix is smaller than in a dense matrix leads to the need for more
	measurements. However, taking advantage of a certain structure in
	the sparse signal allows one to relax the conditions on the measurement
	matrix for the restricted isometry property (RIP) to hold, thus allowing
	for higher compression rate. We systematically provide guarantees
	for a unique solution, and also an efficient recovery method. The
	analysis relies on the RIP of the random BD matrix for signals in
	a particular union of subspaces. Also, we show how our theoretical
	results can be used to analyze the multiple measurement vector (MMV)
	problem.},
  timestamp = {2016-07-11T16:57:37Z},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP), 	2013 IEEE},
  author = {Li, Bo and Petropulu, A.P.},
  month = dec,
  year = {2013},
  keywords = {Analytical models,Atmospheric measurements,block diagonal matrices,block
	diagonal measurement matrices,block sparsity,compressed sensing,compressed
	sensing,compressive sensing,distributed compressive sensing system,Linear
	matrix inequalities,MMV problem,multiple measurement vector problem,multiple
	measurement vectors,random BD matrix,restricted isometry property,RIP,signal
	sampling,sparse matrices,Standards,structured sampling,structured
	signal,Vectors},
  pages = {1009-1012},
  owner = {afdidehf}
}

@article{Li2014b,
  title = {Sensing and measurement dictionaries design for block OMP algorithm},
  volume = {50},
  issn = {0013-5194},
  doi = {10.1049/el.2014.2000},
  abstract = {The block orthogonal matching pursuit (BOMP) algorithm aims to recover
	block sparse signals whose non-zero components have a block structure.
	Sensing and measurement dictionaries are alternatively optimised
	to have small inter- and sub-block mutual coherences. The computational
	complexity of the proposed algorithm is low. With the designed dictionaries,
	the performance of the BOMP algorithm improves.},
  timestamp = {2016-07-10T08:10:31Z},
  number = {19},
  journal = {Electronics Letters},
  author = {Li, Bo and Shen, Yi and Li, Jia and Wu, Zhenghua},
  month = sep,
  year = {2014},
  keywords = {block OMP algorithm,block orthogonal matching pursuit algorithm,block
	sparse signal recovery,block structure,compressed sensing,computational complexity,Computational
	complexity,iterative methods,measurement
	dictionary design,nonzero components,sensing dictionary design,signal
	restoration,small-interblock mutual coherence,subblock mutual coherence},
  pages = {1351-1353},
  owner = {afdidehf}
}

@article{Li2015a,
  title = {Dual Group Structured Tracking},
  volume = {PP},
  issn = {1051-8215},
  doi = {10.1109/TCSVT.2015.2469171},
  abstract = {The �sparse representation�-based tracking framework generally
	considers the testing candidates and dictionary atoms individually,
	thus failing to model the structured information within data. In
	this paper, we present a robust tracking framework by exploiting
	the dual group structure of both candidate samples and dictionary
	templates, and formulate the sparse representation at group level.
	The similar samples are encoded simultaneously by a few atom groups,
	which induces the inter-group sparsity, and also each group enjoys
	different internal sparsity. In this way, not only the potential
	commonality shared by the related candidates is taken into account,
	but also the individual differences between samples are reflected.
	Then we provide two effective optimization methods to solve our formulation
	by block-coordinate gradient descent (BCGD) and alternating direction
	method of multipliers (ADMM), respectively, and make a comparison
	between them in terms of both effectiveness and efficiency. Finally,
	we embed the dual group structure model into the particle filter
	framework for visual tracking. Extensive experimental results demonstrate
	that our tracker achieves favorable performance against the state-of-theart
	tracking methods.},
  timestamp = {2016-07-08T12:16:02Z},
  number = {99},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  author = {Li, F. and Lu, H. and Wang, D. and Wu, Y. and Zhang, K.},
  year = {2015},
  keywords = {ADMM,Dictionaries,encoding,gradient descent,group structure,linear
	programming,mathematical model,Optimization,sparse matrices,Testing,Visual
	tracking},
  pages = {1-1},
  owner = {afdidehf}
}

@inproceedings{Li2013a,
  title = {Efficient imaging and real-time display of Scanning Ion Conductance 	Microscopy based on block compressive sensing},
  doi = {10.1109/3M-NANO.2013.6737395},
  abstract = {Scanning Ion Conductance Microscopy (SICM) is one kind of Scanning
	Probe Microscopies (SPMs), and it can be used in mapping topographical
	features of sample at high - resolution with free contact by measuring
	the ion current of ultra-micropipette. SICM is widely used in imaging
	soft samples for many distinct merits, such as high resolution imaging,
	simple preparation of probe and no harm to sample surface. However,
	it is undeniable that the scanning speed of SICM is much slower than
	other SPMs, especially for large scale and high resolution imaging.
	Fortunately, compressive sensing (CS), which breaks through the Shannon's
	sampling theorem for dramatically reducing sample rate, could improve
	scanning speed tremendously, but it still costs much time in image
	reconstruction. Therefore block compressive sensing was applied to
	SICM imaging for reducing the reconstruction time of sparse signals,
	and it has an anther further and unique application that it can achieve
	the function of image real-time display. In this paper, a new method
	of dividing blocks and a new matrix arithmetic operation was proposed
	to build the block compressive sensing model, and several experiments
	was taken to verified the superiority of block compressive sensing
	in reducing imaging time and image real-time display used SICM.},
  timestamp = {2016-07-08T12:19:13Z},
  booktitle = {Manipulation, Manufacturing and Measurement on the Nanoscale (3M-NANO), 	2013 International Conference on},
  author = {Li, Gongxin and Li, Peng and Wang, Yuechao and Wang, Wenxue and Xi, Ning and Liu, Lianqing},
  month = aug,
  year = {2013},
  keywords = {block compressive sensing,Blocks Compressive Sensing,compressed sensing,Compressive
	Sensing (CS),dividing blocks,Image reconstruction,imaging,matrix
	arithmetic operation,Microscopy,real-time display,Real-time systems,scanning
	ion conductance microscopy,Scanning Ion Conductance Microscopy (SICM),scanning
	probe microscopy,sparse matrices},
  pages = {114-118},
  owner = {afdidehf}
}

@article{Li2014c,
  title = {Perturbation Analysis of Greedy Block Coordinate Descent Under RIP},
  volume = {21},
  issn = {1070-9908},
  doi = {10.1109/LSP.2014.2307116},
  abstract = {Practically, in the underdetermined model Y = AX, where X is a K-group
	sparse matrix (i.e., it has no more than K nonzero rows), both Y
	and A could be totally perturbed. In this paper, based on restricted
	isometry property, for the greedy block coordinate descent algorithm,
	a sufficient condition of exact recovery is presented under the total
	perturbations, to guarantee that the support of the sparse matrix
	X is recovered exactly. It is pointed out that there exists some
	case satisfying our condition, but not the mutual coherence condition.
	We also discuss the upper bound of our sufficient condition.},
  timestamp = {2016-07-10T07:32:19Z},
  number = {5},
  journal = {Signal Processing Letters, IEEE},
  author = {Li, Haifeng and Fu, Yuli and Hu, Rui and Rong, Rong},
  month = may,
  year = {2014},
  keywords = {Coherence,compressed sensing,coordinate descent algorithm,Direction-of-arrival
	estimation,Greedy algorithms,greedy block algorithm,greedy block
	coordinate descent,Indexes,K-group sparse matrix,perturbation,perturbation
	analysis,perturbation techniques,restricted isometry property,restricted
	isometry property,Signal processing algorithms,sparse matrices,underdetermined
	model,Upper bound,Vectors},
  pages = {518-522},
  owner = {afdidehf}
}

@article{Li2015b,
  title = {Improved analysis of greedy block coordinate descent under RIP},
  volume = {51},
  issn = {0013-5194},
  doi = {10.1049/el.2014.3756},
  abstract = {A more relaxed condition means that fewer of measurements are needed
	to ensure the exact sparse recovery from the theoretical aspect.
	The sufficient condition for the greedy block coordinate descent
	(GBCD) algorithm is relaxed using the near-orthogonality property.
	It is also shown that the GBCD algorithm fails when (1/(?K+1)??K+1<;1).},
  timestamp = {2016-07-08T12:48:07Z},
  number = {6},
  journal = {Electronics Letters},
  author = {Li, Haifeng and Ma, Yingbin and Liu, WenAn and Fu, Yuli},
  year = {2015},
  keywords = {direction-of-arrival estimation,DOA estimation,GBCD algorithm,greedy
	block coordinate descent,MMV model,multiple measurements vectors,near-orthogonality
	property,restricted isometry property,RIP,sparse matrices,sparse
	recovery,sufficient condition},
  pages = {488-490},
  owner = {afdidehf}
}

@inproceedings{Li2013b,
  title = {Group sparsity based semi-supervised band selection for hyperspectral 	images},
  doi = {10.1109/ICIP.2013.6738664},
  abstract = {In this paper, we propose a novel group sparsity based semi-supervised
	band selection method. There are three key features in our method.
	First, it fulfills the band selection task by employing group sparsity
	on the regression coefficients in a robust linear regression for
	classification model, so that the selected bands hold lower classification
	errors. Second, the spatial smoothness prior is incorporated to preserve
	the similarity of spatial neighbors in band selection. Third, the
	objective function is efficiently optimized via an alternative iteration
	algorithm. Comparative results on two hyper-spectral data sets validate
	the effectiveness of our method, showing higher classification accuracies.},
  timestamp = {2016-07-08T12:40:52Z},
  booktitle = {Image Processing (ICIP), 2013 20th IEEE International Conference 	on},
  author = {Li, Haichang and Wang, Ying and Duan, Jiangyong and Xiang, Shiming and Pan, Chunhong},
  month = sep,
  year = {2013},
  keywords = {alternative iteration algorithm,Band selection,band selection task,classification
	accuracy,classification model,group sparsity,group sparsity based
	semisupervised band selection method,hyperspectral data sets,hyperspectral
	images,Hyperspectral imaging,image classification,iterative methods,objective
	function,optimisation,regression analysis,regression coefficients,robust
	linear regression,Smoothness prior,spatial neighbors similarity preservation,spatial
	smoothness prior},
  pages = {3225-3229},
  owner = {afdidehf}
}

@inproceedings{Li2015c,
  title = {Robust recovery of wideband block-sparse spectrum based on MAP and 	MMSE estimator},
  doi = {10.1109/I2MTC.2015.7151551},
  abstract = {Indirect spectrum sensing mainly concerns the measurement and analysis
	of primary wideband analog signal. This paper proposes two robust
	algorithms based on maximum a-posteriori probability (MAP) and minimum
	mean-squared error (MMSE) estimators to recover wideband block-sparse
	spectrum and then detect the spectrum holes in compressive spectrum
	sensing (CSS). In each iteration of the referred Block-sparse Orthogonal
	Matching Pursuit based on iterative MAP (BOMP-IMAP) algorithm, one
	index of block is firstly identified to expand the estimated support.
	And then, wideband block-sparse spectrum can be recovered through
	approximating the MAP estimator. Finally, the residual is updated
	and put into next iteration. In order to approximate the MMSE estimator,
	the Random BOMP-IMAP (RandBOMP-IMAP) algorithm utilizes a randomized
	block identification of BOMP-IMAP algorithm to generate multiple
	solutions, which is followed by the fusion of them to obtain the
	final approximation. Numerical simulation results concerning probability
	of detection and detection time under certain noise level or measurement
	number validate the superiority of the proposed algorithms.},
  timestamp = {2016-07-10T08:08:33Z},
  booktitle = {Instrumentation and Measurement Technology Conference (I2MTC), 2015 	IEEE International},
  author = {Li, Jia and Wang, Qiang and Qiu, Jiayan and Dong, Cong},
  month = may,
  year = {2015},
  keywords = {block-sparse orthogonal matching pursuit based on iterative MAP algorithm,block-sparse
	spectrum,compressed sensing,compressive spectrum sensing,Compressive
	spectrum sensing,CSS,iterative methods,least mean squares methods,Logic
	gates,MAP estimator,maximum a posteriori probability,Maximum likelihood
	detection,Maximum likelihood estimation,minimum mean squared error
	estimator,MMSE estimator,primary wideband analog signal analysis,probability,radio
	spectrum management,RandBOMP-IMAP algorithm,random BOMP-IMAP algorithm,spectrum
	hole detection probability,wideband block-sparse spectrum robust
	recovery},
  pages = {1783-1788},
  owner = {afdidehf}
}

@inproceedings{Li2011,
  title = {Robust talking face video verification using joint factor analysis 	and sparse representation on GMM mean shifted supervectors},
  doi = {10.1109/ICASSP.2011.5946773},
  abstract = {It has been previously demonstrated that systems based on block wise
	local features and Gaussian mixture models (GMM) are suitable for
	video based talking face verification due to the best trade-off in
	terms of complexity, robustness and performance. In this paper, we
	propose two methods to enhance the robustness and performance of
	the GMM-ZTnorm baseline system. First, joint factor analysis is performed
	to compensate the session variabilities due to different recording
	devices, lighting conditions, facial expressions, etc. Second, the
	difference between the universal background model (UBM) and the maximum
	a posteriori (MAP) adapted model is mapped into the GMM mean shifted
	supervector whose over-complete dictionary becomes more incoherent.
	Then, for verification purpose, the sparse representation computed
	by l1-minimization with quadratic constraints is employed to model
	these GMM mean shifted supervectors. Experimental results show that
	the proposed system achieved 8.4% (group 1) and 10.5% (group 2) equal
	error rate on the Banca talking face video database following the
	P protocol and outperformed the GMM-ZTnorm baseline by yielding more
	than 20% relative error reduction.},
  timestamp = {2016-07-10T08:08:54Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International 	Conference on},
  author = {Li, Ming and Narayanan, S.},
  month = may,
  year = {2011},
  keywords = {Adaptation models,Dictionaries,Face,face recognition,face video database,face
	video recognition,Gaussian mixture models,Gaussian processes,GMM,GMM
	supervector,GMM-ZTnorm baseline system,image representation,image
	sequences,joint factor analysis,maximum a posteriori,maximum
	likelihood estimation,Maximum likelihood
	estimation,mean shifted super vectors,P protocol,protocols,quadratic
	constraints,quadratic programming,Robustness,sparse representation,speaker
	recognition,Support vector machines,taking face video verification,Training,universal
	background model,video databases},
  pages = {1481-1484},
  owner = {afdidehf}
}

@article{Li2015d,
  title = {Sparse fixed-rank representation for robust visual analysis},
  volume = {110},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.08.026},
  abstract = {Abstract Robust visual analysis plays an important role in a great
	variety of computer vision tasks, such as motion segmentation, pose
	and face analysis. One of the promising real-world applications is
	to recover the clean data representation from the corrupted data
	points for subspace segmentation. Recently, low-rank based methods
	have gained considerable popularity in solving this problem, such
	as Low-Rank Representation (LRR) and Fixed-Rank Representation (FRR).
	They both learn a low-rank data matrix and a sparse error matrix.
	Each new data representation is learnt using the whole dictionary
	covering all data points. However, they neglect a common fact that
	each point can be represented by a linear combination of only a few
	other points w.r.t. a given dictionary, which has been shown in sparse
	learning. Motivated by this, we explicitly impose the sparsity constraint
	on the learnt low-rank representation. To be more efficient, we adopt
	a fixed-rank scheme by minimizing the Frobenius norm of the new representation.
	Hence, in this paper we propose a novel Sparse Fixed-Rank Representation
	(SFRR) approach for robust visual analysis. Specifically, we model
	the corruptions by enforcing a sparse regularizer. This way, we can
	obtain a new data representation with both low-rankness and sparseness
	robustly. Furthermore, we present a generalized alternating direction
	method (ADM) to optimize the objective function. Extensive experiments
	on both synthetic and real-world databases have suggested the effectiveness
	and the robustness of the proposed method.},
  timestamp = {2016-07-11T16:48:53Z},
  journal = {Signal Processing},
  author = {Li, Ping and Bu, Jiajun and Xu, Bin and He, Zhanying and Chen, Chun and Cai, Deng},
  year = {2015},
  note = {Machine learning and signal processing for human pose recovery and
	behavior analysis},
  keywords = {detection,Fixed-rank,learning,Outlier,representation,Robustrecovery,segmentation,Sparse,Subspace},
  pages = {222 - 231},
  owner = {afdidehf}
}

@inproceedings{Li2015f,
  title = {On recovery of sparse signals with block structures},
  doi = {10.1109/ISIT.2015.7282514},
  abstract = {It has been widely recognized that structure information helps in
	sparse signal recovery. In this paper, a general form of block structure
	is considered, which is often referred to hierarchically sparse model.
	It is assumed that the unknown sparse signal can be divided into
	blocks, and a block contains either all zero components or a fraction
	of nonzero components. This model sits between the standard sparse
	model (without block structure) and the strict block sparse model
	(all entries in nonzero blocks are nonzero). The focus of this paper
	is to analyze the convex optimization approach to recover hierarchically
	sparse signals. The technique we employed is based on the approximated
	message passing framework and the associated state evolution. The
	minimum number of measurements required for exact recovery, also
	known as phase transition (PT), has been quantified in an asymptotic
	region. We show that the PT depends on two parameters: the fraction
	of nonzero components in nonzero blocks, and the uniformity of the
	magnitudes of nonzero components. Based on the PT analysis, we characterize
	the regions at which the convex optimization methods designed for
	the standard, hierarchically, and block sparse models are optimal
	respectively.},
  timestamp = {2016-07-10T07:13:27Z},
  booktitle = {Information Theory (ISIT), 2015 IEEE International Symposium on},
  author = {Li, Pan and Dai, Wei and Meng, Huadong and Wang, Xiqin},
  month = jun,
  year = {2015},
  keywords = {compressed sensing,Convex functions,message passing,Noise,Noise measurement,sparse
	matrices,Standards},
  pages = {546-550},
  owner = {afdidehf}
}

@article{Li2015g,
  title = {Sparsity Learning Formulations for Mining Time-Varying Data},
  volume = {27},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2014.2373411},
  abstract = {Traditional clustering and feature selection methods consider the
	data matrix as static. However, the data matrices evolve smoothly
	over time in many applications. A simple approach to learn from these
	time-evolving data matrices is to analyze them separately. Such strategy
	ignores the time-dependent nature of the underlying data. In this
	paper, we propose two formulations for evolutionary co-clustering
	and feature selection based on the fused Lasso regularization. The
	evolutionary co-clustering formulation is able to identify smoothly
	varying hidden block structures embedded into the matrices along
	the temporal dimension. Our formulation is very flexible and allows
	for imposing smoothness constraints over only one dimension of the
	data matrices. The evolutionary feature selection formulation can
	uncover shared features in clustering from time-evolving data matrices.
	We show that the optimization problems involved are non-convex, non-smooth
	and non-separable. To compute the solutions efficiently, we develop
	a two-step procedure that optimizes the objective function iteratively.
	We evaluate the proposed formulations using the Allen Developing
	Mouse Brain Atlas data. Results show that our formulations consistently
	outperform prior methods.},
  timestamp = {2016-07-11T16:55:10Z},
  number = {5},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  author = {Li, Rongjian and Zhang, Wenlu and Zhao, Yao and Zhu, Zhenfeng and Ji, Shuiwang},
  month = may,
  year = {2015},
  keywords = {Allen Developing Mouse Brain Atlas data,Approximation methods,bioinformatics,co-clustering,Data mining,Data
	mining,evolutionary coclustering formulation,evolutionary
	computation,evolutionary feature selection formulation,feature selection,fused
	Lasso regularization,gene expression,learning (artificial intelligence),linear
	programming,matrix algebra,neuroinformatics,optimisation,Optimization,optimization
	problems,pattern clustering,shared features,smoothly varying hidden
	block structures,smoothness constraints,sparse matrices,Sparsity
	learning,sparsity learning formulation,temporal dimension,time-evolving
	data matrices,time-varying data,time-varying data mining,Vectors},
  pages = {1411-1423},
  owner = {afdidehf}
}

@inproceedings{Li2014d,
  title = {Iteratively reweighted least squares for block-sparse recovery},
  doi = {10.1109/ICIEA.2014.6931321},
  abstract = {The compressive sensing (CS) theory has shown that sparse signals
	can be reconstructed exactly from much fewer measurements than traditionally
	believed. What's more, using ?p-norm minimization with p <; 1 can
	do so with much fewer measurements than with p=1. In this paper,
	a novel algorithm is proposed for computing local minima of the nonconvex
	problem in the block-sparse system. A series of experiments are presented
	to show the remarkable performance of our proposed algorithm in block
	sparse signal recovery, and compare the recovery ability of this
	algorithm with the IRLS and BOMP algorithm.},
  timestamp = {2016-07-09T19:45:27Z},
  booktitle = {Industrial Electronics and Applications (ICIEA), 2014 IEEE 9th Conference 	on},
  author = {Li, Shuang and Li, Qiuwei and Li, Gang and He, Xiongxiong and Chang, Liping},
  month = jun,
  year = {2014},
  keywords = {?p-norm minimization,BIRLS,block-sparse recovery,block sparse signal
	reconstruction,compressed sensing,compressive sensing,concave programming,CS
	theory,Dictionaries,Equations,iteratively reweighted least squares,iterative
	methods,least squares approximations,Minimization,nonconvex optimization,nonconvex
	problem,Signal processing algorithms,signal reconstruction,sparse
	matrices,sparse signal reconstruction,underdetermined systems of
	linear equations,Vectors},
  pages = {1061-1066},
  owner = {afdidehf}
}

@inproceedings{Li2013c,
  title = {Simultaneous Sensing Matrix and Sparsifying Dictionary Optimization 	for Block-sparse Compressive Sensing},
  doi = {10.1109/MASS.2013.98},
  abstract = {In this paper, we propose a new method to optimize the sensing matrix
	and the overcomplete dictionary simultaneously in a block-sparse
	system. This method mainly includes two parts: the optimization of
	the sensing matrix for a given dictionary and the optimization of
	the overcomplete dictionary with a block structure for a predefined
	sensing matrix. Simulation results show that our novel method can
	significantly improve the dictionary recovery ability and lower the
	representation error compared with other dictionary learning methods
	in block-sparse systems.},
  timestamp = {2016-07-10T08:18:39Z},
  booktitle = {Mobile Ad-Hoc and Sensor Systems (MASS), 2013 IEEE 10th International 	Conference on},
  author = {Li, Shuang and Li, Qiuwei and Li, Gang and He, Xiongxiong and Chang, Liping},
  month = oct,
  year = {2013},
  keywords = {block sparse compressive sensing,Block-sparsity,CBKSVD,Coherence,compressed
	sensing,compressive sensing,Dictionaries,dictionary learning method,dictionary
	recovery ability,Image reconstruction,learning (artificial intelligence),optimisation,Optimization,overcomplete
	dictionary learning,overcomplete dictionary optimization,predefined
	sensing matrix,projection matrix optimization,sensing matrix optimization,Sensors,simultaneous
	sensing matrix method,sparse matrices,sparsifying dictionary optimization,sparsifying
	dictionary optimization,Vectors},
  pages = {597-602},
  owner = {afdidehf}
}

@inproceedings{Li2013d,
  title = {Projection matrix optimization for block-sparse compressive sensing},
  doi = {10.1109/ICSPCC.2013.6663993},
  abstract = {Traditionally, the projection matrix in compressive sensing (CS) is
	chosen as a random matrix. In recent years, we have seen that the
	performance of CS systems can be improved by using a carefully designed
	projection matrix rather than a random one. In particular, we can
	reduce the coherence between the columns of the equivalent dictionary
	thanks to a well-designed projection matrix. Then, we can get a lower
	reconstruction error and a higher successful reconstruction rate.
	In some applications, the signals of interest have nonzero entries
	occurring in clusters - i.e., block-sparse signals. In this paper,
	we use the equiangular tight frame (ETF) to approach the Gram matrix
	of equivalent dictionary rather than the identity matrix used in
	[1]. Then, we minimize a weighted sum of the subblock coherence and
	the interblock coherence of the equivalent dictionary. The simulation
	results show that our novel method for projection matrix optimization
	significantly improves the ability of block-sparse approximation
	techniques to reconstruct and classify signals than the method proposed
	by Lihi Zelnik-Manor (LZM) [1].},
  timestamp = {2016-07-10T07:39:08Z},
  booktitle = {Signal Processing, Communication and Computing (ICSPCC), 2013 IEEE 	International Conference on},
  author = {Li, Shuang and Zhu, Zhihui and Li, Gang and Chang, Liping and Li, Qiuwei},
  month = aug,
  year = {2013},
  keywords = {Algorithm design and analysis,approximation theory,block-sparse approximation,block-sparse
	compressive sensing,block-sparse signals,Block-sparsity,Coherence,compressed
	sensing,compressive sensing,Dictionaries,equiangular tight frame,equivalent
	dictionary,ETF,gram matrix,interblock coherence,nonzero entries,optimisation,Optimization,projection matrix optimization,projection
	matrix optimization,random matrix,reconstruction
	error,Sensors,sparse matrices,subblock coherence,Vectors},
  pages = {1-4},
  owner = {afdidehf}
}

@article{Li2015h,
  title = {Single Image Superresolution via Directional Group Sparsity and Directional 	Features},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2432713},
  abstract = {Single image superresolution (SR) aims to construct a high-resolution
	version from a single low-resolution (LR) image. The SR reconstruction
	is challenging because of the missing details in the given LR image.
	Thus, it is critical to explore and exploit effective prior knowledge
	for boosting the reconstruction performance. In this paper, we propose
	a novel SR method by exploiting both the directional group sparsity
	of the image gradients and the directional features in similarity
	weight estimation. The proposed SR approach is based on two observations:
	1) most of the sharp edges are oriented in a limited number of directions
	and 2) an image pixel can be estimated by the weighted averaging
	of its neighbors. In consideration of these observations, we apply
	the curvelet transform to extract directional features which are
	then used for region selection and weight estimation. A combined
	total variation regularizer is presented which assumes that the gradients
	in natural images have a straightforward group sparsity structure.
	In addition, a directional nonlocal means regularization term takes
	pixel values and directional information into account to suppress
	unwanted artifacts. By assembling the designed regularization terms,
	we solve the SR problem of an energy function with minimal reconstruction
	error by applying a framework of templates for first-order conic
	solvers. The thorough quantitative and qualitative results in terms
	of peak signal-to-noise ratio, structural similarity, information
	fidelity criterion, and preference matrix demonstrate that the proposed
	approach achieves higher quality SR reconstruction than the state-of-the-art
	algorithms.},
  timestamp = {2016-07-10T08:19:08Z},
  number = {9},
  journal = {Image Processing, IEEE Transactions on},
  author = {Li, Xiaoyan and He, Hongjie and Wang, Ruxin and Tao, Dacheng},
  month = sep,
  year = {2015},
  keywords = {compressed sensing,curvelet transform,curvelet transforms,Dictionaries,directional
	features,directional group sparsity,directional nonlocal means regularization
	term,edge detection,Estimation,extract directional features,Feature
	Extraction,first-order conic solvers,group sparsity structure,Image
	edge detection,image gradients,image pixel,Image reconstruction,Image
	reconstruction,Image resolution,Image super-resolution,information
	fidelity criterion,LR image,natural images,peak signal-to-noise ratio,reconstruction-based,sharp
	edges,similarity weight estimation,single image superresolution,single
	low-resolution image,SR reconstruction,total variation regularizer,Training,transforms},
  pages = {2874-2888},
  owner = {afdidehf}
}

@inproceedings{Li2015i,
  title = {Subspace Learning with Structured Sparsity for Compressive Video 	Sampling},
  doi = {10.1109/DCC.2015.24},
  abstract = {Existing sparse representation with subspace learning is hampered
	by the intersection of subspaces of bases. With structured sparsity
	to enable the prior knowledge of signal statistics, this paper proposes
	a novel compressive video sampling by subspace learning to minimize
	the intersection of subspaces. As the measurement, the block coherence
	is optimized with the regularized learning to generate a class of
	independent bases associated with the subspaces. Thus, the proposed
	framework can make a compact block sparse representation based on
	the derived basis in an efficient and adaptive manner. The block-based
	recovery of video sequences is demonstrated to be stable under the
	constraint of block restricted isometric property (RIP). Experimental
	results show that the proposed method outperforms existing compressive
	video sampling schemes.},
  timestamp = {2016-07-11T16:58:57Z},
  booktitle = {Data Compression Conference (DCC), 2015},
  author = {Li, Yong and Dai, Wenrui and Xiong, Hongkai},
  month = apr,
  year = {2015},
  keywords = {block-based recovery,Coherence,compressive video sampling,data compression,Electronic
	mail,image coding,image sequences,Principal component analysis,restricted
	isometric property,signal statistics,sparse representation,subspace
	learning,Training,video coding,video sequences},
  pages = {456-456},
  owner = {afdidehf}
}

@inproceedings{Li2015j,
  title = {Analysis dictionary learning based on summation of blocked determinants 	measure of sparseness},
  doi = {10.1109/ICDSP.2015.7251864},
  abstract = {This paper addresses the dictionary learning and sparse representation
	with the analysis model. Though it has been studied in the literature,
	there is still not an investigation in the context of dictionary
	learning for nonnegative signal representation. For measuring the
	sparseness, in this paper, we propose a measure that is so called
	the summation of blocked determinants. Based on this measure, a new
	analysis sparse model is derived, and an iterative sparseness maximization
	approach is proposed to solve this model. In the approach, the nonnegative
	sparse representation problem can be cast into row-to-row optimizations
	with respect to the dictionary, and then the quadratic programming
	(QP) technique is used to optimize each row. Numerical experiments
	on recovery of analysis dictionary show the effectiveness of the
	proposed algorithm.},
  timestamp = {2016-07-08T10:28:41Z},
  booktitle = {Digital Signal Processing (DSP), 2015 IEEE International Conference 	on},
  author = {Li, Yujie and Ding, Shuxue and Li, Zhenni},
  month = jul,
  year = {2015},
  keywords = {analysis dictionary learning,analysis sparse model,analysis
	sparse model,blocked determinants measure,blocked determinants
	measure,Cities and towns,determinants,Dictionaries,iterative methods,iterative
	methods,iterative sparseness maximization approach,iterative
	sparseness maximization approach,nonnegative matrix factorization,nonnegative matrix
	factorization,nonnegative signal representation,nonnegative
	signal representation,optimisation,quadratic programming,quadratic
	programming,row-to-row optimizations,row-to-row
	optimizations,signal representation,sparse representation,sparse
	representation,summation of blocked determinants measure of sparseness(SBDMS),summation of blocked determinants
	measure of sparseness(SBDMS)},
  pages = {224-228},
  owner = {afdidehf}
}

@inproceedings{Li2014f,
  title = {Union of Data-Driven Subspaces via Subspace Clustering for Compressive 	Video Sampling},
  doi = {10.1109/DCC.2014.21},
  abstract = {The standard Compressive Sensing (CS) theory indicates that robust
	signals recovery can be obtained from just a few collection of incoherent
	projections. To further decrease the necessary measurements, an alternative
	to the generic CS framework assumes that signals lie on a union of
	subspaces (UoS). However, UoS model is limited to the specific type
	of signal regularity. This paper considers a more general and adaptive
	model which presumes that signals lie on a union of data-driven subspaces
	(UoDS). The UoDS model inherits the merit from UoS that signals have
	structural sparse representation. Meanwhile, it allows to recover
	signals using fewer degrees of freedom for a desirable recovery quality
	than UoS. To construct the UoDS model, a subspace clustering method
	is utilized to form an adaptive group set. The corresponding adaptive
	basis is learned by applying a linear subspace learning (LSL) method
	to each group. A corresponding recovery algorithm with provable performance
	is also given. Experiment results demonstrate that the proposed model
	for video sampling is valid and applicable.},
  timestamp = {2016-07-11T17:10:10Z},
  booktitle = {Data Compression Conference (DCC), 2014},
  author = {Li, Yong and Xiong, Hongkai},
  month = mar,
  year = {2014},
  keywords = {Adaptation models,adaptive group set,compressed sensing,compressive
	video sampling,CS,data compression,data driven subspaces,Decoding,Discrete
	cosine transforms,Image reconstruction,linear subspace learning,LSL
	method,pattern clustering,robust signals recovery,Sensors,signal
	regularity,standard compressive sensing,structural sparse representation,subspace
	clustering,subspace clustering method,union of data-driven subspaces,UoDS
	model,Vectors,video coding},
  pages = {63-72},
  owner = {afdidehf}
}

@inproceedings{Li2013e,
  title = {Compressive video sampling from a union of data-driven subspaces},
  doi = {10.1109/VCIP.2013.6706390},
  abstract = {Recently, compressive sampling (CS) is an active research field of
	signal processing. To further decrease the necessary measurements
	and get more efficient recovery of a signal x, recent approaches
	assume that x lives in a union of subspaces (UoS). Unlike previous
	approaches, this paper proposes a novel method to sample and recover
	an unknown signal from a union of data-driven subspaces (UoDS). Instead
	of a fix set of supports, this UoDS is learned from classified signal
	series which are uniquely formed by block matching. The basis of
	these data-driven subspaces is regularized after dimensionality reduction
	by principal component extraction. A corresponding recovery solution
	with provable performance guarantees is also given, which takes full
	advantage of block-sparsity structure and improves the recovery efficiency.
	In practice, the proposed scheme is fulfilled to sample and recover
	frames in video sequences. The experimental results demonstrate that
	the proposed video sampling behaves better performance in sampling
	and recovery than the classical CS.},
  timestamp = {2016-07-08T12:01:16Z},
  booktitle = {Visual Communications and Image Processing (VCIP), 2013},
  author = {Li, Yong and Xiong, Hongkai and Ye, Xinwei},
  month = nov,
  year = {2013},
  keywords = {block matching,block sparsity structure,compressed sensing,compressive
	sampling,compressive video sampling,CS,Data-driven,data driven subspaces,data
	handling,Decoding,image sequences,PCA,Principal component analysis,Principal
	component analysis,principal component extraction,PSNR,Sensors,signal
	processing,signal series,sparse matrices,Standards,union of data
	driven subspaces,union of subspaces,UoDS,Vectors,video coding,Video
	compression},
  pages = {1-6},
  owner = {afdidehf}
}

@article{Li2015k,
  title = {Color-Direction Patch-Sparsity-Based Image Inpainting Using Multidirection 	Features},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2014.2383322},
  abstract = {This paper proposes a color-direction patch-sparsity-based image inpainting
	method to better maintain structure coherence, texture clarity, and
	neighborhood consistence of the inpainted region of an image. The
	method uses super-wavelet transform to estimate the multi-direction
	features of a degraded image, and combines with color information
	to construct the weighted color-direction distance (WCDD) to measure
	the difference between two patches. Based on the WCDD, the color-direction
	structure sparsity is defined to obtain a more robust filling order
	and more suitable multiple candidate patches are searched. Then,
	the target patches are sparsely represented by the multiple candidate
	patches under neighborhood consistency constraints in both the color
	and the multi-direction spaces. Experimental results are presented
	to demonstrate the effectiveness of the proposed approach on tasks
	such as scratch removal, text removal, block removal, and object
	removal. The effects of super-wavelet transforms and direction features
	are also investigated.},
  timestamp = {2016-07-08T11:51:51Z},
  number = {3},
  journal = {Image Processing, IEEE Transactions on},
  author = {Li, Zhidan and He, Hongjie and Tai, Heng-Ming and Yin, Zhongke and Chen, Fan},
  month = mar,
  year = {2015},
  keywords = {block removal,Coherence,colordirection,color-direction patch-sparsity-based
	image inpainting method,color-direction structure sparsity,difference
	measurement,Equations,feature extraction,Image color analysis,Image
	color analysis,image colour analysis,image
	colour analysis,Image inpainting,image representation,image texture,image
	texture,mathematical model,Mathematical
	model,multi-direction feature,multidirection feature estimation,multidirection
	feature estimation,multiple candidate patch selection,multiple
	candidate patch selection,neighborhood consistence,neighborhood
	consistence,object removal,robust filling order,robust
	filling order,Robustness,scratch removal,scratch
	removal,sparse representation,structure coherence,structure
	coherence,structure sparsity,super-wavelet transform,super-wavelet
	transform,text removal,texture clarity,texture
	clarity,transforms,wavelet transforms,WCDD construction,WCDD
	construction,weighted color-direction distance construction,weighted
	color-direction distance construction},
  pages = {1138-1152},
  owner = {afdidehf}
}

@phdthesis{Lim2013,
  title = {The Group Lasso: Two Novel Applications},
  abstract = {This thesis deals with two problems: learning linear interaction models,
	and electroencephalography (EEG) source estimation in the visual
	cortex. These are quite dierent problems, but they have a common
	theme that brings them together: we propose solutions to each that
	are based on the group-lasso. The group-lasso wasrst introduced
	in [49], and is an example of regularization applied to a supervised
	learning problem. Learning interactions is a dicult problem because
	of the number of variables involved. With a million variables (as
	in the case of genome wide association studies (GWAS), we are already
	looking at a candidate interaction search space of about a trillion
	terms. Even if the computational problems are overcome, there is
	still the statistical issue of spurious correlations and low signal
	to noise ratios inherent in these problems. Past approaches to this
	problem such as hierNet [5] and logic regression [33] are limited
	in either their computational feasibility or the types of variables
	they can accommodate. Our contribution here is glinternet, a method
	for learning pairwise interactions via hierarchical group-lasso regularization.
	We demonstrate that glinternet is competitive with current methods
	(where these methods are feasible), but also that it has the added
	advantage of being able to accommodate both categorical variables
	with arbitrary numbers of levels as well as continuous variables.
	glinternet is available as a package on CRAN for the statistical
	software R. Our EEG source problem arises from visual activation
	studies. Here, subjects are given visual stimuli, and their neural
	response is measured in a non-invasive manner through the use the
	sensors (or channels) placed around the subject's head. The goal
	is to recover the underlying neural activity (the sources) that are
	responsible for the observations recorded with these sensors. One
	method that is currently used to perform source inversion is called
	the minimum norm, which applies ridge regression to the observed
	sensor readings. We make two contributions in this domain. First,
	we show that the group-lasso outperforms the minimum norm inversion,
	and that the group-lasso performance improves with the number of
	subjects. This occurs because the group-lasso is able to pool information
	across multiple subjects, whereas the minimum norm is inherently
	unable to do so. A post-processing step may be applied to the minimum
	norm estimates by averaging across multiple subjects. Our second
	contribution consists of showing that averaging within appropriately
	dened regions of interest (ROIs) in the visual cortex across multiple
	subjects is able to dramatically boost the performance of both the
	minimum norm and group-lasso solutions, and also improves with the
	number of subjects. These two contributions, to the best of our knowledge,
	are novel results. There are four chapters in this thesis. Chapter
	1 introduces the group-lasso and describes some of its properties.
	Chapter 2 consists of the interaction learning problem. We make clear
	the problem statement, make the case for hierarchical interaction
	models, and then present our solution. The EEG source estimation
	problem is tackled in Chapter 3. We introduce the problem, discuss
	past approaches and why they are inadequate before presenting our
	solution that is based on the group-lasso. Finally, we conclude with
	a discussion in Chapter 4.},
  timestamp = {2016-07-11T17:01:20Z},
  school = {STANFORD UNIVERSITY},
  author = {Lim, Michael},
  month = aug,
  year = {2013},
  owner = {Fardin}
}

@techreport{Lim2014,
  title = {Learning interactions via hierarchical group-lasso regularization},
  abstract = {We introduce a method for learning pairwise interactions in a linear
	regression or logistic regression model in a manner that satises
	strong hierarchy: whenever an interaction is estimated to be nonzero,
	both its associated main eects are also included in the model. We
	motivate our approach by modeling pairwise interactions for categorical
	variables with arbitrary numbers of levels, and then show how we
	can accommodate continuous variables as well. Our approach allows
	us to dispense with explicitly applying constraints on the main eects
	and interactions for identiability, which results in interpretable
	interaction models. We compare our method with existing approaches
	on both simulated and real data, including a genome-wide association
	study, all using our R package glinternet.},
  timestamp = {2016-07-09T19:55:26Z},
  institution = {Statistics Department, Stanford University},
  author = {Lim, Michael and Hastie, Trevor},
  year = {2014},
  keywords = {computer intensive,hierarchical,interaction,logistic,regression},
  owner = {Fardin}
}

@inproceedings{Lin2012,
  title = {Time-domain method for DOA estimation of LFM signals based on block-sparsity 	recovery},
  doi = {10.1049/cp.2012.2472},
  abstract = {Under the current array system, a new method for wideband LFM signal
	Direction-of-Arrival (DOA) estimation is proposed, without restriction
	to the Shannon-Nyquist sampling theorem as like as the frequency-domain
	techniques. The key point of this method is to use the blocksparsity
	of several time source signals to build a blocksparse model for the
	array receiving several snapshots, then the DOA estimation problem
	of LFM signals can be reformatted as a block-sparsity recovery problem
	which can be solved by various algorithms. The experimental results
	demonstrate the superior estimation performance of the proposed method.},
  timestamp = {2016-07-11T17:06:43Z},
  booktitle = {Information Science and Control Engineering 2012 (ICISCE 2012), IET 	International Conference on},
  author = {Lin, Bo and Zhang, Zenghui and Zhu, Jubo},
  month = dec,
  year = {2012},
  keywords = {array signal processing,block-sparsity recovery,compressive sensing,Compressive
	sensing,direction-of-arrival estimation,Direction-of-arrival
	estimation,DOA estimation,DOA
	estimation,frequency modulation,LFM signal,LFM singals,Shannon-Nyquist
	sampling theorem,signal sampling,Time-domain analysis,time-domain
	method},
  pages = {1-6},
  owner = {afdidehf}
}

@article{Lin2013,
  title = {Design of Optimal Sparse Feedback Gains via the Alternating Direction 	Method of Multipliers},
  volume = {58},
  issn = {0018-9286},
  doi = {10.1109/TAC.2013.2257618},
  abstract = {We design sparse and block sparse feedback gains that minimize the
	variance amplification (i.e., the H2 norm) of distributed systems.
	Our approach consists of two steps. First, we identify sparsity patterns
	of feedback gains by incorporating sparsity-promoting penalty functions
	into the optimal control problem, where the added terms penalize
	the number of communication links in the distributed controller.
	Second, we optimize feedback gains subject to structural constraints
	determined by the identified sparsity patterns. In the first step,
	the sparsity structure of feedback gains is identified using the
	alternating direction method of multipliers, which is a powerful
	algorithm well-suited to large optimization problems. This method
	alternates between promoting the sparsity of the controller and optimizing
	the closed-loop performance, which allows us to exploit the structure
	of the corresponding objective functions. In particular, we take
	advantage of the separability of the sparsity-promoting penalty functions
	to decompose the minimization problem into sub-problems that can
	be solved analytically. Several examples are provided to illustrate
	the effectiveness of the developed approach.},
  timestamp = {2016-07-08T12:11:19Z},
  number = {9},
  journal = {Automatic Control, IEEE Transactions on},
  author = {Lin, Fu and Fardad, M. and Jovanovic, M.R.},
  month = sep,
  year = {2013},
  keywords = {$ell _{1}$ minimization,ADMM,alternating direction method,alternating
	direction method of multipliers,Alternating direction method of multipliers
	(ADMM),block sparse feedback gains,closed loop performance optimization,closed
	loop systems,communication architectures,communication links,continuation
	methods,controller sparsity,distributed control,distributed controller,distributed
	systems,feedback,feedback gain optimization,minimisation,minimization
	problem,objective functions,optimal control,optimal control problem,optimal
	sparse feedback gain design,Optimization,separable penalty functions,sparsity
	pattern identification,sparsity-promoting optimal control,sparsity-promoting
	penalty functions,sparsity structure,structural constraints,structured
	distributed design,variance amplification minimization},
  pages = {2426-2431},
  owner = {afdidehf}
}

@article{Lina2014,
  title = {Wavelet-Based Localization of Oscillatory Sources From Magnetoencephalography 	Data},
  volume = {61},
  issn = {0018-9294},
  doi = {10.1109/TBME.2012.2189883},
  abstract = {Transient brain oscillatory activities recorded with Electroencephalography
	(EEG) or magnetoencephalography (MEG) are characteristic features
	in physiological and pathological processes. This study is aimed
	at describing, evaluating, and illustrating with clinical data a
	new method for localizing the sources of oscillatory cortical activity
	recorded by MEG. The method combines time-frequency representation
	and an entropic regularization technique in a common framework, assuming
	that brain activity is sparse in time and space. Spatial sparsity
	relies on the assumption that brain activity is organized among cortical
	parcels. Sparsity in time is achieved by transposing the inverse
	problem in the wavelet representation, for both data and sources.
	We propose an estimator of the wavelet coefficients of the sources
	based on the maximum entropy on the mean (MEM) principle. The full
	dynamics of the sources is obtained from the inverse wavelet transform,
	and principal component analysis of the reconstructed time courses
	is applied to extract oscillatory components. This methodology is
	evaluated using realistic simulations of single-trial signals, combining
	fast and sudden discharges (spike) along with bursts of oscillating
	activity. The method is finally illustrated with a clinical application
	using MEG data acquired on a patient with a right orbitofrontal epilepsy.},
  timestamp = {2016-07-11T17:11:37Z},
  number = {8},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Lina, J.M. and Chowdhury, R. and Lemay, E. and Kobayashi, E. and Grova, C.},
  month = aug,
  year = {2014},
  keywords = {bioelectric phenomena,Brain,brain models,clinical application,clinical
	data,cortical parcels,data acquisition,Discrete wavelet transforms,EEG,electroencephalography,Electrophysiological
	imaging,entropic regularization,Epilepsy,inverse problem,inverse
	problem,inverse problems,inverse wavelet transform,magnetoencephalography,magnetoencephalography
	data,Magnetoencephalography (MEG),maximum entropy methods,maximum
	entropy on the mean (MEM),maximum entropy-on-the-mean principle,medical
	signal processing,MEG,MEG data acquisition,orbitofrontal epilepsy,Oscillations,oscillatory
	cortical activity,oscillatory sources,pathological processes,physiological
	processes,Physiology,Principal component analysis,principal component
	analysis,signal reconstruction,single-trial signals,spatial sparsity,Time
	frequency analysis,time-frequency representation,transient brain
	oscillatory activity,wavelet-based localization,Wavelet coefficients,wavelet
	representation,wavelet transforms},
  pages = {2350-2364},
  owner = {afdidehf}
}

@inproceedings{Lindenbaum2013,
  title = {Blind separation of spatially-block-sparse sources from orthogonal 	mixtures},
  doi = {10.1109/MLSP.2013.6661896},
  abstract = {We addresses the classical problem of blind separation of a static
	linear mixture, where separation is not based on statistical assumptions
	(such as independence) regarding the sources, but rather on their
	spatial (block-) sparsity, and with an additional constraint of an
	orthogonal mixing-matrix. An algorithm for this problem was recently
	proposed by Mishali and Eldar, and consists of two steps: one for
	recovering the support of the sources, and a subsequent one for recovering
	their values. That algorithm has two shortcomings: One is an assumption
	that the spatial sparsity level of the sources at each time-instant
	is constant and known; The second is the algorithm's sensitivity
	to the possible presence of temporal �blocks� of the signals
	with identical support. In this work we propose two pre-processing
	stages for improving the applicability and the performance of the
	algorithm. A first stage is aimed at identifying �blocks� of
	similar support, and pruning the data accordingly for the support-recovery
	stage. A second stage is aimed at recovering the sparsity level at
	each time-instant by exploiting observed structural inter-relations
	between the signals at different time-instants. We demonstrate the
	improvement over the original algorithm using both synthetic data
	and mixed text-images. We also show that the algorithm outperforms
	the recovery rate of alternative source separation methods for such
	contexts, including K-SVD, a leading method for dictionary learning.},
  timestamp = {2016-07-08T11:39:00Z},
  booktitle = {Machine Learning for Signal Processing (MLSP), 2013 IEEE International 	Workshop on},
  author = {Lindenbaum, O. and Yeredor, A. and Vitek, R. and Mishali, M.},
  month = sep,
  year = {2013},
  keywords = {blind separation,blind source separation,block sparsity,Block-sparsity,block
	sparsity,Correlation,Dictionaries,dictionary learning,dictionary
	learning,Estimation,K-SVD,matrix algebra,matrix
	algebra,orthogonal mixing-matrix,orthogonal mixture,orthogonal
	mixture,Orthogonal Mixtures,Radio access networks,Radio
	access networks,recovery rate,Signal processing algorithms,Signal
	processing algorithms,Singular Value Decomposition,singular
	value decomposition,source separation,sparse matrices,sparse
	matrices,spatially-block-sparse source separation,spatially-block-sparse
	source separation,spatial sparsity level,spatial sparsity
	level,static linear mixture,structural interrelation,structural
	interrelation,support-recovery stage,support-recovery
	stage},
  pages = {1-6},
  owner = {afdidehf}
}

@article{Ling2013,
  title = {Decentralized Jointly Sparse Optimization by Reweighted $\ell _{q}$ 	Minimization},
  volume = {61},
  issn = {1053-587X},
  doi = {10.1109/TSP.2012.2236830},
  abstract = {A set of vectors (or signals) are jointly sparse if all their nonzero
	entries are found on a small number of rows (or columns). Consider
	a network of agents {i} that collaboratively recover a set of jointly
	sparse vectors {x(i)} from their linear measurements {y(i)}. Assume
	that every agent i collects its own measurement y(i) and aims to
	recover its own vector x(i) taking advantages of the joint sparsity
	structure. This paper proposes novel decentralized algorithms to
	recover these vectors in a way that every agent runs a recovery algorithm
	and exchanges with its neighbors only the estimated joint support
	of the vectors. The agents will obtain their solutions through collaboration
	while keeping their vectors' values and measurements private. As
	such, the proposed approach finds applications in distributed human
	action recognition, cooperative spectrum sensing, decentralized event
	detection, as well as collaborative data mining. We use a non-convex
	minimization model and propose algorithms that alternate between
	support consensus and vector update. The latter step is based on
	reweighted ?q iterations, where q can be 1 or 2. We numerically compare
	the proposed decentralized algorithms with existing centralized and
	decentralized algorithms. Simulation results demonstrate that the
	proposed decentralized approaches have strong recovery performance
	and converge reasonably fast.},
  timestamp = {2016-09-30T11:24:07Z},
  number = {5},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Ling, Qing and Wen, Zaiwen and Yin, Wotao},
  month = mar,
  year = {2013},
  keywords = {cognitive radio,compressed sensing,convex programming,Data mining,Data
	mining,Decentralized algorithm,decentralized jointly sparse optimization,distributed
	human action recognition,jointly sparse optimization,Joints,joint
	support estimation,linear measurements,minimisation,Minimization,nonconvex
	minimization model,non-convex model,Optimization,reweighted ?q minimization,Sensors,support
	consensus,Support vector machines,Vectors},
  pages = {1165--1170},
  owner = {Fardin}
}

@inproceedings{Ling2013a,
  title = {Local sparse represenation for driver drowsiness expression recognition},
  doi = {10.1109/CAC.2013.6775831},
  abstract = {This paper proposes a novel local sparse representation for driver
	drowsiness expression recognition. A discriminative local structured
	dictionary is learned from the non-overlapping sub-blocks of four
	kinds of facial Gabor images. Then, a novel local descriptor is presented
	by using sparse coefficients obtained by the learned dictionary to
	represent the entire human facial expression, and a simple linear
	SVM is used for final recognition. We demonstrate the performance
	of this proposed local sparse representation method on our driver
	drowsiness datasets.},
  timestamp = {2016-07-09T19:58:02Z},
  booktitle = {Chinese Automation Congress (CAC), 2013},
  author = {Ling, Zhigang and Lu, Xiao and Wang, Yaonan and Zhou, Yunpeng and Wang, Guofeng and Li, Jiancheng},
  month = nov,
  year = {2013},
  keywords = {Dictionaries,discriminative local structured dictionary learning,driver drowsiness expression recognition,Driver
	drowsiness expression recognition,face recognition,Face
	recognition,facial Gabor images,Fatigue,Feature
	Extraction,Gabor filter,Gabor filters,human facial expression,image
	representation,learning (artificial intelligence),local sparse representation,local
	structure dictionary,medical image processing,road safety,simple
	linear SVM,sparse coefficients,Sparse Represenation,Support vector machines,support vector
	machines,traffic engineering computing,Training,Vehicles},
  pages = {733-737},
  owner = {afdidehf}
}

@inproceedings{Liu2014,
  title = {Blockwise coordinate descent schemes for sparse representation},
  doi = {10.1109/ICASSP.2014.6854608},
  abstract = {The current sparse representation framework is to decouple it as two
	subproblems, i.e., alternate sparse coding and dictionary learning
	using different optimizers, treating elements in bases and codes
	separately. In this paper, we treat elements both in bases and codes
	ho-mogenously. The original optimization is directly decoupled as
	several blockwise alternate subproblems rather than above two. Hence,
	sparse coding and bases learning optimizations are coupled together.
	And the variables involved in the optimization problems are partitioned
	into several suitable blocks with convexity preserved, making it
	possible to perform an exact block coordinate descent. For each separable
	subproblem, based on the convexity and monotonic property of the
	parabolic function, a closed-form solution is obtained. Thus the
	algorithm is simple, efficient and effective. Experimental results
	show that our algorithm significantly accelerates the learning process.},
  timestamp = {2016-07-08T11:48:49Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Liu, Bao-Di and Wang, Yu-Xiong and Shen, Bin and Zhang, Yu-Jin and Wang, Yan-Jiang},
  month = may,
  year = {2014},
  keywords = {blockwise coordinate descent schemes,Convergence,coordinate descent,Dictionaries,dictionary
	learning,encoding,learning optimizations,learning process,linear
	programming,Minimization,Optimization,parabolic equations,parabolic
	function,Sparse Coding,sparse matrices,sparse representation framework},
  pages = {5267-5271},
  owner = {afdidehf}
}

@inproceedings{Liu2013,
  title = {Self-Explanatory Convex Sparse Representation for Image Classification},
  doi = {10.1109/SMC.2013.363},
  abstract = {Sparse representation technique has been widely used in various areas
	of computer vision over the last decades. Unfortunately, in the current
	formulations, there are no explicit relationship between the learned
	dictionary and the original data. By tracing back and connecting
	sparse representation with the K-means algorithm, a novel variation
	scheme termed as self-explanatory convex sparse representation (SCSR)
	has been proposed in this paper. To be specific, the basis vectors
	of the dictionary are refined as convex combination of the data points.
	The atoms now would capture a notion of centroids similar to K-means,
	leading to enhanced interpretability. Sparse representation and K-means
	are thus unified under the same framework in this sense. Besides,
	an appealing property also emerges that the weight and code matrices
	both tend to be naturally sparse without additional constraints.
	Compared with the standard formulations, SCSR is easier to be extended
	into the kernel space. To solve the corresponding sparse coding sub
	problem and dictionary learning sub problem, block-wise coordinate
	descent and Lagrange multipliers are proposed accordingly. To validate
	the proposed algorithm, it is implemented in image classification,
	a successful applications of sparse representation. Experimental
	results on several benchmark data sets, such as UIUC-Sports, Scene
	15, and Caltech-256 demonstrate the effectiveness of our proposed
	algorithm.},
  timestamp = {2016-07-10T08:10:28Z},
  booktitle = {Systems, Man, and Cybernetics (SMC), 2013 IEEE International Conference 	on},
  author = {Liu, Bao-Di and Wang, Yu-Xiong and Shen, Bin and Zhang, Yu-Jin and Wang, Yan-Jiang and Liu, Wei-Feng},
  month = oct,
  year = {2013},
  keywords = {basis vector,block-wise coordinate descent method,computer vision,convex
	hull,coordinate descent,Dictionaries,dictionary learning subproblem,Educational
	institutions,encoding,image classification,image representation,image
	representation,Kernel,K-means algorithm,K-means
	algorithm,Lagrange multiplier,learning (artificial intelligence),learning
	(artificial intelligence),linear programming,linear
	programming,Minimization,SCSR,self-explanatory convex sparse representation,self-explanatory
	convex sparse representation,sparse matrices,sparse
	matrices,sparse representation,variation scheme,variation
	scheme,Vectors},
  pages = {2120-2125},
  owner = {afdidehf}
}

@inproceedings{Liu2014a,
  title = {Local structure based sparse representation for face recognition 	with single sample per person},
  doi = {10.1109/ICIP.2014.7025143},
  abstract = {In this paper, we propose local structure based sparse representation
	classification (LS SRC) to solve single sample per person (SSPP)
	problem. By adopting the �divide-conquer-aggregate� strategy,
	we successfully alleviate the dilemma of high data dimensionality
	and small samples, where we first divide the face into local blocks,
	and classify each local block, and then integrate all the classification
	results by voting. For each block, we further divide it into overlapped
	patches and assume that these patches lie in a linear subspace. This
	subspace assumption reflects local structure relationship of the
	overlapped patches and makes SRC feasible for SSPP problem. To lighten
	the computing burden, we further propose local structure based collaborative
	representation classification (LS CRC). Experimental results on three
	public face databases show that our methods not only generalize well
	to SSPP problem but also have strong robustness to expression, illumination,
	little pose variation, occlusion and time variation.},
  timestamp = {2016-07-09T19:58:10Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Liu, Fan and Tang, Jinhui and Song, Yan and Xiang, Xinguang and Tang, Zhenmin},
  month = oct,
  year = {2014},
  keywords = {Bismuth,Collaboration,collaborative representation,computing burden,Databases,data
	dimensionality,divide and conquer methods,divide-conquer-aggregate
	strategy,Face,face recognition,illumination,image classification,Image
	classification,image representation,image
	representation,lighting,linear subspace,local blocks,local
	blocks,local structure,local structure based collaborative representation
	classification,local
	structure based collaborative representation classification,local structure based sparse representation classification,local
	structure based sparse representation classification,LS_SRC,occlusion,pose
	estimation,pose variation,single sample per person,single sample
	per person problem,sparse representation,SSPP,time variation,Training},
  pages = {713-717},
  owner = {afdidehf}
}

@inproceedings{Liu2012,
  title = {Efficient recovery of block sparse signals via zero-point attracting 	projection},
  doi = {10.1109/ICASSP.2012.6288629},
  abstract = {In this paper, we consider compressed sensing (CS) of block-sparse
	signals, i.e., sparse signals that have nonzero coefficients occurring
	in clusters. An efficient algorithm, called zero-point attracting
	projection (ZAP) algorithm, is extended to the scenario of block
	CS. The block version of ZAP algorithm employs an approximate l2,0
	norm as the cost function, and finds its minimum in the solution
	space via iterations. For block sparse signals, an analysis of the
	stability of the local minimums of this cost function under the perturbation
	of noise reveals an advantage of the proposed algorithm over its
	original non-block version in terms of reconstruction error. Finally,
	numerical experiments show that the proposed algorithm outperforms
	other state of the art methods for the block sparse problem in various
	respects, especially the stability under noise.},
  timestamp = {2016-07-08T12:19:44Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International 	Conference on},
  author = {Liu, Jingbo and Jin, Jian and Gu, Yuantao},
  month = mar,
  year = {2012},
  keywords = {Algorithm design and analysis,Approximation algorithms,block sparse,block
	sparse problem,block sparse signals,compressed sensing,compressed
	sensing,Cost function,efficient recovery,Matching pursuit algorithms,Noise,Noise
	measurement,noise perturbation,perturbation techniques,reconstruction
	error,sparse recovery,ZAP algorithm,zero-point attracting projection},
  pages = {3333-3336},
  owner = {afdidehf}
}

@inproceedings{Liu2014b,
  title = {Facial expression recognition under random block occlusion based 	on maximum likelihood estimation sparse representation},
  doi = {10.1109/IJCNN.2014.6889744},
  abstract = {Occlusion is a big challenge for facial expression recognition and
	previous efforts are largely limited to a few occlusion types without
	considering the random characteristic of occlusion. Since the original
	sparse coding model actually assumes that the coding residual follows
	the Gaussian distribution, which may not be accurate enough to describe
	the coding errors in practice, so we propose a new scheme by modeling
	the sparse coding as a sparsely constrained robust regression problem
	in this paper. Firstly, in order to reduce the influence of occlusion
	for facial expression, the test facial expression image will be assigned
	different weights in all pixels. Secondly, because the occluded pixels
	should have lower weight values, hence, we update the weight through
	constant iterative until the convergence is achieved. Finally, the
	final sparse representation of test image can be calculated using
	the optimal weight matrix. And the class of test image can be determined
	by the minimal residual which associated with each class of training
	samples to the test image. The proposed method achieves better performance
	in JAFFE database and Cohn-Kanade database and experimental results
	show that it is robust to facial expression recognition under random
	block occlusion.},
  timestamp = {2016-07-08T12:27:52Z},
  booktitle = {Neural Networks (IJCNN), 2014 International Joint Conference on},
  author = {Liu, S.S. and Zhang, Y. and Liu, K.P.},
  month = jul,
  year = {2014},
  keywords = {coding residual,Cohn-Kanade database,Databases,encoding,face recognition,Facial
	expression recognition,Gaussian distribution,image coding,image representation,JAFFE
	database,Maximum likelihood estimation,maximum
	likelihood estimation sparse representation,maximum likelihood estimation
	sparse representation,random block occlusion,Robustness,sparse coding
	model,sparse
	coding model,Training},
  pages = {1285-1290},
  owner = {afdidehf}
}

@inproceedings{Liu2013a,
  title = {A sparse multi-class classifier for biomarker screening},
  doi = {10.1109/GlobalSIP.2013.6736817},
  abstract = {We introduce an approach to sparsity penalized multi-class classifier
	design that accounts for multi-block structure of the data. The unified
	multi-class classifier is parameterized by a set of weights defined
	over the classes and over the blocks. The proposed sparse multi-block
	multi-class classifier imposes structured sparsity on the weights
	so that the same variables are selected for all classes and all blocks.
	The classifier is trained to minimize an objective function that
	captures the unified miss-classification probabilities of error over
	the classes in addition to the sparsity of the weights. The optimization
	of the objective function is implemented by a convex augmented Lagrangian
	and variable splitting method. This results in a classifier that
	automatically selects biomarkers for inclusion or exclusion in the
	classifier and results in significantly improved classifier performance.
	The approach is illustrated on publicly available longitudinal gene
	microarray data.},
  timestamp = {2016-07-08T11:29:06Z},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP), 	2013 IEEE},
  author = {Liu, Tzu-Yu and Wiesel, A. and Hero, A.O.},
  month = dec,
  year = {2013},
  keywords = {augmented Lagrangian optimization,biomarker screening,Buildings,convex
	augmented Lagrangian,data multiblock structure,data structures,Dimension
	Reduction,Educational institutions,Input variables,linear programming,longitudinal
	gene microarray data,Multi-class classification,objective function,optimisation,Optimization,pattern
	classification,probability,Sparsity,sparsity penalized multiclass
	classifier design,Support vector machines,Training,unified missclassification
	probabilities,variable selection,variable splitting method},
  pages = {77-80},
  owner = {afdidehf}
}

@article{Liu2001,
  title = {Cramer-Rao lower bounds for low-rank decomposition of multidimensional 	arrays},
  volume = {49},
  issn = {1053-587X},
  doi = {10.1109/78.942635},
  abstract = {Unlike low-rank matrix decomposition, which is generically nonunique
	for rank greater than one, low-rank three-and higher dimensional
	array decomposition is unique, provided that the array rank is lower
	than a certain bound, and the correct number of components (equal
	to array rank) is sought in the decomposition. Parallel factor (PARAFAC)
	analysis is a common name for low-rank decomposition of higher dimensional
	arrays. This paper develops Cramer-Rao bound (CRB) results for low-rank
	decomposition of three- and four-dimensional (3-D and 4-D) arrays,
	illustrates the behavior of the resulting bounds, and compares alternating
	least squares algorithms that are commonly used to compute such decompositions
	with the respective CRBs. Simple-to-check necessary conditions for
	a unique low-rank decomposition are also provided},
  timestamp = {2016-07-08T12:06:16Z},
  number = {9},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Liu, Xiangqian and Sidiropoulos, N.D.},
  month = sep,
  year = {2001},
  keywords = {3D arrays,4D arrays,alternating least squares algorithms,array decomposition,array
	rank,array signal processing,code division multiple access,code division
	multiple access,Cramer-Rao lower bounds,Cramer-Rao
	lower bounds,Data models,DS-CDMA,four-dimensional arrays,four-dimensional
	arrays,least squares approximations,least squares
	approximations,Least squares methods,low-rank decomposition,low-rank
	decomposition,low-rank matrix decomposition,low-rank
	matrix decomposition,matrix decomposition,Monte Carlo simulation,Monte
	Carlo simulation,Multiaccess communication,Multiaccess
	communication,multidimensional arrays,multidimensional signal processing,multidimensional
	signal processing,Multidimensional systems,Multidimensional
	systems,multiuser channels,necessary conditions,necessary
	conditions,parallel factor analysis,signal processing,signal
	processing,Signal processing algorithms,Signal processing
	algorithms,Singular Value Decomposition,spread spectrum communication,spread
	spectrum communication,three-dimensional arrays,three-dimensional
	arrays,Two dimensional displays},
  pages = {2074-2086},
  owner = {Fardin}
}

@inproceedings{Liu2013b,
  title = {Sparsity-based soft decoding of compressed images in transform domain},
  doi = {10.1109/ICIP.2013.6738116},
  abstract = {We propose a sparsity-based soft decoding approach to restore compressed
	images directly in the transform domain of compression (DCT domain
	specifically examined in this paper). Restoring transform coefficients
	rather than pixel values prevents the propagation of quantization
	errors in the image domain. As natural images are statistically non-stationary
	with spatially varying sparse representations, we develop an adaptive
	block-wise sparsity-based restoration method that learns and exploits
	local statistics. Specially, for each DCT block, we collect sample
	blocks via non-local patch grouping to learn a compact dictionary
	based on principal component analysis. The resulting block-specific
	dictionary is used to estimate the corresponding DCT coefficients
	by a technique of collaborative sparse coding, in which the similarity
	between sample DCT patches used in dictionary construction is further
	considered. Experimental results are encouraging and demonstrate
	that the proposed soft decoding approach performs competitively on
	restoring compressed images against existing methods.},
  timestamp = {2016-07-11T16:55:07Z},
  booktitle = {Image Processing (ICIP), 2013 20th IEEE International Conference 	on},
  author = {Liu, Xianming and Wu, Xiaolin and Zhao, Debin},
  month = sep,
  year = {2013},
  keywords = {adaptive block-wise sparsity-based restoration method,collaborative
	sparse coding technique,compact dictionary learning,compressed images,Data
	Compression,DCT domain,Decoding,Dictionaries,discrete cosine transform
	domain,discrete cosine transforms,image coding,image domain,image
	domain,image representation,image
	representation,image restoration,learning (artificial intelligence),learning
	(artificial intelligence),local statistics,local
	statistics,nonlocal patch grouping,pixel values,pixel
	values,Principal component analysis,principal component
	analysis,quantization errors,Quantization (signal),Quantization
	(signal),sparsity-based soft decoding approach,sparsity-based
	soft decoding approach,spatially varying sparse representations,spatially varying
	sparse representations,Transform coding,transform
	coding},
  pages = {563-566},
  owner = {afdidehf}
}

@article{Liuxxxx,
  title = {Research Statement},
  timestamp = {2016-07-10T08:02:37Z},
  author = {Liu, Yang},
  year = {xxxx},
  owner = {Fardin}
}

@inproceedings{Liu2014d,
  title = {Group sparsity in dimensionality reduction of sparse representation},
  doi = {10.1109/WPMC.2014.7014877},
  abstract = {Group sparsity is extensively used in developing Compressed Sensing
	recovery algorithms, but it is ignored in dimensionality reduction.
	In this paper, we define supervised group distance based on group
	sparsity to measure the similarity between recovered sparse coefficients
	from Compressed Sensing for signal classification, especially the
	sparse coefficients intra-class. And a graph embedding dimensionality
	reduction algorithm is then proposed based on the defined supervised
	group distance. Experimental results on both synthetic and real-world
	data demonstrate that the proposed method achieves superior performance
	on both classification error rate and visualization.},
  timestamp = {2016-07-08T12:41:01Z},
  booktitle = {Wireless Personal Multimedia Communications (WPMC), 2014 International 	Symposium on},
  author = {Liu, Yang and Li, Xueming and Liu, Chenyu and Tang, Yufang},
  month = sep,
  year = {2014},
  keywords = {compressed sensing,compressed sensing recovery algorithms,Error analysis,error
	rate classification,error statistics,Euclidean distance,graph embedding
	dimensionality reduction algorithm,group sparsity,principal component
	analysis,signal classification,sparse coefficients,sparse matrices,sparse
	representation,supervised group distance,Wireless communication,wireless
	sensor networks},
  pages = {541-546},
  owner = {afdidehf}
}

@inproceedings{Liu2007,
  title = {Adaptive Wiener filter formulation on the fMRI-EEG integrated spatiotemporal 	neuroimaging},
  doi = {10.1109/NFSI-ICFBI.2007.4387714},
  abstract = {In response to a need of establishing a high-resolution spatiotemporal
	neuroimaging technology, tremendous efforts have been directed upon
	developing multimodal neuroimaging strategies that combine the complementary
	advantages of high-spatial-resolution functional magnetic resonance
	imaging (fMRI) and high-temporal-resolution electroencephalography
	(EEG) or magnetoencephalography (MEG). A critical challenge to the
	fMRI-EEG/MEG integration lies in the spatial mismatches between fMRI
	activations and instantaneous electrical source activities. Such
	mismatches are fundamentally due to the fact that fMRI and EEG/MEG
	signals are generated and collected in highly different time scales.
	In this paper, we propose a new theoretical framework to solve the
	problem of fMRI-EEG integrated cortical source imaging. The new framework
	has two principal technical advancements. First, by assuming a linear
	neurovascular coupling, a method is rigorously derived to quantify
	the fMRI signal in each voxel as proportional to the time integral
	of the power of local electrical current source over the period of
	event related potentials (ERP). Second, the EEG inverse problem is
	solved for every time instant using an adaptive Wiener filter, in
	which the prior time-variant source covariance matrix is estimated
	by combining the quantified fMRI responses and the segmented EEG
	signals before response averaging. Our preliminary computer simulation
	results suggest that the proposed method provides a reliable technique
	for the fMRI-EEG integration and represents a significant advancement
	over the conventional fMRI-weighted EEG (or MEG) source imaging techniques,
	and is also applicable to fMRI-MEG integrated source imaging.},
  timestamp = {2016-07-08T10:12:36Z},
  booktitle = {Noninvasive Functional Source Imaging of the Brain and Heart and 	the International Conference on Functional Biomedical Imaging, 2007. 	NFSI-ICFBI 2007. Joint Meeting of the 6th International Symposium 	on},
  author = {Liu, Zhongming and He, Bin},
  month = oct,
  year = {2007},
  keywords = {adaptive filters,Adaptive Wiener filter formulation,bioelectric phenomena,biomedical
	MRI,Covariance matrix,electrical source activities,electroencephalography,Enterprise
	resource planning,fMRI-EEG integrated cortical source imaging,fMRI-EEG
	integrated spatiotemporal neuroimaging,functional magnetic resonance
	imaging,inverse problem,inverse problems,linear neurovascular coupling,linear
	neurovascular coupling,Magnetic resonance imaging,Magnetic
	Resonance Imaging,magnetoencephalography,medical signal processing,medical
	signal processing,Neuroimaging,neurophysiology,Signal generators,Signal
	generators,spatiotemporal phenomena,spatiotemporal
	phenomena,time-variant source covariance matrix,time-variant source covariance
	matrix,Wiener filter,Wiener filters,Wiener
	filters},
  pages = {159-161},
  owner = {afdidehf}
}

@inproceedings{Lu2013,
  title = {Correlation Adaptive Subspace Segmentation by Trace Lasso},
  doi = {10.1109/ICCV.2013.170},
  abstract = {This paper studies the subspace segmentation problem. Given a set
	of data points drawn from a union of subspaces, the goal is to partition
	them into their underlying subspaces they were drawn from. The spectral
	clustering method is used as the framework. It requires to find an
	affinity matrix which is close to block diagonal, with nonzero entries
	corresponding to the data point pairs from the same subspace. In
	this work, we argue that both sparsity and the grouping effect are
	important for subspace segmentation. A sparse affinity matrix tends
	to be block diagonal, with less connections between data points from
	different subspaces. The grouping effect ensures that the highly
	corrected data which are usually from the same subspace can be grouped
	together. Sparse Subspace Clustering (SSC), by using ?1-minimization,
	encourages sparsity for data selection, but it lacks of the grouping
	effect. On the contrary, Low-Rank Representation (LRR), by rank minimization,
	and Least Squares Regression (LSR), by ?2-regularization, exhibit
	strong grouping effect, but they are short in subset selection. Thus
	the obtained affinity matrix is usually very sparse by SSC, yet very
	dense by LRR and LSR. In this work, we propose the Correlation Adaptive
	Subspace Segmentation (CASS) method by using trace Lasso. CASS is
	a data correlation dependent method which simultaneously performs
	automatic data selection and groups correlated data together. It
	can be regarded as a method which adaptively balances SSC and LSR.
	Both theoretical and experimental results show the effectiveness
	of CASS.},
  timestamp = {2016-07-08T12:05:13Z},
  booktitle = {Computer Vision (ICCV), 2013 IEEE International Conference on},
  author = {Lu, Canyi and Feng, Jiashi and Lin, Zhouchen and Yan, Shuicheng},
  month = dec,
  year = {2013},
  keywords = {?1-minimization,?2-regularization,automatic data selection,block diagonal,CASS
	method,Correlation,correlation adaptive subspace segmentation method,correlation
	methods,data correlation dependent method,data points,data selection,grouping
	effect,image representation,Image segmentation,least square regression,least
	square regression,least squares approximations,Least
	squares approximations,low-rank representation,low-rank
	representation,LRR,LSR,minimisation,Noise,nonzero entries,nonzero
	entries,pattern clustering,pattern
	clustering,rank minimization,regression analysis,set theory,set
	theory,Silicon,sparse affinity matrix,sparse
	affinity matrix,sparse matrices,sparse subspace clustering,sparse subspace
	clustering,spectral clustering method,spectral
	clustering method,SSC,subset selection,subspace segmentation problem,subspace
	segmentation problem,trace Lasso,trace
	Lasso,Vectors},
  pages = {1345-1352},
  owner = {afdidehf}
}

@article{Lu2008,
  title = {A Theory for Sampling Signals From a Union of Subspaces},
  volume = {56},
  issn = {1053-587X},
  doi = {10.1109/TSP.2007.914346},
  abstract = {One of the fundamental assumptions in traditional sampling theorems
	is that the signals to be sampled come from a single vector space
	(e.g., bandlimited functions). However, in many cases of practical
	interest the sampled signals actually live in a union of subspaces.
	Examples include piecewise polynomials, sparse representations, nonuniform
	splines, signals with unknown spectral support, overlapping echoes
	with unknown delay and amplitude, and so on. For these signals, traditional
	sampling schemes based on the single subspace assumption can be either
	inapplicable or highly inefficient. In this paper, we study a general
	sampling framework where sampled signals come from a known union
	of subspaces and the sampling operator is linear. Geometrically,
	the sampling operator can be viewed as projecting sampled signals
	into a lower dimensional space, while still preserving all the information.
	We derive necessary and sufficient conditions for invertible and
	stable sampling operators in this framework and show that these conditions
	are applicable in many cases. Furthermore, we find the minimum sampling
	requirements for several classes of signals, which indicates the
	power of the framework. The results in this paper can serve as a
	guideline for designing new algorithms for various applications in
	signal processing and inverse problems.},
  timestamp = {2016-07-08T11:33:22Z},
  number = {6},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Lu, Y.M. and Do, M.N.},
  month = jun,
  year = {2008},
  keywords = {inverse problem,Linear operators,projections,Sampling,shift-invariant
	spaces,signal processing,signal representations,signal sampling,single
	vector subspace assumption,stable,union of subspaces},
  pages = {2334-2345},
  owner = {afdidehf}
}

@article{Lu2008a,
  title = {Sampling Signals from a Union of Subspaces},
  volume = {25},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914999},
  abstract = {The single linear vector space assumption is widely used in modeling
	the signal classes, mainly due to its simplicity and mathematical
	tractability. In certain signals, a union of subspaces can be a more
	appropriate model. This paper provides a new perspective for signal
	sampling by considering signals from a union of subspaces instead
	of a single space.},
  timestamp = {2016-09-30T11:25:00Z},
  number = {2},
  journal = {Signal Processing Magazine, IEEE},
  author = {Lu, Y.M. and Do, M.N.},
  month = mar,
  year = {2008},
  keywords = {Delay,Filtering,Geophysics,Low pass filters,mathematical model,Radar
	applications,Sampling methods,signal processing,signal sampling,subspace
	union,Vectors},
  pages = {41--47},
  owner = {afdidehf}
}

@inproceedings{Lu2015a,
  title = {Image deblocking via group sparsity optimization},
  doi = {10.1109/ISCAS.2015.7168950},
  abstract = {Block-wise compressed image often suffers from the blocking artifacts.
	In this paper, we propose a novel deblocking scheme for compressed
	image, by combining image's sparse property and its self-similarity
	together, called group sparsity optimization. Instead of processing
	each image patch individually, in the proposed scheme, similar patches
	in one group are required to be well-represented on learned dictionary
	collaboratively, using group sparsity regularization. The group sparsity
	not only imposes every patch's representation to be sparse, bus also
	requires patches' coefficients in the group share the similar pattern.
	The experiment results on standard test images demonstrate that our
	scheme can improve the PSNR of the compressed images by an average
	of 1.25 dB, and outperform state of the art deblocking approaches.},
  timestamp = {2016-07-08T12:46:08Z},
  booktitle = {Circuits and Systems (ISCAS), 2015 IEEE International Symposium on},
  author = {Lu, Zhenbo and Li, Houqiang and Li, Weiping},
  month = may,
  year = {2015},
  keywords = {blocking artifacts,blockwise compressed image,data compression,Dictionaries,Discrete
	cosine transforms,group sparsity optimization,image coding,image deblocking,Image
	Deblocking,image patch,image representation,image restoration,image
	restoration,image sparse property,JPEG compression,learned
	dictionary,optimisation,patch representation,Quantization (signal),sparsity
	regularization,Standards,Transform coding},
  pages = {1582-1585},
  owner = {afdidehf}
}

@inproceedings{Luo2014,
  title = {Sparse fuzzy c-regression models with application to T-S fuzzy systems 	identification},
  doi = {10.1109/FUZZ-IEEE.2014.6891567},
  abstract = {In this paper, the objective function of fuzzy c-regression models
	(FCRM) is modified to develop a novel fuzzy partition method on the
	basis of block structured sparse representation, namely as sparse
	fuzzy c-regression model. This method takes advantage of the block
	structured information in the objective function of FCRM and casts
	fuzzy partition into an optimization problem by making a tradeoff
	between traditional FCRM and the number of prototypes of hyper-plane
	with nonzero parameters. An alternating direction method of multipliers
	(ADMM) based algorithm is exploited to address the proposed optimization
	problem. Furthermore, based on sparse fuzzy c-regression models,
	a novel T-S fuzzy systems identification method is developed for
	reduction of fuzzy rules. Finally, examples on well-known benchmark
	data set are carried out to illustrate the effectiveness of the proposed
	methods.},
  timestamp = {2016-07-11T16:48:55Z},
  booktitle = {Fuzzy Systems (FUZZ-IEEE), 2014 IEEE International Conference on},
  author = {Luo, Minnan and Sun, Fuchun and Liu, Huaping},
  month = jul,
  year = {2014},
  keywords = {ADMM,alternating direction method of multipliers based algorithm,block
	structured information,block structured sparse representation,Data
	analysis,Data models,FCRM,fuzzy partition method,fuzzy rules,fuzzy
	set theory,fuzzy systems,hyper-plane prototypes,linear programming,nonzero
	parameters,optimisation,Optimization,optimization problem,Partitioning
	algorithms,Prototypes,regression analysis,sparse fuzzy c-regression
	models,T-S fuzzy systems identification,Vectors},
  pages = {1571-1577},
  owner = {afdidehf}
}

@article{Luo2013,
  title = {Hierarchical Structured Sparse Representation for T-S Fuzzy Systems 	Identification},
  volume = {21},
  issn = {1063-6706},
  doi = {10.1109/TFUZZ.2013.2240690},
  abstract = {�The curse of dimensionality� has become a significant bottleneck
	for fuzzy system identification and approximation. In this paper,
	we cast the Takagi-Sugeno (T-S) fuzzy system identification into
	a hierarchical sparse representation problem, where our goal is to
	establish a T-S fuzzy system with a minimal number of fuzzy rules,
	which simultaneously have a minimal number of nonzero consequent
	parameters. The proposed method, which is called hierarchical sparse
	fuzzy inference systems ( H-sparseFIS), explicitly takes into account
	the block-structured information that exists in the T-S fuzzy model
	and works in an intuitive way: First, initial fuzzy rule antecedent
	part is extracted automatically by an iterative vector quantization
	clustering method; then, with block-structured sparse representation,
	the main important fuzzy rules are selected, and the redundant ones
	are eliminated for better model accuracy and generalization performance;
	moreover, we simplify the selected fuzzy rules consequent with sparse
	regularization such that more consequent parameters can approximate
	to zero. This algorithm is very efficient and shows good performance
	in well-known benchmark datasets and real-world problems.},
  timestamp = {2016-07-08T12:41:53Z},
  number = {6},
  journal = {Fuzzy Systems, IEEE Transactions on},
  author = {Luo, Minnan and Sun, Fuchun and Liu, Huaping},
  month = dec,
  year = {2013},
  keywords = {Accuracy,block-structured information,blockstructured sparse representation,Block-structured
	sparse representation,Clustering algorithms,Dictionaries,dimensionality
	curse,fuzzy reasoning,fuzzy rules,fuzzy rules reduction,fuzzy system
	identification,fuzzy systems,hierarchical sparse fuzzy inference
	systems,hierarchical structured sparse representation problem,H-sparseFIS,identification,initial
	fuzzy rule antecedent part,iterative methods,iterative vector quantization
	clustering method,Matching pursuit algorithms,nonzero consequent
	parameters,Optimization,pattern clustering,sparse representation,Takagi-Sugeno
	fuzzy system,T-S fuzzy model,T-S fuzzy systems identification,vector
	quantisation,Vectors},
  pages = {1032-1043},
  owner = {afdidehf}
}

@inproceedings{Luo2014a,
  title = {Bayesian compressive sensing using adaptive threshold for block sparse 	wideband signal recovery},
  doi = {10.1109/HMWC.2014.7000216},
  abstract = {By using Relevance Vector Machine (RVM) to solve the problem of sparse
	signal recovery, Bayesian Compressive Sensing (BCS) can obtain good
	performance in spectral discrete spike signal detection. However,
	in cognitive radio (CR) system, the spectrum of primary user's signal,
	which is continuous in narrowband and is block sparse in wideband,
	cannot be exactly recovered by BCS. In this paper, a Bayesian Compressive
	Sensing Using Adaptive Threshold (AT-BCS) is proposed, in which we
	improve the iterative algorithm of RVM for enhancing the accuracy
	of block sparse wideband signal recovery. Specifically, before every
	iteration, we set a threshold to select potentially valuable weights
	for signal recovery so that we can reduce the total number of iterations
	and develop the accuracy of signal recovery simultaneously. Furthermore,
	we take the change of noise into account, exploiting Support Vector
	Machine (SVM) to fit a function to describe the relationship between
	SNR and threshold. Thus, we can offer different thresholds for each
	signal recovery procedure with different SNR to guarantee the robustness
	of our algorithm.},
  timestamp = {2016-07-08T11:37:11Z},
  booktitle = {High Mobility Wireless Communications (HMWC), 2014 International 	Workshop on},
  author = {Luo, Xudong and Sun, Xuekang and Guo, Caili},
  month = nov,
  year = {2014},
  keywords = {Accuracy,adaptive threshold,Bayesian compressive sensing,Bayes methods,block sparse,block
	sparse,block sparse wideband signal recovery,cognitive
	radio system,compressed sensing,iterative algorithm,Noise,relevance
	vector machine,RVM,signal recovery procedure,SNR,spectral discrete
	spike signal detection,support vector machine,Support vector machines,Support
	vector machines,SVM,Vectors,Wideband,wideband signal recovery},
  pages = {68-72},
  owner = {afdidehf}
}

@article{Lustig2007,
  title = {Sparse MRI: The application of compressed sensing for rapid MR imaging},
  volume = {53},
  abstract = {The sparsity which is implicit in MR images is exploited to significantly
	undersample k-space. Some MR images such as angiograms are already
	sparse in the pixel representation; other, more complicated images
	have a sparse representation in some transform domain-for example,
	in terms of spatial finite-differences or their wavelet coefficients.
	According to the recently developed mathematical theory of compressed-sensing,
	images with a sparse representation can be recovered from randomly
	undersampled k-space data, provided an appropriate nonlinear recovery
	scheme is used. Intuitively, artifacts due to random undersampling
	add as noise-like interference. In the sparse transform domain the
	significant coefficients stand out above the interference. A nonlinear
	thresholding scheme can recover the sparse coefficients, effectively
	recovering the image itself. In this article, practical incoherent
	undersampling schemes are developed and analyzed by means of their
	aliasing interference. Incoherence is introduced by pseudo-random
	variable-density undersampling of phase-encodes. The reconstruction
	is performed by minimizing the l(1) norm of a transformed image,
	subject to data fidelity constraints. Examples demonstrate improved
	spatial resolution and accelerated acquisition for multislice fast
	spin-echo brain imaging and 3D contrast enhanced angiography},
  timestamp = {2016-07-11T16:49:31Z},
  number = {6},
  journal = {Magnetic Resonance in Medicine},
  author = {Lustig, Michael and Donoho, David and Pauly, John M.},
  year = {2007},
  pages = {1182-95},
  owner = {afdidehf}
}

@article{Lustig2008,
  title = {Compressed Sensing MRI},
  volume = {25},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914728},
  abstract = {This article reviews the requirements for successful compressed sensing
	(CS), describes their natural fit to MRI, and gives examples of four
	interesting applications of CS in MRI. The authors emphasize on an
	intuitive understanding of CS by describing the CS reconstruction
	as a process of interference cancellation. There is also an emphasis
	on the understanding of the driving factors in applications, including
	limitations imposed by MRI hardware, by the characteristics of different
	types of images, and by clinical concerns.},
  timestamp = {2016-09-29T14:48:10Z},
  number = {2},
  journal = {IEEE Signal Process. Mag.},
  author = {Lustig, M. and Donoho, D.L. and Santos, J.M. and Pauly, J.M.},
  month = mar,
  year = {2008},
  keywords = {biomedical imaging,biomedical MRI,compressed sensing,encoding,Image
	coding,Image reconstruction,interference cancellation,Magnetic resonance imaging,Magnetic resonance
	imaging,Magnetization,medical image processing,MRI,Protons,Radio
	frequency,review,reviews,wavelet transforms},
  pages = {72--82},
  annote = {Signal Processing Magazine, IEEE},
  annote = {Signal Processing Magazine, IEEE},
  annote = {Signal Processing Magazine, IEEE},
  annote = {Signal Processing Magazine, IEEE},
  annote = {Signal Processing Magazine, IEEE},
  owner = {afdidehf}
}

@article{Lv2011,
  title = {The Group Lasso for Stable Recovery of Block-Sparse Signal Representations},
  volume = {59},
  issn = {1053-587X},
  doi = {10.1109/TSP.2011.2105478},
  abstract = {Group Lasso is a mixed l1/l2-regularization method for a block-wise
	sparse model that has attracted a lot of interests in statistics,
	machine learning, and data mining. This paper establishes the possibility
	of stably recovering original signals from the noisy data using the
	adaptive group Lasso with a combination of sufficient block-sparsity
	and favorable block structure of the overcomplete dictionary. The
	corresponding theoretical results about the solution uniqueness,
	support recovery and representation error bound are derived based
	on the properties of block-coherence and subcoherence. Compared with
	the theoretical results on the parametrized quadratic program of
	conventional sparse representation, our stability results are more
	general. A comparison with block-based orthogonal greedy algorithm
	is also presented. Numerical experiments demonstrate the validity
	and correctness of theoretical derivation and also show that in case
	of noisy situation, the adaptive group Lasso has a better reconstruction
	performance than the quadratic program approach if the observed sparse
	signals have a natural block structure.},
  timestamp = {2016-09-30T13:55:17Z},
  number = {4},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Lv, Xiaolei and Bi, Guoan and Wan, C.},
  month = apr,
  year = {2011},
  keywords = {block-coherence,block-sparse signal representation,Greedy algorithms,group lasso,group
	Lasso,group theory,l1/l2-regularization method,orthogonal greedy algorithm,orthogonal
	greedy algorithm,parametrized quadratic
	program,quadratic programming,signal representation,sparse representation,stable
	recovery,subcoherence},
  pages = {1371--1382},
  owner = {afdidehf}
}

@inproceedings{Ma2010,
  title = {Head Pose Estimation Using Sparse Representation},
  volume = {2},
  doi = {10.1109/ICCEA.2010.226},
  abstract = {This paper proposes a novel method using sparse representation to
	improve the performance of head pose estimation. Sparse Representation
	Classifier (SRC) has been applied in face recognition and the related
	problem. In this paper, we first argue that SRC is efficient in head
	pose estimation. Then we propose the Block based Spare Representation
	Classifier (BSRC) method to reduce the influence of the background
	in the multi-view face images. The motivation of BSRC is that since
	the face shapes are different for the discretional head poses, the
	background in the multi-view face image is difficult to be eliminated
	by the traditional ways, such as feature extraction. As a result,
	the features of the multi-view face contain the noise of the background,
	which caused the descend of the performance of head pose estimation.
	In this paper, a face region is transformed into many images with
	the different blocks are discarded based on the assume that the block
	maybe the background at the specifically pose. By this way, there
	is an image in which the influence of the background can be reduced
	greatly. Specifically, SRC is applied on each image and the final
	class label of the input face region is decided by maximizing of
	Sparsity Concentration Index (SCI). The experimental results on the
	head pose database show the effectiveness of BSRC.},
  timestamp = {2016-07-08T12:41:40Z},
  booktitle = {Computer Engineering and Applications (ICCEA), 2010 Second International 	Conference on},
  author = {Ma, Bingpeng and Wang, Tianjiang},
  month = mar,
  year = {2010},
  keywords = {Application software,block based spare representation classifier,Computer
	applications,Computer science,Face detection,face recognition,Head,head
	pose estimation,Image databases,image representation,Layout,multiview
	face images,pose estimation,Shape,sparsity concentration index,System
	testing},
  pages = {389-392},
  owner = {afdidehf}
}

@phdthesis{Ma2011,
  title = {Localisation de sources par m{\'e}thodes {\`a} haute r{\'e}solution 	et par analyse parcimonieuse},
  abstract = {This thesis concerns the problem of sensor array source localization
	and power estimation by an acoustical array of sensors. In first
	the acoustical array directivity is treated. It is shown that such
	array is not useful for the localization of multiple sources. Adaptive
	arrays and high resolution methods are then introduced. They are
	based on the estimation of the sensor output covariance matrix and
	their performances overcome the natural limitations of the weighted
	beamforming processing. However, these methods require the use of
	a propagation model and are not robust to model errors. We present
	a new method which is an application of sparse regularization methodology
	to acoustical source localization using an acoustical array. Its
	performances are better than high-resolution methods and this method
	works very well in the case of correlated or uncorrelated signals,
	narrow band or wideband signals, near field or far field environments.
	Finally, a power estimation of sound sources by an acoustical array
	is presented. Numerical and experimental results in an anechoic room
	are presented showing the effectiveness of theoretical results.},
  timestamp = {2016-07-09T19:57:29Z},
  school = {Universite de Franche-Comte, Sciences pour l'Ing{\'e}nieur},
  author = {Ma, Hua},
  month = jul,
  year = {2011},
  keywords = {Acoustical array,array directivity.,high resolution algorithms,Source
	localization,sparse representation},
  owner = {Fardin}
}

@article{Machizawa2009,
  title = {MfD EEG/MEG Source Localization},
  timestamp = {2016-07-09T20:11:51Z},
  author = {Machizawa, Maro and Sabir, Himn and Litvak, Vladimir},
  month = feb,
  year = {2009},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{MacKay2004,
  title = {Sparse-graph codes for quantum error correction},
  volume = {50},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.834737},
  abstract = {Sparse-graph codes appropriate for use in quantum error-correction
	are presented. Quantum error-correcting codes based on sparse graphs
	are of interest for three reasons. First, the best codes currently
	known for classical channels are based on sparse graphs. Second,
	sparse-graph codes keep the number of quantum interactions associated
	with the quantum error-correction process small: a constant number
	per quantum bit, independent of the block length. Third, sparse-graph
	codes often offer great flexibility with respect to block length
	and rate. We believe some of the codes we present are unsurpassed
	by previously published quantum error-correcting codes.},
  timestamp = {2016-07-11T16:48:58Z},
  number = {10},
  journal = {Information Theory, IEEE Transactions on},
  author = {MacKay, D.J.C. and Mitchison, G. and McFadden, P.L.},
  month = oct,
  year = {2004},
  keywords = {block codes,block length,channel coding,classical channel,Decoding,error correction codes,Error-correction codes,Error
	correction codes,Error-correction
	codes,Geometry,Graph theory,Information rates,Information
	rates,information theory,Information
	theory,Laboratories,Noise level,parity check codes,parity
	check codes,Physics,probabilistic decoding,probabilistic
	decoding,Quantum computing,Quantum error correction,quantum error-correction,quantum
	error-correction,Quantum mechanics,Quantum
	mechanics,sparse-graph code,sparse-graph
	code,sparse matrices},
  pages = {2315-2330},
  owner = {afdidehf}
}

@article{Mairal2013,
  title = {Optimization for Sparse Estimation and Structured Sparsity},
  timestamp = {2016-07-10T07:20:03Z},
  author = {Mairal, Julien},
  month = jun,
  year = {2013},
  owner = {afdidehf}
}

@inproceedings{Majumdar2010,
  title = {Compressive color imaging with group-sparsity on analysis prior},
  doi = {10.1109/ICIP.2010.5653685},
  abstract = {Compressed sensing (CS) of color images can be formulated as a group-sparsity
	promoting inverse problem. In the past, group-sparsity constraint
	was imposed on the CS synthesis prior formulation with an orthogonal
	transform to solve the inverse problem. The objective of this work
	is to empirically show that better results can be obtained if a group-sparsity
	constraint is imposed on the CS analysis prior formulation with a
	redundant transform. This problem requires solving a group-sparsity
	promoting inverse problem which has not been addressed earlier. Therefore
	we derive a new algorithm for solving it based on the Majorization-Minimization
	approach. Experimental results corroborate that analysis prior with
	a redundant transform gives far superior (about 1.5dB) improvement
	compared to synthesis prior with orthogonal transform.},
  timestamp = {2016-07-08T11:59:05Z},
  booktitle = {Image Processing (ICIP), 2010 17th IEEE International Conference 	on},
  author = {Majumdar, A. and Ward, R.K.},
  month = sep,
  year = {2010},
  keywords = {Algorithm design and analysis,analysis prior,Color,color images,color
	imaging,compressed sensing,compressive color imaging,data compression,group
	sparsity,group-sparsity constraint,image coding,image colour analysis,Image
	reconstruction,inverse problem,inverse transforms,majorization-minimization
	approach,Noise,Optimization,orthogonal transform,synthesis prior,transforms,Wavelet
	analysis},
  pages = {1337-1340},
  owner = {afdidehf}
}

@inproceedings{Malek-Mohammadi2014,
  title = {DOA estimation in partially correlated noise using low-rank/sparse 	matrix decomposition},
  doi = {10.1109/SAM.2014.6882419},
  abstract = {We consider the problem of direction-of-arrival (DOA) estimation in
	unknown partially correlated noise environments where the noise covariance
	matrix is sparse. A sparse noise covariance matrix is a common model
	for a sparse array of sensors consisted of several widely separated
	subarrays. Since interelement spacing among sensors in a subarray
	is small, the noise in the subarray is in general spatially correlated,
	while, due to large distances between subarrays, the noise between
	them is uncorrelated. Consequently, the noise covariance matrix of
	such an array has a block diagonal structure which is indeed sparse.
	Moreover, in an ordinary nonsparse array, because of small distance
	between adjacent sensors, there is noise coupling between neighboring
	sensors, whereas one can assume that non-adjacent sensors have spatially
	uncorrelated noise which makes again the array noise covariance matrix
	sparse. Utilizing some recently available tools in low-rank/sparse
	matrix decomposition, matrix completion, and sparse representation,
	we propose a novel method which can resolve possibly correlated or
	even coherent sources in the aforementioned partly correlated noise.
	In particular, when the sources are uncorrelated, our approach involves
	solving a second-order cone programming (SOCP), and if they are correlated
	or coherent, one needs to solve a computationally harder convex program.
	We demonstrate the effectiveness of the proposed algorithm by numerical
	simulations and comparison to the Cramer-Rao bound (CRB).},
  timestamp = {2016-07-08T12:15:27Z},
  booktitle = {Sensor Array and Multichannel Signal Processing Workshop (SAM), 2014 	IEEE 8th},
  author = {Malek-Mohammadi, M. and Jansson, M. and Owrang, A. and Koochakzadeh, A. and Babaie-Zadeh, M.},
  month = jun,
  year = {2014},
  keywords = {Antenna arrays,array elements,Arrays,array signal processing,block
	diagonal structure,convex program,convex programming,correlation
	methods,covariance matrices,direction-of-arrival estimation,Direction-of-arrival
	estimation,direction-of-arrival estimation problem,DOA estimation
	problem,Estimation,inter element spacing,low-rank matrix decomposition,matrix
	completion,matrix decomposition,neighboring sensors,Noise,noise coupling,numerical
	simulations,ordinary nonsparse array,second-order cone programming,signal
	representation,sparse matrices,sparse matrix decomposition,sparse
	noise covariance matrix,sparse representation,sparse sensor array,unknown
	partially correlated noise environments},
  pages = {373-376},
  owner = {afdidehf}
}

@article{Malioutov2005,
  title = {A sparse signal reconstruction perspective for source localization 	with sensor arrays},
  volume = {53},
  issn = {1053-587X},
  doi = {10.1109/TSP.2005.850882},
  abstract = {We present a source localization method based on a sparse representation
	of sensor measurements with an overcomplete basis composed of samples
	from the array manifold. We enforce sparsity by imposing penalties
	based on the ?1-norm. A number of recent theoretical results on sparsifying
	properties of ?1 penalties justify this choice. Explicitly enforcing
	the sparsity of the representation is motivated by a desire to obtain
	a sharp estimate of the spatial spectrum that exhibits super-resolution.
	We propose to use the singular value decomposition (SVD) of the data
	matrix to summarize multiple time or frequency samples. Our formulation
	leads to an optimization problem, which we solve efficiently in a
	second-order cone (SOC) programming framework by an interior point
	implementation. We propose a grid refinement method to mitigate the
	effects of limiting estimates to a grid of spatial locations and
	introduce an automatic selection criterion for the regularization
	parameter involved in our approach. We demonstrate the effectiveness
	of the method on simulated data by plots of spatial spectra and by
	comparing the estimator variance to the Cramer-Rao bound (CRB).
	We observe that our approach has a number of advantages over other
	source localization techniques, including increased resolution, improved
	robustness to noise, limitations in data quantity, and correlation
	of the sources, as well as not requiring an accurate initialization.},
  timestamp = {2017-06-23T13:14:12Z},
  number = {8},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Malioutov, D. and Cetin, M. and Willsky, A.S.},
  month = aug,
  year = {2005},
  keywords = {Acoustic sensors,array signal processing,automatic selection criterion,data
	matrix,direction-of-arrival estimation,grid refinement method,grid
	refinement method,Maximum likelihood estimation,maximum
	likelihood estimation,Multiple signal classification,Optimization,overcomplete
	representation,regularization parameter,second-order cone programming
	framework,sensor array processing,Sensor arrays,Sensor
	arrays,signal reconstruction,signal representation,signal
	representation,signal representations,signal resolution,Signal
	resolution,signal sampling,signal superresolution,signal
	superresolution,Singular Value Decomposition,singular
	value decomposition,source localization,source localization method,source
	localization method,sparse representation,sparse
	representation,sparse signal reconstruction perspective,sparse signal reconstruction
	perspective,sparse signal representation,sparse signal
	representation,Spatial resolution,spatial spectrum estimation,spatial
	spectrum estimation,superresolution,time-frequency analysis,time-frequency
	analysis},
  pages = {3010--3022},
  owner = {afdidehf}
}

@inproceedings{Malioutov2004,
  title = {Optimal sparse representations in general overcomplete bases},
  volume = {2},
  doi = {10.1109/ICASSP.2004.1326377},
  abstract = {We consider the problem of enforcing a sparsity prior in underdetermined
	linear problems, which is also known as sparse signal representation
	in overcomplete bases. The problem is combinatorial in nature, and
	a direct approach is computationally intractable even for moderate
	data sizes. A number of approximations have been considered in the
	literature, including stepwise regression, matching pursuit and its
	variants, and recently, basis pursuit (`1) and also `p-norm relaxations
	with p < 1. Although the exact notion of sparsity (expressed by an
	`0-norm) is replaced by `1 and `p norms in the latter two, it can
	be shown that under some conditions these relaxations solve the original
	problem exactly. The seminal paper of Donoho and Huo establishes
	this fact for `1 (basis pursuit) for a special case where the linear
	operator is composed of an orthogonal pair. In this paper, we extend
	their results to a general underdetermined linear operator. Furthermore,
	we derive conditions for the equivalence of `0 and `p problems, and
	extend the results to the problem of enforcing sparsity with respect
	to a transformation (which includes total variation priors as a special
	case). Finally, we describe an interesting result relating the sign
	patterns of solutions to the question of `1-`0 equivalence.},
  timestamp = {2016-07-10T07:19:57Z},
  booktitle = {International Conference on Acoustics, Speech, and Signal Processing},
  author = {Malioutov, Dmitry M. and Cetin, Mujdat and Willsky, Alan S.},
  month = may,
  year = {2004},
  pages = {793�796},
  masid = {3529063},
  owner = {afdidehf}
}

@book{Mallat2008,
  title = {A Wavelet Tour of Signal Processing},
  timestamp = {2017-06-23T13:13:36Z},
  author = {Mallat, Stéphane},
  year = {2008},
  owner = {afdidehf}
}

@inproceedings{Mallat2009,
  title = {Structured pursuits for geometric super-resolution},
  doi = {10.1109/ICIP.2009.5414549},
  abstract = {Super-resolution image zooming is possible when the image has some
	geometric regularity. We introduce a general class of non-linear
	inverse estimators, which combines linear estimators with mixing
	weights in a frame providing a sparse representation. Mixing weights
	are computed with a block decomposition, which minimizes a Tikhonov
	energy penalized by an 11 norm of the mixing weights. A fast orthogonal
	matching pursuit algorithm computes the mixing weights. Adaptive
	directional image interpolations are calculated with mixing weights
	in a wavelet frame.},
  timestamp = {2016-07-11T16:57:34Z},
  booktitle = {Image Processing (ICIP), 2009 16th IEEE International Conference 	on},
  author = {Mallat, S. and Yu, Guoshen},
  month = nov,
  year = {2009},
  keywords = {adaptive directional image interpolations,block decomposition,fast
	orthogonal matching pursuit algorithm,Fourier transforms,geometric
	regularity,geometric super-resolution,Image Processing,Image resolution,image
	zooming,interpolation,inverse problems,linear estimators,Low pass
	filters,Matching pursuit algorithms,mixing weights,Noise measurement,Nonlinear
	filters,non-linear inverse estimators,Pursuit algorithms,sparse representation,Spline,structured
	pursuits,Tikhonov energy},
  pages = {1477-1480},
  owner = {afdidehf}
}

@article{Mallat1993,
  title = {Matching pursuits with time-frequency dictionaries},
  volume = {41},
  issn = {1053-587X},
  doi = {10.1109/78.258082},
  abstract = {The authors introduce an algorithm, called matching pursuit, that
	decomposes any signal into a linear expansion of waveforms that are
	selected from a redundant dictionary of functions. These waveforms
	are chosen in order to best match the signal structures. Matching
	pursuits are general procedures to compute adaptive signal representations.
	With a dictionary of Gabor functions a matching pursuit defines an
	adaptive time-frequency transform. They derive a signal energy distribution
	in the time-frequency plane, which does not include interference
	terms, unlike Wigner and Cohen class distributions. A matching pursuit
	isolates the signal structures that are coherent with respect to
	a given dictionary. An application to pattern extraction from noisy
	signals is described. They compare a matching pursuit decomposition
	with a signal expansion over an optimized wavepacket orthonormal
	basis, selected with the algorithm of Coifman and Wickerhauser see
	(IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
  timestamp = {2016-09-29T14:53:46Z},
  number = {12},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Mallat, S.G. and Zhang, Z.},
  month = dec,
  year = {1993},
  keywords = {adaptive signal representations,adaptive time-frequency transform,Dictionaries,Fourier
	transforms,Gabor functions,Interference,linear waveform expansion,matching
	pursuit algorithm,Matching pursuit algorithms,matching pursuit decomposition,Natural
	languages,noisy signals,optimized wavepacket orthonormal basis,pattern
	extraction,Pursuit algorithms,signal energy distribution,signal expansion,signal
	processing,Signal processing algorithms,signal representations,signal
	structures,time-frequency analysis,Time frequency analysis,time-frequency
	dictionaries,time-frequency plane,Vocabulary,wavelet transforms},
  pages = {3397--3415},
  owner = {afdidehf}
}

@inproceedings{Mallat1992,
  title = {Adaptive time-frequency decomposition with matching pursuits},
  doi = {10.1109/TFTSA.1992.274245},
  abstract = {An adaptive time-frequency decomposition is introduced. It represents
	signals as a linear expansion of local time-frequency waveforms,
	selected in order to match the signal structures. The decomposition
	is performed with an algorithm, called matching pursuit, whose convergence
	is guaranteed. A new definition of signal energy density in the time-frequency
	plane is derived by summing the Wigner distribution of each local
	time-frequency waveform. This energy density does not have any interference
	terms, unlike Wigner and Cohen class distributions. Numerical examples
	are described},
  timestamp = {2016-07-08T10:12:27Z},
  booktitle = {Time-Frequency and Time-Scale Analysis, 1992., Proceedings of the 	IEEE-SP International Symposium},
  author = {Mallat, S. and Zhang, Z.},
  month = oct,
  year = {1992},
  keywords = {adaptive time-frequency decomposition,algorithm,Convergence,Dictionaries,Fourier
	transforms,Interference,linear expansion,local time-frequency waveforms,matching
	pursuit,Matching pursuit algorithms,Pursuit algorithms,signal energy
	density,signal processing,Signal processing algorithms,time-frequency analysis,Time frequency analysis,time-frequency
	analysis,Time frequency
	analysis,time-frequency plane,Vectors,wavelet
	transforms,Wigner distribution},
  pages = {7-10},
  owner = {afdidehf}
}

@inproceedings{Mancera2006,
  title = {L0-Norm-Based Sparse Representation Through Alternate Projections},
  doi = {10.1109/ICIP.2006.312819},
  abstract = {We present a simple and robust method for finding sparse representations
	in overcomplete transforms, based on minimization of the L0-norm.
	Our method is better than current solutions based on minimization
	of the L1-norm in terms of energy compaction. These results strongly
	question the equivalence of minimizing both norms in real conditions.
	We also show application to in-painting (interpolation of lost pixels).},
  timestamp = {2016-07-09T19:52:26Z},
  booktitle = {Image Processing, 2006 IEEE International Conference on},
  author = {Mancera, L. and Portilla, J.},
  month = oct,
  year = {2006},
  keywords = {Artificial Intelligence,Compaction,Computer science,Degradation,Dictionaries,Equations,image
	representation,Information processing,L0-norm,Minimization methods,restoration,Robustness,sparse
	representation,Vectors},
  pages = {2089-2092},
  owner = {Fardin}
}

@article{Mansour2015a,
  title = {Recovery Analysis for Weighted $\ell_1$-Minimization Using a Null 	space property},
  abstract = {We study the recovery of sparse signals from underdetermined linear
	measurements when a poten- tially erroneous support estimate is available.
	Our results are twofold. First, we derive necessary and sufficient
	conditions for signal recovery from compressively sampled measurements
	using weighted ?1- norm minimization. These conditions, which depend
	on the choice of weights as well as the size and accuracy of the
	support estimate, are on the null space of the measurement matrix.
	They can guarantee recovery even when standard ?1 minimization fails.
	Second, we derive bounds on the number of Gaus- sian measurements
	for these conditions to be satisfied, i.e., for weighted ?1 minimization
	to successfully recover all sparse signals whose support has been
	estimated sufficiently accurately. Our bounds show that weighted
	?1 minimization requires significantly fewer measurements than standard
	?1 minimization when the support estimate is relatively accurate.},
  timestamp = {2016-07-10T07:42:03Z},
  journal = {International Conference on Acoustics, Speech, and Signal Processing 	(ICASSP2015)},
  author = {Mansour, Hassan and Saab, Rayan},
  year = {2015},
  owner = {afdidehf}
}

@article{Maris2007,
  title = {Nonparametric statistical testing of EEG- and MEG-data},
  volume = {164},
  issn = {0165-0270},
  doi = {http://dx.doi.org/10.1016/j.jneumeth.2007.03.024},
  abstract = {In this paper, we show how ElectroEncephaloGraphic (EEG) and MagnetoEncephaloGraphic
	(MEG) data can be analyzed statistically using nonparametric techniques.
	Nonparametric statistical tests offer complete freedom to the user
	with respect to the test statistic by means of which the experimental
	conditions are compared. This freedom provides a straightforward
	way to solve the multiple comparisons problem (MCP) and it allows
	to incorporate biophysically motivated constraints in the test statistic,
	which may drastically increase the sensitivity of the statistical
	test. The paper is written for two audiences: (1) empirical neuroscientists
	looking for the most appropriate data analysis method, and (2) methodologists
	interested in the theoretical concepts behind nonparametric statistical
	tests. For the empirical neuroscientist, a large part of the paper
	is written in a tutorial-like fashion, enabling neuroscientists to
	construct their own statistical test, maximizing the sensitivity
	to the expected effect. And for the methodologist, it is explained
	why the nonparametric test is formally correct. This means that we
	formulate a null hypothesis (identical probability distribution in
	the different experimental conditions) and show that the nonparametric
	test controls the false alarm rate under this null hypothesis.},
  timestamp = {2016-07-10T07:00:25Z},
  number = {1},
  journal = {Journal of Neuroscience Methods},
  author = {Maris, Eric and Oostenveld, Robert},
  year = {2007},
  keywords = {EEG,Hypothesis testing,MEG,Multiple comparisons problem,Nonparametric
	statistical testing},
  pages = {177 - 190},
  owner = {Fardin}
}

@article{Marvasti2012,
  title = {A unified approach to sparse signal processing},
  volume = {2012},
  abstract = {A unified view of the area of sparse signal processing is presented
	in tutorial form by bringing together various fields in which the
	property of sparsity has been successfully exploited. For each of
	these fields, various algorithms and techniques, which have been
	developed to leverage sparsity, are described succinctly. The common
	potential benefits of significant reduction in sampling rate and
	processing manipulations through sparse signal processing are revealed.
	The key application domains of sparse signal processing are sampling,
	coding, spectral estimation, array processing, component analysis,
	and multipath channel estimation. In terms of the sampling process
	and reconstruction algorithms, linkages are made with random sampling,
	compressed sensing, and rate of innovation. The redundancy introduced
	by channel coding in finite and real Galois fields is then related
	to over-sampling with similar reconstruction algorithms. The error
	locator polynomial (ELP) and iterative methods are shown to work
	quite effectively for both sampling and coding applications. The
	methods of Prony, Pisarenko, and MUltiple SIgnal Classification (MUSIC)
	are next shown to be targeted at analyzing signals with sparse frequency
	domain representations. Specifically, the relations of the approach
	of Prony to an annihilating filter in rate of innovation and ELP
	in coding are emphasized; the Pisarenko andMUSICmethods are further
	improvements of the Prony method under noisy environments. The iterative
	methods developed for sampling and coding applications are shown
	to be powerful tools in spectral estimation. Such narrowband spectral
	estimation is then related to multi-source location and direction
	of arrival estimation in array processing. Sparsity in unobservable
	source signals is also shown to facilitate source separation in sparse
	component analysis; the algorithms developed in this area such as
	linear programming and matching pursuit are also widely used in compressed
	sensing. Finally, the multipath channel estimation problem is shown
	to have a sparse formulation; algorithms similar to sampling and
	coding are used to estimate typical multicarrier communication channels.},
  timestamp = {2016-07-08T11:34:52Z},
  number = {44},
  journal = {EURASIP Journal on Advances in Signal Processing},
  author = {Marvasti, Farokh and Arash Amini1, Farzan Haddadi2, Mahdi Soltanolkotabi1, Babak Hossein Khalaj1, Akram Aldroubi3, Saeid Sanei4 and Chambers5, Janathon},
  year = {2012},
  owner = {Fardin}
}

@inproceedings{Marvasti2012a,
  title = {Sparse signal processing using iterative method with adaptive thresholding 	(IMAT)},
  doi = {10.1109/ICTEL.2012.6221328},
  abstract = {Classical sampling theorem states that by using an anti-aliased low-pass
	filter at the Nyquist rate, one can transmit and retrieve the filtered
	signal. This approach, which has been used for decades in signal
	processing, is not good for high quality speech, image and video
	signals where the actual signals are not low-pass but rather sparse.
	The traditional sampling theorems do not work for sparse signals.
	Modern approach, developed by statisticians at Stanford (Donoho and
	Candes), give some lower bounds for the minimum sampling rate such
	that a sparse signal can be retrieved with high probability. However,
	their approach, using a sampling matrix called compressive matrix,
	has certain drawbacks: Compressive matrices require the knowledge
	of all the samples, which defeats the whole purpose of compressive
	sampling! Moreover, for real signals, one does not need a compressive
	matrix and we shall show in this invited paper that random sampling
	performs as good as or better than compressive sampling. In addition,
	we show that greedy methods such as Orthogonal Matching Pursuit (OMP)
	are too complex with inferior performance compared to IMAT and other
	iterative methods. Furthermore, we shall compare IMAT to OMP and
	other reconstruction methods in term of complexity and show the advantages
	of IMAT. Various applications such as image and speech recovery from
	random or block losses, salt & pepper noise, OFDM channel estimation,
	MRI, and finally spectral estimation will be discussed and simulated.},
  timestamp = {2016-07-11T16:53:03Z},
  booktitle = {Telecommunications (ICT), 2012 19th International Conference on},
  author = {Marvasti, F. and Azghani, M. and Imani, P. and Pakrouh, P. and Heydari, S.J. and Golmohammadi, A. and Kazerouni, A. and Khalili, M.M.},
  month = apr,
  year = {2012},
  keywords = {adaptive thresholding,antialiased low-pass filter,classical sampling
	theorem,compressive matrix,filtering theory,high probability,high
	quality image,high quality speech,high quality video signal,image
	recovery,IMAT,iterative method,iterative methods,Magnetic resonance
	imaging,matrix algebra,MRI,Noise measurement,Nyquist rate,OFDM,OFDM channel estimation,OFDM
	Channel Estimation,OMP,orthogonal matching
	pursuit,pepper noise,probability,Salt\\\\\\\\\\\\\\\&Pepper and clippling Noise,Salt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\&Pepper and clippling
	Noise,Salt\\\&Pepper and clippling Noise,Salt\\\\\\\&Pepper and clippling Noise,signal processing,signal
	processing,Signal to noise ratio,Sparse,sparse signal
	processing,sparse signal processing,Spectral Estimation,Spectral
	Estimation,speech recovery,transforms},
  pages = {1-6},
  owner = {afdidehf}
}

@book{MarvinMarcus1992,
  title = {Survey of matrix theory and matrix inequalities},
  isbn = {0-486-67102-X 978-0-486-67102-4},
  timestamp = {2016-07-11T16:59:17Z},
  publisher = {{\{Dover Publications\}}},
  author = {Marvin Marcus, Henryk Minc},
  year = {1992},
  owner = {Fardin}
}

@inproceedings{Masood2013,
  title = {Support agnostic Bayesian matching pursuit for block sparse signals},
  doi = {10.1109/ICASSP.2013.6638540},
  abstract = {A fast matching pursuit method using a Bayesian approach is introduced
	for block-sparse signal recovery. This method performs Bayesian estimates
	of block-sparse signals even when the distribution of active blocks
	is non-Gaussian or unknown. It is agnostic to the distribution of
	active blocks in the signal and utilizes a priori statistics of additive
	noise and the sparsity rate of the signal, which are shown to be
	easily estimated from data and no user intervention is required.
	The method requires a priori knowledge of block partition and utilizes
	a greedy approach and order-recursive updates of its metrics to find
	the most dominant sparse supports to determine the approximate minimum
	mean square error (MMSE) estimate of the block-sparse signal. Simulation
	results demonstrate the power and robustness of our proposed estimator.},
  timestamp = {2016-07-11T16:59:04Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Masood, M. and Al-Naffouri, T.Y.},
  month = may,
  year = {2013},
  keywords = {Additive noise,agnostic Bayesian matching pursuit,Bayesian estimation,Bayesian
	matching pursuit,Bayes methods,block partition,block sparse signal
	recovery,block sparse signals,Clustering algorithms,compressed sensing,fast
	matching pursuit method,Greedy algorithms,greedy approach,iterative
	methods,least mean squares methods,Matching pursuit algorithms,minimum
	mean square error estimation,MMSE estimation,Noise,order recursive
	update,Robustness,SABMP,sparse signal recovery,time-frequency analysis,Vectors},
  pages = {4643-4647},
  owner = {afdidehf}
}

@inproceedings{Matsuzono2014,
  title = {Structured random linear codes (SRLC): Bridging the gap between block 	and convolutional codes},
  doi = {10.1109/GLOCOM.2014.7036974},
  abstract = {Several types of AL-FEC (Application-Level FEC) codes for the Packet
	Erasure Channel exist. Random Linear Codes (RLC), where redundancy
	packets consist of random linear combinations of source packets over
	a certain finite field, are a simple yet efficient coding technique,
	for instance massively used for Network Coding applications. However
	the price to pay is a high encoding and decoding complexity, especially
	when working on GF(28), which seriously limits the number of packets
	in the encoding window. On the opposite, structured block codes have
	been designed for situations where the set of source packets is known
	in advance, for instance with file transfer applications. Here the
	encoding and decoding complexity is controlled, even for huge block
	sizes, thanks to the sparse nature of the code and advanced decoding
	techniques that exploit this sparseness (e.g., Structured Gaussian
	Elimination). But their design also prevents their use in convolutional
	use-cases featuring an encoding window that slides over a continuous
	set of incoming packets. In this work we try to bridge the gap between
	these two code classes, bringing some structure to RLC codes in order
	to enlarge the use-cases where they can be efficiently used: in convolutional
	mode (as any RLC code), but also in block mode with either tiny,
	medium or large block sizes. We also demonstrate how to design compact
	signaling for these codes (for encoder/decoder synchronization),
	which is an essential practical aspect.},
  timestamp = {2016-07-11T16:57:35Z},
  booktitle = {Global Communications Conference (GLOBECOM), 2014 IEEE},
  author = {Matsuzono, K. and Roca, V. and Asaeda, H.},
  month = dec,
  year = {2014},
  keywords = {advanced decoding techniques,AL-FEC,application level FEC,block codes,block
	mode,communication complexity,Convolution,convolutional codes,convolutional
	mode,Decoding,decoding complexity,encoding,encoding complexity,encoding
	window,finite field,forward erasure correction,forward error correction,Galois
	fields,linear codes,Maintenance engineering,network coding,packet
	erasure channel,packet radio networks,random codes,Receivers,redundancy,redundancy
	packets,Reliability,RLC codes,source packets,structured block codes,structured
	random linear codes},
  pages = {1211-1217},
  owner = {afdidehf}
}

@article{Mattout2008,
  title = {EEG/MEG Source Localisation},
  timestamp = {2016-07-08T12:17:41Z},
  author = {Mattout, J�r�mie and Phillips, Christophe},
  month = oct,
  year = {2008},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {SPM Course � Wellcome Trust Centre for Neuroimaging},
  owner = {Fardin}
}

@article{Mattoutxxxx,
  title = {EEG/MEG source reconstruction},
  timestamp = {2016-07-08T12:17:55Z},
  author = {Mattout, J�r�mie and Phillips, Christophe and Friston, Karl},
  year = {xxxx},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Maung2015,
  title = {Improved Greedy Algorithms for Sparse Approximation of a Matrix in 	Terms of Another Matrix},
  volume = {27},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2014.2349910},
  abstract = {We consider simultaneously approximating all the columns of a data
	matrix in terms of few selected columns of another matrix that is
	sometimes called �the dictionary�. The challenge is to determine
	a small subset of the dictionary columns that can be used to obtain
	an accurate prediction of the entire data matrix. Previously proposed
	greedy algorithms for this task compare each data column with all
	dictionary columns, resulting in algorithms that may be too slow
	when both the data matrix and the dictionary matrix are large. A
	recent approach for accelerating the run time requires large amounts
	of memory to keep temporary values during the run of the algorithm.
	We propose two new algorithms that can be used even when both the
	data matrix and the dictionary matrix are large. The first algorithm
	is exact, with output identical to some previously proposed greedy
	algorithms. It takes significantly less memory when compared to the
	current state-of-the-art, and runs much faster when the dictionary
	matrix is sparse. The second algorithm uses a low rank approximation
	to the data matrix to further improve the run time. The algorithms
	use new recursive formulas for computing the greedy selection criterion.
	The formulas enable decoupling most of the computations related to
	the data matrix from the computations related to the dictionary matrix.},
  timestamp = {2016-07-08T12:48:11Z},
  number = {3},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  author = {Maung, C. and Schweitzer, H.},
  month = mar,
  year = {2015},
  keywords = {another matrix,Approximation algorithms,Approximation methods,approximation
	theory,data matrix columns,Dictionaries,dictionary columns,dictionary
	matrix,Greedy algorithms,greedy selection criterion,improved greedy
	algorithms,low rank approximation,Matching pursuit algorithms,Signal
	processing algorithms,sparse approximation,sparse matrices,Vectors},
  pages = {769-780},
  owner = {Fardin}
}

@article{Meier2008,
  title = {The group lasso for logistic regression},
  volume = {70},
  abstract = {The group lasso is an extension of the lasso to do variable selection
	on (predefined) groups of variables in linear regression models.
	The estimates have the attractive property of being invariant under
	groupwise orthogonal reparameterizations. We extend the group lasso
	to logistic regression models and present an efficient algorithm,
	that is especially suitable for high dimensional problems, which
	can also be applied to generalized linear models to solve the corresponding
	convex optimization problem. The group lasso estimator for logistic
	regression is shown to be statistically consistent even if the number
	of predictors is much larger than sample size but with sparse true
	underlying structure.We further use a two-stage procedure which aims
	for sparser models than the group lasso, leading to improved prediction
	performance for some cases. Moreover, owing to the two-stage nature,
	the estimates can be constructed to be hierarchical. The methods
	are used on simulated and real data sets about splice site detection
	in DNA sequences.},
  timestamp = {2017-06-23T13:18:04Z},
  number = {1},
  journal = {J. R. Statist. Soc. B},
  author = {Meier, Lukas and Geer, Sara van de and Bühlmann, Peter},
  year = {2008},
  keywords = {Categorical data,Co-ordinate descent algorithm,DNA splice site,Group
	variable selection,High dimensional generalized linear model,Penalized
	likelihood},
  pages = {53--71},
  owner = {afdidehf}
}

@article{Meinshausen2006,
  title = {High-dimensional graphs and variable selection with the Lasso},
  volume = {34},
  doi = {10.1214/009053606000000281},
  abstract = {The pattern of zero entries in the inverse covariance matrix of a
	multivariate normal distribution corresponds to conditional independence
	restrictions between variables. Covariance selection aims at estimating
	those structural zeros from data. We show that neighborhood selection
	with the Lasso is a computationally attractive alternative to standard
	covariance selection for sparse high-dimensional graphs. Neighborhood
	selection estimates the conditional independence restrictions separately
	for each node in the graph and is hence equivalent to variable selection
	for Gaussian linear models. We show that the proposed neighborhood
	selection scheme is consistent for sparse high-dimensional graphs.
	Consistency hinges on the choice of the penalty parameter. The oracle
	value for optimal prediction does not lead to a consistent neighborhood
	estimate. Controlling instead the probability of falsely joining
	some distinct connectivity components of the graph, consistent estimation
	for sparse graphs is achieved (with exponential rates), even when
	the number of variables grows like the number of observations raised
	to an arbitrary power.},
  timestamp = {2016-07-08T12:43:00Z},
  number = {3},
  journal = {Annals of Statistics},
  author = {Meinshausen, Nicolai and Buhlmann, Peter},
  year = {2006},
  pages = {1436-1462},
  masid = {3183035},
  owner = {afdidehf}
}

@article{Meinshausen2007,
  title = {Discussion: A Tale Of Three Cousins: Lasso, L2boosting And Dantzig},
  volume = {35},
  doi = {10.1214/009053607000000460},
  timestamp = {2016-07-08T12:12:13Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Meinshausen, N. and Rocha, G. and Yu, B.},
  year = {2007},
  pages = {2373�2384},
  owner = {afdidehf}
}

@article{Meinshausen2009,
  title = {Lasso-type recovery of sparse representations for high-dimensional 	data},
  volume = {37},
  doi = {10.1214/07-AOS582},
  abstract = {The Lasso is an attractive technique for regularization and variable
	selection for high-dimensional data, where the number of predictor
	variables pn is potentially much larger than the number of samples
	n. However, it was recently discovered that the sparsity pattern
	of the Lasso estimator can only be asymptotically identical to the
	true sparsity pattern if the design matrix satisfies the so-called
	irrepresentable condition. The latter condition can easily be violated
	in the presence of highly correlated variables. Here we examine the
	behavior of the Lasso estimators if the irrepresentable condition
	is relaxed. Even though the Lasso cannot recover the correct sparsity
	pattern, we show that the estimator is still consistent in the 2-norm
	sense for fixed designs under conditions on (a) the number sn of
	nonzero components of the vector ?n and (b) the minimal singular
	values of design matrices that are induced by selecting small subsets
	of variables. Furthermore, a rate of convergence result is obtained
	on the 2 error with an appropriate choice of the smoothing parameter.
	The rate is shown to be optimal under the condition of bounded maximal
	and minimal sparse eigenvalues. Our results imply that, with high
	probability, all important variables are selected. The set of selected
	variables is a meaningful reduction on the original set of variables.
	Finally, our results are illustrated with the detection of closely
	adjacent frequencies, a problem encountered in astrophysics.},
  timestamp = {2016-09-30T10:51:46Z},
  number = {1},
  journal = {The Annals of Statistics},
  author = {Meinshausen, Nicolai and Yu, Bin},
  year = {2009},
  keywords = {high-dimensional data,Lasso,Shrinkage estimation,Sparsity},
  pages = {246--270},
  annote = {read},
  owner = {Fardin}
}

@article{Menke2005,
  title = {A Priori Information and Weighted Least Squared},
  timestamp = {2016-07-08T11:27:11Z},
  author = {Menke, William},
  year = {2005},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Professor of Earth and Environmental Sciences Lamont-Doherty Earth
	Observatory and The Earth Institute at Columbia University},
  owner = {afdidehf}
}

@article{Meyer2002,
  title = {Multilayered image representation: application to image compression},
  volume = {11},
  issn = {1057-7149},
  doi = {10.1109/TIP.2002.802527},
  abstract = {The main contribution of this work is a new paradigm for image representation
	and image compression. We describe a new multilayered representation
	technique for images. An image is parsed into a superposition of
	coherent layers: piecewise smooth regions layer, textures layer,
	etc. The multilayered decomposition algorithm consists in a cascade
	of compressions applied successively to the image itself and to the
	residuals that resulted from the previous compressions. During each
	iteration of the algorithm, we code the residual part in a lossy
	way: we only retain the most significant structures of the residual
	part, which results in a sparse representation. Each layer is encoded
	independently with a different transform, or basis, at a different
	bitrate, and the combination of the compressed layers can always
	be reconstructed in a meaningful way. The strength of the multilayer
	approach comes from the fact that different sets of basis functions
	complement each others: some of the basis functions will give reasonable
	account of the large trend of the data, while others will catch the
	local transients, or the oscillatory patterns. This multilayered
	representation has a lot of beautiful applications in image understanding,
	and image and video coding. We have implemented the algorithm and
	we have studied its capabilities.},
  timestamp = {2016-07-09T20:15:28Z},
  number = {9},
  journal = {Image Processing, IEEE Transactions on},
  author = {Meyer, F.G. and Averbuch, A.Z. and Coifman, R.R.},
  month = sep,
  year = {2002},
  keywords = {basis functions,Bit rate,cosine transforms,data compression,Discrete
	cosine transforms,Discrete wavelet transforms,image coding,Image
	compression,Image reconstruction,image representation,Libraries,multilayered
	decomposition algorithm,multilayered representation,Nonhomogeneous
	media,piecewise smooth regions layer,Pursuit algorithms,residual
	part,sparse representation,textures layer,Transform coding,transform
	coding,video coding,wavelet transforms},
  pages = {1072-1080},
  owner = {afdidehf}
}

@article{Michel2004,
  title = {EEG Source Imaging},
  volume = {115},
  abstract = {Electroencephalography (EEG) is an important tool for studying the
	temporal dynamics of the human brain�s large-scale neuronal circuits.
	However, most EEG applications fail to capitalize on all of the data�s
	available information, particularly that concerning the location
	of active sources in the brain. Localizing the sources of a given
	scalp measurement is only achieved by solving the so-called inverse
	problem. By introducing reasonable a priori constraints, the inverse
	problem can be solved and the most probable sources in the brain
	at every moment in time can be accurately localized. Here, we review
	the different EEG source localization procedures applied during the
	last two decades. Additionally, we detail the importance of those
	procedures preceding and following source estimation that are intimately
	linked to a successful, reliable result. We discuss (1) the number
	and positioning of electrodes, (2) the varieties of inverse solution
	models and algorithms, (3) the integration of EEG source estimations
	with MRI data, (4) the integration of time and frequency in source
	imaging, and (5) the statistical analysis of inverse solution results.
	We show that modern EEG source imaging simultaneously details the
	temporal and spatial dimensions of brain activity, making it an important
	and affordable tool to study the properties of cerebral neural networks
	in cognitive and clinical neurosciences.},
  timestamp = {2016-07-08T12:18:00Z},
  number = {10},
  journal = {Clinical Neurophysiology},
  author = {Michel, Christoph M. and Murray, Micah M. and Lantz, G�ran and Gonzalez, Sara and Spinelli, Laurent and Peralta, Rolando Grave de},
  month = oct,
  year = {2004},
  note = {read},
  keywords = {EEG,Neuroimaging,source localization},
  pages = {2195�2222},
  owner = {Fardin}
}

@inproceedings{Mideksa2013,
  title = {Dipole source analysis for readiness potential and field using simultaneously 	measured EEG and MEG signals},
  doi = {10.1109/EMBC.2013.6609762},
  abstract = {Various source localization techniques have indicated the generators
	of each identifiable component of movement-related cortical potentials,
	since the discovery of the surface negative potential prior to self-paced
	movement by Kornhuber and Decke. Readiness potentials and fields
	preceding self-paced finger movements were recorded simultaneously
	using multichannel electroencephalography (EEG) and magnetoencephalography
	(MEG) from five healthy subjects. The cortical areas involved in
	this paradigm are the supplementary motor area (SMA) (bilateral),
	pre-SMA (bilateral), and contralateral motor area of the moving finger.
	This hypothesis is tested in this paper using the dipole source analysis
	independently for only EEG, only MEG, and both combined. To localize
	the sources, the forward problem is first solved by using the boundary-element
	method for realistic head models and by using a locally-fitted-sphere
	approach for spherical head models consisting of a set of connected
	volumes, typically representing the scalp, skull, and brain. In the
	source reconstruction it is to be expected that EEG predominantly
	localizes radially oriented sources while MEG localizes tangential
	sources at the desired region of the cortex. The effect of MEG on
	EEG is also observed when analyzing both combined data. When comparing
	the two head models, the spherical and the realistic head models
	showed similar results. The significant points for this study are
	comparing the source analysis between the two modalities (EEG and
	MEG) so as to assure that EEG is sensitive to mostly radially orientated
	sources while MEG is only sensitive to only tangential sources, and
	comparing the spherical and individual head models.},
  timestamp = {2016-10-21T13:56:17Z},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 35th Annual 	International Conference of the IEEE},
  author = {Mideksa, K.G. and Hellriegel, H. and Hoogenboom, N. and Krause, H. and Schnitzler, A. and Deuschl, G. and Raethjen, J. and Heute, U. and Muthuraman, M.},
  month = jul,
  year = {2013},
  keywords = {bioelectric potentials,biomechanics,boundary-element method,boundary-elements
	methods,Brain,brain models,contralateral motor area,cortex region,dipole
	source analysis,EEG signal measurement,Electric potential,electroencephalography,forward
	problem,Head,locally-fitted-sphere approach,Magnetic heads,Magnetic
	Resonance Imaging,magnetoencephalography,medical signal processing,MEG
	signal measurement,movement-related cortical potential,multichannel
	electroencephalography,multichannel magnetoencephalography,neurophysiology,physiological
	models,readiness potential,realistic head model,Scalp,self-paced
	finger movement recording,Skull,source localization technique,source
	reconstruction,spherical head model,supplementary motor area,surface
	negative potential,tangential source},
  pages = {1362--1365},
  owner = {Fardin}
}

@article{Mircea2014,
  title = {Basis Pursuit and Null Space Property},
  timestamp = {2016-07-08T11:36:56Z},
  author = {Mircea, Maria},
  month = nov,
  year = {2014},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {afdidehf}
}

@article{Mishali2010,
  title = {From Theory to Practice: Sub-Nyquist Sampling of Sparse Wideband 	Analog Signals},
  volume = {4},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2010.2042414},
  abstract = {Conventional sub-Nyquist sampling methods for analog signals exploit
	prior information about the spectral support. In this paper, we consider
	the challenging problem of blind sub-Nyquist sampling of multiband
	signals, whose unknown frequency support occupies only a small portion
	of a wide spectrum. Our primary design goals are efficient hardware
	implementation and low computational load on the supporting digital
	processing. We propose a system, named the modulated wideband converter,
	which first multiplies the analog signal by a bank of periodic waveforms.
	The product is then low-pass filtered and sampled uniformly at a
	low rate, which is orders of magnitude smaller than Nyquist. Perfect
	recovery from the proposed samples is achieved under certain necessary
	and sufficient conditions. We also develop a digital architecture,
	which allows either reconstruction of the analog input, or processing
	of any band of interest at a low rate, that is, without interpolating
	to the high Nyquist rate. Numerical simulations demonstrate many
	engineering aspects: robustness to noise and mismodeling, potential
	hardware simplifications, real-time performance for signals with
	time-varying support and stability to quantization effects. We compare
	our system with two previous approaches: periodic nonuniform sampling,
	which is bandwidth limited by existing hardware devices, and the
	random demodulator, which is restricted to discrete multitone signals
	and has a high computational load. In the broader context of Nyquist
	sampling, our scheme has the potential to break through the bandwidth
	barrier of state-of-the-art analog conversion technologies such as
	interleaved converters.},
  timestamp = {2016-09-30T11:19:09Z},
  number = {2},
  journal = {IEEE J. Sel. Topics Signal Process.},
  author = {Mishali, M. and Eldar, Y.C.},
  month = apr,
  year = {2010},
  keywords = {analog input reconstruction,analog-to-digital conversion,Analog-to-digital
	conversion (ADC),analogue-digital conversion,blind source separation,blind
	sub-Nyquist sampling,compressive sampling (CS),demodulators,digital
	architecture,digital processing,discrete multitone signal,hardware
	device,infinite measurement vectors (IMV),low-pass filter,low-pass
	filters,modulated wideband converter,multiband sampling,multiband
	signal,numerical analysis,Numerical simulation,periodic nonuniform
	sampling,periodic waveform,random demodulator,signal reconstruction,signal
	sampling,sparse wideband analog signal,spectral support,spectrum-blind
	reconstruction,sub-Nyquist sampling,waveform analysis},
  pages = {375--391},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  owner = {afdidehf}
}

@article{Mishali2009,
  title = {Blind Multiband Signal Reconstruction: Compressed Sensing for Analog 	Signals},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2012791},
  abstract = {We address the problem of reconstructing a multiband signal from its
	sub-Nyquist pointwise samples, when the band locations are unknown.
	Our approach assumes an existing multi-coset sampling. To date, recovery
	methods for this sampling strategy ensure perfect reconstruction
	either when the band locations are known, or under strict restrictions
	on the possible spectral supports. In this paper, only the number
	of bands and their widths are assumed without any other limitations
	on the support. We describe how to choose the parameters of the multi-coset
	sampling so that a unique multiband signal matches the given samples.
	To recover the signal, the continuous reconstruction is replaced
	by a single finite-dimensional problem without the need for discretization.
	The resulting problem is studied within the framework of compressed
	sensing, and thus can be solved efficiently using known tractable
	algorithms from this emerging area. We also develop a theoretical
	lower bound on the average sampling rate required for blind signal
	reconstruction, which is twice the minimal rate of known-spectrum
	recovery. Our method ensures perfect reconstruction for a wide class
	of signals sampled at the minimal rate, and provides a first systematic
	study of compressed sensing in a truly analog setting. Numerical
	experiments are presented demonstrating blind sampling and reconstruction
	with minimal sampling rate.},
  timestamp = {2017-06-23T13:15:58Z},
  number = {3},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Mishali, M. and Eldar, Y.C.},
  month = mar,
  year = {2009},
  keywords = {analog signals,average sampling rate,blind multiband signal reconstruction,compressed
	sensing,data compression,multiband,multicoset
	sampling,multiple measurement vectors (MMV),nonuniform periodic sampling,sampling
	strategy,signal reconstruction,signal sampling,single finite-dimensional
	problem,Sparsity,spectrum recovery,sub-Nyquist pointwise samples},
  pages = {993--1009},
  owner = {afdidehf}
}

@article{Mishali2008,
  title = {Reduce and Boost: Recovering Arbitrary Sets of Jointly Sparse Vectors},
  volume = {56},
  issn = {1053-587X},
  doi = {10.1109/TSP.2008.927802},
  abstract = {The rapid developing area of compressed sensing suggests that a sparse
	vector lying in a high dimensional space can be accurately and efficiently
	recovered from only a small set of nonadaptive linear measurements,
	under appropriate conditions on the measurement matrix. The vector
	model has been extended both theoretically and practically to a finite
	set of sparse vectors sharing a common sparsity pattern. In this
	paper, we treat a broader framework in which the goal is to recover
	a possibly infinite set of jointly sparse vectors. Extending existing
	algorithms to this model is difficult due to the infinite structure
	of the sparse vector set. Instead, we prove that the entire infinite
	set of sparse vectors can be recovered by solving a single, reduced-size
	finite-dimensional problem, corresponding to recovery of a finite
	set of sparse vectors. We then show that the problem can be further
	reduced to the basic model of a single sparse vector by randomly
	combining the measurements. Our approach is exact for both countable
	and uncountable sets, as it does not rely on discretization or heuristic
	techniques. To efficiently find the single sparse vector produced
	by the last reduction step, we suggest an empirical boosting strategy
	that improves the recovery ability of any given suboptimal method
	for recovering a sparse vector. Numerical experiments on random data
	demonstrate that, when applied to infinite sets, our strategy outperforms
	discretization techniques in terms of both run time and empirical
	recovery rate. In the finite model, our boosting algorithm has fast
	run time and much higher recovery rate than known popular methods.},
  timestamp = {2016-09-30T11:20:49Z},
  number = {10},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Mishali, M. and Eldar, Y.C.},
  month = oct,
  year = {2008},
  keywords = {arbitrary sets,Basis pursuit,boosting algorithm,compressed sensing,empirical
	boosting strategy,finite-dimensional problem,jointly sparse vectors,multiple
	measurement vectors (MMV),multiple measurement vectors (MMVs),signal
	representation,single sparse vector,sparse representation,suboptimal
	method},
  pages = {4692--4702},
  owner = {afdidehf}
}

@article{Mishali2011,
  title = {Xampling: Signal Acquisition and Processing in Union of Subspaces},
  volume = {59},
  issn = {1053-587X},
  doi = {10.1109/TSP.2011.2161472},
  abstract = {We introduce Xampling, a unified framework for signal acquisition
	and processing of signals in a union of subspaces. The main functions
	of this framework are two: Analog compression that narrows down the
	input bandwidth prior to sampling with commercial devices followed
	by a nonlinear algorithm that detects the input subspace prior to
	conventional signal processing. A representative union model of spectrally
	sparse signals serves as a test-case to study these Xampling functions.
	We adopt three metrics for the choice of analog compression: robustness
	to model mismatch, required hardware accuracy, and software complexities.
	We conduct a comprehensive comparison between two sub-Nyquist acquisition
	strategies for spectrally sparse signals, the random demodulator
	and the modulated wideband converter (MWC), in terms of these metrics
	and draw operative conclusions regarding the choice of analog compression.
	We then address lowrate signal processing and develop an algorithm
	for that purpose that enables convenient signal processing at sub-Nyquist
	rates from samples obtained by the MWC. We conclude by showing that
	a variety of other sampling approaches for different union classes
	fit nicely into our framework.},
  timestamp = {2016-07-11T17:12:03Z},
  number = {10},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Mishali, M. and Eldar, Y.C. and Elron, A.J.},
  month = oct,
  year = {2011},
  keywords = {analog compression,Analog to digital conversion,baseband processing,compressed
	sensing,Demodulation,Digital signal processing,Hardware,lowrate signal
	processing,measurement,modulated wideband converter,nonlinear algorithm,random
	demodulator,representative union model,signal acquisition,signal
	detection,signal sampling,Software,software complexity,sparse signal,sub-Nyquist,sub-Nyquist
	acquisition strategy,Wideband,Xampling},
  pages = {4719-4734},
  owner = {afdidehf}
}

@article{Mishra2015,
  title = {Spectral Super-Resolution With Prior Knowledge},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2452223},
  abstract = {We address the problem of super-resolution frequency recovery using
	prior knowledge of the structure of a spectrally sparse, undersampled
	signal. In many applications of interest, some structure information
	about the signal spectrum is often known. The prior information might
	be simply knowing precisely some signal frequencies or the likelihood
	of a particular frequency component in the signal. We devise a general
	semidefinite program to recover these frequencies using theories
	of positive trigonometric polynomials. Our theoretical analysis shows
	that, given sufficient prior information, perfect signal reconstruction
	is possible using signal samples no more than thrice the number of
	signal frequencies. Numerical experiments demonstrate great performance
	enhancements using our method. We show that the nominal resolution
	necessary for the grid-free results can be improved if prior information
	is suitably employed.},
  timestamp = {2016-07-11T16:55:30Z},
  number = {20},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Mishra, K.V. and Cho, Myung and Kruger, A. and Xu, Weiyu},
  month = oct,
  year = {2015},
  keywords = {Atomic clocks,atomic norm,block prior,compressed sensing,Estimation,frequency
	component,frequency-domain analysis,general semidefinite program,known
	poles,Minimization,optimisation,Polynomials,positive trigonometric
	polynomials,prior knowledge,probabilistic prior,signal reconstruction,signal
	reconstruction,signal resolution,signal sample,signal sampling,signal
	sampling,signal spectrum,signal
	spectrum,Spectral analysis,spectrally sparse signal,spectral super-resolution
	frequency recovery,spectral
	super-resolution frequency recovery,super-resolution,undersampled signal,undersampled
	signal},
  pages = {5342-5357},
  owner = {afdidehf}
}

@inproceedings{Mohapatra2015,
  title = {An experimental analysis of an advanced OMP algorithm for image compression 	in an OSCC approach},
  doi = {10.1109/EESCO.2015.7253823},
  abstract = {Image compression has been a widely researched field for decades.
	Several methods are available and standards exist that provide high
	compression ratios for a given subjective quality. From pervious
	analysis the DCT and wavelet based compression methods suffer from
	blocking and ringing artifacts and also they are unable to capture
	the directional information. So, in this context an investigation
	has been made to this paper by using sparse coding method by an advanced
	orthogonal matching pursuit (OMP). In this proposed framework, the
	conventional DCT can be replaced by a set of trained dictionaries.
	For dictionary construction we have used a combination of DCT and
	Gabor basis and to encode the trained dictionary elements we have
	employed advanced OMP algorithm. Experimental results demonstrate
	that the proposed method provides gains in rate distortion (RD) performance
	and improvements in perceptual quality.},
  timestamp = {2016-07-08T11:23:49Z},
  booktitle = {Electrical, Electronics, Signals, Communication and Optimization 	(EESCO), 2015 International Conference on},
  author = {Mohapatra, S.K. and Swain, B.R. and Mahapatra, S.K.},
  month = jan,
  year = {2015},
  keywords = {Approximation algorithms,Approximation methods,blocking artifacts,compressed
	sensing,data compression,DCT,Dictionaries,discrete cosine transform,discrete cosine transforms,Discrete
	cosine transforms,Gabor basis,Gabor filters,Image
	coding,image compression,Image resolution,iterative methods,iterative
	methods,Matching pursuit algorithms,Matching
	pursuit algorithms,OMP algorithm,orthogonal matching pursuit,orthogonal matching
	pursuit,orthogonal
	matching pursuit,OSCC approach,over-complete dictionary,perceptual
	quality,ringing artifacts,Sparse Coding,sparse coding method,time-frequency
	analysis,Transform coding,Visual Perception,wavelet based compression
	method,wavelet transforms},
  pages = {1-5},
  owner = {afdidehf}
}

@article{Mohimani2009,
  title = {A Fast Approach for Overcomplete Sparse Decomposition Based on Smoothed 	$\ell ^{0}$ Norm},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2008.2007606},
  abstract = {In this paper, a fast algorithm for overcomplete sparse decomposition,
	called SL0, is proposed. The algorithm is essentially a method for
	obtaining sparse solutions of underdetermined systems of linear equations,
	and its applications include underdetermined sparse component analysis
	(SCA), atomic decomposition on overcomplete dictionaries, compressed
	sensing, and decoding real field codes. Contrary to previous methods,
	which usually solve this problem by minimizing the l 1 norm using
	linear programming (LP) techniques, our algorithm tries to directly
	minimize the l 1 norm. It is experimentally shown that the proposed
	algorithm is about two to three orders of magnitude faster than the
	state-of-the-art interior-point LP solvers, while providing the same
	(or better) accuracy.},
  timestamp = {2016-07-08T10:15:24Z},
  number = {1},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Mohimani, H. and Babaie-Zadeh, Massoud and Jutten, C.},
  month = jan,
  year = {2009},
  keywords = {atomic decomposition,blind source separation (BSS),compressed sensing,compressed
	sensing,linear equations,linear programming,linear programming techniques,LP
	solvers,minimisation,Minimization,overcomplete dictionary,overcomplete
	signal representation,overcomplete sparse decomposition,Principal
	component analysis,real field codes,signal representation,SL0,Sparse
	component analysis,sparse component analysis (SCA),sparse decomposition,sparse
	source separation,underdetermined systems},
  pages = {289-301},
  owner = {Fardin}
}

@inproceedings{Mohseni2013,
  title = {A new approach to the fusion of EEG and MEG signals using the LCMV 	beamformer},
  doi = {10.1109/ICASSP.2013.6637841},
  abstract = {In this paper, we demonstrate a new approach for the fusion of multichannel
	signals. We show how this method can be used to combine signals from
	magnetometer and gradiometer sensors used in magnetoencephalography
	(MEG). This approach works by assuming that the lead-fields have
	multiplicative errors which in turn leads to an under-determined
	problem. To solve this problem, we impose two constraints that result
	in closed-from solutions: i) one set of sensors is error-free, ii)
	the norm of the multiplicative error is bounded. These prior assumptions
	to estimate the error are used in the linearly constraint minimum
	variance (LCMV) spatial filter to improve the optimisation. Although
	we focus on the fusion of MEG sensors, this approach can be employed
	for multimodal fusion of other multichannel signals such as MEG and
	EEG signals.},
  timestamp = {2016-10-21T13:53:56Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), IEEE International 	Conference on},
  author = {Mohseni, H.R. and Kringelbach, M.L. and Woolrich, M.W. and Aziz, T.Z. and Smith, P.P.},
  month = may,
  year = {2013},
  keywords = {array signal processing,EEG signal fusion,electroencephalography,Face,gradiometer,gradiometer
	sensors,LCMV beamformer,linearly constraint minimum variance spatial
	filter,Magnetic resonance imaging,Magnetic sensors,magnetoencephalography,magnetometer,magnetometers,magnetometer
	sensors,medical signal processing,MEG sensors,MEG signal fusion,multichannel
	signal fusion,multimodal fusion,multiplicative errors,optimisation,sensor fusion,sensor
	fusion,Signal to noise ratio,spatial filters,under-determined
	problem},
  pages = {1202--1206},
  annote = {read},
  owner = {Fardin}
}

@article{Mohseni2012,
  title = {Fusion of Magnetometer and Gradiometer Sensors of MEG in the Presence 	of Multiplicative Error},
  volume = {59},
  issn = {0018-9294},
  doi = {10.1109/TBME.2012.2195001},
  abstract = {Novel neuroimaging techniques have provided unprecedented information
	on the structure and function of the living human brain. Multimodal
	fusion of data from different sensors promises to radically improve
	this understanding, yet optimal methods have not been developed.
	Here, we demonstrate a novel method for combining multichannel signals.
	We show how this method can be used to fuse signals from the magnetometer
	and gradiometer sensors used in magnetoencephalography (MEG), and
	through extensive experiments using simulation, head phantom and
	real MEG data, show that it is both robust and accurate. This new
	approach works by assuming that the lead fields have multiplicative
	error. The criterion to estimate the error is given within a spatial
	filter framework such that the estimated power is minimized in the
	worst case scenario. The method is compared to, and found better
	than, existing approaches. The closed-form solution and the conditions
	under which the multiplicative error can be optimally estimated are
	provided. This novel approach can also be employed for multimodal
	fusion of other multichannel signals such as MEG and EEG. Although
	the multiplicative error is estimated based on beamforming, other
	methods for source analysis can equally be used after the lead-field
	modification.},
  timestamp = {2016-10-21T13:51:12Z},
  number = {7},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Mohseni, H.R. and Woolrich, M.W. and Kringelbach, M.L. and Luckhoo, H. and Smith, P.P. and Aziz, T.Z.},
  month = jul,
  year = {2012},
  keywords = {Brain,closed-form solution,Computer-Assisted,Computer Simulation,Covariance
	matrix,gradiometer,gradiometer sensors,head phantom,Humans,Image
	Processing,imaging,Lead,lead-field modification,living human brain,Magnetic
	sensors,magnetoencephalography,Magnetoencephalography (MEG),magnetometer,magnetometers,medical
	signal processing,MEG data,Monte Carlo Method,multichannel signals,multimodal
	data fusion,multiplicative error,neuroimaging techniques,neuroimaging
	techniques,optimal methods,optimal
	methods,phantoms,Photic Stimulation,sensor fusion,signal processing,signal
	processing,Signal to noise ratio,Signal
	to noise ratio,source analysis,spatial filter framework,worst
	case scenario,worst case
	scenario},
  pages = {1951--1961},
  owner = {afdidehf}
}

@inproceedings{Montoya-Martinez2012,
  title = {Structured sparsity regularization approach to the EEG inverse problem},
  doi = {10.1109/CIP.2012.6232898},
  abstract = {Localization of brain activity involves solving the EEG inverse problem,
	which is an undetermined ill-posed problem. We propose a novel approach
	consisting in estimating, using structured sparsity regularization
	techniques, the Brain Electrical Sources (BES) matrix directly in
	the spatio-temporal source space. We use proximal splitting optimization
	methods, which are efficient optimization techniques, with good convergence
	rates and with the ability to handle large nonsmooth convex problems,
	which is the typical scenario in the EEG inverse problem. We have
	evaluated our approach under a simulated scenario, consisting in
	estimating a synthetic BES matrix with 5124 sources. We report results
	using ?1 (LASSO), ?1/?2 (Group LASSO) and ?1 + ?1/?2 (Sparse Group
	LASSO) regularizers.},
  timestamp = {2016-07-11T16:58:37Z},
  booktitle = {Cognitive Information Processing (CIP), 2012 3rd International Workshop 	on},
  author = {Montoya-Martinez, J. and Artes-Rodriguez, A. and Hansen, L.K. and Pontil, M.},
  month = may,
  year = {2012},
  keywords = {BES,brain electrical sources matrix,Brain modeling,Conferences,EEG
	inverse problem,Electrodes,electroencephalography,good convergence,inverse problems,Inverse
	problems,large nonsmooth convex problems,medical
	signal processing,optimisation,Optimization,proximal splitting optimization
	methods,sparse matrices,spatio-temporal source space,structured sparsity
	regularization approach,undetermined ill-posed problem},
  pages = {1-6},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@inproceedings{Morikawa2013,
  title = {A sparse optimization approach to supervised NMF based on convex 	analytic method},
  doi = {10.1109/ICASSP.2013.6638832},
  abstract = {In this paper, we propose a novel scheme to supervised nonnegative
	matrix factorization (NMF). We formulate the supervised NMF as a
	sparse optimization problem assuming the availability of a set of
	basis vectors, some of which are irrelevant to a given matrix to
	be decomposed. The proposed scheme is presented in the context of
	music transcription and musical instrument recognition. In addition
	to the nonnegativity constraint, we introduce three regularization
	terms: (i) a block ?1 norm to select relevant basis vectors, and
	(ii) a temporal-continuity term plus the popular ?1 norm to estimate
	correct activation vectors. We present a state-of-the-art convex-analytic
	iterative solver which ensures global convergence. The number of
	basis vectors to be actively used is obtained as a consequence of
	optimization. Simulation results show the efficacy of the proposed
	scheme both in the case of perfect/imperfect basis matrices.},
  timestamp = {2016-07-08T11:29:16Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Morikawa, Yu. and Yukawa, M.},
  month = may,
  year = {2013},
  keywords = {Convergence,convergence of numerical methods,convex analysis,convex-analytic
	iterative solver,convex analytic method-based supervised NMF,Convex
	functions,convex programming,global convergence,Instruments,iterative
	methods,matrix decomposition,MUSIC,musical instrument recognition,musical instrument
	recognition,musical instruments,musical
	instruments,music transcription,nonnegativity constraint,nonnegativity
	constraint,Optimization,perfect-imperfect basis matrices,perfect-imperfect
	basis matrices,sparse matrices,sparse optimization,sparse
	optimization,sparse optimization approach,sparse optimization
	approach,supervised nonnegative matrix factorization,supervised nonnegative
	matrix factorization,temporal-continuity term,temporal-continuity
	term,Vectors},
  pages = {6078-6082},
  owner = {afdidehf}
}

@article{Morrow2011,
  title = {Norms},
  timestamp = {2016-07-10T07:00:34Z},
  author = {Morrow, Jim},
  month = sep,
  year = {2011},
  owner = {afdidehf}
}

@inproceedings{Mosher1995,
  title = {Matrix kernels for MEG and EEG source localization and imaging},
  volume = {5},
  doi = {10.1109/ICASSP.1995.479462},
  abstract = {The most widely used model for electroencephalography (EEG) and magnetoencephalography
	(MEG) assumes a quasi-static approximation of Maxwell's equations
	and a piecewise homogeneous conductor model. Both models contain
	an incremental field element that linearly relates an incremental
	source element (current dipole) to the field or voltage at a distant
	point. The explicit form of the field element is dependent on the
	head modeling assumptions and sensor configuration. Proper characterization
	of this incremental element is crucial to the inverse problem. The
	field element can be partitioned into the product of a vector dependent
	on sensor characteristics and a matrix kernel dependent only on head
	modeling assumptions. The authors present the matrix kernels for
	the general boundary element model (BEM) and for MEG spherical models.
	They show how these kernels are easily interchanged in a linear algebraic
	framework that includes sensor specifics such as orientation and
	gradiometer configuration. They then describe how this kernel is
	easily applied to �gain� or �transfer� matrices used in multiple
	dipole and source imaging models},
  timestamp = {2016-07-09T20:08:58Z},
  booktitle = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 	International Conference on},
  author = {Mosher, J.C. and Leahy, R.M. and Lewis, P.S.},
  month = may,
  year = {1995},
  keywords = {boundary element model,boundary-elements methods,Brain modeling,Conductors,EEG,electroencephalography,gradiometer
	configuration,head modeling,Image reconstruction,imaging,incremental
	field element,inverse problem,inverse problems,Kernel,linear algebraic
	framework,Magnetic heads,magnetoencephalography,matrix kernel,Maxwell
	equations,Maxwell's equations,medical image processing,MEG,multiple
	dipole model,orientation,piecewise homogeneous conductor model,quasi-static
	approximation,sensor configuration,Sensor phenomena and characterization,Source
	localization,transfer function matrices,transfer matrices,Voltage},
  pages = {2943-2946 vol.5},
  owner = {afdidehf}
}

@inproceedings{Mosher1992,
  title = {Error bounds for MEG and EEG source localization},
  volume = {1},
  doi = {10.1109/ACSSC.1992.269277},
  abstract = {Localization error bounds are presented for both electroencephalograms
	(EEGs) and magnetoencephalograms (MEGs) as graphical error contours
	for a 37-sensor arrangement. Both one and two dipole cases were examined
	for all possible dipole orientations and locations within a head
	quadrant. The results show a strong dependence on absolute dipole
	location and orientation. The results also show that fusion of the
	EEG and MEG measurements into a combined model reduces the lower
	bound. A Monte Carlo simulation was performed to check the tightness
	of the bounds for a selected case. The simple head model, the white
	and relatively low power noise, and the few relatively strong dipoles
	were all selected in this study as optimistic conditions to establish
	possibly fundamental resolution limits for any localization effort},
  timestamp = {2016-10-21T13:25:49Z},
  booktitle = {Signals, Systems and Computers, Conference Record of The 	Twenty-Sixth Asilomar Conference on},
  author = {Mosher, J.C. and Spencer, M.E. and Leahy, R.M. and Lewis, P.S.},
  month = oct,
  year = {1992},
  keywords = {37-sensor arrangement,biomagnetism,Biomedical measurement,Brain modeling,dipole
	orientations,EEG source localization,electroencephalography,fundamental
	resolution limits,graphical error contours,head quadrant,Humans,Image
	Processing,image sensors,Laboratories,magnetic field measurement,Magnetic
	heads,Measurement errors,MEG source localization,Monte Carlo simulation,Optimization
	methods,signal processing,simple head model},
  pages = {150--155},
  owner = {afdidehf}
}

@inproceedings{Mun2009,
  title = {Block compressed sensing of images using directional transforms},
  doi = {10.1109/ICIP.2009.5414429},
  abstract = {Block-based random image sampling is coupled with a projection-driven
	compressed-sensing recovery that encourages sparsity in the domain
	of directional transforms simultaneously with a smooth reconstructed
	image. Both contourlets as well as complex-valued dual-tree wavelets
	are considered for their highly directional representation, while
	bivariate shrinkage is adapted to their multiscale decomposition
	structure to provide the requisite sparsity constraint. Smoothing
	is achieved via a Wiener filter incorporated into iterative projected
	Landweber compressed-sensing recovery, yielding fast reconstruction.
	The proposed approach yields images with quality that matches or
	exceeds that produced by a popular, yet computationally expensive,
	technique which minimizes total variation. Additionally, reconstruction
	quality is substantially superior to that from several prominent
	pursuits-based algorithms that do not include any smoothing.},
  timestamp = {2016-07-08T11:41:27Z},
  booktitle = {Image Processing (ICIP), 2009 16th IEEE International Conference 	on},
  author = {Mun, Sungkwang and Fowler, J.E.},
  month = nov,
  year = {2009},
  keywords = {bivariate shrinkage,block-based random image sampling,block compressed
	sensing,complex-valued dual-tree wavelets,compressed sensing,compressed
	sensing,Contourlets,data compression,directional transforms,Discrete
	wavelet transforms,dual-tree discrete wavelet transform,highly directional
	representation,image coding,Image reconstruction,image sampling,image
	sampling,image smooth reconstruction,Landweber compressed-sensing
	recovery,Matching pursuit algorithms,projection-driven compressed-sensing
	recovery,pursuits-based algorithms,signal sampling,smoothing methods,smoothing
	methods,sparse matrices,transforms,trees (mathematics),wavelet transforms,Wiener
	filter,Wiener filters},
  pages = {3021-3024},
  owner = {afdidehf}
}

@article{Muravchik2001,
  title = {EEG/MEG Error Bounds For A Static Dipole Source With A Realistic 	Head Model},
  volume = {49},
  issn = {1053-587X},
  doi = {10.1109/78.905859},
  abstract = {We derive Cramer-Rao bounds (CRBs) on the errors of estimating the
	parameters (location and moment) of a static current dipole source
	using data from electro-encephalography (EEG), magneto-encephalography
	(MEG), or the combined EEG/MEG modality. We use a realistic head
	model based on knowledge of surfaces separating tissues of different
	conductivities obtained from magnetic resonance (MR) or computer
	tomography (CT) imaging systems. The electric potentials and magnetic
	field components at the respective sensors are functions of the source
	parameters through integral equations. These potentials and field
	are formulated for solving them by the boundary or the finite element
	method (BEM or FEM) with a weighted residuals technique. We present
	a unified framework for the measurements computed by these methods
	that enables the derivation of the bounds. The resulting bounds may
	be used, for instance, to choose the best configuration of the sensors
	for a given patient and region of expected source location. Numerical
	results are used to demonstrate an application for showing expected
	accuracies in estimating the source parameters as a function of its
	position in the brain, based on real EEG/MEG system and MR or CT
	images},
  timestamp = {2016-07-08T12:17:24Z},
  number = {3},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Muravchik, C.H. and Nehorai, Arye},
  month = mar,
  year = {2001},
  keywords = {BEM,biomedical MRI,boundary element method,boundary-elements methods,Brain,Brain
	modeling,Computed tomography,Computer errors,computerised tomography,computer
	tomography,Conductivity,Cramer-Rao bounds,CT images,CT imaging system,diagnostic
	radiography,EEG/MEC error bounds,Electric potential,electric potentials,electroencephalography,FEM,finite
	element analysis,finite element method,head model,integral equations,location
	estimation,magnetic field components,magnetic field measurement,Magnetic
	fields,Magnetic heads,Magnetic resonance imaging,Magnetic sensors,Magnetic
	separation,magnetoencephalography,medical image processing,moment
	estimation,MRI,parameter estimation,patient,Sensors,source parameters,source
	parameters,static current dipole source,static
	current dipole source,static dipole source,tissue conductivity,tissue
	conductivity,weighted residuals technique,weighted
	residuals technique},
  pages = {470-484},
  owner = {afdidehf}
}

@inproceedings{Muravchik2000,
  title = {Error bounds of EEG/MEG for a stationary dipole source with a realistic 	head model},
  volume = {6},
  doi = {10.1109/ICASSP.2000.860221},
  abstract = {We derive Cramer-Rao bounds (CRBs) on the errors of estimating the
	parameters (location and moment) of a current dipole source using
	data from electro-encephalography (EEG), magneto-encephalography
	(MEG), or the combined EEG/MEG modality. We use a realistic head
	model based on knowledge of surfaces separating tissues of different
	conductivities, obtained from magnetic resonance (MR) or computer
	tomography (CT) imaging systems. The electric potentials and magnetic
	field components at the respective sensors are functions of the source
	parameters through integral equations. These potentials and field
	are computed using the boundary or the finite element method (BEM
	or FEM), with a weighted residuals technique. We present a unified
	framework for the measurements computed by these methods that enables
	the derivation of the bounds. The resulting bounds may be used, for
	instance, to choose the best configuration of the sensors for a given
	patient and region of expected source location. Numerical results
	are used to demonstrate an application for shelving expected accuracies
	in estimating the source parameters as a function of its position
	in the brain, based on real EEG/MEG system and MR or CT images. The
	results include contours of equal precision in the estimation and
	surfaces showing the size of the 90% confidence volume for a dipole
	on a sphere inside the brain},
  timestamp = {2016-07-08T12:21:32Z},
  booktitle = {Acoustics, Speech, and Signal Processing, 2000. ICASSP '00. Proceedings. 	2000 IEEE International Conference on},
  author = {Muravchik, C.H. and Nehorai, Arye},
  year = {2000},
  keywords = {BEM,bioelectric potentials,biomedical MRI,Brain,Brain modeling,Computed
	tomography,Computer errors,computerised tomography,computer tomography,Conductivity,confidence
	volume,Cramer-Rao bounds,CT imaging systems,EEG/MEG,electric potentials,electroencephalography,Electro-encephalography,error
	bounds,FEM,finite element analysis,finite element method,head model,Integral
	equations,magnetic field components,magnetic field measurement,Magnetic
	fields,Magnetic heads,magnetic resonance,Magnetic sensors,Magnetic
	separation,magnetoencephalography,Magneto-encephalography,MR imaging
	systems,parameter estimation,parameter estimation errors,patient,patient
	care,source parameters,sphere,stationary dipole source,tissue conductivity,weighted
	residuals technique},
  pages = {3763-3766 vol.6},
  owner = {afdidehf}
}

@inproceedings{Muravchik1997,
  title = {MEG/EEG numerical error bounds for a dipole source with a realistic 	head model},
  volume = {3},
  doi = {10.1109/IEMBS.1997.756590},
  abstract = {Measures of performance for magnetoencephalography (MEG), electroencephalography
	(EEG), and the combined MEG/EEG modality are needed in order to compare,
	improve, and to optimize existing systems and their configuration.
	We numerically compute bounds on the mean-squared errors when estimating
	a current dipole's source parameters for a three layered head model
	using measurements of EEG, MEG and their combination. The electric
	potentials and magnetic field components are related to the source
	through integral equations obtained via the boundary element method
	(BEM). Discretization is achieved with a weighted residuals technique,
	producing a center of gravity method. The bounds obtained assume
	knowledge of the layer's conductivities and an accurate representation
	of the interface surfaces, as obtained from MR or CT imaging},
  timestamp = {2016-07-09T20:10:42Z},
  booktitle = {Engineering in Medicine and Biology Society, 1997. Proceedings of 	the 19th Annual International Conference of the IEEE},
  author = {Muravchik, C.H. and Nehorai, Arye},
  month = oct,
  year = {1997},
  keywords = {boundary element method,Boundary element methods,boundary-elements
	methods,Brain modeling,center of gravity method,combined MEG/EEG
	modality,Cramer-Rao bounds,Current distribution,Current measurement,dipole
	source,discretization,electric field integral equations,Electric
	potential,electric potentials,electroencephalography,Error analysis,Gravity,integral equations,Integral
	equations,interface surfaces representation,magnetic
	field components,magnetic field integral equations,magnetic field
	measurement,Magnetic heads,magnetoencephalography,mean-squared errors,mean
	square error methods,medical signal processing,numerical error bounds,parameter
	estimation,physiological models,position error bounds,realistic head
	model,source parameters estimation,three layered head model,weighted
	residuals technique},
  pages = {1233-1236 vol.3},
  owner = {afdidehf}
}

@article{Nagmote2013,
  title = {Review: Sparse Representation for Face Recognition Application},
  volume = {4},
  timestamp = {2016-07-10T08:04:00Z},
  number = {5},
  journal = {International Journal of Engineering Trends and Technology (IJETT)},
  author = {Nagmote, Minakshi S. and Mushrif, Milind M.},
  month = may,
  year = {2013},
  keywords = {face recognition,l1-minimization,sparse representation},
  pages = {1772-1775},
  owner = {Fardin}
}

@inproceedings{Naleer2012,
  title = {A new two-step face hallucination through block of coefficients},
  volume = {1},
  doi = {10.1109/CSAE.2012.6272588},
  abstract = {We introduce a two-step face hallucination frame work as one of classifying
	among sparse residual compensation model. In the first step, the
	optimal coefficients of the interpolated training images are used
	to construct a global face image. In the second step, a class of
	priors is computed based on mixing a set of linear priors related
	to dissimilar priors. The blocks of coefficients are considered to
	find the sparse mixing weights. In order to find the best improved
	information of the face image in the residual compensation of step-two,
	a sparse signal representation is considered over coefficients in
	a frame. Finally, we obtain a hallucinated face image by integrating
	these two steps. The extensive experiments on publicly available
	database show the effectiveness of the framework.},
  timestamp = {2016-07-08T11:23:38Z},
  booktitle = {Computer Science and Automation Engineering (CSAE), 2012 IEEE International 	Conference on},
  author = {Naleer, H.M.M. and Lu, Yao},
  month = may,
  year = {2012},
  keywords = {block coefficients,Dictionaries,dissimilar priors,Equations,Face,Face
	recognition,global face image construction,image classification,Image
	classification,Image resolution,Integration,interpolated training
	images,interpolation,Learning method,linear priors,mathematical model,minimisation,mixing
	prior,optimal coefficients,residual compensation,sparse matrices,sparse
	mixing weights,sparse residual compensation model,sparse signal representation,Training,two-step
	face hallucination frame work,Vectors},
  pages = {237-241},
  owner = {afdidehf}
}

@inproceedings{Nana2011,
  title = {Block adaptive compressed sensing of SAR images based on statistical 	character},
  doi = {10.1109/IGARSS.2011.6049210},
  abstract = {Block-based processing has shown promise to reduce computation complexity
	and storage space for image Compressed Sensing. In this paper, a
	new architecture for SAR images is proposed, as an improvement for
	traditional Block Compressed Sensing of natural images. The proposed
	scheme adopts the basic structure of existing Block Compressed Sensing,
	and studies the character of SAR images. Based on the difference
	of statistical property among sub blocks, the proposed scheme can
	adaptively select the number of measurements that needed to take
	for every sub blocks. Different from equality measurement, adaptive
	sampling can sufficiently capture the diversity between sub blocks
	and keep their properties well. Several numeral experiments also
	demonstrate that the proposed approach outperforms the existing scheme,
	achieving comparable reconstruction quality via fewer measurements.},
  timestamp = {2016-07-08T11:40:35Z},
  booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2011 IEEE International},
  author = {Nana, Wang and Jingwen, Li},
  month = jul,
  year = {2011},
  keywords = {Adaptive sampling,block adaptive compressed sensing,block-based processing,compressed sensing,compressed
	sensing,computational complexity,image coding,Image
	coding,image compressed sensing,Image edge detection,Image Processing,Image
	reconstruction,image sampling,image sensors,natural images,PSNR,reconstruction
	quality,SAR image,SAR images,sparse matrices,Sparsity,statistical
	analysis,statistical character,statistical properties,storage space,synthetic
	aperture radar,transforms},
  pages = {640-643},
  owner = {afdidehf}
}

@article{Needell2015,
  title = {Recovering overcomplete sparse representations from structured sensing},
  abstract = {In many signal processing applications, one wishes to acquire images
	that are approximately sparse in transform domains such as wavelets
	using frequency domain samples. Often the quality of the sparsity
	based model significantly improves when one considers redundant representation
	systems such as wavelet frames. To date, compressed sensing with
	redundant representation systems has, however, only been studied
	for measurement systems that have certain concentration properties,
	which is not the case for frequency domain samples. In this talk,
	we close this gap, providing more general reconstruction guarantees
	for signals that are sparse with respect to redundant systems. This
	is joint work with Felix Krahmer and Rachel Ward.},
  timestamp = {2016-07-10T07:41:56Z},
  author = {Needell, Deanna},
  month = feb,
  year = {2015},
  note = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Needell2009a,
  title = {Uniform Uncertainty Principle and Signal Recovery via Regularized 	Orthogonal Matching Pursuit},
  volume = {9},
  issn = {1615-3375},
  doi = {10.1007/s10208-008-9031-3},
  abstract = {This paper seeks to bridge the two major algorithmic approaches to
	sparse signal recovery from an incomplete set of linear measurements�L1-minimization
	methods and iterative methods (Matching Pursuits). We find a simple
	regularized version of Orthogonal Matching Pursuit (ROMP) which has
	advantages of both approaches: the speed and transparency of OMP
	and the strong uniform guarantees of L1-minimization. Our algorithm,
	ROMP, reconstructs a sparse signal in a number of iterations linear
	in the sparsity, and the reconstruction is exact provided the linear
	measurements satisfy the uniform uncertainty principle.},
  language = {English},
  timestamp = {2016-07-11T17:10:06Z},
  number = {3},
  journal = {Foundations of Computational Mathematics},
  author = {Needell, Deanna and Vershynin, Roman},
  year = {2009},
  keywords = {41A46,65T50,68W20,Basis pursuit,compressed sensing,orthogonal matching
	pursuit,Restricted isometry condition,signal recovery,Signal recovery
	algorithms,sparse approximation,uncertainty principle},
  pages = {317-334},
  owner = {afdidehf}
}

@article{Negahban2011,
  title = {Simultaneous Support Recovery in High Dimensions: Benefits and Perils 	of Block $\ell_{1}/\ell_{\infty }$-Regularization},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2144150},
  abstract = {Given a collection of r ? 2 linear regression problems in p dimensions,
	suppose that the regression coefficients share partially common supports
	of size at most s. This set-up suggests the use of ?1/??-regularized
	regression for joint estimation of the p�r matrix of regression
	coefficients. We analyze the high-dimensional scaling of ?1/??-regularized
	quadratic programming, considering both consistency rates in ??-norm,
	and how the minimal sample size n required for consistent variable
	selection scales with model dimension, sparsity, and overlap between
	the supports. We first establish bounds on the ??-error as well sufficient
	conditions for exact variable selection for fixed design matrices,
	as well as for designs drawn randomly from general Gaussian distributions.
	Specializing to the case r = 2 linear regression problems with standard
	Gaussian designs whose supports overlap in a fraction ? ? [0,1] of
	their entries, we prove that ?1/??-regularized method undergoes a
	phase transition characterized by the rescaled sample size ?1,?(n,
	p, s, ?) = n/{(4 - 3 ?) s log(p-(2- ?) s)}. An implication is that
	the use of ?1/??-regularization yields improved statistical efficiency
	if the overlap parameter is large enough ( ? >; 2/3), but has worse
	statistical efficiency than a naive Lasso-based approach for moderate
	to small overlap (? <; 2/3 ). Empirical simulations illustrate the
	close agreement between theory and actual behavior in practice. These
	results show that caution must be exercised in applying ?1/?? block
	regularization: if the data does not match its structure very closely,
	it can impair statistical performance relative to computationally
	less expensive schemes.},
  timestamp = {2016-07-10T08:19:05Z},
  number = {6},
  journal = {Information Theory, IEEE Transactions on},
  author = {Negahban, S.N. and Wainwright, M.J.},
  month = jun,
  year = {2011},
  keywords = {$ell _{1}$-constraints,block ?1/??-regularization,compressed sensing,convex
	relaxation,Estimation,Gaussian distribution,Gaussian distributions,group
	Lasso,high-dimensional inference,Input variables,Joints,Linear regression,Linear
	regression,Model selection,Multivariate regression,Noise,phase transitions,quadratic programming,quadratic
	programming,regression analysis,simultaneous
	support recovery,sparse approximation,subset selection,Symmetric
	matrices},
  pages = {3841-3863},
  owner = {afdidehf}
}

@inproceedings{Nguyen2011,
  title = {Video error concealment using sparse recovery and local dictionaries},
  doi = {10.1109/ICASSP.2011.5946606},
  abstract = {Video error concealment is a post-processing technique that conceals
	the errors in a decoded video sequence based on data available only
	at the decoder. Most of the current techniques adopt the approach
	that recovers the Motion Vector (MV) of a lost image block, uses
	that MV to look for data to fill in the blank then performs some
	refinements. We propose a method that does not rely on MV recovery,
	but essentially bases on sparse representation of image patches on
	local temporal dictionaries. Experiment results show a large improvement
	over Boundary Matching Algorithm (BMA), the standard method used
	in reference software for H.264 video codec.},
  timestamp = {2016-07-11T17:11:33Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International 	Conference on},
  author = {Nguyen, Dzung and Dao, Minh and Tran, Trac},
  month = may,
  year = {2011},
  keywords = {BMA,boundary matching algorithm,decoded video sequence,Decoding,Dictionaries,encoding,Error
	concealment,H.264 video codec,image matching,image motion analysis,image
	patches,Image reconstruction,image representation,image sequences,l1
	minimization,Local dictionary,local temporal dictionary,lost image
	block,motion vector,post-processing technique,PSNR,sparse recovery,Streaming
	media,video codecs,video coding,video error concealment},
  pages = {1125-1128},
  owner = {afdidehf}
}

@inproceedings{Nikolakopoulos2014,
  title = {NCDREC: A Decomposability Inspired Framework for Top-N Recommendation},
  volume = {1},
  doi = {10.1109/WI-IAT.2014.32},
  abstract = {Building on the intuition behind Nearly Decomposable systems, we propose
	NCDREC, a top-N recommendation framework designed to exploit the
	innately hierarchical structure of the item space to alleviate Sparsity,
	and the limitations it imposes to the quality of recommendations.
	We decompose the item space to define blocks of closely related elements
	and we introduce corresponding indirect proximity components that
	try to fill in the gap left by the inherent sparsity of the data.
	We study the theoretical properties of the decomposition and we derive
	sufficient conditions that guarantee full item space coverage even
	in cold-start recommendation scenarios. A comprehensive set of experiments
	on the Movie Lens and the Yahoo!R2Music datasets, using several widely
	applied performance metrics, support our model's theoretically predicted
	properties and verify that NCDREC outperforms several state-of-the-art
	algorithms, in terms of recommendation accuracy, diversity and sparseness
	insensitivity.},
  timestamp = {2016-07-10T06:48:10Z},
  booktitle = {Web Intelligence (WI) and Intelligent Agent Technologies (IAT), 2014 	IEEE/WIC/ACM International Joint Conferences on},
  author = {Nikolakopoulos, A.N. and Garofalakis, J.D.},
  month = aug,
  year = {2014},
  keywords = {cold-start recommendation scenarios,Computational modeling,Decomposability,decomposability
	inspired framework,full item space coverage,indirect proximity components,information filtering,information
	filtering,item space decomposition,Long-Tail
	Recommendation,Markov Chain Models,Markov processes,matrix decomposition,measurement,MovieLens
	dataset,NCDREC,nearly decomposable systems,performance metrics,Ranking,recommendation
	accuracy,recommendation diversity,recommender systems,Recommender
	systems,sparse matrices,sparseness insensitivity,Sparsity,Sufficient
	conditions,top-N recommendation framework,Vectors,Yahoo!R2Music dataset},
  pages = {183-190},
  owner = {afdidehf}
}

@unpublished{Nolte2012,
  address = {Berlin},
  title = {Tutorial on EEG/MEG inverse source reconstruction},
  timestamp = {2016-10-26T14:07:02Z},
  author = {Nolte, Guido and Haufe, Stefan},
  year = {2012},
  note = {BBCI Summer School 2012, Berlin},
  keywords = {beamformers,dipole fit,distributed inverse solutions,EEG,inverse source
	reconstruction,MEG,subspace methods},
  annote = {http://videolectures.net/bbci2012_haufe_nolte_source_reconstruction/},
  annote = {http://videolectures.net/bbci2012_haufe_nolte_source_reconstruction/},
  annote = {http://videolectures.net/bbci2012_haufe_nolte_source_reconstruction/},
  annote = {http://videolectures.net/bbci2012_haufe_nolte_source_reconstruction/},
  annote = {http://videolectures.net/bbci2012_haufe_nolte_source_reconstruction/},
  annote = {PPT},
  annote = {read},
  owner = {Fardin}
}

@unpublished{Noorzad1390,
  title = {Sparse Coding and Dictionary Learning},
  timestamp = {2016-07-11T16:44:02Z},
  author = {Noorzad, Pardis},
  month = jun,
  year = {1390},
  note = {Amirkabir University of Technology},
  annote = {Amirkabir University of Technology PPT},
  annote = {Amirkabir University of Technology PPT},
  annote = {Amirkabir University of Technology PPT},
  annote = {Amirkabir University of Technology PPT},
  annote = {Amirkabir University of Technology PPT},
  owner = {Fardin}
}

@article{Nucinkis2015,
  title = {Matrix norms},
  timestamp = {2016-07-09T20:09:01Z},
  author = {Nucinkis, Daniel},
  year = {2015},
  owner = {afdidehf}
}

@inproceedings{OHanlon2012,
  title = {Structured sparsity for automatic music transcription},
  doi = {10.1109/ICASSP.2012.6287911},
  abstract = {Sparse representations have previously been applied to the automatic
	music transcription (AMT) problem. Structured sparsity, such as group
	and molecular sparsity allows the introduction of prior knowledge
	to sparse representations. Molecular sparsity has previously been
	proposed for AMT, however the use of greedy group sparsity has not
	previously been proposed for this problem. We propose a greedy sparse
	pursuit based on nearest subspace classification for groups with
	coherent blocks, based in a non-negative framework, and apply this
	to AMT. Further to this, we propose an enhanced molecular variant
	of this group sparse algorithm and demonstrate the effectiveness
	of this approach.},
  timestamp = {2016-07-11T16:57:39Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International 	Conference on},
  author = {O'Hanlon, K. and Nagano, H. and Plumbley, M.D.},
  month = mar,
  year = {2012},
  keywords = {Approximation algorithms,Approximation methods,Artificial neural networks,automatic
	music transcription,coherent blocks,Dictionaries,encoding,Greedy
	algorithms,greedy group sparsity,group sparse algorithm,Matching
	pursuit algorithms,measurement,non-negative,nonnegative framework,signal
	representation,sparse representations,Structured Sparsity,structured
	sparsity,subspace classification,Transcription},
  pages = {441-444},
  owner = {afdidehf}
}

@inproceedings{OHanlon2014,
  title = {Polyphonic piano transcription using non-negative Matrix Factorisation 	with group sparsity},
  doi = {10.1109/ICASSP.2014.6854173},
  abstract = {Non-negative Matrix Factorisation (NMF) is a popular tool in musical
	signal processing. However, problems using this methodology in the
	context of Automatic Music Transcription (AMT) have been noted resulting
	in the proposal of supervised and constrained variants of NMF for
	this purpose. Group sparsity has previously been seen to be effective
	for AMT when used with stepwise methods. In this paper group sparsity
	is introduced to supervised NMF decompositions and a dictionary tuning
	approach to AMT is proposed based upon group sparse NMF using the
	?-divergence. Experimental results are given showing improved AMT
	results over the state-of-the-art NMF-based AMT system.},
  timestamp = {2016-07-10T07:32:42Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {O'Hanlon, K. and Plumbley, M.D.},
  month = may,
  year = {2014},
  keywords = {?-divergence,AMT,automatic music transcription,Cost function,Cost
	function,decomposition,Dictionaries,dictionary tuning approach,dictionary
	tuning approach,group sparsity,group
	sparsity,Harmonic analysis,matrix decomposition,MUSIC,musical signal
	processing,musical
	signal processing,Narrowband,NMF,nonnegative matrix factorisation,polyphonic
	piano transcription,signal processing,stepwise method,Tuning},
  pages = {3112-3116},
  owner = {afdidehf}
}

@article{Oikonomou2012,
  title = {A Sparse and Spatially Constrained Generative Regression Model for 	fMRI Data Analysis},
  volume = {59},
  issn = {0018-9294},
  doi = {10.1109/TBME.2010.2104321},
  abstract = {In this study, we present an advanced Bayesian framework for the analysis
	of functional magnetic resonance imaging (fMRI) data that simultaneously
	employs both spatial and sparse properties. The basic building block
	of our method is the general linear regression model that constitutes
	a well-known probabilistic approach. By treating regression coefficients
	as random variables, we can apply an enhanced Gibbs distribution
	function that captures spatial constrains and at the same time allows
	sparse representation of fMRI time series. The proposed scheme is
	described as a maximum a posteriori approach, where the known expectation
	maximization algorithm is applied offering closed-form update equations
	for the model parameters. We have demonstrated that our method produces
	improved performance and functional activation detection capabilities
	in both simulated data and real applications.},
  timestamp = {2016-07-08T11:28:58Z},
  number = {1},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Oikonomou, V.P. and Blekas, K. and Astrakas, L.},
  month = jan,
  year = {2012},
  keywords = {advanced Bayesian framework,Analytical models,biomedical MRI,Brain,closed-form
	update equations,Computer-Assisted,Computer Simulation,Correlation,Data
	Interpretation,Data models,Estimation,expectation maximization algorithm,Expectation
	maximization (EM) algorithm,fMRI data analysis,fMRI time series,functional
	magnetic resonance imaging,functional magnetic resonance imaging
	(fMRI) analysis,general linear regression model (GLM),Gibbs distribution
	function,Humans,Image Interpretation,Magnetic resonance imaging,Markov
	processes,Markov random field (MRF),mathematical model,maximum a
	posteriori approach,medical computing,Models,Nerve Net,Neurological,Noise,probabilistic
	approach,regression analysis,regression coefficients,relevance vector
	machine (RVM),sparse generative regression model,spatial constrains,spatially
	constrained generative regression model,Statistical},
  pages = {58-67},
  owner = {afdidehf}
}

@article{Olier2013,
  title = {A switching multi-scale dynamical network model of EEG/MEG},
  volume = {83},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2013.04.046},
  abstract = {Abstract We introduce a new generative model of the Encephalography
	(EEG/MEG) data, the inversion of which allows for inferring the locations
	and temporal evolution of the underlying sources as well as their
	dynamical interactions. The proposed Switching Mesostate Space Model
	(SMSM) builds on the multi-scale generative model for EEG/MEG by
	Daunizeau and Friston (2007). \{SMSM\} inherits the assumptions that
	(1) bioelectromagnetic activity is generated by a set of distributed
	sources, (2) the dynamics of these sources can be modelled as random
	fluctuations about a small number of mesostates, and (3) the number
	of mesostates engaged by a cognitive task is small. Additionally,
	four generalising assumptions are now included: (4) the mesostates
	interact according to a full Dynamical Causal Network (DCN) that
	can be estimated; (5) the dynamics of the mesostates can switch between
	multiple approximately linear operating regimes; (6) each operating
	regime remains stable over finite periods of time (temporal clusters);
	and (7) the total number of times the mesostates' dynamics can switch
	is small. The proposed model adds, therefore, a level of flexibility
	by accommodating complex brain processes that cannot be characterised
	by purely linear and stationary Gaussian dynamics. Importantly, the
	\{SMSM\} furnishes a new interpretation of the EEG/MEG data in which
	the source activity may have multiple discrete modes of behaviour,
	each with approximately linear dynamics. This is modelled by assuming
	that the connection strengths of the underlying mesoscopic \{DCN\}
	are time-dependent but piecewise constant, i.e. they can undergo
	discrete changes over time. A Variational Bayes inversion scheme
	is derived to estimate all the parameters of the model by maximising
	a (Negative Free Energy) lower bound on the model evidence. This
	bound is used to select among different model choices that are defined
	by the number of mesostates as well as by the number of stationary
	linear regimes. The full model is compared to a simplified version
	that uses no dynamical assumptions as well as to a standard \{EEG\}
	inversion technique. The comparison is carried out using an extensive
	set of simulations, and the application of \{SMSM\} to a real data
	set is also demonstrated. Our results show that for experimental
	situations in which we have some a priori belief that there are multiple
	approximately linear dynamical regimes, the proposed \{SMSM\} provides
	a natural modelling tool.},
  timestamp = {2016-07-08T11:33:06Z},
  journal = {NeuroImage},
  author = {Olier, Iv�n and Trujillo-Barreto, Nelson J. and El-Deredy, Wael},
  year = {2013},
  keywords = {localisation,Source},
  pages = {262 - 287},
  owner = {afdidehf}
}

@inproceedings{Olivi2011,
  title = {Handling white-matter anisotropy in BEM for the EEG forward problem},
  doi = {10.1109/ISBI.2011.5872526},
  abstract = {Solving the inverse problem of source localization in MEG or EEG,
	requires appropriate electrophysiological modeling of the head. Conductivity
	of tissues in the vicinity of the sources is especially influential
	on the MEG and EEG forward fields. Those tissues include white matter,
	whose conductivity is anisotropic because of its fiber structure.
	While white matter anisotropy can be measured thanks to Diffusion-Weighted
	MRI, it is rarely incorporated in MEG and EEG head models. Boundary
	Element Methods can only deal with piecewise constant conductivities,
	therefore ruling out white matter anisotropy that has a complex structure,
	and Finite Element Method have been developed to deal with anisotropic
	conductivity, but require very fine meshes, thus huge linear systems.
	The purpose of this paper is to extend the BEM framework to incorporate
	white matter anisotropy by treating anisotropic conductivity as a
	perturbation of an isotropic one.},
  timestamp = {2016-07-08T12:41:18Z},
  booktitle = {Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium 	on},
  author = {Olivi, E. and Papadopoulo, T. and Clerc, M.},
  month = mar,
  year = {2011},
  keywords = {Anisotropic magnetoresistance,Anisotropy,boundary element method,boundary
	element method,boundary-elements methods,brain models,Conductivity,Diffusion-Weighted
	MRI,EEG,EEG forward problem,electroencephalography,electrophysiological
	modeling,finite element analysis,finite element method,Finite element
	methods,MEG,MEG forward field,Tensile stress,tissue conductivity,white
	matter,white-matter anisotropy},
  pages = {799-802},
  owner = {afdidehf}
}

@article{Olshausen2004,
  title = {Sparse coding of sensory inputs},
  volume = {14},
  issn = {0959-4388},
  doi = {http://dx.doi.org/10.1016/j.conb.2004.07.007},
  abstract = {Several theoretical, computational, and experimental studies suggest
	that neurons encode sensory information using a small number of active
	neurons at any given point in time. This strategy, referred to as
	‘sparse coding’, could possibly confer several advantages. First,
	it allows for increased storage capacity in associative memories;
	second, it makes the structure in natural signals explicit; third,
	it represents complex data in a way that is easier to read out at
	subsequent levels of processing; and fourth, it saves energy. Recent
	physiological recordings from sensory neurons have indicated that
	sparse coding could be a ubiquitous strategy employed in several
	different modalities across different organisms.},
  timestamp = {2016-07-11T16:44:06Z},
  number = {4},
  journal = {Current Opinion in Neurobiology},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {2004},
  pages = {481 - 487},
  owner = {Fardin}
}

@article{Osborne2000,
  title = {A new approach to variable selection in least squares problems},
  volume = {20},
  abstract = {The title Lasso has been suggested by Tibshirani (1996) as a colourful
	name for a technique of variable selection which requires the minimization
	of a sum of squares subject to an l1 bound ? on the solution. This
	forces zero components in the minimizing solution for small values
	of ?. Thus this bound can function as a selection parameter. This
	paper makes two contributions to computational problems associated
	with implementing the Lasso: (1) a compact descent method for solving
	the constrained problem for a particular value of ? is formulated,
	and (2) a homotopy method, in which the constraint bound ? becomes
	the homotopy parameter, is developed to completely describe the possible
	selection regimes. Both algorithms have a finite termination property.
	It is suggested that modified Gram-Schmidt orthogonalization applied
	to an augmented design matrix provides an effective basis for implementing
	the algorithms.},
  timestamp = {2016-07-08T10:31:25Z},
  number = {3},
  journal = {IMA Journal of Numerical Analysis},
  author = {Osborne, Mr and Presnella, B. and Turlacha, Ba},
  year = {2000},
  pages = {389-403},
  owner = {afdidehf}
}

@article{Osborne2000a,
  title = {On the LASSO and its Dual},
  volume = {9},
  abstract = {Proposed by Tibshirani, the least absolute shrinkage and selection
	operator (LASSO) estimates a vector of regression coefficients by
	minimizing the residual sum of squares subject to a constraint on
	the 11-norm of the coefficient vector. The LASSO estimator typically
	has one or more zero elements and thus shares characteristicso f
	both shrinkage estimation and variable selection. In this article
	we treat the LASSO as a convex programming problem and derive its
	dual. Consideration of the primal and dual problems together leads
	to importantn ew insights into the characteristicso f the LASSO estimator
	and to an improved method for estimating its covariance matrix. Using
	these results we also develop an efficient algorithm for computing
	LASSO estimates which is usable even in cases where the number of
	regressors exceeds the number of observations. An S-Plus library
	based on this algorithm is available from StatLib.},
  timestamp = {2016-07-10T07:16:53Z},
  number = {2},
  journal = {Journal of Computational and Graphical Statistics},
  author = {Osborne, Michael R. and Presnell, Brett and Turlac, Berwin A.},
  year = {2000},
  keywords = {convex programming,Dual problem,partial least squares,Penalized regression,Quadraticp
	rogramming,R egression,shrinkage,S ubset selection,Variables election.},
  pages = {319-337},
  owner = {afdidehf}
}

@inproceedings{Otsubo2012,
  title = {Source localization using event related beamformer of magnetoencephalography 	for interictal spikes in pediatric neocortical epilepsy},
  doi = {10.1109/ICCME.2012.6275692},
  abstract = {Purpose: We studied the relationship between source localization of
	interictal discharges using an event-related beamformer (ERB), equivalent
	current dipole analysis and the ictal onset zone on intracranial
	video EEG (IVEEG). Methods: We acquired interictal MEG data using
	a whole-head 151channel gradiometer system in 35 children with intractable
	neocortical epilepsy. We visually identified the earliest peak of
	each spike, by examining the raw MEG recordings using a band pass
	filter of 15-70 Hz. We used a spatiotemporal beamforming method to
	estimate the spatial distribution of source power in individual interictal
	spikes over the whole brain. The resulting volumetric source power
	images of individual spikes were then averaged and the results displayed
	on the patient's MRI. We compared the results to the localization
	obtained using the equivalent current dipole model and to the ictal
	onset zones on IVEEG. Results: Thirty-one patients had a single MEG
	dipole cluster. Four patients had more than 2 MEG spike source (MEGSS)
	clusters. Twenty-three patients showed focal ERB, including a single
	focal ERB 16 (46%) patients, and more than 2 ERBs in 7 (20%) patients.
	The Euclidean distance between the centroid of the MEGSS cluster
	and the ERB was less than 2 cm in 27 (77%) patients. The distance
	between the centroid of the MEGSS cluster and ERB was less than 2
	cm in 21/23 patients with focal ERB (mean, 1.19cm; SD, 0.65cm ).
	ERB was localized within the ictal onset zone gyral in 24 (69 %)
	patients, regional concordance was seen in 8 (23 %) patients and
	discordant in 3 (8 %) patients. A focal ERB was associated with concordant
	seizure localization (p=0.02) while all three patients with discordant
	IVEEG and ERB activation had multiple ERB foci. Maximum ERB was included
	in the resection margin in 28 (80 %) patients. In 23 patients with
	focal ERB, the ERB area was included in the resection margin in 22
	(95 %) patients. A favorable surgical outcome was obtained in 17
	(74 %) with a- focal ERB. Conclusion: ERB for interictal MEG spikes
	highly corresponded to the center of the MEGSS clusters and ictal
	onset zones localized on IVEEG. The frequency analysis of interictal
	MEG spikes may correlate with a subset of the epileptogenic high
	frequency oscillations.},
  timestamp = {2016-07-10T08:23:31Z},
  booktitle = {Complex Medical Engineering (CME), 2012 ICME International Conference 	on},
  author = {Otsubo, H. and Mohamed, I. and Cheyne, D.},
  month = jul,
  year = {2012},
  keywords = {Abstracts,array signal processing,band pass filter,concordant seizure
	localization,diseases,electroencephalography,epileptogenic high frequency
	oscillations,equivalent current dipole analysis,ERB,Euclidean distance,event
	related beamformer,frequency 15 Hz to 70 Hz,Frequency conversion,ictal
	onset zone,interictal discharges,interictal MEG data,interictal spikes,intracranial
	video EEG,intractable neocortical epilepsy,IVEEG,Magnetic analysis,Magnetic
	Resonance Imaging,magnetoencephalography,medical signal processing,MEG
	dipole cluster,MEG spike source clusters,MEGSS cluster centroid,MEGSS
	clusters,paediatrics,pediatric neocortical epilepsy,Planning,Source
	localization,source power spatial distribution,spatiotemporal beamforming
	method,Surgery,whole head gradiometer system},
  pages = {288-291},
  owner = {afdidehf}
}

@article{Ou2009,
  title = {A distributed spatio-temporal EEG/MEG inverse solver},
  volume = {44},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2008.05.063},
  abstract = {We propose a novel l1l2-norm inverse solver for estimating the sources
	of EEG/MEG signals. Based on the standard l1-norm inverse solvers,
	this sparse distributed inverse solver integrates the l1-norm spatial
	model with a temporal model of the source signals in order to avoid
	unstable activation patterns and “spiky�? reconstructed signals
	often produced by the currently used sparse solvers. The joint spatio-temporal
	model leads to a cost function with an l1l2-norm regularizer whose
	minimization can be reduced to a convex second-order cone programming
	(SOCP) problem and efficiently solved using the interior-point method.
	The efficient computation of the \{SOCP\} problem allows us to implement
	permutation tests for estimating statistical significance of the
	inverse solution. Validation with simulated and human \{MEG\} data
	shows that the proposed solver yields source time course estimates
	qualitatively similar to those obtained through dipole fitting, but
	without the need to specify the number of dipole sources in advance.
	Furthermore, the l1l2-norm solver achieves fewer false positives
	and a better representation of the source locations than the conventional
	l2 minimum-norm estimates.},
  timestamp = {2016-07-08T10:12:50Z},
  number = {3},
  journal = {NeuroImage},
  author = {Ou, Wanmei and H�m�l�inen, Matti S. and Golland, Polina},
  year = {2009},
  keywords = {?1-norm,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\(EEG\\\\\\\\\\\\\\\\,\\\\\\\\(EEG\\\\\\\\,\\\\(EEG\\\\,Inverse solver,MEG,second-order cone programming,second-order
	cone programming,Temporal basis functions,Temporal
	basis functions},
  pages = {932 - 946},
  owner = {Fardin}
}

@article{Owen2006,
  title = {Regularization: Ridge Regression and the LASSO},
  timestamp = {2016-07-10T07:50:42Z},
  author = {Owen, Art},
  month = nov,
  year = {2006},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Statistics 305: Autumn Quarter 2006/2007},
  owner = {Fardin}
}

@article{Oymak2010,
  title = {New Null Space Results and Recovery Thresholds for Matrix Rank Minimization},
  volume = {abs/1011.6},
  abstract = {Nuclear norm minimization (NNM) has recently gained significant attention
	for its use in rank minimization problems. Similar to compressed
	sensing, using null space characterizations, recovery thresholds
	for NNM have been studied in [12, 4]. However simulations show that
	the thresholds are far from optimal, especially in the low rank region.
	In this paper we apply the recent analysis of Stojnic for compressed
	sensing [18] to the null space conditions of NNM. The resulting thresholds
	are significantly better and in particular our weak threshold appears
	to match with simulation results. Further our curves suggest for
	any rank growing linearly with matrix size n we need only three times
	of oversampling (the model complexity) for weak recovery. Similar
	to [12] we analyze the conditions for weak, sectional and strong
	thresholds. Additionally a separate analysis is given for special
	case of positive semidefinite matrices. We conclude by discussing
	simulation results and future research directions.},
  timestamp = {2016-07-10T06:53:21Z},
  journal = {Computing Research Repository},
  author = {Oymak, Samet and Hassibi, Babak},
  month = nov,
  year = {2010},
  owner = {afdidehf}
}

@inproceedings{Ozog2013,
  title = {Inspector-Executor Load Balancing Algorithms for Block-Sparse Tensor 	Contractions},
  doi = {10.1109/ICPP.2013.12},
  abstract = {Developing effective yet scalable load-balancing methods for irregular
	computations is critical to the successful application of simulations
	in a variety of disciplines at petascale and beyond. This paper explores
	a set of static and dynamic scheduling algorithms for block-sparse
	tensor contractions within the NWChem computational chemistry code
	for different degrees of sparsity (and therefore load imbalance).
	In this particular application, a relatively large amount of task
	information can be obtained at minimal cost, which enables the use
	of static partitioning techniques that take the entire task list
	as input. However, fully static partitioning is incapable of dealing
	with dynamic variation of task costs, such as from transient network
	contention or operating system noise, so we also consider hybrid
	schemes that utilize dynamic scheduling within subgroups. These two
	schemes, which have not been previously implemented in NWChem or
	its proxies (i.e. quantum chemistry mini-apps) are compared to the
	original centralized dynamic load-balancing algorithm as well as
	improved centralized scheme. In all cases, we separate the scheduling
	of tasks from the execution of tasks into an inspector phase and
	an executor phase. The impact of these methods upon the application
	is substantial on a large InfiniBand cluster: execution time is reduced
	by as much as 50% at scale. The technique is applicable to any scientific
	application requiring load balance where performance models or estimations
	of kernel execution times are available.},
  timestamp = {2016-07-08T12:50:25Z},
  booktitle = {Parallel Processing (ICPP), 2013 42nd International Conference on},
  author = {Ozog, D. and Hammond, J.R. and Dinan, J. and Balaji, P. and Shende, S. and Malony, A.},
  month = oct,
  year = {2013},
  keywords = {block-sparse tensor contractions,chemistry computing,Computational
	efficiency,Computational modeling,Dynamic Load Balancing,dynamic
	scheduling algorithms,Global Arrays,InfiniBand cluster,inspector-executor
	load balancing algorithms,kernel execution times,Load management,Load
	modeling,NWChem computational chemistry code,operating system noise,Quantum
	Chemistry,Radiation detectors,resource allocation,scheduling,sparsity
	degree,Static Partitioning,static partitioning techniques,static
	scheduling algorithms,task scheduling,Tensile stress,Tensor Contractions,Tiles,transient
	network contention},
  pages = {30-39},
  owner = {afdidehf}
}

@book{Pajarinen2010,
  edition = {1},
  series = {Lecture Notes in Computer Science 6323},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, 	Part III},
  isbn = {978-3-642-15879-7 3-642-15879-X 978-3-642-15882-7 3-642-15882-X 978-3-642-15938-1 3-642-15938-9},
  timestamp = {2016-10-24T16:40:04Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Pajarinen, Joni and Peltonen, Jaakko and Hottinen, Ari and A. Uusitalo, Mikko},
  editor = {Balc�zar, Jos� Luis and Bonchi, Francesco and Gionis, Aristides and Sebag, Mich�le},
  year = {2010},
  owner = {Fardin}
}

@inproceedings{Pal2013,
  title = {Correlation-aware sparse support recovery: Gaussian sources},
  doi = {10.1109/ICASSP.2013.6638792},
  abstract = {Consider a multiple measurement vector (MMV) model given by y[n] =
	Axs[n]; 1 ? n ? L where equation denote the L measurement vectors,
	A ? RM�N is the measurement matrix and xs[n] ? RN are the unknown
	vectors with same sparsity support denoted by the set S0 with |S0|
	= D. It has been shown in a recent paper by the authors that when
	the elements of xs[n] are uncorrelated from each other, one can recover
	sparsity levels as high as O(M2) for suitably designed measurement
	matrix. The recovery is exact when support recovery algorithms are
	applied on the ideal correlation matrix. When we only have estimates
	of the correlation, it is still possible to probabilistically argue
	the recovery of sparsity levels (using a coherence based argument)
	that is much higher than that guaranteed by existing coherence based
	results. However the lower bound on the probability of success is
	found to increase rather slowly with L (as 1-C/L for some constant
	C > 0) without any further assumption on the distribution of the
	source vectors. In this paper, we demonstrate that when the source
	vectors belong to a Gaussian distribution with diagonal covariance
	matrix, it is possible to guarantee the recovery of original support
	with overwhelming probability. We also provide numerical simulations
	to demonstrate the effectiveness of the proposed strategy by comparing
	it with other popular MMV based methods.},
  timestamp = {2016-07-08T12:05:16Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Pal, P. and Vaidyanathan, P.P.},
  month = may,
  year = {2013},
  keywords = {block sparsity,Coherence,Correlation,correlation-aware sparse support
	recovery,correlation methods,covariance matrices,diagonal covariance
	matrix,Gaussian distribution,Gaussian sources,ideal correlation matrix,Joints,Lasso,measurement
	matrix,MMV model,multiple measurement vector,Multiple Measurement
	Vector (MMV),Probabilistic logic,probability,Random variables,signal
	processing,source vectors,sparse matrices,sparsity levels,support
	recovery,Vectors},
  pages = {5880-5884},
  owner = {afdidehf}
}

@inproceedings{Pal2012,
  title = {On application of LASSO for sparse support recovery with imperfect 	correlation awareness},
  doi = {10.1109/ACSSC.2012.6489158},
  abstract = {In this paper, the problem of identifying the common sparsity support
	of multiple measurement vectors (MMV) is considered. The model is
	given by y[n] = Axs[n], 1 ? n ? L where {y[n]}n=1L denote the L measurement
	vectors, A ? RM�N is the measurement matrix and xs[n] ? RN are
	the unknown vectors with same sparsity support denoted by the set
	S0 with |S0| = D. It has been shown in a recent paper by the authors
	that when the elements of xs[n] are uncorrelated from each other,
	one can recover sparsity levels as high as O(M2) for suitably designed
	measurement matrix. This result was shown assuming the knowledge
	that the nonzero elements are perfectly uncorrelated and that we
	have perfect estimates for the data correlation matrix, (the latter
	is true in the limit as L ? ?). In this paper, we formulate the problem
	of support recovery in the non ideal setting, i.e., when the correlation
	matrix is estimated with finite L. The resulting support recovery
	problem which explicitly utilizes the correlation knowledge, can
	be formulated as a LASSO. The performance of such �correlation
	aware� LASSO is analyzed by providing lower bounds on the probability
	of successful recovery as a function of the number L of measurement
	vectors. Numerical results are also provided to demonstrate the superior
	performance of the proposed correlation aware framework over conventional
	MMV techniques under identical conditions.},
  timestamp = {2016-07-10T07:10:52Z},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2012 Conference Record 	of the Forty Sixth Asilomar Conference on},
  author = {Pal, P. and Vaidyanathan, P.P.},
  month = nov,
  year = {2012},
  keywords = {block sparsity,Correlation,correlation aware LASSO,correlation methods,data
	correlation matrix,imperfect correlation awareness,Lasso,matrix algebra,measurement
	matrix,multiple measurement vector,Multiple Measurement Vector (MMV),probability,sparse
	support recovery,support recovery},
  pages = {958-962},
  owner = {afdidehf}
}

@inproceedings{Pan2014,
  title = {Minimum fourier measurements for stable recovery of block sparse 	signal},
  doi = {10.1109/ICASSP.2014.6854379},
  abstract = {Model based sparse signal recovery requires fewer measurements and
	has attracted lots of attention recently. One prototypical sparsity
	model is block sparsity whose stability is guaranteed from block
	restricted isometry property (RIP). However, the existing block RIP
	methods in the l2 norm space only consider Gaussian measurement case.
	In this paper, we extend the block RIP to the Fourier measurement
	case and demonstrate that the minimum number of measurements satisfying
	block RIP is as low as O (sd log q log(sd log q)log2 s), where d
	is the block size, s represents the block sparsity, and N is the
	length of unknowns satisfying N = qd for some integer q.},
  timestamp = {2016-07-09T20:11:54Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Pan, Junjie and Gao, Feifei},
  month = may,
  year = {2014},
  keywords = {Block restricted isometry property,block sparse signal,block sparsity,compressed
	sensing,Educational institutions,Fourier transforms,Gaussian distribution,Gaussian
	measurement,minimum Fourier measurements,prototypical sparsity model,Q
	measurement,Random variables,RIP,Size measurement,sparse signal recovery,stable
	recovery,Standards,Vectors},
  pages = {4131-4135},
  owner = {afdidehf}
}

@inproceedings{Pant2013,
  title = {Reconstruction of ECG signals for compressive sensing by promoting 	sparsity on the gradient},
  doi = {10.1109/ICASSP.2013.6637798},
  abstract = {A new algorithm for the reconstruction of signals in compressive sensing
	framework is proposed. The algorithm is based on a least-squares
	method which incorporates a regularization to promote sparsity on
	the gradient of the signal. It uses a sequential basic conjugate-gradient
	method, and it is especially suited for the reconstruction of signals
	which exhibit temporal correlation, e.g., electrocardiogram (ECG)
	signals. Simulation results are presented which demonstrate that
	the proposed algorithm yields upto 80.28% reduction in mean square
	error and from 49.95% to 65.64% reduction in the required amount
	of computation, relative to the state-of-the-art block sparse Bayesian
	learning bound-optimization algorithm.},
  timestamp = {2016-07-10T07:41:41Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Pant, J.K. and Krishnan, S.},
  month = may,
  year = {2013},
  keywords = {Approximation methods,Bayes methods,belief networks,block sparse Bayesian
	learning bound-optimization algorithm,compressed sensing,Compressive
	sensing,compressive sensing framework,conjugate gradient,conjugate
	gradient methods,correlation methods,ECG signal reconstruction,electrocardiogram,electrocardiogram
	signals,electrocardiography,least-squares method,Matching pursuit
	algorithms,mean square error,mean square error methods,medical signal
	processing,optimisation,Optimization,regularization,sequential basic
	conjugate-gradient method,signal gradient sparsity,Signal processing
	algorithms,signal reconstruction,sparse gradient,temporal correlation},
  pages = {993-997},
  owner = {afdidehf}
}

@inproceedings{Pant2012,
  title = {Reconstruction of block-sparse signals by using an $\ell_{2/p}$-regularized 	least-squares algorithm},
  doi = {10.1109/ISCAS.2012.6271884},
  abstract = {A new algorithm for the reconstruction of so called block-sparse signals
	in a compressive sensing framework is presented. The algorithm is
	based on minimizing an ?2/p-norm regularized l2 error. The minimization
	is carried out by using a sequential conjugate-gradient algorithm
	where the line search involved is carried out using a technique based
	on Banach's fixed-point theorem. Simulation results are presented
	which show that for large-size data the proposed algorithm yields
	improved reconstruction performance and requires a reduced amount
	of computation relative to several known algorithms.},
  timestamp = {2016-07-10T07:41:09Z},
  booktitle = {Circuits and Systems (ISCAS), 2012 IEEE International Symposium on},
  author = {Pant, J.K. and Lu, Wu-Sheng and Antoniou, A.},
  month = may,
  year = {2012},
  keywords = {?2/p-regularized least-squares algorithm,Approximation algorithms,Banach
	fixed-point theorem,Banach spaces,block-sparse signal reconstruction,compressed
	sensing,compressive sensing framework,conjugate gradient methods,large-size
	data,least squares approximations,Matching pursuit algorithms,Minimization,Noise
	measurement,Optimization,sequential conjugate-gradient algorithm,Signal
	processing algorithms,signal reconstruction},
  pages = {277-280},
  owner = {afdidehf}
}

@inproceedings{Parvaresh2008,
  title = {Explicit measurements with almost optimal thresholds for compressed 	sensing},
  doi = {10.1109/ICASSP.2008.4518494},
  abstract = {We consider the deterministic construction of a measurement matrix
	and a recovery method for signals that are block sparse. A signal
	that has dimension N = nd, which consists of n blocks of size d,
	is called (s, d)-block sparse if only s blocks out of n are nonzero.
	We construct an explicit linear mapping Phi that maps the (s, d)
	-block sparse signal to a measurement vector of dimension M, where
	s - d < N (1- (1- M/N)d/d+1) - o(1). We show that if the (s,d)- block
	sparse signal is chosen uniformly at random then the signal can almost
	surely be reconstructed from the measurement vector in O(N3) computations.},
  timestamp = {2016-07-08T12:27:20Z},
  booktitle = {Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE 	International Conference on},
  author = {Parvaresh, F. and Hassibi, B.},
  month = mar,
  year = {2008},
  keywords = {block sparse signal reconstruction,compressed sensing,convex optimization,convex
	optimization,decoding algorithms,decoding
	algorithms,Electric variables measurement,Equations,explicit
	linear mapping,explicit linear
	mapping,Linear systems,Mathematics,measurement vector matrix,optimal
	threshold,Optimized production technology,Reed-Solomon codes,Sampling
	methods,Signal mapping,signal reconstruction,sparse matrices,sparse
	signals,Vectors},
  pages = {3853-3856},
  owner = {afdidehf}
}

@article{Parvaresh2008a,
  title = {Recovering Sparse Signals Using Sparse Measurement Matrices in Compressed 	DNA Microarrays},
  volume = {2},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2008.924384},
  abstract = {Microarrays (DNA, protein, etc.) are massively parallel affinity-based
	biosensors capable of detecting and quantifying a large number of
	different genomic particles simultaneously. Among them, DNA microarrays
	comprising tens of thousands of probe spots are currently being employed
	to test multitude of targets in a single experiment. In conventional
	microarrays, each spot contains a large number of copies of a single
	probe designed to capture a single target, and, hence, collects only
	a single data point. This is a wasteful use of the sensing resources
	in comparative DNA microarray experiments, where a test sample is
	measured relative to a reference sample. Typically, only a fraction
	of the total number of genes represented by the two samples is differentially
	expressed, and, thus, a vast number of probe spots may not provide
	any useful information. To this end, we propose an alternative design,
	the so-called compressed microarrays, wherein each spot contains
	copies of several different probes and the total number of spots
	is potentially much smaller than the number of targets being tested.
	Fewer spots directly translates to significantly lower costs due
	to cheaper array manufacturing, simpler image acquisition and processing,
	and smaller amount of genomic material needed for experiments. To
	recover signals from compressed microarray measurements, we leverage
	ideas from compressive sampling. For sparse measurement matrices,
	we propose an algorithm that has significantly lower computational
	complexity than the widely used linear-programming-based methods,
	and can also recover signals with less sparsity.},
  timestamp = {2016-09-29T14:52:30Z},
  number = {3},
  journal = {IEEE J. Sel. Topics Signal Process.},
  author = {Parvaresh, F. and Vikalo, H. and Misra, S. and Hassibi, B.},
  month = jun,
  year = {2008},
  keywords = {biocomputing,bioinformatics,biosensors,compressed DNA microarrays,compressive
	sampling,computational complexity,Costs,DNA,DNA microarrays,genetics,genomic
	particles,Genomics,image acquisition,image coding,Image Processing,linear
	programming,Probes,Proteins,signal processing,sparse matrices,sparse
	measurement matrices,sparse measurements,sparse signals,Testing},
  pages = {275--285},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  annote = {Selected Topics in Signal Processing, IEEE Journal of},
  owner = {afdidehf}
}

@article{Pascual-Marqui1999,
  title = {Review of methods for solving the EEG inverse problem},
  volume = {1},
  abstract = {This paper reviews the class of instantaneous, 3D, discrete, linear
	solutions for the EEG inverse problem. Five different inverse methods
	are analyzed and compared: minimum norm, weighted minimum norm, Backus
	and Gilbert, weighted resolution optimization (WROP), and low resolution
	brain electromagnetic tomography (LORETA). The inverse methods are
	compared by testing localization errors in the estimation of single
	and multiple sources. These tests constitute the minimum necessary
	condition to be satisfied by any tomography. Of the five inverse
	solutions tested, only LORETA demonstrates the ability of correct
	localization in 3D space. The other four inverse solutions should
	not be used if the research aim is to localize the neuronal generators
	of EEG in a 3D brain. In this sense, minimum norm, weighted minimum
	norm, Backus and Gilbert, and WROP can be likened to x-rays, where
	depth information is totally lacking. For the sake of reproducible
	research, all the material and methods used in this part of the study,
	consisting of computer programs (source code and executables) and
	data, are available upon request to the author. In this way, all
	the results and conclusions can be checked, reproduced, and validated
	by the interested reader. In the final part of this paper, LORETA
	in the standard Talairach human brain is presented. This technique
	allows the quantitative neuroanatomical localization of neuronal
	electric activity. A computer program for LORETA in Talairach space
	is available upon request from the author.},
  timestamp = {2016-07-10T08:03:50Z},
  number = {1},
  journal = {International Journal of Bioelectromagnetism},
  author = {Pascual-Marqui, Roberto Domingo},
  year = {1999},
  pages = {75-86},
  owner = {Fardin}
}

@inproceedings{Pati1993,
  title = {Orthogonal matching pursuit: recursive function approximation with 	applications to wavelet decomposition},
  doi = {10.1109/ACSSC.1993.342465},
  abstract = {We describe a recursive algorithm to compute representations of functions
	with respect to nonorthogonal and possibly overcomplete dictionaries
	of elementary building blocks e.g. affine (wavelet) frames. We propose
	a modification to the matching pursuit algorithm of Mallat and Zhang
	(1992) that maintains full backward orthogonality of the residual
	(error) at every step and thereby leads to improved convergence.
	We refer to this modified algorithm as orthogonal matching pursuit
	(OMP). It is shown that all additional computation required for the
	OMP algorithm may be performed recursively},
  timestamp = {2016-07-10T07:20:36Z},
  booktitle = {Signals, Systems and Computers, 1993. 1993 Conference Record of The 	Twenty-Seventh Asilomar Conference on},
  author = {Pati, Y.C. and Rezaiifar, R. and Krishnaprasad, P.S.},
  month = nov,
  year = {1993},
  keywords = {affine wavelet frames,approximation theory,backward orthogonality,Convergence,convergence
	of numerical methods,Dictionaries,Educational institutions,Function
	approximation,Information systems,Iterative algorithms,Laboratories,matching
	pursuit algorithm,Matching pursuit algorithms,orthogonal matching
	pursuit,overcomplete dictionaries,Pursuit algorithms,recursive algorithm,recursive
	estimation,recursive function approximation,signal representation,signal
	representation,wavelet decomposition,wavelet transforms,Zinc},
  pages = {40-44 vol.1},
  owner = {Fardin}
}

@article{Patrascu2015,
  title = {Random Coordinate Descent Methods for $\ell _{0}$ Regularized Convex 	Optimization},
  volume = {60},
  issn = {0018-9286},
  doi = {10.1109/TAC.2015.2390551},
  abstract = {In this paper, we study the minimization of \ell _{0} regularized
	optimization problems, where the objective function is composed of
	a smooth convex function and the \ell _{0} regularization. We analyze
	optimality conditions for this nonconvex problem which lead to the
	separation of local minima into two restricted classes that are nested
	and around the set of global minima. Based on these restricted classes
	of local minima, we devise two new random coordinate descent type
	methods for solving these problems. In particular, we analyze the
	convergence properties of an iterative hard thresholding based random
	coordinate descent algorithm for which we prove that any limit point
	is a local minimum from the first restricted class of local minimizers.
	Then, we analyze the convergence of a random proximal alternating
	minimization method and show that any limit point of this algorithm
	is a local minima from the second restricted class of local minimizers.
	We also provide numerical experiments which show the superior behavior
	of our methods in comparison with the usual iterative hard thresholding
	algorithm.},
  timestamp = {2016-07-10T07:40:06Z},
  number = {7},
  journal = {Automatic Control, IEEE Transactions on},
  author = {Patrascu, A. and Necoara, I.},
  month = jul,
  year = {2015},
  keywords = {Local minima,packetized predictive control,random coordinate descent,Sparse
	regularization},
  pages = {1811-1824}
}

@inproceedings{Patrascu2015a,
  title = {Random Coordinate Descent Methods for Sparse Optimization: Application 	to Sparse Control},
  doi = {10.1109/CSCS.2015.140},
  abstract = {In this paper we analyze a family of general random block coordinate
	descent methods for the minimization of ?0 regularized optimization
	problems, i.e. The objective function is composed of a smooth convex
	function and the ?0 regularization. Our family of methods covers
	particular cases such as random block coordinate gradient descent
	and random proximal coordinate descent methods. We analyze necessary
	optimality conditions for this nonconvex ?0 regularized problem and
	devise a separation of the set of local minima into restricted classes
	based on approximation versions of the objective function. We provide
	a unified analysis of the almost sure convergence for this family
	of block coordinate descent algorithms and prove that, for each approximation
	version, the limit points are local minima from the corresponding
	restricted class of local minimizers.},
  timestamp = {2016-07-10T07:40:09Z},
  booktitle = {Control Systems and Computer Science (CSCS), 2015 20th International 	Conference on},
  author = {Patrascu, A. and Necoara, I.},
  month = may,
  year = {2015},
  keywords = {Algorithm design and analysis,Approximation algorithms,Approximation
	methods,concave programming,Convergence,convergence to local minima,convex
	programming,general random block coordinate descent methods,l0 regularization,l0
	regularized optimization,l0 regularized optimization problems,linear
	programming,minimisation,Minimization,necessary optimality conditions,nonconvex
	l0 regularized problem,objective function,Optimization,random coordinate
	descent,random proximal coordinate descent methods,smooth convex
	function,sparse control,sparse optimization,sparse predictive control},
  pages = {909-914},
  owner = {afdidehf}
}

@inproceedings{Peng2013,
  title = {Parallel and distributed sparse optimization},
  doi = {10.1109/ACSSC.2013.6810364},
  abstract = {This paper proposes parallel and distributed algorithms for solving
	very large-scale sparse optimization problems on computer clusters
	and clouds. Modern datasets usually have a large number of features
	or training samples, and they are usually stored in a distributed
	manner. Motivated by the need of solving sparse optimization problems
	with large datasets, we propose two approaches including (i) distributed
	implementations of prox-linear algorithms and (ii) GRock, a parallel
	greedy block coordinate descent method. Different separability properties
	of the objective terms in the problem enable different data distributed
	schemes along with their corresponding algorithm implementations.
	We also establish the convergence of GRock and explain why it often
	performs exceptionally well for sparse optimization. Numerical results
	on a computer cluster and Amazon EC2 demonstrate the efficiency and
	elasticity of our algorithms.},
  timestamp = {2016-07-10T07:25:21Z},
  booktitle = {Signals, Systems and Computers, 2013 Asilomar Conference on},
  author = {Peng, Zhimin and Yan, Ming and Yin, Wotao},
  month = nov,
  year = {2013},
  keywords = {Amazon EC2,Broadcasting,Clustering algorithms,computer clusters,Convergence,convex
	optimization,convex programming,data distributed schemes,distributed
	algorithms,Distributed databases,distributed sparse optimization,Greedy
	algorithms,GRock,GRock convergence,l1 minimization,Lasso,Logistics,mathematics
	computing,objective term separability property,Optimization,Parallel
	algorithms,parallel and distributed computing,parallel greedy block
	coordinate descent method,parallel sparse optimization,prox-linear
	algorithms,sparse optimization,Vectors,very large-scale sparse optimization
	problem solving},
  pages = {659-646},
  owner = {afdidehf}
}

@article{Peotta2007,
  title = {Matching Pursuit With Block Incoherent Dictionaries},
  volume = {55},
  issn = {1053-587X},
  doi = {10.1109/TSP.2007.896022},
  abstract = {Recently, there has been an intense activity in the field of sparse
	approximations with redundant dictionaries, largely motivated by
	the practical performances of algorithms such as matching pursuit
	(MP) and basis pursuit (BP). However, most of the theoretical results
	obtained so far are valid only for the restricted class of incoherent
	dictionaries. This paper investigates a new class of overcomplete
	dictionaries, called block incoherent dictionaries, where coherence
	can be arbitrarily big. We show that a simple greedy algorithm can
	correctly identify stable subdictionaries (called blocks) and demonstrate
	how one can use the extra coherence freedom for approximation purposes.},
  timestamp = {2016-09-30T13:51:38Z},
  number = {9},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Peotta, L.. and Vandergheynst, P.},
  month = sep,
  year = {2007},
  keywords = {Approximation methods,approximation theory,basis pursuit algorithm,block incoherent dictionaries,Block-incoherent dictionaries,Block-incoherent
	dictionaries,block
	incoherent dictionaries,data processing,Dictionaries,greedy
	algorithm,Greedy algorithms,Harmonic analysis,Helium,matching
	pursuit algorithm,matching pursuit
	algorithm,Matching pursuit algorithms,matching pursuit (MP),Pursuit
	algorithms,redundant dictionaries,signal representation,signal representations,Signal
	synthesis,sparse signal approximation,sparse signal representation},
  pages = {4549--4557},
  owner = {afdidehf}
}

@article{Peters2000,
  title = {Undersampled projection reconstruction applied to MR angiography},
  volume = {43},
  issn = {1522-2594},
  doi = {10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4},
  abstract = {Undersampled projection reconstruction (PR) is investigated as an
	alternative method for MRA (MR angiography). In conventional 3D Fourier
	transform (FT) MRA, resolution in the phase-encoding direction is
	proportional to acquisition time. Since the PR resolution in all
	directions is determined by the readout resolution, independent of
	the number of projections (Np), high resolution can be generated
	rapidly. However, artifacts increase for reduced Np. In X-ray CT,
	undersampling artifacts from bright objects like bone can dominate
	other tissue. In MRA, where bright, contrast-filled vessels dominate,
	artifacts are often acceptable and the greater resolution per unit
	time provided by undersampled PR can be realized. The resolution
	increase is limited by SNR reduction associated with reduced voxel
	size. The hybrid 3D sequence acquires fractional echo projections
	in the kx–ky plane and phase encodings in kz. PR resolution and
	artifact characteristics are demonstrated in a phantom and in contrast-enhanced
	volunteer studies. Magn Reson Med 43:91–101, 2000. © 2000 Wiley-Liss,
	Inc.},
  timestamp = {2016-09-29T14:46:45Z},
  number = {1},
  journal = {Magnetic Resonance in Medicine},
  author = {Peters, Dana C. and Korosec, Frank R. and Grist, Thomas M. and Block, Walter F. and Holden, James E. and Vigen, Karl K. and Mistretta, Charles A.},
  year = {2000},
  keywords = {contrast-enhanced,projection reconstruction,rapid imaging,sparse sampling},
  pages = {91--101},
  annote = {http://dx.doi.org/10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4},
  annote = {http://dx.doi.org/10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4},
  annote = {http://dx.doi.org/10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4},
  annote = {http://dx.doi.org/10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4},
  annote = {http://dx.doi.org/10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4},
  owner = {afdidehf}
}

@book{Peterson1972,
  edition = {second edition},
  title = {Error-Correcting Codes},
  isbn = {0-262-16039-0 978-0-262-16039-1 978-0-585-30709-1},
  timestamp = {2016-07-08T12:21:36Z},
  author = {Peterson, W. Wesley and Weldon, E. J.},
  year = {1972},
  owner = {Fardin}
}

@inproceedings{Peyre2011,
  title = {Group sparsity with overlapping partition functions},
  abstract = {This paper introduces a novel and versatile group sparsity prior for
	denoising and to regularize inverse problems. The sparsity is enforced
	through arbitrary block-localization operators, such as for instance
	smooth localized partition functions. The resulting blocks can have
	an arbitrary overlap, which is important to reduce visual artifacts
	thanks to the increased translation invariance of the prior. They
	are moreover not necessarily binary, and allow for non-integer block
	sizes. We develop two schemes, one primal and another primal-dual,
	originating from the non-smooth convex optimization realm, to efficiently
	solve a wide class of inverse problems regularized using this overlapping
	group sparsity prior. This scheme is flexible enough to handle both
	penalized and constrained versions of the optimization problems at
	hand. Numerical results on denoising and compressed sensing are reported
	and show the improvement brought by the overlap and the smooth partition
	functions with respect to classical group sparsity.},
  timestamp = {2016-07-08T12:41:10Z},
  booktitle = {Signal Processing Conference, 2011 19th European},
  author = {Peyre, G. and Fadili, J.},
  month = aug,
  year = {2011},
  keywords = {Algorithm design and analysis,arbitrary block-localization operators,compressed
	sensing,convex programming,image denoising,inverse problems,Inverse
	problems,Minimization,Noise reduction,nonsmooth convex optimization
	realm,novel versatile group sparsity,overlapping group sparsity prior,overlapping
	partition functions,PSNR,Signal processing algorithms,translation
	invariance},
  pages = {303-307},
  owner = {Fardin}
}

@inproceedings{Peyre2011a,
  title = {Adaptive structured block sparsity via dyadic partitioning},
  abstract = {This paper proposes a novel method to adapt the block-sparsity structure
	to the observed noisy data. Towards this goal, the Stein risk estimator
	framework is exploited, and the block-sparsity is dyadically organized
	in a tree. The adaptation of the sparsity structure is obtained by
	finding the best recursive dyadic partition, whose terminal nodes
	(leaves) are the blocks, that minimizes a data-driven estimator of
	the risk. Our main contributions are (i) analytical expression of
	the risk; (ii) a novel estimator of the risk; (iii) a fast algorithm
	that yields the best partition. Numerical results on wavelet-domain
	denoising of synthetic and natural images illustrate the improvement
	brought by our adaptive approach.},
  timestamp = {2016-07-08T10:12:15Z},
  booktitle = {Signal Processing Conference, 2011 19th European},
  author = {Peyre, G. and Fadili, J. and Chesneau, C.},
  month = aug,
  year = {2011},
  keywords = {adaptive structured block sparsity,Approximation methods,best recursive
	dyadic partition,Estimation,Heuristic algorithms,image denoising,Noise
	measurement,Noise reduction,Partitioning algorithms,Stein risk estimator
	framework,Wavelet domain,wavelet-domain denoising,wavelet transforms},
  pages = {1455-1459},
  owner = {afdidehf}
}

@article{Peyre2009a,
  title = {Sparse Modeling of Textures},
  volume = {34},
  issn = {0924-9907},
  doi = {10.1007/s10851-008-0120-3},
  language = {English},
  timestamp = {2016-07-11T16:49:19Z},
  number = {1},
  journal = {Journal of Mathematical Imaging and Vision},
  author = {Peyr{\'e}, Gabriel},
  year = {2009},
  keywords = {Image Processing,inpainting,Learning dictionaries,sparse representation,Texture
	synthesis},
  pages = {17-31},
  __markedentry = {[afdidehf:]},
  owner = {afdidehf}
}

@article{Pham2015,
  title = {On group-wise regularization: Theory and efficient algorithms},
  volume = {48},
  issn = {0031-3203},
  doi = {http://dx.doi.org/10.1016/j.patcog.2015.05.009},
  abstract = {Abstract Following advances in compressed sensing and high-dimensional
	statistics, many pattern recognition methods have been developed
	with l1 regularization, which promotes sparse solutions. In this
	work, we instead advocate the use of l p ( 2 ≥ p &gt; 1 ) regularization
	in a group setting which provides a better trade-off between sparsity
	and algorithmic stability. We focus on the simplest case with squared
	loss, which is known as group bridge regression. On the theoretical
	side, we prove that group bridge regression is uniformly stable and
	thus generalizes, which is an important property of a learning method.
	On the computational side, we make group bridge regression more practically
	attractive by deriving provably convergent and computationally efficient
	optimization algorithms. We show that there are at least several
	values of p over (1,2) at which the iterative update is analytical,
	thus it is even suitable for large-scale settings. We demonstrate
	the clear advantage of group bridge regression with the proposed
	algorithms over other competitive alternatives on several datasets.
	As l p -regularization allows one to achieve flexibility in sparseness/denseness
	of the solution, we hope that the algorithms will be useful for future
	applications of this regularization.},
  timestamp = {2016-07-10T07:12:05Z},
  number = {11},
  journal = {Pattern Recognition},
  author = {Pham, Duc-Son},
  year = {2015},
  keywords = {ADMM,Algorithmic,Bridge,bridgeregression,Convexoptimizationalgorithms,detection,FISTA,group,Lasso,lp,regression,regularization,Splice,Stability},
  pages = {3728 - 3738},
  owner = {afdidehf}
}

@article{Phillips2012,
  title = {M/EEG source reconstruction: problems \& solutions},
  timestamp = {2016-07-09T20:10:12Z},
  author = {Phillips, C.},
  month = apr,
  year = {2012},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {SPM-M/EEG course},
  owner = {Fardin}
}

@article{Phillips1997,
  title = {Imaging neural activity using MEG and EEG},
  volume = {16},
  issn = {0739-5175},
  doi = {10.1109/51.585515},
  abstract = {The authors have developed a Bayesian framework for image estimation
	from combined MEG/EEG data. Their results indicate that performance
	of their imaging approach is superior to that of weighted minimum
	norm when the image is sparse and focal. Note however that if the
	image is not sparse, the authors' method would perform poorly since
	their prior is specifically designed to give sparse focal sources.
	This observation serves to emphasize the fact that the use of prior
	information is crucial in extracting useful spatial information from
	the data. The authors have also found that combining MEG and EEG
	gives superior results when compared to using the modalities individually.
	This improvement is due not only to increasing the number of measurements,
	but also because of the complimentary nature of MEG/EEG. Even when
	working with the two modalities in combination, significant limitations
	to electromagnetic imaging exist. Regardless of the number and placement
	of sensors, reconstructions are generally only reliable if relatively
	few source clusters exist. If a large number of distributed sources
	exist, no imaging technique can hope to reconstruct them accurately
	strictly from the MEG/EEG data given. Such complex distributions
	will generally be matched as well or better by simpler solutions.
	Thus, if used on their own, the authors expect MEG/EEG data to be
	most useful when the number of activated sites is small. Alternatively,
	when used in combination with fMRI or PET, it may be possible to
	produce dynamic images of more complex processes},
  timestamp = {2016-10-21T13:27:22Z},
  number = {3},
  journal = {Engineering in Medicine and Biology Magazine, IEEE},
  author = {Phillips, J.W. and Leahy, R.M. and Mosher, J.C. and Timsari, B.},
  month = may,
  year = {1997},
  keywords = {activated sites,Bayesian framework,Bayes methods,biosensors,combined
	MEG/EEG data,complex distributions,distributed sources,dynamic images,Electrodes,electroencephalography,electromagnetic
	imaging,fMRI,image estimation,Image reconstruction,magnetic field
	measurement,Magnetic flux,Magnetic recording,Magnetic sensors,Magnetic
	shielding,magnetoencephalography,medical diagnostic imaging,medical
	image processing,neural activity imaging,neurophysiology,PET,reconstructions,Scalp,sparse
	focal image,SQUIDs,weighted minimum norm},
  pages = {34--42}
}

@inproceedings{Pieloth2013,
  title = {An online system for neuroelectromagnetic source imaging},
  volume = {01},
  doi = {10.1109/IDAACS.2013.6662687},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) provide
	insight into neuronal processes in the brain in a real-time scale.
	Brain activity can be modeled in terms of a source distribution found
	by solving the bioelectromagnetic inverse problem, e.g. using linear
	source reconstruction methods. Such methods are particularly suitable
	to be used on modern highly parallel processing systems, such as
	widely available graphic processing units (GPUs). The utilization
	of these capabilities paves the way for online neuroelectromagnetic
	source imaging. We present a system that, according to its modular
	scheme, can be configured in a very flexible way using graphical
	building blocks. It allows to use different preprocessing algorithms
	together with a linear source reconstruction method. The algorithms
	use both CPU and GPU resources.},
  timestamp = {2016-07-08T11:24:44Z},
  booktitle = {Intelligent Data Acquisition and Advanced Computing Systems (IDAACS), 	2013 IEEE 7th International Conference on},
  author = {Pieloth, C. and Pizarro, J.M. and Knosche, T. and Maess, B. and Fuchs, M.},
  month = sep,
  year = {2013},
  keywords = {bioelectromagnetic inverse problem,brain activity,brain models,CPU,data
	structures,EEG,electroencephalography,GPU resources,graphic processing
	units,graphics processing units,Lead,linear
	source reconstruction method,linear source reconstruction
	method,magnetoencephalography,medical image processing,medical image
	processing,MEG,online neuroelectromagnetic source imaging,online
	neuroelectromagnetic source imaging,online system,online
	system,parallel processing systems,parallel processing
	systems,Signal processing algorithms,source distribution,source
	distribution,source reconstruction},
  pages = {270-274},
  owner = {afdidehf}
}

@book{Pinkus1985,
  title = {N-Width in Approximation Theory},
  timestamp = {2016-07-10T07:10:28Z},
  publisher = {{\{Springer, Berlin\}}},
  author = {Pinkus, Allan},
  year = {1985},
  owner = {Fardin}
}

@inproceedings{Pope2012,
  title = {Sparse signal recovery in Hilbert spaces},
  doi = {10.1109/ISIT.2012.6283506},
  abstract = {This paper reports an effort to consolidate numerous coherence-based
	sparse signal recovery results available in the literature. We present
	a single theory that applies to general Hilbert spaces with the sparsity
	of a signal defined as the number of (possibly infinite-dimensional)
	subspaces participating in the signal's representation. Our general
	results recover uncertainty relations and coherence-based recovery
	thresholds for sparse signals, block-sparse signals, multi-band signals,
	signals in shift-invariant spaces, and signals in finite unions of
	(possibly infinite-dimensional) subspaces. Moreover, we improve upon
	and generalize several of the existing results and, in many cases,
	we find shortened and simplified proofs.},
  timestamp = {2016-07-11T16:53:09Z},
  booktitle = {Information Theory Proceedings (ISIT), 2012 IEEE International Symposium 	on},
  author = {Pope, G. and Bolcskei, H.},
  month = jul,
  year = {2012},
  keywords = {block-sparse signals,Coherence,coherence-based recovery thresholds,coherence-based
	sparse signal recovery,Hilbert space,Hilbert spaces,infinite-dimensional
	subspaces,Kernel,Matching pursuit algorithms,multiband signals,signal
	representation,Sparks,Uncertainty,Vectors},
  pages = {1463-1467},
  owner = {afdidehf}
}

@inproceedings{Potter2008,
  title = {A fast posterior update for sparse underdetermined linear models},
  doi = {10.1109/ACSSC.2008.5074527},
  abstract = {A Bayesian approach is adopted for linear regression, and a fast algorithm
	is given for updating posterior probabilities. Emphasis is given
	to the underdetermined and sparse case, i.e., fewer observations
	than regression coefficients and the belief that only a few regression
	coefficients are non-zero. The fast update allows for a low-complexity
	method of reporting a set of models with high posterior probability
	and their exact posterior odds. As a byproduct, this Bayesian model
	averaged approach yields the minimum mean squared error estimate
	of unknown coefficients. Algorithm complexity is linear in the number
	of unknown coefficients, the number of observations and the number
	of nonzero coefficients. For the case in which hyperparameters are
	unknown, a maximum likelihood estimate is found by a generalized
	expectation maximization algorithm.},
  timestamp = {2016-07-08T10:17:10Z},
  booktitle = {Signals, Systems and Computers, 2008 42nd Asilomar Conference on},
  author = {Potter, L.C. and Schniter, P. and Ziniel, J.},
  month = oct,
  year = {2008},
  keywords = {algorithm complexity,Bayesian approach,Bayesian methods,Bayesian model,Bayes
	methods,channel estimation,expectation-maximisation algorithm,generalized
	expectation maximization algorithm,linear programming,Linear regression,Linear
	regression,low-complexity method,Matching pursuit algorithms,maximum
	likelihood estimate,Maximum likelihood estimation,mean square error
	methods,Medical treatment,minimum mean squared error estimate,nonzero
	coefficients,posterior probability,posterior update,probability,Radar
	imaging,Random variables,regression analysis,regression coefficients,sparse
	underdetermined linear models,Yield estimation},
  pages = {838-842},
  owner = {Fardin}
}

@inproceedings{Prasad2014,
  title = {Nested Sparse Bayesian Learning for block-sparse signals with intra-block 	correlation},
  doi = {10.1109/ICASSP.2014.6854994},
  abstract = {In this work, we address the recovery of block sparse vectors with
	intra-block correlation, i.e., the recovery of vectors in which the
	correlated nonzero entries are constrained to lie in a few clusters,
	from noisy underdetermined linear measurements. Among Bayesian sparse
	recovery techniques, the cluster Sparse Bayesian Learning (SBL) is
	an efficient tool for block-sparse vector recovery, with intrablock
	correlation. However, this technique uses a heuristic method to estimate
	the intra-block correlation. In this paper, we propose the Nested
	SBL (NSBL) algorithm, which we derive using a novel Bayesian formulation
	that facilitates the use of the monotonically convergent nested Expectation
	Maximization (EM) and a Kalman filtering based learning framework.
	Unlike the cluster-SBL algorithm, this formulation leads to closed-form
	EM updates for estimating the correlation coefficient. We demonstrate
	the efficacy of the proposed NSBL algorithm using Monte Carlo simulations.},
  timestamp = {2016-07-10T06:49:16Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Prasad, R. and Murthy, C.R. and Rao, B.D.},
  month = may,
  year = {2014},
  keywords = {Bayesian sparse recovery,Bayes methods,block sparse signals,block
	sparse vector recovery,Clustering algorithms,cluster sparse bayesian
	learning,Correlation,correlation coefficient,correlation methods,EM,expectation-maximisation
	algorithm,Expectation maximization,intrablock correlation,Kalman
	filtering,Kalman filters,learning (artificial intelligence),Mathematical
	model,Monte Carlo methods,Monte Carlo simulations,nested sparse Bayesian
	learning,NSBL,Signal processing algorithms,signal reconstruction,Signal
	to noise ratio,Vectors},
  pages = {7183-7187},
  owner = {afdidehf}
}

@article{Prasad2014a,
  title = {Joint Approximately Sparse Channel Estimation and Data Detection 	in OFDM Systems Using Sparse Bayesian Learning},
  volume = {62},
  issn = {1053-587X},
  doi = {10.1109/TSP.2014.2329272},
  abstract = {It is well known that the impulse response of a wideband wireless
	channel is approximately sparse, in the sense that it has a small
	number of significant components relative to the channel delay spread.
	In this paper, we consider the estimation of the unknown channel
	coefficients and its support in OFDM systems using a sparse Bayesian
	learning (SBL) framework for exact inference. In a quasi-static,
	block-fading scenario, we employ the SBL algorithm for channel estimation
	and propose a joint SBL (J-SBL) and a low-complexity recursive J-SBL
	algorithm for joint channel estimation and data detection. In a time-varying
	scenario, we use a first-order autoregressive model for the wireless
	channel and propose a novel, recursive, low-complexity Kalman filtering-based
	SBL (KSBL) algorithm for channel estimation. We generalize the KSBL
	algorithm to obtain the recursive joint KSBL algorithm that performs
	joint channel estimation and data detection. Our algorithms can efficiently
	recover a group of approximately sparse vectors even when the measurement
	matrix is partially unknown due to the presence of unknown data symbols.
	Moreover, the algorithms can fully exploit the correlation structure
	in the multiple measurements. Monte Carlo simulations illustrate
	the efficacy of the proposed techniques in terms of the mean-square
	error and bit error rate performance.},
  timestamp = {2016-07-09T19:47:25Z},
  number = {14},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Prasad, R. and Murthy, C.R. and Rao, B.D.},
  month = jul,
  year = {2014},
  keywords = {approximately sparse channel estimation,a-sparse,autoregressive processes,Bayes
	methods,block fading channel,channel delay spread,channel estimation,Channel
	estimation,data detection,Estimation,exact inference,Expectation
	maximization,fading channels,first-order autoregressive model,impulse
	response,Inference algorithms,Joints,Kalman filtering and smoothing,Kalman
	filters,learning (artificial intelligence),low complexity Kalman
	filter,low complexity recursive J-SBL algorithm,OFDM,OFDM modulation,OFDM
	systems,quasistatic channel,recursive Kalman filter,signal detection,Signal
	processing algorithms,sparse Bayesian learning,telecommunication
	computing,time varying channel,time-varying channels,unknown channel
	coefficient,Vectors,wideband wireless channel},
  pages = {3591-3603},
  owner = {afdidehf}
}

@inproceedings{Qi2015,
  title = {Sparse channel estimation based on compressed sensing for massive 	MIMO systems},
  doi = {10.1109/ICC.2015.7249041},
  abstract = {The sparse channel estimation which sufficiently exploits the inherent
	sparsity of wireless channels, is capable of improving the channel
	estimation performance with less pilot overhead. To reduce the pilot
	overhead in massive MIMO systems, sparse channel estimation exploring
	the joint channel sparsity is first proposed, where the channel estimation
	is modeled as a joint sparse recovery problem. Then the block coherence
	of MIMO channels is analyzed for the proposed model, which shows
	that as the number of antennas at the base station grows, the probability
	of joint recovery of the positions of nonzero channel entries will
	increase. Furthermore, an improved algorithm named block optimized
	orthogonal matching pursuit (BOOMP) is also proposed to obtain an
	accurate channel estimate for the model. Simulation results verify
	our analysis and show that the proposed scheme exploring joint channel
	sparsity substantially outperforms the existing methods using individual
	sparse channel estimation.},
  timestamp = {2016-07-11T16:40:06Z},
  booktitle = {Communications (ICC), 2015 IEEE International Conference on},
  author = {Qi, Chenhao and Huang, Yongming and Jin, Shi and Wu, Lenan},
  month = jun,
  year = {2015},
  keywords = {Antennas,block optimized orthogonal matching pursuit,BOOMP,Channel
	estimation,compressed sensing,Compressed Sensing (CS),Downlink,joint
	channel sparsity,Joints,large-scale MIMO,massive MIMO,massive MIMO
	systems,Matching pursuit algorithms,MIMO,MIMO channels,MIMO communication,OFDM,probability,sparse channel estimation,sparse
	channel estimation,wireless channels,wireless
	channels},
  pages = {4558-4563},
  owner = {afdidehf}
}

@article{Qi2014,
  title = {Uplink channel estimation for massive MIMO systems exploring joint 	channel sparsity},
  volume = {50},
  issn = {0013-5194},
  doi = {10.1049/el.2014.2769},
  abstract = {The joint sparsity of uplink channels in massive multi-input-multi-output
	(MIMO) systems is explored and a block sparse model is proposed for
	joint channel estimation. The block coherence of this model is analysed.
	It is indicated that as the number of antennas at the base station
	grows to be infinity, the block coherence will be zero. Then a block
	optimised orthogonal matching pursuit (BOOMP) algorithm is proposed.
	Simulation results verify the analysis and show that the joint estimation
	using the BOOMP algorithm can significantly improve the channel estimation
	performance.},
  timestamp = {2016-07-11T17:10:13Z},
  number = {23},
  journal = {Electronics Letters},
  author = {Qi, Chenhao and Wu, Lenan},
  year = {2014},
  keywords = {Antenna arrays,approximation theory,base station,block coherence analysis,block
	optimised orthogonal matching pursuit algorithm,BOOMP algorithm,Channel
	estimation,joint channel sparsity,massive MIMO systems,massive multiinput-multioutput
	systems,MIMO communication,uplink channel estimation,wireless channels},
  pages = {1770-1772},
  owner = {afdidehf}
}

@article{Qian2015,
  title = {Robust Visual Tracking via Sparse Representation under Subclass Discriminant 	Constraint},
  volume = {PP},
  issn = {1051-8215},
  doi = {10.1109/TCSVT.2015.2424091},
  abstract = {In this paper, we propose a method for visual tracking based on local
	sparse representation. Image patches from the object and the background
	are split into image blocks to construct local representations. Within
	the subclass discriminant framework, a discriminative subspace is
	learned to distinguish the object image blocks from the background
	image blocks while preserving their multimodal structure. A dictionary
	is constructed using the centers of the object subclasses. With this
	dictionary, sparse coding is implemented on the projected vectors
	corresponding to the image blocks, and the sparse coefficients are
	concatenated to obtain a local sparse code as the feature that represents
	the image patch. Considering the subclass discriminant constraint
	and the sparsity constraint imposed on the sparse coding, the subspace
	learning and sparse representation problems are converted into a
	joint optimization problem with respect to a transformation matrix
	and sparse coefficients. To enhance the tracking accuracy, two dictionaries
	are devised, one to incorporate the original observations of the
	target and the other to incorporate the latest observations, thereby
	providing two templates to characterize the appearance of the target.
	Histogram intersection over the local sparse codes provides an evaluation
	of confidence. Finally, the candidate with the maximal confidence
	is selected as the object image patch. Compared with several state-of-the-art
	algorithms, our method demonstrates superior performance when applied
	to challenging sequences.},
  timestamp = {2016-07-10T08:09:05Z},
  number = {99},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  author = {Qian, C. and Xu, Z.},
  year = {2015},
  keywords = {Dictionaries,dictionary learning,encoding,Optimization,sparse matrices,sparse
	representation,subclass discriminant constraint,target tracking,Training,Visualization,Visual
	tracking},
  pages = {1-1},
  owner = {afdidehf}
}

@inproceedings{Qian2014,
  title = {Nonuniform quantization for block-based compressed sensing of images 	in differential pulse-code modulation framework},
  doi = {10.1109/ICSAI.2014.7009392},
  abstract = {In practical signal processing, it is necessary to quantize the sampled
	signals. Quantization is considered a necessary step to digitalize
	signals and realize the high-efficient transmission of digital signals.
	As a new signal processing theory, compressed sensing (CS) which
	is promoted as a joint sampling and compression approach for sparse
	signals has caused wide public concern in the field of image processing.
	In a practical application, although quantization is unavoidable
	for CS measurements, CS literature has largely avoided to discuss
	the topic of quantization. In this paper, differential pulse-code
	modulation(DPCM) is coupled with nonuniform scalar quantization(nonuniform
	SQ) to provide block-based compressed sensing (BCS) quantization
	of images. This paper analyzes the distribution of prediction errors
	in DPCM framework and draws a conclusion that in statistical sense
	such distribution is consistent with the characteristics of nonuniform
	scalar quantization. This discovery provides a theoretical basis
	for the proposed quantization method. Experimental results show that
	the proposed quantization scheme effectively increases the quantized
	signal to noise ratio(SNR), meanwhile improves the quality of reconstructed
	images.},
  timestamp = {2016-07-10T07:00:30Z},
  booktitle = {Systems and Informatics (ICSAI), 2014 2nd International Conference 	on},
  author = {Qian, Cheng and Zheng, Baoyu and Lin, Bilan},
  month = nov,
  year = {2014},
  keywords = {BCS quantization,block-based compressed sensing,block compressed sensing,compressed
	sensing,CS measurements,Current measurement,differential pulse-code
	modulation,digital signals,DPCM,image coding,Image Processing,Image
	Processing,Image reconstruction,nonuniform quantization,nonuniform
	scalar quantization,prediction errors distribution,pulse code modulation,quantisation
	(signal),Quantization,Quantization (signal),Rate-distortion,reconstructed
	images,signal processing,Signal to noise ratio,Size measurement,SNR,Vectors},
  pages = {791-765},
  owner = {afdidehf}
}

@article{Qian2016,
  title = {On stepwise pattern recovery of the fused Lasso},
  volume = {94},
  issn = {0167-9473},
  doi = {http://dx.doi.org/10.1016/j.csda.2015.08.013},
  abstract = {Abstract We study the property of the Fused Lasso Signal Approximator
	(FLSA) for estimating a blocky signal sequence with additive noise.
	We transform the \{FLSA\} to an ordinary Lasso problem, and find
	that in general the resulting design matrix does not satisfy the
	irrepresentable condition that is known as an almost necessary and
	sufficient condition for exact pattern recovery. We give necessary
	and sufficient conditions on the expected signal pattern such that
	the irrepresentable condition holds in the transformed Lasso problem.
	However, these conditions turn out to be very restrictive. We apply
	the newly developed preconditioning method — Puffer Transformation
	(Jia and Rohe, 2015) to the transformed Lasso and call the new procedure
	the preconditioned fused Lasso. We give non-asymptotic results for
	this method, showing that as long as the signal-to-noise ratio is
	not too small, our preconditioned fused Lasso estimator always recovers
	the correct pattern with high probability. Theoretical results give
	insight into what controls the ability of recovering the pattern
	— it is the noise level instead of the length of the signal sequence.
	Simulations further confirm our theorems and visualize the significant
	improvement of the preconditioned fused Lasso estimator over the
	vanilla \{FLSA\} in exact pattern recovery.},
  timestamp = {2016-07-10T07:14:10Z},
  journal = {Computational Statistics \& Data Analysis},
  author = {Qian, Junyang and Jia, Jinzhu},
  year = {2016},
  keywords = {Fused,Lasso,Non-asymptotic,Pattern,Preconditioning,recovery},
  pages = {221 - 237},
  owner = {afdidehf}
}

@article{Qian1994,
  title = {Signal representation using adaptive normalized Gaussian functions},
  volume = {36},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/0165-1684(94)90174-0},
  abstract = {In this paper, a new joint time-frequency signal representation, the
	adaptive Gaussian basis representation (AGR), is presented. Unlike
	the Gabor expansion and the wavelet decomposition, the bandwidth
	and time-frequency centers of the localized Gaussian elementary functions
	hp(t) used in the \{AGR\} can be adjusted to best match the analyzed
	signal. Each expansion coefficient Bp is defined as the inner product
	sp(t) and hp(t), where sp(t) is the remainder of the orthogonal projection
	of sp−1(t) onto hp−1(t). Consequently, the \{AGR\} not only accurately
	captures signal local behavior, but also has a monotonically decreasing
	reconstruction error ‖sp(t)‖2. By combining the \{AGR\} and the
	Wigner-Ville distribution, we further develop an adaptive spectrogram
	that is non-negative, cross-term free, and of high resolution. Finally,
	an efficient numerical algorithm to compute the optimal Gaussian
	elementary functions hp(t) is discussed.},
  timestamp = {2016-09-29T16:07:26Z},
  number = {1},
  journal = {Signal Processing},
  author = {Qian, Shie and Chen, Dapang},
  year = {1994},
  keywords = {expansion,Gabor},
  pages = {1--11},
  owner = {afdidehf}
}

@article{Qian2013,
  title = {Double-level binary tree Bayesian compressed sensing for block structured 	sparse signals},
  volume = {7},
  issn = {1751-9675},
  doi = {10.1049/iet-spr.2012.0180},
  abstract = {Sparsity is one of the key points in the compressed sensing (CS) theory,
	which provides a sub-Nyquist sampling paradigm. Nevertheless, apart
	from sparsity, structures on the sparse patterns such as block structures
	and tree structures can also be exploited to improve the reconstruction
	performance and further reduce the sampling rate in CS framework.
	Based on the fact that the block structure is also sparse for a widely
	studied block sparse signal, in this study, a double-level binary
	tree (DBT) hierarchical Bayesian model is proposed under the Bayesian
	CS (BCS) framework. The authors exploit a recovery algorithm with
	the proposed DBT structured model, and the block clustering in the
	proposed algorithm can be achieved fastly and correctly using the
	Markov Chain Monte Carlo method. The experimental results demonstrate
	that, compared with most existing CS algorithms for block sparse
	signals, our proposed DBT-based BCS algorithm can obtain good recovery
	results with less time consuming.},
  timestamp = {2016-07-08T12:15:58Z},
  number = {8},
  journal = {Signal Processing, IET},
  author = {Qian, Yongqing and Sun, Hong and Le Ruyet, D.},
  month = oct,
  year = {2013},
  keywords = {Bayesian CS framework,belief networks,block clustering,block structured
	sparse signals,block structures,compressed sensing,CS framework,DBT-based
	BCS algorithm,DBT structured model,double-level binary tree Bayesian
	compressed sensing,Markov chain Monte Carlo method,Markov processes,Monte
	Carlo methods,reconstruction performance,sampling rate,signal reconstruction,sparse
	patterns,Sparsity,sub-Nyquist sampling paradigm,trees (mathematics),tree
	structures},
  pages = {774-782},
  owner = {afdidehf}
}

@article{Qian2013a,
  title = {Hyperspectral Imagery Restoration Using Nonlocal Spectral-Spatial 	Structured Sparse Representation With Noise Estimation},
  volume = {6},
  issn = {1939-1404},
  doi = {10.1109/JSTARS.2012.2232904},
  abstract = {Noise reduction is an active research area in image processing due
	to its importance in improving the quality of image for object detection
	and classification. In this paper, we develop a sparse representation
	based noise reduction method for hyperspectral imagery, which is
	dependent on the assumption that the non-noise component in an observed
	signal can be sparsely decomposed over a redundant dictionary while
	the noise component does not have this property. The main contribution
	of the paper is in the introduction of nonlocal similarity and spectral-spatial
	structure of hyperspectral imagery into sparse representation. Non-locality
	means the self-similarity of image, by which a whole image can be
	partitioned into some groups containing similar patches. The similar
	patches in each group are sparsely represented with a shared subset
	of atoms in a dictionary making true signal and noise more easily
	separated. Sparse representation with spectral-spatial structure
	can exploit spectral and spatial joint correlations of hyperspectral
	imagery by using 3-D blocks instead of 2-D patches for sparse coding,
	which also makes true signal and noise more distinguished. Moreover,
	hyperspectral imagery has both signal-independent and signal-dependent
	noises, so a mixed Poisson and Gaussian noise model is used. In order
	to make sparse representation be insensitive to the various noise
	distribution in different blocks, a variance-stabilizing transformation
	(VST) is used to make their variance comparable. The advantages of
	the proposed methods are validated on both synthetic and real hyperspectral
	remote sensing data sets.},
  timestamp = {2016-07-08T12:44:45Z},
  number = {2},
  journal = {Selected Topics in Applied Earth Observations and Remote Sensing, 	IEEE Journal of},
  author = {Qian, Yuntao and Ye, Minchao},
  month = apr,
  year = {2013},
  keywords = {Correlation,Dictionaries,Gaussian noise,Gaussian noise model,geophysical
	image processing,Hyperspectral imagery,hyperspectral imagery restoration,Hyperspectral imaging,Hyperspectral
	imaging,hyperspectral remote sensing data sets,image
	denoising,Image Processing,image quality,image representation,image
	restoration,image self-similarity,mixed Poisson noise model,noise
	estimation,Noise reduction,nonlocality,nonlocal similarity,nonlocal
	spectral-spatial structured sparse representation,nonnoise component,object
	classification,object detection,redundant dictionary,remote sensing,sparse
	representation,variance-stabilizing transformation},
  pages = {499-515},
  owner = {afdidehf}
}

@inproceedings{Qian2012,
  title = {Noise reduction of hyperspectral imagery using nonlocal sparse representation 	with spectral-spatial structure},
  doi = {10.1109/IGARSS.2012.6350674},
  abstract = {Noise reduction is always an active research area in image processing
	due to its importance for the sequential tasks such as object classification
	and detection. In this paper, we develop a sparse representation
	based noise reduction method for hyperspectral imagery, which is
	dependent on the assumption that the non-noise component in the signal
	can be approximated by only a small number of atoms in a dictionary
	while noise component has not this property. The main contribution
	of the paper is in introducing nonlocal similarity and spectral-spatial
	structure of hyperspectral imagery into sparse representation. Non-locality
	means the self-similarity of image, by which the whole image can
	be partitioned into some groups containing similar patches. The similar
	patches in each group is sparsely represented with shared atoms making
	the signal and noise more easily separated. Sparse representation
	with spectral-spatial structure can exploit spectral and spatial
	joint correlations of hyperspectral imagery also making the signal
	and noise more distinguished, in which 3-D blocks are instead of
	2-D patches for sparse coding. The experimental results indicate
	that the proposed method has a good quality of restoring the true
	signal from the noisy observation.},
  timestamp = {2016-07-10T06:55:02Z},
  booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2012 IEEE International},
  author = {Qian, Yuntao and Ye, Minchao and Wang, Qi},
  month = jul,
  year = {2012},
  keywords = {2D patch,3D block,Abstracts,approximation theory,Dictionary,encoding,geophysical
	image processing,hyperspectral image processing,Hyperspectral imagery,Image
	classification,image coding,image denoising,image representation,image
	restoration,image sensors,image sequences,image sequential task,Noise
	reduction,noise reduction method,noise separation,nonlocal similarity,nonlocal
	sparse representation,nonnoise component signal approximation,object
	classification,object detection,signal restoration,signal separation,Sparse
	coding,sparse representation,spatial correlation,spectral correlation,spectral-spatial structure,spectral-spatial
	structure},
  pages = {3467-3470},
  owner = {afdidehf}
}

@article{Qu2011,
  title = {Magneto-photo-acoustic imaging},
  abstract = {Magneto-photo-acoustic imaging, a technique based on the synergy of
	magneto-motive ultrasound, photoacoustic and ultrasound imaging,
	is introduced. Hybrid nanoconstructs, liposomes encapsulating gold
	nanorods and iron oxide nanoparticles, were used as a dual-contrast
	agent for magneto-photo-acoustic imaging. Tissue-mimicking phantom
	and macrophage cells embedded in ex vivo porcine tissue were used
	to demonstrate that magneto-photo-acoustic imaging is capable of
	visualizing the location of cells or tissues labeled with dual-contrast
	nanoparticles with sufficient contrast, excellent contrast resolution
	and high spatial resolution in the context of the anatomical structure
	of the surrounding tissues. Therefore, magneto-photo-acoustic imaging
	is capable of identifying the nanoparticle-labeled pathological regions
	from the normal tissue, providing a promising platform to noninvasively
	diagnose and characterize pathologies.},
  timestamp = {2016-07-09T20:06:51Z},
  journal = {Biomedical Optics Express Optical Society of America},
  author = {Qu, Min and Mallidi, Srivalleesha and Mehrmohammadi, Mohammad and Truby, Ryan and Homan, Kimberly and Joshi, Pratixa and Chen, Yun-Sheng and Sokolov, Konstantin and Emelianov, and {Stanislav}},
  year = {2011},
  pages = {385�396},
  owner = {Fardin}
}

@article{Quan2014,
  title = {Comment on 'Sparse block circulant matrices for compressed sensing'},
  volume = {8},
  issn = {1751-8628},
  doi = {10.1049/iet-com.2014.0032},
  abstract = {In `Sparse block circulant matrices for compressed sensing', in order
	to apply Lemma 4, every off-diagonal element of Gram matrix for the
	sparse block circulant matrix was separated into two component sums
	to make sure the terms in each sum are independent. In this comment,
	however, the authors show that separating every element into two
	sums is not sufficient to guarantee the independency of the terms
	in each sum. The authors also prove that the entries should be split
	into three parts instead of two to satisfy the requirements of Lemma
	4. Finally, the authors modify the deduction and the result.},
  timestamp = {2016-07-08T11:53:44Z},
  number = {11},
  journal = {Communications, IET},
  author = {Quan, Lei and Xiao, Song and Wang, Mengsi},
  month = jul,
  year = {2014},
  keywords = {component sums,compressed sensing,gram matrix,Lemma 4,Lemma
	4,off-diagonal element,off-diagonal
	element,sparse block circulant matrices,sparse matrices,terms independency,terms
	independency},
  pages = {2054-2055},
  owner = {afdidehf}
}

@article{RahnamaRad2011,
  title = {Nearly Sharp Sufficient Conditions on Exact Sparsity Pattern Recovery},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2145670},
  abstract = {Consider the n-dimensional vector y=X?+? where ? ? BBRp has only k
	nonzero entries and ? ? BBRn is a Gaussian noise. This can be viewed
	as a linear system with sparsity constraints corrupted by noise,
	where the objective is to estimate the sparsity pattern of ? given
	the observation vector y and the measurement matrix X. First, we
	derive a nonasymptotic upper bound on the probability that a specific
	wrong sparsity pattern is identified by the maximum-likelihood estimator.
	We find that this probability depends (inversely) exponentially on
	the difference of ||X?||2 and the l2 -norm of X? projected onto the
	range of columns of X indexed by the wrong sparsity pattern. Second,
	when X is randomly drawn from a Gaussian ensemble, we calculate a
	nonasymptotic upper bound on the probability of the maximum-likelihood
	decoder not declaring (partially) the true sparsity pattern. Consequently,
	we obtain sufficient conditions on the sample size n that guarantee
	almost surely the recovery of the true sparsity pattern. We find
	that the required growth rate of sample size n matches the growth
	rate of previously established necessary conditions.},
  timestamp = {2016-07-10T06:48:14Z},
  number = {7},
  journal = {Information Theory, IEEE Transactions on},
  author = {Rahnama Rad, K.},
  month = jul,
  year = {2011},
  keywords = {Decoding,Eigenvalues and eigenfunctions,Error probability,exact sparsity
	pattern recovery,Gaussian ensemble,Gaussian noise,Hypothesis testing,Hypothesis
	testing,linear system,linear
	system,mathematical model,maximum likelihood decoder,maximum
	likelihood estimation,Maximum likelihood
	estimation,maximum likelihood estimator,Noise measurement,nonasymptotic
	upper bound,random projections,sharp sufficient condition,sparsity
	pattern recovery,subset selection,Sufficient conditions,underdetermined
	systems of equations,Upper bound,wrong sparsity pattern},
  pages = {4672-4679},
  owner = {Fardin}
}

@inproceedings{Rao2004,
  title = {Diversity measure minimization based method for computing sparse 	solutions to linear inverse problems with multiple measurement vectors},
  volume = {2},
  doi = {10.1109/ICASSP.2004.1326271},
  abstract = {The problem of computing sparse solutions to linear inverse problems
	arises in a large number of signal processing application areas.
	We address the problem of finding sparse solutions to linear inverse
	problems when there are multiple measurement vectors (MMV) and the
	solutions are assumed to have a common, but unknown, sparsity profile.
	This is an important extension to the single measurement sparse solution
	problem that has been extensively studied in the past. Of particular
	interest are methods based on minimizing diversity measures. A measure
	appropriate for the multiple measurement problem is developed, and
	an algorithm is derived based on its minimization. The algorithm
	developed, M-FOCUSS, generalizes the focal underdetermined system
	solver (FOCUSS) algorithm developed for the single measurement case.
	The convergence of the algorithm is established and a simulation
	study is conducted to evaluate its effectiveness. The results clearly
	show the ability of M-FOCUSS to utilize multiple measurement vectors
	for accurate identification of the sparsity structure and sparse
	solution computation.},
  timestamp = {2016-07-08T12:15:22Z},
  booktitle = {Acoustics, Speech, and Signal Processing, 2004. Proceedings. (ICASSP 	'04). IEEE International Conference on},
  author = {Rao, B.D. and Engan, K. and Cotter, S.},
  month = may,
  year = {2004},
  keywords = {Computational modeling,Dictionaries,diversity measure minimization
	method,Diversity methods,Electric variables measurement,focal underdetermined
	system solver,Focusing,inverse problems,Linear inverse problems,Matching
	pursuit algorithms,minimisation,Minimization methods,multiple measurement
	vectors,Particle measurements,signal processing,sparse solutions,sparsity
	profile,Vectors},
  pages = {ii-369-72 vol.2},
  owner = {Fardin}
}

@article{Rao1999,
  title = {An affine scaling methodology for best basis selection},
  volume = {47},
  issn = {1053-587X},
  doi = {10.1109/78.738251},
  abstract = {A methodology is developed to derive algorithms for optimal basis
	selection by minimizing diversity measures proposed by Wickerhauser
	(1994) and Donoho (1994). These measures include the p-norm-like
	(l(p?1)) diversity measures and the Gaussian and Shannon entropies.
	The algorithm development methodology uses a factored representation
	for the gradient and involves successive relaxation of the Lagrangian
	necessary condition. This yields algorithms that are intimately related
	to the affine scaling transformation (AST) based methods commonly
	employed by the interior point approach to nonlinear optimization.
	The algorithms minimizing the (l(p?1)) diversity measures are equivalent
	to a previously developed class of algorithms called focal underdetermined
	system solver (FOCUSS). The general nature of the methodology provides
	a systematic approach for deriving this class of algorithms and a
	natural mechanism for extending them. It also facilitates a better
	understanding of the convergence behavior and a strengthening of
	the convergence results. The Gaussian entropy minimization algorithm
	is shown to be equivalent to a well-behaved p=0 norm-like optimization
	algorithm. Computer experiments demonstrate that the p-norm-like
	and the Gaussian entropy algorithms perform well, converging to sparse
	solutions. The Shannon entropy algorithm produces solutions that
	are concentrated but are shown to not converge to a fully sparse
	solution},
  timestamp = {2016-07-08T10:28:34Z},
  number = {1},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Rao, B.D. and Kreutz-Delgado, K.},
  month = jan,
  year = {1999},
  keywords = {affine scaling methodology,algorithm development methodology,Area
	measurement,AST,best basis selection,convergence behavior,convergence
	of numerical methods,Dictionaries,diversity measures,Entropy,factored
	representation,focal underdetermined system solver,FOCUSS,Gaussian
	entropy,Gaussian entropy minimization algorithm,Gaussian processes,gradient,interior
	point approach,Lagrangian functions,Lagrangian necessary condition,Matching
	pursuit algorithms,minimisation,Minimization methods,nonlinear optimization,optimal
	basis selection,Optimization methods,p-norm-like diversity measure,Shannon
	entropy,Signal processing algorithms,signal representation,sparse
	matrices,sparse solution,successive relaxation,Terminology,transforms,Wavelet
	packets},
  pages = {187-200},
  owner = {Fardin}
}

@inproceedings{Rao2012,
  title = {Sparse signal recovery in the presence of intra-vector and inter-vector 	correlation},
  doi = {10.1109/SPCOM.2012.6290242},
  abstract = {This work discusses the problem of sparse signal recovery when there
	is correlation among the values of nonzero entries. We examine intra-vector
	correlation in the context of the block sparse model and inter-vector
	correlation in the context of the multiple measurement vector model,
	as well as their combination. Algorithms based on the sparse Bayesian
	learning are presented and the benefits of incorporating correlation
	at the algorithm level are discussed. The impact of correlation on
	the limits of support recovery is also discussed highlighting the
	different impact intra-vector and inter-vector correlations have
	on such limits.},
  timestamp = {2016-07-11T16:53:17Z},
  booktitle = {Signal Processing and Communications (SPCOM), 2012 International 	Conference on},
  author = {Rao, B.D. and Zhang, Zhilin and Jin, Yuzhe},
  month = jul,
  year = {2012},
  keywords = {Adaptation models,Bayesian methods,Context,Context modeling,Correlation,correlation
	methods,inter-vector correlation,intra-vector correlation,multiple
	measurement vector model,signal processing,sparse Bayesian learning,sparse matrices,sparse
	matrices,sparse signal recovery,Vectors},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Rath2008,
  title = {Sparse approximations for joint source-channel coding},
  doi = {10.1109/MMSP.2008.4665126},
  abstract = {This paper considers the application of sparse approximations in a
	joint source-channel (JSC) coding framework. The considered JSC coded
	system employs a real number BCH code on the input signal before
	the signal is quantized and further processed. Under an impulse channel
	noise model, the decoding of error is posed as a sparse approximation
	problem. The orthogonal matching pursuit (OMP) and basis pursuit
	(BP) algorithms are compared with the syndrome decoding algorithm
	in terms of mean square reconstruction error. It is seen that, with
	a Gauss-Markov source and Bernoulli-Gaussian channel noise, the BP
	outperforms the syndrome decoding and the OMP at higher noise levels.
	In the case of image transmission with channel bit errors, the BP
	outperforms the other two decoding algorithms consistently.},
  timestamp = {2016-07-11T16:39:02Z},
  booktitle = {Multimedia Signal Processing, 2008 IEEE 10th Workshop on},
  author = {Rath, Gagan and Guillemot, C. and Fuchs, J.-J.},
  month = oct,
  year = {2008},
  keywords = {basis pursuit algorithms,BCH codes,Bernoulli-Gaussian channel noise,bit
	error rate,Bose-Chaudhuri-Hocquenghem codes,channel bit errors,Channel
	coding,combined source-channel coding,Decoding,Discrete Fourier transforms,Equations,Error
	correction,error correction codes,error decoding,error statistics,Gaussian
	channels,Gauss-Markov source channel noise,image coding,image transmission,impulse
	channel noise model,joint source-channel coding,JSC,Matching pursuit
	algorithms,mean square error methods,mean square reconstruction error,Noise
	level,OMP,orthogonal matching pursuit,Pursuit algorithms,signal processing,signal
	quantization,sparse approximations,sparse matrices},
  pages = {481-485},
  owner = {Fardin}
}

@article{Rathi2014,
  title = {Multi-shell diffusion signal recovery from sparse measurements},
  volume = {18},
  issn = {1361-8415},
  doi = {http://dx.doi.org/10.1016/j.media.2014.06.003},
  abstract = {Abstract For accurate estimation of the ensemble average diffusion
	propagator (EAP), traditional multi-shell diffusion imaging (MSDI)
	approaches require acquisition of diffusion signals for a range of
	b-values. However, this makes the acquisition time too long for several
	types of patients, making it difficult to use in a clinical setting.
	In this work, we propose a new method for the reconstruction of diffusion
	signals in the entire q-space from highly undersampled sets of \{MSDI\}
	data, thus reducing the scan time significantly. In particular, to
	sparsely represent the diffusion signal over multiple q-shells, we
	propose a novel extension to the framework of spherical ridgelets
	by accurately modeling the monotonically decreasing radial component
	of the diffusion signal. Further, we enforce the reconstructed signal
	to have smooth spatial regularity in the brain, by minimizing the
	total variation (TV) norm. We combine these requirements into a novel
	cost function and derive an optimal solution using the Alternating
	Directions Method of Multipliers (ADMM) algorithm. We use a physical
	phantom data set with known fiber crossing angle of 45° to determine
	the optimal number of measurements (gradient directions and b-values)
	needed for accurate signal recovery. We compare our technique with
	a state-of-the-art sparse reconstruction method (i.e., the \{SHORE\}
	method of Cheng et al. (2010)) in terms of angular error in estimating
	the crossing angle, incorrect number of peaks detected, normalized
	mean squared error in signal recovery as well as error in estimating
	the return-to-origin probability (RTOP). Finally, we also demonstrate
	the behavior of the proposed technique on human in vivo data sets.
	Based on these experiments, we conclude that using the proposed algorithm,
	at least 60 measurements (spread over three b-value shells) are needed
	for proper recovery of \{MSDI\} data in the entire q-space.},
  timestamp = {2016-07-10T06:47:52Z},
  number = {7},
  journal = {Medical Image Analysis},
  author = {Rathi, Y. and Michailovich, O. and Laun, F. and Setsompop, K. and Grant, P. E. and Westin, C.-F.},
  year = {2014},
  keywords = {Compressed,Diffusion,imaging,Kurtosis,MRI,propagator,sensing,spectrum},
  pages = {1143 - 1156},
  owner = {afdidehf}
}

@article{Rauhut2015,
  title = {Interpolation via weighted minimization},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2015.02.003},
  abstract = {Abstract Functions of interest are often smooth and sparse in some
	sense, and both priors should be taken into account when interpolating
	sampled data. Classical linear interpolation methods are effective
	under strong regularity assumptions, but cannot incorporate nonlinear
	sparsity structure. At the same time, nonlinear methods such as l
	1 minimization can reconstruct sparse functions from very few samples,
	but do not necessarily encourage smoothness. Here we show that weighted
	l 1 minimization effectively merges the two approaches, promoting
	both sparsity and smoothness in reconstruction. More precisely, we
	provide specific choices of weights in the l 1 objective to achieve
	approximation rates for functions with coefficient sequences in weighted
	l p spaces with p ≤ 1 . We consider implications of these results
	for spherical harmonic and polynomial interpolation, in the univariate
	and multivariate setting. Along the way, we extend concepts from
	compressive sensing such as the restricted isometry property and
	null space property to accommodate weighted sparse expansions; these
	developments should be of independent interest in the study of structured
	sparse approximations and continuous-time compressive sensing problems.},
  timestamp = {2016-07-08T12:50:28Z},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Rauhut, Holger and Ward, Rachel},
  year = {2015},
  keywords = {Bounded,Compressive,interpolation,L1,Minimization,orthonormal,sensing,Sparsity,systems,Weighted},
  pages = {-},
  owner = {afdidehf}
}

@article{Rebollo-Neira2013,
  title = {Hierarchized Block Wise Image Approximation by Greedy Pursuit Strategies},
  volume = {20},
  issn = {1070-9908},
  doi = {10.1109/LSP.2013.2283510},
  abstract = {An approach for effective implementation of greedy selection methodologies,
	to approximate an image partitioned into blocks, is proposed. The
	method is specially designed for approximating partitions on a transformed
	image. It evolves by selecting, at each iteration step, i) the elements
	for approximating each of the blocks partitioning the image and ii)
	the hierarchized sequence in which the blocks are approximated to
	reach the required global condition on sparsity.},
  timestamp = {2016-07-08T12:42:03Z},
  number = {12},
  journal = {Signal Processing Letters, IEEE},
  author = {Rebollo-Neira, L. and Maciol, R. and Bibi, S.},
  month = dec,
  year = {2013},
  keywords = {approximating partitions,Approximation algorithms,Approximation methods,approximation
	theory,block partitioning,Dictionaries,Greedy algorithms,greedy pursuit
	strategy,greedy selection methodology,hierarchized block wise image
	approximation,hierarchized image sequence,High quality sparse image
	approximation with separable dictionaries,image coding,Image segmentation,image
	sequences,iterative methods,Matching pursuit algorithms,Orthogonal
	Matching Pursuit for sparse representation of partitions in the wavelet
	domain,Wavelet domain},
  pages = {1175-1178},
  owner = {afdidehf}
}

@article{Recht2011,
  title = {Null space conditions and thresholds for rank minimization},
  volume = {127},
  doi = {10.1007/s10107-010-0422-2},
  abstract = {Minimizing the rank of a matrix subject to constraints is a challenging
	problem that arises in many applications in machine learning, control
	theory, and discrete geometry. This class of optimization problems,
	known as rank minimization, is NP-hard, and for most practical problems
	there are no efficient algorithms that yield exact solutions. A popular
	heuristic replaces the rank function with the nuclear norm�equal
	to the sum of the singular values�of the decision variable and
	has been shown to provide the optimal low rank solution in a variety
	of scenarios. In this paper, we assess the practical performance
	of this heuristic for finding the minimum rank matrix subject to
	linear equality constraints. We characterize properties of the null
	space of the linear operator defining the constraint set that are
	necessary and sufficient for the heuristic to succeed. We then analyze
	linear constraints sampled uniformly at random, and obtain dimension-free
	bounds under which our null space properties hold almost surely as
	the matrix dimensions tend to infinity. Finally, we provide empirical
	evidence that these probabilistic bounds provide accurate predictions
	of the heuristic�s performance in non-asymptotic scenarios.},
  timestamp = {2016-07-10T07:02:15Z},
  number = {1},
  journal = {Mathematical Programming},
  author = {Recht, Benjamin and Xu, Weiyu and Hassibi, Babak},
  year = {2011},
  keywords = {�,Compressed,Convex,Gaussian,matrices,Matrix,norms,Optimization,processes,Random,rank,sensing},
  pages = {175-202},
  masid = {10787874},
  owner = {afdidehf}
}

@inproceedings{Ren2014,
  title = {Joint sensors-sources association and tracking},
  doi = {10.1109/SAM.2014.6882376},
  abstract = {This paper considers the problem of tracking multiple sources using
	observations acquired at spatially scattered sensors. Kalman filtering
	and smoothing techniques are combined with a sparse matrix estimation
	framework. A pertinent normone regularized minimization formulation
	is proposed that jointly searches for source-informative sensors,
	associates sources with sensors and tracks the unknown sources. Block
	coordinate descent techniques are used to recover the unknown sparse
	observation matrix, and subsequently obtain source state estimates.
	Numerical tests are provided to demonstrate the potential of the
	novel approach to identify the source-informative sensors and accurately
	track the field sources.},
  timestamp = {2016-07-09T19:49:33Z},
  booktitle = {Sensor Array and Multichannel Signal Processing Workshop (SAM), 2014 	IEEE 8th},
  author = {Ren, Guohua and Schizas, I.D. and Maroulas, V.},
  month = jun,
  year = {2014},
  keywords = {associate source,Attenuation,block coordinate descent technique,Covariance
	matrices,field source tracking,joint sensor-source association,joint
	sensor-source tracking,Kalman filtering technique,Kalman filters,minimisation,Minimization,multiple-source
	tracking problem,Noise,numerical test,pertinent normone regularized
	minimization formulation,Sensors,smoothing methods,smoothing technique,source-informative
	sensors,source state estimation,sparse matrices,sparse matrix estimation
	framework,sparse
	matrix estimation framework,spatially-scattered sensors,unknown sparse observation
	matrix recovery,unknown sparse
	observation matrix recovery},
  pages = {205-208},
  owner = {afdidehf}
}

@article{Ritov2007,
  title = {Discussion: the dantzig selector: statistical estimation when p is 	much larger than n},
  volume = {35},
  doi = {10.1214/009053607000000451},
  timestamp = {2016-07-08T12:13:27Z},
  number = {6},
  journal = {The Annals of Statistics},
  author = {Ritov, Ya�acov},
  year = {2007},
  pages = {2370�2372},
  owner = {afdidehf}
}

@book{Robards2011,
  edition = {1},
  series = {Lecture Notes in Computer Science 6913 Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, 	Part III},
  isbn = {3-642-23807-6 978-3-642-23807-9 978-3-642-23808-6 3-642-23808-4},
  timestamp = {2016-10-24T16:41:02Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Robards, Matthew and Sunehag, Peter and Marthi, Scott Sanner Bhaskara},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  owner = {Fardin}
}

@inproceedings{Romer2014,
  title = {Sparsity order estimation for single snapshot compressed sensing},
  doi = {10.1109/ACSSC.2014.7094653},
  abstract = {In this paper we discuss the estimation of the spar-sity order for
	a Compressed Sensing scenario where only a single snapshot is available.
	We demonstrate that a specific design of the sensing matrix based
	on Khatri-Rao products enables us to transform this problem into
	the estimation of a matrix rank in the presence of additive noise.
	Thereby, we can apply existing model order selection algorithms to
	determine the sparsity order. The matrix is a rearranged version
	of the observation vector which can be constructed by concatenating
	a series of non-overlapping or overlapping blocks of the original
	observation vector. In both cases, a Khatri-Rao structured measurement
	matrix is required with the main difference that in the latter case,
	one of the factors must be a Vandermonde matrix. We discuss the choice
	of the parameters and show that an increasing amount of block overlap
	improves the sparsity order estimation but it increases the coherence
	of the sensing matrix. We also explain briefly that the proposed
	measurement matrix design introduces certain multilinear structures
	into the observations which enables us to apply tensor-based signal
	processing, e.g., for enhanced denoising or improved sparsity order
	estimation.},
  timestamp = {2016-07-11T16:55:14Z},
  booktitle = {Signals, Systems and Computers, 2014 48th Asilomar Conference on},
  author = {Romer, F. and Lavrenko, A. and Del Galdo, G. and Hotz, T. and Arikan, O. and Thoma, R.S.},
  month = nov,
  year = {2014},
  keywords = {Additive noise,Coherence,compressed sensing,Estimation,Estimation
	theory,Khatri-Rao products,Khatri-Rao structured measurement matrix
	design,matrix algebra,matrix rank estimation,model selection algorithms,nonoverlapping
	blocks,overlapping blocks,sensing matrix,Sensors,Signal to noise
	ratio,single snapshot compressed sensing,sparse matrices,sparsity
	order estimation,Tensile stress,tensor-based signal processing,Vandermonde
	matrix},
  pages = {1220-1224},
  owner = {afdidehf}
}

@inproceedings{Rusu2013,
  title = {Block orthonormal overcomplete dictionary learning},
  abstract = {In the field of sparse representations, the overcomplete dictionary
	learning problem is of crucial importance and has a growing application
	pool where it is used. In this paper we present an iterative dictionary
	learning algorithm based on the singular value decomposition that
	efficiently construct unions of orthonormal bases. The important
	innovation described in this paper, that affects positively the running
	time of the learning procedures, is the way in which the sparse representations
	are computed - data are reconstructed in a single orthonormal base,
	avoiding slow sparse approximation algorithms - how the bases in
	the union are used and updated individually and how the union itself
	is expanded by looking at the worst reconstructed data items. The
	numerical experiments show conclusively the speedup induced by our
	method when compared to previous works, for the same target representation
	error.},
  timestamp = {2016-07-08T11:41:34Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2013 Proceedings of the 21st 	European},
  author = {Rusu, C. and Dumitrescu, B.},
  month = sep,
  year = {2013},
  keywords = {Approximation algorithms,Approximation methods,block orthonormal,compressed
	sensing,data reconstruction,Dictionaries,iterative dictionary learning
	algorithm,iterative methods,learning (artificial intelligence),Learning
	systems,Matching pursuit algorithms,orthogonal blocks,orthonormal
	base,overcomplete dictionary learning,overcomplete dictionary learning
	problem,signal representation,Singular Value Decomposition,slow sparse
	approximation algorithms,sparse matrices,sparse representations,target
	representation error,Training},
  pages = {1-5},
  owner = {afdidehf}
}

@book{Saha2013,
  edition = {1},
  series = {Lecture Notes in Computer Science 8190},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, 	Part III},
  isbn = {978-3-642-40993-6 978-3-642-40994-3},
  timestamp = {2016-10-24T16:41:23Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Saha, Baidya Nath and Kunapuli, Gautam and Ray, Nilanjan and Maldjian, Joseph A. and Natarajan, Sriraam},
  editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and �elezn�, Filip},
  year = {2013},
  owner = {Fardin}
}

@article{Sanandaji2012,
  title = {A tutorial on recovery conditions for compressive system identification 	of sparse channels},
  issn = {0743-1546},
  doi = {10.1109/CDC.2012.6426078},
  abstract = {In this tutorial, we review some of the recent results concerning
	Compressive System Identification (CSI) (identification from few
	measurements) of sparse channels (and in general, Finite Impulse
	Response (FIR) systems) when it is known a priori that the impulse
	response of the system under study is sparse (high-dimensional but
	with few non-zero entries) in an appropriate basis. For the systems
	under study in this tutorial, the system identification problem boils
	down to an inverse problem of the form Ax = b, where the vector x
	? ?N is high-dimensional but with K ? N nonzero entries and the matrix
	A ? ?M�N is underdetermined (i.e., M <; N). Over the past few years,
	several algorithms with corresponding recovery conditions have been
	proposed to perform such a recovery. These conditions provide the
	number of measurements sufficient for correct recovery. In this note,
	we review alternate approaches to derive such recovery conditions
	concerning CSI of FIR systems whose impulse response is known to
	be sparse.},
  timestamp = {2016-07-08T11:33:56Z},
  journal = {Decision and Control (CDC), 2012 IEEE 51st Annual Conference on},
  author = {Sanandaji, B.M. and Vincent, T.L. and Poolla, K. and Wakin, M.B.},
  month = dec,
  year = {2012},
  keywords = {channel estimation,compressed sensing,compressive system identification,Convolution,CSI,Eigenvalues
	and eigenfunctions,Finite impulse response filter,finite impulse
	response systems,FIR filters,FIR systems,identification,impulse response,inverse
	problem,inverse problems,Linear matrix inequalities,matrix algebra,nonzero
	entries,Random variables,recovery conditions,sparse channels,transient
	response,Tutorials},
  pages = {6277-6283},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@article{Sanandaji2014,
  title = {Observability With Random Observations},
  volume = {59},
  issn = {0018-9286},
  doi = {10.1109/TAC.2014.2351693},
  abstract = {Recovery of the initial state of a high-dimensional system can require
	a large number of measurements. In this paper, we explain how this
	burden can be significantly reduced when randomized measurement operators
	are employed. Our work builds upon recent results from Compressive
	Sensing (CS). In particular, we make the connection to CS analysis
	for random block diagonal matrices. By deriving Concentration of
	Measure (CoM) inequalities, we show that the observability matrix
	satisfies the Restricted Isometry Property (RIP) (a sufficient condition
	for stable recovery of sparse vectors) under certain conditions on
	the state transition matrix. For example, we show that if the state
	transition matrix is unitary, and if independent, randomly-populated
	measurement matrices are employed, then it is possible to uniquely
	recover a sparse high-dimensional initial state when the total number
	of measurements scales linearly in the sparsity level (the number
	of non-zero entries) of the initial state and logarithmically in
	the state dimension. We further extend our RIP analysis for scaled
	unitary and symmetric state transition matrices. We support our analysis
	with a case study of a diffusion process.},
  timestamp = {2016-07-10T07:10:42Z},
  number = {11},
  journal = {Automatic Control, IEEE Transactions on},
  author = {Sanandaji, B.M. and Wakin, M.B. and Vincent, T.L.},
  month = nov,
  year = {2014},
  keywords = {Atmospheric measurements,block diagonal matrices,CoM inequalities,compressed
	sensing,compressive sensing,Concentration of measure,concentration-of-measure
	inequalities,CS analysis,diffusion process,high-dimensional system,linearly
	scaled measurements,Linear matrix inequalities,logarithmically scaled
	measurements,nonzero entries,observability,observability matrix,random
	block diagonal matrices,randomized measurement operators,randomly-populated
	measurement matrices,random observations,random processes,Random
	variables,restricted isometry property,RIP analysis,RIP
	analysis,scaled symmetric state transition matrices,scaled symmetric
	state transition matrices,scaled unitary state transition matrices,scaled unitary
	state transition matrices,sparse high-dimensional initial state recovery,sparse
	high-dimensional initial state recovery,sparse matrices,sparse
	matrices,sparse vectors,sparsity level,sparsity
	level,stable recovery,state dimension,sufficient condition,sufficient
	condition,Symmetric matrices,Symmetric
	matrices,Vectors},
  pages = {3002-3007},
  owner = {afdidehf}
}

@inproceedings{Sandeep2014,
  title = {Supervised dictionary learning for signals from union of subspaces},
  doi = {10.1109/SPCOM.2014.6983917},
  abstract = {Dictionary learning algorithms are used to train an overcomplete dictionary
	from a set of signal examples such that the learnt dictionary provides
	sparse representations for a class of signals from which the training
	examples are sampled. In this work, we consider a specific class
	of signals, i.e., signals which belong to a union of subspaces, and
	we propose a dictionary learning algorithm for such type of signals
	by extending the popular K-SVD algorithm. Apart from the traditional
	sparsity model, we also incorporate the union of subspaces model
	into the dictionary learning algorithm. Various experiments using
	synthetic and real data demonstrate that the proposed algorithm recovers
	a dictionary which is closer to the underlying unknown dictionary
	than the one obtained from a simple K-SVD algorithm which do not
	make use of the additional structure contained in the signal examples.},
  timestamp = {2016-07-11T16:59:03Z},
  booktitle = {Signal Processing and Communications (SPCOM), 2014 International 	Conference on},
  author = {Sandeep, P. and Jacob, T.},
  month = jul,
  year = {2014},
  keywords = {Dictionaries,dictionary learning algorithm,K-SVD algorithm,learning
	(artificial intelligence),learnt dictionary,Matching pursuit algorithms,overcomplete
	dictionary,signal examples,Signal processing algorithms,signal representation,Signal
	to noise ratio,Singular Value Decomposition,sparse matrices,sparse
	representations,sparsity model,subspaces union,supervised dictionary
	learning,Training,Vectors},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Schepker2012,
  title = {Compressive Sensing Multi-User Detection with Block-Wise Orthogonal 	Least Squares},
  doi = {10.1109/VETECS.2012.6240301},
  abstract = {One challenging future application in digital communications is the
	wireless uplink transmission in sensor networks. This application
	is characterized by sporadic transmissions by a large number of sensors
	over a random multiple access channel. To reduce control signaling
	overhead, we propose that sensors do not transmit their activity
	states; instead sensor activity is detected at the receiver. As sensors
	have low activity probabilities, the multi-user vector is in general
	sparse. This enables Compressive Sensing (CS) detectors to perform
	joint Multi-User Detection (MUD) of activity and data, by exploiting
	the sparsity. Since sensors are either active or inactive for several
	symbol durations, block-wise CS detection can be applied to improve
	the activity detection. In this paper, we introduce blockwise greedy
	CS MUD, compare it to symbol-wise greedy CS MUD, and show that statistically
	independent channels for each symbol further improve the activity
	detection for block-wise CS detection. Herein, we use Code Division
	Multiple Access (CDMA) as a multiple access scheme.},
  timestamp = {2016-07-08T12:00:54Z},
  booktitle = {Vehicular Technology Conference (VTC Spring), 2012 IEEE 75th},
  author = {Schepker, H.F. and Dekorsy, A.},
  month = may,
  year = {2012},
  keywords = {blockwise greedy CS MUD,blockwise orthogonal least squares,CDMA,code
	division multiple access,compressive sensing multiuser detection,Detectors,digital
	communications,Estimation,Matching pursuit algorithms,Multiaccess
	communication,multiuser detection,multiuser vector,random multiple
	access channel,Receivers,sensor networks,sporadic transmissions,symbolwise
	greedy CS MUD,Vectors,Wireless sensor networks,wireless uplink transmission},
  pages = {1-5},
  owner = {afdidehf}
}

@article{Schimpf2002,
  title = {Dipole models for the EEG and MEG},
  volume = {49},
  issn = {0018-9294},
  doi = {10.1109/10.995679},
  abstract = {The current dipole is a widely used source model in forward and inverse
	electroencephalography and magnetoencephalography applications. Analytic
	solutions to the governing field equations have been developed for
	several approximations of the human head using ideal dipoles as the
	source model. Numeric approaches such as the finite-element and finite-difference
	methods have become popular because they allow the use of anatomically
	realistic head models and the increased computational power that
	they require has become readily available. Although numeric methods
	can represent more realistic domains, the sources in such models
	are an approximation of the ideal dipole. In this paper, we examine
	several methods for representing dipole sources in finite-element
	models and compare the resulting surface potentials and external
	magnetic field with those obtained from analytic solutions using
	ideal dipoles.},
  timestamp = {2016-07-08T12:11:32Z},
  number = {5},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Schimpf, P.H. and Ramon, Ceon and Haueisen, J.},
  month = may,
  year = {2002},
  keywords = {anatomically realistic head models,bioelectric potentials,boundary
	condition,Brain Mapping,Brain modeling,brain models,Computer-Assisted,Computer
	Simulation,current dipole,dipole models,EEG,Electric Conductivity,electroencephalography,Electromagnetic
	Fields,Electrophysiology,Equations,external magnetic field,finite-difference
	method,Finite difference methods,finite element analysis,finite element
	analysis,finite-element method,Finite element methods,forward problem,Galerkin method,Galerkin
	method,hexahedral parallelipipeds,Humans,ideal dipoles,Inverse
	problems,Magnetic analysis,Magnetic fields,Magnetic heads,magnetoencephalography,MEG,Membrane
	Potentials,Models,Neurological,numerical analysis,Poisson equation,source
	model,surface potentials,transverse dipoles,volume discretizing numeric
	methods},
  pages = {409-418},
  owner = {Fardin}
}

@article{Schmidt2007,
  title = {Fast Optimization Methods for $L_1$ Regularization: A Comparative 	Study and Two New Approaches},
  volume = {4701},
  abstract = {L1 regularization is effective for feature selection, but the resulting
	optimization is challenging due to the non-differentiability of the
	1-norm. In this paper we compare state-of-the-art optimization techniques
	to solve this problem across several loss functions. Furthermore,
	we propose two new techniques. The first is based on a smooth (differentiable)
	convex approximation for the L1 regularizer that does not depend
	on any assumptions about the loss function used. The other technique
	is a new strategy that addresses the non-differentiability of the
	L1-regularizer by casting the problem as a constrained optimization
	problem that is then solved using a specialized gradient projection
	method. Extensive comparisons show that our newly proposed approaches
	consistently rank among the best in terms of convergence speed and
	efficiency by measuring the number of function evaluations required.},
  timestamp = {2016-07-08T12:28:06Z},
  journal = {Machine Learning: ECML 2007},
  author = {Schmidt, Mark and Fung, Glenn and Rosales, R�mer},
  year = {2007},
  pages = {286-297},
  owner = {afdidehf}
}

@inproceedings{Schmutzhard2010,
  title = {A lower bound on the estimator variance for the sparse linear model},
  doi = {10.1109/ACSSC.2010.5757886},
  abstract = {We study the performance of estimators of a sparse nonrandom vector
	based on an observation which is linearly transformed and corrupted
	by white Gaussian noise. Using the framework of reproducing kernel
	Hilbert spaces, we derive a new lower bound on the estimator variance
	for a given differentiable bias function (including the unbiased
	case) and an almost arbitrary transformation matrix (including the
	underdetermined case considered in compressed sensing theory). For
	the special case of a sparse vector corrupted by white Gaussian noise-i.e.,
	without a linear transformation-and unbiased estimation, our lower
	bound improves on a previously proposed bound.},
  timestamp = {2016-07-08T10:25:57Z},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2010 Conference Record 	of the Forty Fourth Asilomar Conference on},
  author = {Schmutzhard, S. and Jung, A. and Hlawatsch, F. and Ben-Haim, Z. and Eldar, Y.C.},
  month = nov,
  year = {2010},
  keywords = {arbitrary transformation matrix,denoising,differentiable bias function,estimator
	variance,Gaussian noise,Hilbert space,Hilbert spaces,Indexes,Kernel,kernel
	Hilbert spaces,lower bound,matrix algebra,Maximum likelihood estimation,parameter
	estimation,reproducing kernel Hilbert space,RKHS,signal reconstruction,Signal
	to noise ratio,sparse linear model,sparse nonrandom vector,Sparsity,variance
	bound,white Gaussian noise},
  pages = {1976-1980},
  owner = {afdidehf}
}

@article{Schneiderxxxx,
  title = {Choice of regularization parameter},
  timestamp = {2016-07-08T11:50:03Z},
  author = {Schneider, Tapio},
  year = {xxxx},
  owner = {Fardin}
}

@article{Schoffelen2014,
  title = {FieldTrip, a tool for GUI-less exploration of brain dynamics},
  timestamp = {2016-07-08T12:30:24Z},
  author = {Schoffelen, Jan-Mathijs},
  year = {2014},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Schoffelen2009,
  title = {Source Connectivity Analysis With MEG and EEG},
  volume = {30},
  doi = {doi:\\\%002010.1002/hbm.20745},
  abstract = {Interactions between functionally specialized brain regions are crucial
	for normal brain function. Magnetoencephalography (MEG) and electroencephalography
	(EEG) are techniques suited to capture these interactions, because
	they provide whole head measurements of brain activity in the millisecond
	range. More than one sensor picks up the activity of an underlying
	source. This field spread severely limits the utility of connectivity
	measures computed directly between sensor recordings. Consequentially,
	neuronal interactions should be studied on the level of the reconstructed
	sources. This article reviews several methods that have been applied
	to investigate interactions between brain regions in source space.
	We will mainly focus on the different measures used to quantify connectivity,
	and on the different strategies adopted to identify regions of interest.
	Despite various successful accounts of MEG and EEG source connectivity,
	caution with respect to the interpretation of the results is still
	warranted. This is due to the fact that effects of field spread can
	never be completely abolished in source space. However, in this very
	exciting and developing field of research this cautionary note should
	not discourage researchers from further investigation into the connectivity
	between neuronal sources.},
  timestamp = {2016-07-10T08:22:47Z},
  number = {6},
  journal = {Human Brain Mapping},
  author = {Schoffelen, Jan-Mathijs and Gross, Joachim},
  year = {2009},
  keywords = {Coherence,connectivity,EEG,electroencephalography,magnetoencephalography,MEG,Source
	localization,synchronization},
  pages = {1857�1865},
  owner = {Fardin}
}

@techreport{Selesnick2013,
  title = {Least Squares with Examples in Signal Processing},
  abstract = {These notes address (approximate) solutions to linear equations by
	least squares. We deal with the `easy' case wherein the system matrix
	is full rank. If the system matrix is rank decient, then other methods
	are needed, e.g., QR decomposition, singular value decomposition,
	or the pseudo-inverse, [2, 3]. In these notes, least squares is illustrated
	by applying it to several basic problems in signal processing: 1.
	Linear prediction 2. Smoothing 3. Deconvolution 4. System identification
	5. Estimating missing data For the use of least squares in lter
	design, see [1].},
  timestamp = {2016-07-09T19:56:55Z},
  author = {Selesnick, Ivan},
  year = {2013},
  owner = {Fardin}
}

@inproceedings{Sengul2004,
  title = {Estimation of human head tissue conductivities by using in vivo E/MEG 	data and comparison of three different estimation algorithm results},
  doi = {10.1109/SIU.2004.1338271},
  abstract = {Knowledge of tissue conductivities is needed to construct reliable
	volume conductor models of the human body and the head in solving
	forward and inverse bioelectric field problems. In this study three
	different estimation algorithms are applied to in vivo human head
	tissue resistivity estimation by using EEG and MEG data. The applied
	algorithms are conventional least-squared error algorithm (LSEE),
	Bayesian MAP algorithm and statistically constrained minimum mean
	squared error estimator (MiMSEE). The algorithms intake a priori
	information on body geometry (realistic boundary element model),
	statistical properties of regional conductivities (assumed to be
	uniformly distributed between upper and lower bounds), linearization
	error and instrumentation noise. The EEG-MEG data set has been obtained
	from a source localization experiment in which the median nerve has
	been stimulated. The anatomical boundary information has been extracted
	from 256 T1-weighted MRI images. The MEG data have been obtained
	by using a 31-channel magnetometer over the somatosensory cortex.
	By using the data, scalp, skull and brain conductivities have been
	estimated and estimation variances are calculated by using the algorithms.
	It is shown that MiMSEE algorithm gives lower error rates than the
	other two algorithms. The calculated error rates are 90 % for the
	LSEE, 20.5 % for the Bayesian MAP estimator and 12.5 % for the MiMSEE.},
  timestamp = {2016-07-08T12:24:49Z},
  booktitle = {Signal Processing and Communications Applications Conference, 2004. 	Proceedings of the IEEE 12th},
  author = {Sengul, G. and Baysal, U. and Haueisen, J.},
  month = apr,
  year = {2004},
  keywords = {31-channel magnetometer,Bayesian MAP algorithm,Bayesian methods,Bioelectric
	phenomena,Biological system modeling,biological tissues,biomedical
	MRI,boundary element model,boundary-elements methods,brain conductivity,Brain
	modeling,Conductivity,Conductors,electrical conductivity,electrical
	resistivity,electroencephalography,Error analysis,error rates,error
	statistics,human head tissue conductivities,Humans,instrumentation
	noise,In vivo,in vivo EEG/MEG data,least mean squares methods,least-squared
	error algorithm,linearization error,LSEE,Magnetic heads,magnetoencephalography,median
	nerve,medical image processing,MiMSEE,regional conductivities,resistivity
	estimation,scalp conductivity,skull conductivity,somatosensory cortex,somatosensory
	phenomena,source localization,statistically constrained minimum mean
	squared error estimator,T1-weighted MRI images},
  pages = {114-117},
  owner = {afdidehf}
}

@inproceedings{Sermwuthisarn2009,
  title = {A fast image recovery using compressive sensing technique with block 	based Orthogonal Matching Pursuit},
  doi = {10.1109/ISPACS.2009.5383863},
  abstract = {Traditionally, the problems of applying orthogonal matching pursuit
	(OMP) to large images are its high computing time and its requirement
	for a large matrix. In this paper, we propose a fast image recovery
	algorithm by dividing the image into block of n�?n pixels and applying
	OMP to each n�?n block instead of the entire image. The key idea
	is that small matrix requires less computing time and less memory.
	In the experiment, the block based OMP was applied to three standard
	test images: Lena, Mandrill and Pirate. Compared to standard OMP,
	block based OMP required less computing time while giving comparable
	PSNR.},
  timestamp = {2016-07-08T10:17:06Z},
  booktitle = {Intelligent Signal Processing and Communication Systems, 2009. ISPACS 	2009. International Symposium on},
  author = {Sermwuthisarn, P. and Auethavekiat, S. and Patanavijit, V.},
  month = jan,
  year = {2009},
  keywords = {block based OMP,block based orthogonal matching pursuit,Communication
	systems,compressive sensing,data compression,image block,image coding,image
	matching,image recovery,linear programming,Matching pursuit algorithms,Pixel,PSNR,signal
	processing,Signal processing algorithms,Size measurement,sparse matrices,Testing},
  pages = {212-215},
  owner = {afdidehf}
}

@article{Sezer2015,
  title = {Approximation and Compression With Sparse Orthonormal Transforms},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2414879},
  abstract = {We propose a new transform design method that targets the generation
	of compression-optimized transforms for next-generation multimedia
	applications. The fundamental idea behind transform compression is
	to exploit regularity within signals such that redundancy is minimized
	subject to a fidelity cost. Multimedia signals, in particular images
	and video, are well known to contain a diverse set of localized structures,
	leading to many different types of regularity and to nonstationary
	signal statistics. The proposed method designs sparse orthonormal
	transforms (SOTs) that automatically exploit regularity over different
	signal structures and provides an adaptation method that determines
	the best representation over localized regions. Unlike earlier work
	that is motivated by linear approximation constructs and model-based
	designs that are limited to specific types of signal regularity,
	our work uses general nonlinear approximation ideas and a data-driven
	setup to significantly broaden its reach. We show that our SOT designs
	provide a safe and principled extension of the Karhunen-Loeve transform
	(KLT) by reducing to the KLT on Gaussian processes and by automatically
	exploiting non-Gaussian statistics to significantly improve over
	the KLT on more general processes. We provide an algebraic optimization
	framework that generates optimized designs for any desired transform
	structure (multiresolution, block, lapped, and so on) with significantly
	better n-term approximation performance. For each structure, we propose
	a new prototype codec and test over a database of images. Simulation
	results show consistent increase in compression and approximation
	performance compared with conventional methods.},
  timestamp = {2016-07-08T11:27:02Z},
  number = {8},
  journal = {Image Processing, IEEE Transactions on},
  author = {Sezer, O.G. and Guleryuz, O.G. and Altunbasak, Y.},
  month = aug,
  year = {2015},
  keywords = {algebraic optimization framework,Algorithm design and analysis,approximation
	theory,compression-optimized transform generation,data compression,data-driven
	setup,Gaussian process,Gaussian processes,image codec,image coding,Image
	compression,image representation,Karhunen-Loeve transform,Karhunen-Loeve
	transforms,KLT,Linear approximation,linear representation,Machine
	learning,multimedia communication,multimedia signal,next-generation
	multimedia application,non-Gaussian statistics,nonlinear approximation,nonstationary
	signal statistic,optimisation,Quantization (signal),SOT,sparse lapped
	transforms,sparse multi-resolution transforms,sparse orthonormal
	transform,Sparse orthonormal transforms,transform optimization,transform
	optimization,transforms,video codecs,video
	codecs,video coding},
  pages = {2328-2343},
  owner = {afdidehf}
}

@article{Shaban2015,
  title = {From Local Similarities to Global Coding: A Framework for Coding 	Applications},
  volume = {PP},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2465171},
  abstract = {Feature coding has received great attention in recent years as a building
	block of many image processing algorithms. In particular, the importance
	of the locality assumption in coding approaches has been studied
	in many previous works. We review this assumption and claim that
	using the similarity of data points to a more global set of anchor
	points does not necessarily weaken the coding method, as long as
	the underlying structure of the anchor points is taken into account.
	We propose to capture the underlying structure by assuming a random
	walker over the anchor points. We also show that our method is a
	fast approximation to the diffusion map kernel. Experiments on various
	datasets show that with a knowledge of the underlying structure of
	anchor points, different state-of-the-art coding algorithms may boost
	their performance in different learning tasks by utilizing the proposed
	method},
  timestamp = {2016-07-08T12:33:32Z},
  number = {99},
  journal = {Image Processing, IEEE Transactions on},
  author = {Shaban, A. and RABIEE, H. and Najibi, M. and Yousefi, S.},
  year = {2015},
  keywords = {Dictionaries,diffusion kernel,encoding,image coding,Image reconstruction,Kernel,local
	coordinate coding,Manifolds,Sparse Coding,Support vector machines},
  pages = {1-1},
  owner = {afdidehf}
}

@inproceedings{Shafiee2014,
  title = {Multimodal sparse representation classification with Fisher discriminative 	sample reduction},
  doi = {10.1109/ICIP.2014.7026051},
  abstract = {This paper presents a method to perform sparse representation based
	classification (SRC) in a more accurate and efficient way. In this
	method, training data is first mapped into different feature spaces
	and multiple dictionaries are built by utilizing a Fisher discriminative
	based method. These dictionaries can be considered as efficient representations
	of the data which are then used in a multimodal SRC framework to
	classify test samples. In comparison to the original SRC method where
	only one modality of training space is utilized, the proposed method
	classifies test samples in a more accurate and efficient way. Experimental
	results from two different face datasets show that the proposed multimodal
	method has higher recognition rate compared to single-modality SRC
	based methods. The accuracy of the proposed method is also compared
	to other multi-modality classifiers and the results confirm that
	higher recognition rates are achieved in comparison with other common
	classification algorithms.},
  timestamp = {2016-07-09T20:16:34Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Shafiee, S. and Kamangar, F. and Athitsos, V. and Huang, Junzhou and Ghandehari, L.},
  month = oct,
  year = {2014},
  keywords = {Accuracy,data training mapping,Dictionaries,Face,face recognition,Fisher
	discrimination,Fisher discriminative sample reduction,image classification,Image
	recognition,image representation,image sampling,learning (artificial
	intelligence),modal analysis,Multimodal classification,multimodality
	classification,multimodal sparse representation classification,multimodal
	SRC framework,multiple dictionary,single-modality SRC based method,sparse
	representation,Training,Training data,training space modality,Vectors},
  pages = {5192-5196},
  owner = {Fardin}
}

@article{Shang2015,
  title = {A Robust Algorithm for Joint Sparse Recovery in Presence of Impulsive 	Noise},
  volume = {22},
  issn = {1070-9908},
  doi = {10.1109/LSP.2014.2387435},
  abstract = {This letter presents a robust solution for joint sparse recovery (JSR)
	under impulsive noise. The unknown measurement noise is endowed with
	the Student-t distribution, then a novel Bayesian probabilistic model
	is proposed to describe the JSR problem. To effectively recover the
	joint row sparse signal, variational Bayes (VB) method is introduced
	for Bayesian theory based JSR algorithms such that it overcomes the
	intractable integrations inherent. Simulation results verify that
	the proposed algorithm significantly outperforms the existing algorithms
	under impulsive noise.},
  timestamp = {2016-07-08T11:27:38Z},
  number = {8},
  journal = {Signal Processing Letters, IEEE},
  author = {Shang, Jiadong and Wang, Zulin and Huang, Qin},
  month = aug,
  year = {2015},
  keywords = {Approximation algorithms,Bayesian inference,Bayesian theory,Bayes
	methods,compressed sensing,impulsive noise,Joints,joint sparse recovery,joint
	sparse recovery,JSR,Noise,Noise measurement,novel Bayesian probabilistic
	model,robust algorithm,Robustness,Signal processing algorithms,Student-t
	distribution,variational Bayes method,VB method},
  pages = {1166-1170},
  owner = {Fardin}
}

@inproceedings{She2009,
  title = {Spatially adaptive image reconstruction via compressive sensing},
  abstract = {Compressive sensing (CS) is an emerging model-based framework for
	signal recovery at a rate significantly below the Nyquist sampling
	rate. The CS theory states that a signal having a sparse representation
	in some bases can be reconstructed from a small set of random projections.
	In this paper, a reconstruction method is developed based on block
	CS and adaptive choice of frame expansions according to spatial features
	of partitioned regions. Natural image is divided into different types
	of regions, the discrete cosine transform (DCT) is used for smooth
	regions, while the bi-orthogonal wavelet transform (OWT) is chosen
	for uneven regions. Several experiments are conducted on benchmark
	images to verify the efficacy of the proposed method. Experimental
	results show that it achieves improved quality in both subjective
	and objective measurement as compared with existing methods.},
  timestamp = {2016-07-11T16:55:16Z},
  booktitle = {Asian Control Conference, 2009. ASCC 2009. 7th},
  author = {She, Qingshan and Luo, Zhizeng and Zhu, Yaping and Zou, Hongbo and Chen, Yun},
  month = aug,
  year = {2009},
  keywords = {Automation,bi-orthogonal wavelet transform,compressive sensing theory,DCT,discrete
	cosine transform,discrete cosine transforms,Discrete transforms,Discrete
	wavelet transforms,image coding,Image reconstruction,image representation,image
	sampling,monitoring,Reconstruction algorithms,sparse representation,spatial
	adaptive image reconstruction,Transform coding,wavelet transforms},
  pages = {1570-1575},
  owner = {afdidehf}
}

@inproceedings{Shekaramiz2014,
  title = {Hierarchical Bayesian approach for jointly-sparse solution of multiple-measurement 	vectors},
  doi = {10.1109/ACSSC.2014.7094813},
  abstract = {It is well-known that many signals of interest can be well-estimated
	via just a small number of supports under some specific basis. Here,
	we consider finding sparse solution for Multiple Measurement Vectors
	(MMVs) in case of having both jointly sparse and clumpy structure.
	Most of the previous work for finding such sparse representations
	are based on greedy and sub-optimal algorithms such as Basis Pursuit
	(BP), Matching Pursuit (MP), and Orthogonal Matching Pursuit (OMP).
	In this paper, we first propose a hierarchical Bayesian model to
	deal with MMVs that have jointly-sparse structure in their solutions.
	Then, the model is modified to account for clumps of the neighbor
	supports (block sparsity) in the solution structure, as well. Several
	examples are considered to illustrate the merit of the proposed hierarchical
	Bayesian model compared to OMP and a modified version of the OMP
	algorithm.},
  timestamp = {2016-07-08T12:41:48Z},
  booktitle = {Signals, Systems and Computers, 2014 48th Asilomar Conference on},
  author = {Shekaramiz, M. and Moon, T.K. and Gunther, J.H.},
  month = nov,
  year = {2014},
  keywords = {Bayes methods,compressed sensing,greedy algorithm,Greedy algorithms,hierarchical
	Bayesian approach,Joints,joint sparse and clumpy structure,Matching
	pursuit algorithms,mathematical model,MMV,multiple measurement vectors
	joint sparse solution,Noise,Sensors,signal representation,sparse
	matrices,sparse representation,suboptimal algorithm},
  pages = {1962-1966},
  owner = {afdidehf}
}

@article{Shekhar2014,
  title = {Joint Sparse Representation for Robust Multimodal Biometrics Recognition},
  volume = {36},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.109},
  abstract = {Traditional biometric recognition systems rely on a single biometric
	signature for authentication. While the advantage of using multiple
	sources of information for establishing the identity has been widely
	recognized, computational models for multimodal biometrics recognition
	have only recently received attention. We propose a multimodal sparse
	representation method, which represents the test data by a sparse
	linear combination of training data, while constraining the observations
	from different modalities of the test subject to share their sparse
	representations. Thus, we simultaneously take into account correlations
	as well as coupling information among biometric modalities. A multimodal
	quality measure is also proposed to weigh each modality as it gets
	fused. Furthermore, we also kernelize the algorithm to handle nonlinearity
	in data. The optimization problem is solved using an efficient alternative
	direction method. Various experiments show that the proposed method
	compares favorably with competing fusion-based methods.},
  timestamp = {2016-07-09T19:49:43Z},
  number = {1},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  author = {Shekhar, S. and Patel, V.M. and Nasrabadi, N.M. and Chellappa, R.},
  month = jan,
  year = {2014},
  keywords = {Algorithms,alternative direction method,Biometric Identification,biometrics
	(access control),Classification algorithms,competing fusion-based
	methods,computational models,Databases,Dermatoglyphics,Face,Factual,feature
	fusion,Humans,image fusion,Image recognition,image representation,Iris,Joints,joint
	sparse representation,Kernel,Multimodal biometrics,multimodal quality
	measure,multimodal sparse representation method,optimisation,Optimization,optimization
	problem,robust multimodal biometrics recognition system,Robustness,sparse
	matrices,sparse representation,Training data},
  pages = {113-126},
  owner = {Fardin}
}

@inproceedings{Shen2014,
  title = {Pattern-coupled sparse Bayesian learning for recovery of block-sparse 	signals},
  doi = {10.1109/ICASSP.2014.6853928},
  abstract = {In this paper, we develop a new sparse Bayesian learning method for
	recovery of block-sparse signals with unknown cluster patterns. A
	pattern-coupled hierarchical Gaussian prior model is introduced to
	characterize the statistical dependencies among coefficients, where
	a set of hyperparameters are employed to control the sparsity of
	signal coefficients. Unlike the conventional sparse Bayesian learning
	framework in which each individual hyperparameter is associated independently
	with each coefficient, in this paper, the prior for each coefficient
	not only involves its own hyperparameter, but also the hyperparameters
	of its immediate neighbors. In doing this way, the sparsity patterns
	of neighboring coefficients are related to each other and the hierarchical
	model has the potential to encourage structured-sparse solutions.
	The hyperparameters, along with the sparse signal, are learned by
	maximizing their posterior probability via an expectation-maximization
	(EM) algorithm.},
  timestamp = {2016-07-10T07:31:42Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Shen, Yanning and Duan, Huiping and Fang, Jun and Li, Hongbin},
  month = may,
  year = {2014},
  keywords = {Bayes methods,Block-sparse signal recovery,Clustering algorithms,Clustering
	algorithms,compressed sensing,compressed
	sensing,Computational modeling,covariance matrices,expectation-maximisation
	algorithm,expectation-maximization algorithm,Gaussian distribution,hyperparameters,immediate
	neighbors,neighboring coefficients,pattern-coupled hierarchical Gaussian
	prior model,pattern-coupled hierarchical model,pattern-coupled sparse
	Bayesian learning,posterior probability,signal coefficients,signal processing,signal
	processing,Signal processing algorithms,sparse
	Bayesian learning,sparsity control,statistical dependency,unknown
	cluster patterns},
  pages = {1896-1900},
  owner = {afdidehf}
}

@inproceedings{Shen2013,
  title = {Motion estimation for video coding based on sparse representation},
  doi = {10.1109/ICASSP.2013.6637880},
  abstract = {This paper describes a motion estimation algorithm based on sparse
	representation, which can be applied in video coding to reduce the
	temporal redundancy. The sparse coefficients are firstly calculated
	in support region by orthogonal matching pursuit (OMP) algorithm
	using the reference blocks as dictionary elements, and then these
	optimal sparse coefficients are utilized to predict the current block.
	To get the same prediction in decoder, the number of iterations in
	OMP is transmitted to decoder as side information. Simulation results
	show that gain up to 2.87dB in terms of the PSNR when compared with
	traditional translational motion estimation model.},
  timestamp = {2016-07-09T20:15:09Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Shen, Yanfei and Li, Jintao and Zhu, Zhenmin},
  month = may,
  year = {2013},
  keywords = {Approximation methods,compressed sensing,decoder,Dictionaries,dictionary
	elements,image matching,motion estimation,motion estimation algorithm,motion
	estimation algorithm,OMP algorithm,OMP
	algorithm,optimal sparse coefficients,orthogonal
	matching pursuit algorithm,orthogonal matching pursuit
	algorithm,Prediction algorithms,PSNR,reference blocks,same prediction,same
	prediction,Signal processing algorithms,Signal
	processing algorithms,sparse representation,temporal redundancy,temporal
	redundancy,translational motion estimation model,translational
	motion estimation model,Vectors,video coding,video
	coding},
  pages = {1394-1398},
  owner = {afdidehf}
}

@article{Shervashidze2015,
  title = {Learning the Structure for Structured Sparsity},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2446432},
  abstract = {Structured sparsity has recently emerged in statistics, machine learning
	and signal processing as a promising paradigm for learning in high-dimensional
	settings. All existing methods for learning under the assumption
	of structured sparsity rely on prior knowledge on how to weight (or
	how to penalize) individual subsets of variables during the subset
	selection process, which is not available in general. Inferring group
	weights from data is a key open research problem in structured sparsity.
	In this paper, we propose a Bayesian approach to the problem of group
	weight learning. We model the group weights as hyperparameters of
	heavy-tailed priors on groups of variables and derive an approximate
	inference scheme to infer these hyperparameters. We empirically show
	that we are able to recover the model hyperparameters when the data
	are generated from the model, and we demonstrate the utility of learning
	weights in synthetic and real denoising problems.},
  timestamp = {2016-07-09T19:55:50Z},
  number = {18},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Shervashidze, N. and Bach, F.},
  month = sep,
  year = {2015},
  keywords = {approximate inference scheme,Bayesian approach,Bayesian statistics,Bayes methods,Bayes
	methods,Biological system modeling,compressed sensing,Computational
	modeling,Data models,diseases,Gaussian scale mixture,group weight
	learning problem,heavy-tailed priors hyperparameter recovery,image
	denoising,inference mechanisms,key open research problem,learning
	(artificial intelligence),Machine learning,Probabilistic logic,probabilistic
	modeling,real denoising problem,signal processing,structured learning,structured
	sparsity,subset selection process,super-Gaussian prior,synthetic
	denoising problem,variational inference},
  pages = {4894-4902},
  owner = {afdidehf}
}

@article{Shi2015,
  title = {LRTV: MR Image Super-Resolution with Low-Rank and Total Variation 	Regularizations},
  volume = {PP},
  issn = {0278-0062},
  doi = {10.1109/TMI.2015.2437894},
  abstract = {Image super-resolution (SR) aims to recover highresolution images
	from their low-resolution counterparts for improving image analysis
	and visualization. Interpolation methods, widely used for this purpose,
	often result in images with blurred edges and blocking effects. More
	advanced methods such as total variation (TV) retain edge sharpness
	during image recovery. However, these methods only utilize information
	from local neighborhoods, neglecting useful information from remote
	voxels. In this paper, we propose a novel image SR method that integrates
	both local and global information for effective image recovery. This
	is achieved by, in addition to TV, low-rank regularization that enables
	utilization of information throughout the image. The optimization
	problem can be solved effectively via alternating direction method
	of multipliers (ADMM). Experiments on MR images of both adult and
	pediatric subjects demonstrate that the proposed method enhances
	the details in the recovered high-resolution images, and outperforms
	methods such as the nearest-neighbor interpolation, cubic interpolation,
	iterative back projection (IBP), non-local means (NLM), and TVbased
	up-sampling.},
  timestamp = {2016-07-09T20:00:00Z},
  number = {99},
  journal = {Medical Imaging, IEEE Transactions on},
  author = {Shi, F. and Cheng, J. and Wang, L. and Yap, P. and Shen, D.},
  year = {2015},
  keywords = {Cost function,Image Enhancement,Image reconstruction,Image resolution,image
	sampling,interpolation,matrix completion,Signal to noise ratio,sparse
	learning,Spatial resolution,TV},
  pages = {1-1},
  owner = {afdidehf}
}

@inproceedings{Shi2014,
  title = {Ground moving target indication for single SAR imagery based on sparse 	representation},
  doi = {10.1109/ICOSP.2014.7015367},
  abstract = {A new ground moving target indication method for single synthetic
	aperture radar (SAR) image is presented, it is based on sparse representation.
	According to the Range Doppler imaging of static scene, a ground
	moving target imaging model is achieved. Based on the model, the
	over-complete dictionary of targets sample images with different
	speeds is constructed. Then the test SAR images are blocked into
	sub-images and the corresponding coefficients are calculated depend
	on the dictionary. There is a significant difference between the
	coefficients of background clutter and moving objects. On the basis
	of the difference, whether there is a moving target can be judged,
	simulation results have proved the validity of the proposed method.},
  timestamp = {2016-07-08T12:39:59Z},
  booktitle = {Signal Processing (ICSP), 2014 12th International Conference on},
  author = {Shi, Hongyin and Zhao, Xinyue},
  month = oct,
  year = {2014},
  keywords = {background clutter coefficient,Clutter,Dictionaries,Doppler radar,ground
	moving target imaging model,ground moving target indication method,image
	representation,imaging,moving object coefficient,moving targets detection,object
	detection,over-complete dictionary,over-complete target sample image
	dictionary,radar imaging,range Doppler imaging,single SAR imagery,single
	synthetic aperture radar imagery,sparse representation,synthetic aperture radar,synthetic
	aperture radar,Vectors},
  pages = {2110-2115},
  owner = {afdidehf}
}

@article{Shiga2015,
  title = {Non-Negative Matrix Factorization with Auxiliary Information on Overlapping 	Groups},
  volume = {27},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2014.2373361},
  abstract = {Matrix factorization is useful to extract the essential low-rank structure
	from a given matrix and has been paid increasing attention. A typical
	example is non-negative matrix factorization (NMF), which is one
	type of unsupervised learning, having been successfully applied to
	a variety of data including documents, images and gene expression,
	where their values are usually non-negative. We propose a new model
	of NMF which is trained by using auxiliary information of overlapping
	groups. This setting is very reasonable in many applications, a typical
	example being gene function estimation where functional gene groups
	are heavily overlapped with each other. To estimate true groups from
	given overlapping groups efficiently, our model incorporates latent
	matrices with the regularization term using a mixed norm. This regularization
	term allows group-wise sparsity on the optimized low-rank structure.
	The latent matrices and other parameters are efficiently estimated
	by a block coordinate gradient descent method. We empirically evaluated
	the performance of our proposed model and algorithm from a variety
	of viewpoints, comparing with four methods including MMF for auxiliary
	graph information, by using both synthetic and real world document
	and gene expression data sets.},
  timestamp = {2016-07-10T07:00:17Z},
  number = {6},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  author = {Shiga, M. and Mamitsuka, H.},
  month = jun,
  year = {2015},
  keywords = {auxiliary graph information,auxiliary information,block coordinate
	gradient descent method,Data mining,functional gene group,gene expression,gene
	expression data set,gene function estimation,genetic algorithms,gradient
	methods,Graph theory,group-wise sparsity,Jacobian matrices,latent
	matrices,matrix decomposition,mixed norm,NMF,nonnegative matrix factorization,non-negative
	matrix factorization,Optimization,optimized low-rank structure,overlapping
	group,performance evaluation,real world document,regularization term,regularization
	term,semi-supervised
	learning,Semisupervised learning,semi-supervised learning,Semisupervised
	learning,sparse matrices,sparse
	structured norm,sparse structured
	norm,synthetic document,unsupervised learning,unsupervised
	learning,Vectors},
  pages = {1615-1628},
  owner = {afdidehf}
}

@inproceedings{Shuangshuang2011,
  title = {Video foreground detection via sparse representation},
  abstract = {In this article we propose a novel background model method based on
	sparse representation theory. In this framework the image is divided
	into N�N non-overlapping blocks, each block of a new input image
	has a sparse representation in the space spanned by background model.
	The sparse representation is achieved by solving an ?1-regularized
	least squares problem which is computed by the preconditioned conjugate
	gradients (PCG) algorithm. Then the block with large error vector
	is taken as the foreground region. The experimental results show
	that proposed algorithm produces more accurate and stable results.},
  timestamp = {2016-07-11T17:11:34Z},
  booktitle = {Control Conference (CCC), 2011 30th Chinese},
  author = {Shuangshuang, Liu and Bo, Wang and Zhihui, Zheng},
  month = jul,
  year = {2011},
  keywords = {?1-regularized least square problem,background model method,Background
	Subtraction,Computational modeling,conjugate gradient methods,error
	vector,Foreground Segmentation,Image color analysis,image representation,Image
	segmentation,Least squares approximation,least squares approximations,Minimization,nonoverlapping
	blocks,object detection,preconditioned conjugate gradient algorithm,preconditioned conjugate
	gradient algorithm,Spares Representation,Spares
	Representation,sparse matrices,sparse representation theory,sparse representation
	theory,Training,Vectors,video foreground detection,video
	foreground detection,video signal processing},
  pages = {3073-3077},
  owner = {afdidehf}
}

@article{Silva2004,
  series = {Proceedings of the International School on Magnetic Resonance and Brain Function Frontiers of Brain Functional MRI and Electrophysiological Methods},
  title = {Functional localization of brain sources using EEG and/or MEG data: 	volume conductor and source models},
  volume = {22},
  issn = {0730-725X},
  doi = {http://dx.doi.org/10.1016/j.mri.2004.10.010},
  abstract = {In this overview we examine the basic principles of properties of
	electroencephalogram and magnetoencephalogram and the corresponding
	models of sources and of the volume conductor. In particular we show
	how the dipolar model is anchored in neurophysiological findings
	and how the different conductivities of the brain and the tissue
	surrounding it can be estimated. Using these basic models as tools
	we show how the functional localization of the neural sources of
	rhythmic activities (alpha and mu rhythms and sleep spindles) and
	of epileptiform activities can be estimated and integrated with structural
	data of the brain obtained with MRI.},
  timestamp = {2016-10-21T13:33:47Z},
  number = {10},
  journal = {Magnetic Resonance Imaging},
  author = {Silva, Fernando Lopes da},
  year = {2004},
  keywords = {EEG,fMRI,Functional localization,MEG,Volume conductor},
  pages = {1533--1538},
  owner = {Fardin}
}

@article{Simon2012,
  title = {A Sparse-Group Lasso},
  volume = {just-accep},
  doi = {10.1080/10618600.2012.681250},
  abstract = {For high dimensional supervised learning problems, often using problem
	specific assumptions can lead to greater accuracy. For problems with
	grouped covariates, which are believed to have sparse effects both
	on a group and within group level, we introduce a regularized model
	for linear regression with `1 and `2 penalties. We discuss the sparsity
	and other regularization properties of the optimal fit for this model,
	and show that it has the desired effect of group-wise and within
	group sparsity. We propose an algorithm to fit the model via accelerated
	generalized gradient descent, and extend this model and algorithm
	to convex loss functions. We also demonstrate the efficacy of our
	model and the efficiency of our algorithm on simulated data.},
  timestamp = {2016-07-08T11:29:01Z},
  number = {just-accep},
  journal = {Journal of Computational and Graphical Statistics},
  author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2012},
  keywords = {model,nesterov,penalize,regression,regularize},
  masid = {57675260},
  owner = {Fardin}
}

@inproceedings{Singh2015,
  title = {Under-sampled functional MRI using low-rank plus sparse matrix decomposition},
  doi = {10.1109/ICASSP.2015.7178099},
  abstract = {High spatial resolution in functional magnetic resonance imaging improves
	its sensitivity to brain activation signals by reducing partial volume
	effects. However, the long acquisition times required for high spatial
	resolution limit the temporal resolution in fMRI studies. Consequently,
	the low temporal sampling bandwidth leads to increase in physiological
	noise and poor modeling of the functional activation dynamics. Thus,
	fast techniques capable of recovering fMRI time-series from under-sampled
	data are desirable to improve the sensitivity and specificity of
	fMRI for functional brain mapping. This paper presents an under-sampled
	fMRI recovery using low-rank plus sparse matrix decomposition signal
	model. This model is suited for blocked or slow event-related fMRI
	studies, where the low-rank matrix captures the temporally static
	T*2-weighted image patterns and, the sparse matrix captures the pseudo-periodic
	brain activation signal. The preliminary results of under-sampled
	recovery on in-vivo fMRI data show recovery of BOLD activation in
	human superior colliculus with contrast-to-noise ratio ? 4.4 (85%
	of reference) up to acceleration factors of 3.},
  timestamp = {2016-07-11T17:10:00Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International 	Conference on},
  author = {Singh, V. and Tewfik, A.H. and Ress, D.B.},
  month = apr,
  year = {2015},
  keywords = {Acceleration,acceleration factors,biomedical imaging,biomedical MRI,BOLD
	activation,Brain,Brain modeling,compressed sensing,contrast-to-noise
	ratio,fMRI time-series,functional activation dynamics,functional
	brain mapping,functional magnetic resonance imaging,functional MRI,human
	superior colliculus,image colour analysis,image sampling,in-vivo
	fMRI data,Low-rank methods,low-rank plus sparse matrix decomposition
	signal model,Magnetic resonance imaging,matrix decomposition,medical
	image processing,partial volume effects,physiological noise,Physiology,pseudoperiodic
	brain activation signal,sparse matrices,sparse recovery,Spatial resolution,Spatial
	resolution,T*2-weighted image patterns,T*2-weighted
	image patterns,temporal resolution,temporal sampling bandwidth,temporal
	sampling bandwidth,Time series,Time
	series,Transmission line matrix methods,under-sampled fMRI recovery,under-sampled
	fMRI recovery,under-sampled functional MRI,under-sampled
	functional MRI},
  pages = {897-901},
  owner = {afdidehf}
}

@inproceedings{Skretting2001,
  title = {A simple design of sparse signal representations using overlapping 	frames},
  doi = {10.1109/ISPA.2001.938667},
  abstract = {The use of frames and matching pursuits for signal representation
	are receiving increased attention due to their perceived potential
	in various signal processing applications. Good design algorithms
	for block oriented frames have recently been published. Viewing these
	block oriented frames as generalizations of block oriented transforms,
	it is natural to seek corresponding generalizations of critically
	sampled filter banks leading to overlapping frames. Here we show
	that a large class of overlapping frames can be decomposed into a
	critically sampled orthogonal filter bank-that can be chosen prior
	to the design-and a block oriented frame. Based on this, we show
	that excellently performing overlapping frames can be designed using
	the already established and simple theory for the design of block
	oriented frames},
  timestamp = {2016-07-08T11:28:40Z},
  booktitle = {Image and Signal Processing and Analysis, 2001. ISPA 2001. Proceedings 	of the 2nd International Symposium on},
  author = {Skretting, K. and Husoy, J.H. and Aase, S.O.},
  year = {2001},
  keywords = {Algorithm design and analysis,block oriented frames,block oriented
	transforms,channel bank filters,critically sampled filter banks,design
	algorithms,Equations,Karhunen-Loeve transforms,Matching pursuit algorithms,matching
	pursuits,orthogonal filter bank,overlapping frames,Signal design,signal
	processing,Signal processing algorithms,signal processing applications,signal
	representation,signal representations,Signal synthesis,sparse signal
	representations,transforms,Vectors},
  pages = {424-428},
  owner = {afdidehf}
}

@inproceedings{Sole2009,
  title = {Joint sparsity-based optimization of a set of orthonormal 2-D separable 	block transforms},
  doi = {10.1109/ICIP.2009.5413929},
  abstract = {We propose an iterative method for the optimization of a set of 2-D
	separable transforms for a given training data set. The method outputs
	orthonormal transforms, each one being optimal for a subset of the
	data with respect to a sparsity-based objective function. The vertical
	and horizontal directions of the transform may be different, thus
	allowing directional-adapted transforms (in contrast to the usual
	DCT). Additionally, we relate the reconstruction error and the sparsity
	cost terms through the quantization step. To prove the validity of
	our approach, experimental results concerning coding applications
	are provided.},
  timestamp = {2016-07-09T19:49:53Z},
  booktitle = {Image Processing (ICIP), 2009 16th IEEE International Conference 	on},
  author = {Sole, J. and Yin, Peng and Zheng, Yunfei and Gomila, C.},
  month = nov,
  year = {2009},
  keywords = {coding,Cost function,data compression,directional-adapted transforms,Discrete
	cosine transforms,Discrete transforms,Frequency,image coding,Image
	reconstruction,iterative method,iterative methods,joint sparsity-based
	optimization,Karhunen-Loeve transforms,optimisation,Optimization
	methods,orthonormal 2-D separable block transforms,Quantization,reconstruction
	error,Separable Transforms,Sparsity,sparsity-based objective function,Training
	data,transform optimization,transforms,video coding},
  pages = {9-12},
  owner = {afdidehf}
}

@inproceedings{Song2012,
  title = {Sparse representation for video super-resolution},
  doi = {10.1109/ICSAI.2012.6223456},
  abstract = {Sparse representations of signals have been developed rapidly in recent
	years, such as in the field of super-resolution. The theory that
	an image is able to be decomposed into a sparse representation over
	an over-complete dictionary of atoms ensures the feasibility of such
	applications. In this paper, we produce a pair of well-trained dictionaries
	that has both high and low resolution patches followed by restoration
	using sparse representations, and then the steering kernel regression
	is added into the framework to restrict the whole video and reduce
	noise and block effects. The experiments show that the quality of
	recovered super-resolution video is better acceptable and competitive.},
  timestamp = {2016-07-11T16:51:48Z},
  booktitle = {Systems and Informatics (ICSAI), 2012 International Conference on},
  author = {Song, Bingjie and Hao, Pengwei},
  month = may,
  year = {2012},
  keywords = {Dictionaries,Image decomposition,image denoising,Image resolution,image
	restoration,interpolation,Kernel,Noise reduction,over-complete dictionary,PSNR,regression
	analysis,restoration,signal representation,signal resolution,Sparse
	coding,sparse representation,sparse signal representation,spase representation,steering kernel regression,steering
	kernel regression,Training,video signal
	processing,video super-resolution,well-trained dictionaries},
  pages = {2055-2058},
  owner = {afdidehf}
}

@article{Song2015,
  title = {Compressed sensing image reconstruction using intra prediction},
  volume = {151, Part 3},
  issn = {0925-2312},
  doi = {http://dx.doi.org/10.1016/j.neucom.2014.05.088},
  abstract = {Abstract Compressed sensing (CS) provides a general signal acquisition
	framework that enables the reconstruction of sparse signals from
	a small number of linear measurements. In this article we present
	a \{CS\} image reconstruction algorithm using intra prediction method
	based on block-based \{CS\} image framework. The current reconstruction
	block is firstly predicted by its surrounding reconstructed pixels,
	and then its prediction residual will be reconstructed. Because the
	sparsity level of prediction residual is higher than its original
	image block, the performance of our proposed \{CS\} image reconstruction
	algorithm is significantly superior to the traditional \{CS\} reconstruction
	algorithm. Furthermore, total variation model is also used to suppress
	the blocking artifacts caused by intra prediction and measurement
	noise. Experimental results also show the competitive performance
	with respect to peak signal-to-noise ratio and subjective visual
	quality.},
  timestamp = {2016-07-08T11:56:01Z},
  journal = {Neurocomputing},
  author = {Song, Yun and Cao, Wei and Shen, Yanfei and Yang, Gaobo},
  year = {2015},
  keywords = {Compressed,Image,Intra,Post,prediction,processing,reconstruction,sensing,Total,variation},
  pages = {1171 - 1179},
  owner = {afdidehf}
}

@article{Sorber2015,
  title = {Structured Data Fusion},
  volume = {9},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2015.2400415},
  abstract = {We present structured data fusion (SDF) as a framework for the rapid
	prototyping of knowledge discovery in one or more possibly incomplete
	data sets. In SDF, each data set-stored as a dense, sparse, or incomplete
	tensor-is factorized with a matrix or tensor decomposition. Factorizations
	can be coupled, or fused, with each other by indicating which factors
	should be shared between data sets. At the same time, factors may
	be imposed to have any type of structure that can be constructed
	as an explicit function of some underlying variables. With the right
	choice of decomposition type and factor structure, even well-known
	matrix factorizations such as the eigenvalue decomposition, singular
	value decomposition and QR factorization can be computed with SDF.
	A domain specific language (DSL) for SDF is implemented as part of
	the software package Tensorlab, with which we offer a library of
	tensor decompositions and factor structures to choose from. The versatility
	of the SDF framework is demonstrated by means of four diverse applications,
	which are all solved entirely within Tensorlab's DSL.},
  timestamp = {2016-07-11T16:57:18Z},
  number = {4},
  journal = {Selected Topics in Signal Processing, IEEE Journal of},
  author = {Sorber, L. and Van Barel, M. and De Lathauwer, L.},
  month = jun,
  year = {2015},
  keywords = {Approximation methods,Big data,block term decomposition,canonical
	polyadic decomposition,covariance matrices,data fusion,data integration,Data
	mining,domain specific language,DSL,eigenvalue decomposition,eigenvalue
	decomposition,Eigenvalues and eigenfunctions,Eigenvalues
	and eigenfunctions,knowledge discovery,matrix decomposition,Matrix
	decomposition,matrix factorization,QR factorization,rapid prototyping,rapid
	prototyping,SDF framework,SDF
	framework,sensor fusion,signal processing,Singular Value Decomposition,singular
	value decomposition,software package,software
	package,specification languages,structured data fusion,structured
	data fusion,structured factors,structured
	factors,structured matrices,Tensile stress,tensor,Tensor decomposition,Tensor
	decomposition,Tensorlab,tensors,Vectors},
  pages = {586-600},
  owner = {afdidehf}
}

@article{Sprechmann2011,
  title = {C-HiLasso: A Collaborative Hierarchical Sparse Modeling Framework},
  volume = {59},
  issn = {1053-587X},
  doi = {10.1109/TSP.2011.2157912},
  abstract = {Sparse modeling is a powerful framework for data analysis and processing.
	Traditionally, encoding in this framework is performed by solving
	an l1-regularized linear regression problem, commonly referred to
	as Lasso or Basis Pursuit. In this work we combine the sparsity-inducing
	property of the Lasso at the individual feature level, with the block-sparsity
	property of the Group Lasso, where sparse groups of features are
	jointly encoded, obtaining a sparsity pattern hierarchically structured.
	This results in the Hierarchical Lasso (HiLasso), which shows important
	practical advantages. We then extend this approach to the collaborative
	case, where a set of simultaneously coded signals share the same
	sparsity pattern at the higher (group) level, but not necessarily
	at the lower (inside the group) level, obtaining the collaborative
	HiLasso model (C-HiLasso). Such signals then share the same active
	groups, or classes, but not necessarily the same active set. This
	model is very well suited for applications such as source identification
	and separation. An efficient optimization procedure, which guarantees
	convergence to the global optimum, is developed for these new models.
	The underlying presentation of the framework and optimization approach
	is complemented by experimental examples and theoretical results
	regarding recovery guarantees.},
  timestamp = {2016-07-08T11:50:00Z},
  number = {9},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Sprechmann, P. and Ramirez, I. and Sapiro, G. and Eldar, Y.C.},
  month = sep,
  year = {2011},
  keywords = {active set,Basis pursuit,block-sparsity property,C-HiLasso,Collaboration,Collaborative
	coding,collaborative hierarchical sparse modeling framework,collaborative
	HiLasso model,Convergence,data analysis,Data models,data processing,Dictionaries,encoding,hierarchical
	Lasso,hierarchical models,image coding,Instruments,Lasso pursuit,optimisation,Optimization,optimization
	procedure,regression analysis,regularized linear regression problem,simultaneously
	coded signals,source identification,source separation,sparse models,sparsity-inducing
	property,sparsity pattern,Structured Sparsity},
  pages = {4183-4198},
  owner = {afdidehf}
}

@inproceedings{Sprechmann2010,
  title = {Collaborative hierarchical sparse modeling},
  doi = {10.1109/CISS.2010.5464845},
  abstract = {Sparse modeling is a powerful framework for data analysis and processing.
	Traditionally, encoding in this framework is done by solving an ¿1-regularized
	linear regression problem, usually called Lasso. In this work we
	first combine the sparsity-inducing property of the Lasso model,
	at the individual feature level, with the block-sparsity property
	of the group Lasso model, where sparse groups of features are jointly
	encoded, obtaining a sparsity pattern hierarchically structured.
	This results in the hierarchical Lasso, which shows important practical
	modeling advantages. We then extend this approach to the collaborative
	case, where a set of simultaneously coded signals share the same
	sparsity pattern at the higher (group) level but not necessarily
	at the lower one. Signals then share the same active groups, or classes,
	but not necessarily the same active set. This is very well suited
	for applications such as source separation. An efficient optimization
	procedure, which guarantees convergence to the global optimum, is
	developed for these new models. The underlying presentation of the
	new framework and optimization approach is complemented with experimental
	examples and preliminary theoretical results.},
  timestamp = {2016-07-08T11:51:45Z},
  booktitle = {Information Sciences and Systems (CISS), 2010 44th Annual Conference 	on},
  author = {Sprechmann, P. and Ramirez, I. and Sapiro, G. and Eldar, Yonina},
  month = mar,
  year = {2010},
  keywords = {block-sparsity property,Collaboration,collaborative hierarchical sparse
	modeling,Collaborative work,data analysis,data processing,Dictionaries,encoding,group
	Lasso model,group theory,hierarchical Lasso model,Instruments,Linear
	regression,linear regression problem,optimisation,optimization procedure,regression
	analysis,Robustness,signal processing,simultaneously coded signals,source
	separation,sparsity-inducing property},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Srinivas2011,
  title = {Sparsity-based face recognition using discriminative graphical models},
  doi = {10.1109/ACSSC.2011.6190206},
  abstract = {A key recent advance in face recognition which models a test face
	image as a sparse linear combination of training face images has
	demonstrated robustness against a variety of distortions, albeit
	under the restrictive assumption of perfect image registration. To
	overcome this misalignment problem, we propose a graphical learning
	framework for robust automatic face recognition, utilizing sparse
	signal representations from face images as features for classification.
	Our approach combines two key ideas from recent work in: (i) locally
	adaptive block-based sparsity for face recognition, and (ii) discriminative
	learning of graphical models. In particular, we learn discriminative
	graphs on sparse representations obtained from distinct local slices
	of a face. The graphical models are learnt in a manner such that
	conditional correlations between these sparse features are first
	discovered (in the training phase), and subsequently exploited to
	bring about significant improvements in recognition rates. Experimental
	results show that the complementary merits of existing sparsity-based
	face recognition techniques - which use class specific reconstruction
	error as a recognition statistic - in comparison with our proposed
	approach can further be mined into building a powerful meta-classifier
	for face recognition.},
  timestamp = {2016-07-11T16:55:06Z},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2011 Conference Record 	of the Forty Fifth Asilomar Conference on},
  author = {Srinivas, U. and Monga, V. and Chen, Yi and Tran, T.D.},
  month = nov,
  year = {2011},
  keywords = {class specific reconstruction error,conditional correlation,discriminative
	graphical model,discriminative graph learning,Face,face image feature,Face
	recognition,face slices,feature extraction,graphical learning framework,Graphical
	models,Graph theory,image classification,image distortion,Image reconstruction,image
	registration,image representation,learning (artificial intelligence),locally
	adaptive block-based sparsity,metaclassifier,misalignment problem,recognition
	statistic,robust automatic face recognition,Robustness,sparse features,sparse
	linear combination,sparse signal representation,sparsity-based face
	recognition,statistical analysis,Training,Vectors},
  pages = {1204-1208},
  owner = {afdidehf}
}

@inproceedings{Sriram2012,
  title = {Burst error correction using partial fourier matrices and block sparse 	representation},
  doi = {10.1109/NCC.2012.6176836},
  abstract = {There is a strong relation between sparse signal recovery and error
	control coding. It is known that burst errors are block sparse in
	nature. So, here we attempt to solve burst error correction problem
	using block sparse signal recovery methods. We construct partial
	Fourier based encoding and decoding matrices using results on difference
	sets. These constructions offer guaranteed and efficient error correction
	when used in conjunction with reconstruction algorithms which exploit
	block sparsity.},
  timestamp = {2016-07-08T11:49:51Z},
  booktitle = {Communications (NCC), 2012 National Conference on},
  author = {Sriram, N.M. and Adiga, B.S. and Hari, K.V.S.},
  month = feb,
  year = {2012},
  keywords = {block codes,block sparse representation,block sparse signal recovery
	methods,block sparsity,burst error correction,Coherence,compressed
	sensing,Decoding,Difference Set,encoding,error control coding,Error
	correction,error correction codes,Fourier analysis,inverse problems,partial
	Fourier based decoding matrices,partial Fourier based encoding matrices,partial
	Fourier matrix,Reconstruction algorithms,sparse matrices,Vectors},
  pages = {1-5},
  owner = {afdidehf}
}

@article{Sriram2014,
  title = {Grassmannian fusion frames and its use in block sparse recovery},
  volume = {94},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2013.07.016},
  abstract = {Abstract Tight fusion frames which form optimal packings in Grassmannian
	manifolds are of interest in signal processing and communication
	applications. In this paper, we study optimal packings and fusion
	frames having a specific structure for use in block sparse recovery
	problems. The paper starts with a sufficient condition for a set
	of subspaces to be an optimal packing. Further, a method of using
	optimal Grassmannian frames to construct tight fusion frames which
	form optimal packings is given. Then, we derive a lower bound on
	the block coherence of dictionaries used in block sparse recovery.
	From this result, we conclude that the Grassmannian fusion frames
	considered in this paper are optimal from the block coherence point
	of view.},
  timestamp = {2016-07-08T12:38:47Z},
  journal = {Signal Processing},
  author = {Sriram, N. Mukund and Adiga, B. S. and Hari, K. V. S.},
  year = {2014},
  keywords = {Block,bound,Coherence,frame,Fusion,Grassmannian,Optimal,packing,Simplex,Sparsity},
  pages = {498 - 502},
  owner = {afdidehf}
}

@inproceedings{Stahlhut2013,
  title = {A hierarchical Bayesian M/EEG imagingmethod correcting for incomplete 	spatio-temporal priors},
  doi = {10.1109/ISBI.2013.6556536},
  abstract = {In this paper we present a hierarchical Bayesian model, to tackle
	the highly ill-posed problem that follows with MEG and EEG source
	imaging. Our model promotes spatiotemporal patterns through the use
	of both spatial and temporal basis functions. While in contrast to
	most previous spatio-temporal inverse M/EEG models, the proposed
	model benefits of consisting of two source terms, namely, a spatiotemporal
	pattern term limiting the source configuration to a spatio-temporal
	subspace and a source correcting term to pick up source activity
	not covered by the spatio-temporal prior belief. Both artificial
	data and real EEG data is used to demonstrate the efficacy of the
	model.},
  timestamp = {2016-07-08T10:17:39Z},
  booktitle = {Biomedical Imaging (ISBI), 2013 IEEE 10th International Symposium 	on},
  author = {Stahlhut, C. and Attias, H.T. and Sekihara, K. and Wipf, D. and Hansen, L.K. and Nagarajan, S.S.},
  month = apr,
  year = {2013},
  keywords = {artificial data,Bayes methods,Brain modeling,Computational modeling,Data
	models,EEG,EEG source imaging,electroencephalography,hierarchical
	Bayesian EEG imaging method,hierarchical Bayesian MEG imaging method,hierarchical
	Bayesian model,ill-posed problem,imaging,inverse problem,Inverse
	problems,magnetoencephalography,medical image processing,MEG,MEG
	source imaging,real EEG data,source activity,source configuration,source
	correcting term,spatial basis function,spatiotemporal inverse M/EEG
	model,spatiotemporal pattern,spatiotemporal phenomena,spatio-temporal
	prior,spatiotemporal prior belief,spatiotemporal subspace,temporal
	basis function,Variational Bayes},
  pages = {560-563},
  owner = {afdidehf}
}

@inproceedings{Stankovic2014,
  title = {Image reconstruction from a reduced set of pixels using a simplified 	gradient algorithm},
  doi = {10.1109/TELFOR.2014.7034455},
  abstract = {A reconstruction of images in the DCT transformation domain based
	on the adaptive gradient algorithm is considered in this paper. Two
	approaches are used in the reconstruction. In the first approach,
	the image is pre-processed using 8�8 blocks, such that the smallest
	DCT coefficients are set to zero in order to make the image sparse.
	The second approach reconstructs the image without the pre-processing
	step. It has been assumed that the sparsity is an intrinsic property
	of the analyzed image. An adaptive gradient based algorithm is used
	to recover a large number of missing pixels in the image. In order
	to improve the calculation complexity, in this paper we propose an
	improved version of recently proposed adaptive gradient algorithm,
	which is now reduced to a single, automatically determined parameter.
	The previous reconstruction of black and white and colour images
	is repeated with a significant calculation efficiency improvement.},
  timestamp = {2016-07-08T12:46:46Z},
  booktitle = {Telecommunications Forum Telfor (TELFOR), 2014 22nd},
  author = {Stankovic, I. and Orovic, I. and Stankovic, S.},
  month = nov,
  year = {2014},
  keywords = {adaptive gradient algorithm,black and white images,calculation complexity,colour
	images,compressive sensing,computational complexity,DCT coefficients,DCT
	transformation domain,Decision support systems,discrete cosine transforms,gradient-based
	algorithm,gradient methods,image colour analysis,Image Processing,Image
	reconstruction,sparse signals,Telecommunications},
  pages = {497-500},
  owner = {afdidehf}
}

@inproceedings{Starck2009,
  title = {An Overview Of Inverse Problem Regularization Using Sparsity},
  doi = {10.1109/ICIP.2009.5414556},
  abstract = {Sparsity constraints are now very popular to regularize inverse problems.
	We review several approaches which have been proposed in the last
	ten years to solve inverse problems such as inpainting, deconvolution
	or blind source separation. We will focus especially on optimization
	methods based on iterative thresholding methods to derive the solution.},
  timestamp = {2016-07-08T11:25:51Z},
  booktitle = {Image Processing (ICIP), 2009 16th IEEE International Conference 	on},
  author = {Starck, J.L. and Fadili, M.J.},
  month = nov,
  year = {2009},
  keywords = {blind source separation,compressed sensing,Deconvolution,Dictionaries,inpainting,Inverse
	problems,iterative methods,iterative thresholding,iterative thresholding
	methods,optimisation,Optimization methods,Sampling methods,Signal
	design,Sparsity,sparsity constraints,wavelet transforms},
  pages = {1453-1456},
  owner = {Fardin}
}

@article{Starck2002,
  title = {The curvelet transform for image denoising},
  volume = {11},
  issn = {1057-7149},
  doi = {10.1109/TIP.2002.1014998},
  abstract = {We describe approximate digital implementations of two new mathematical
	transforms, namely, the ridgelet transform and the curvelet transform.
	Our implementations offer exact reconstruction, stability against
	perturbations, ease of implementation, and low computational complexity.
	A central tool is Fourier-domain computation of an approximate digital
	Radon transform. We introduce a very simple interpolation in the
	Fourier space which takes Cartesian samples and yields samples on
	a rectopolar grid, which is a pseudo-polar sampling set based on
	a concentric squares geometry. Despite the crudeness of our interpolation,
	the visual performance is surprisingly good. Our ridgelet transform
	applies to the Radon transform a special overcomplete wavelet pyramid
	whose wavelets have compact support in the frequency domain. Our
	curvelet transform uses our ridgelet transform as a component step,
	and implements curvelet subbands using a filter bank of a` trous
	wavelet filters. Our philosophy throughout is that transforms should
	be overcomplete, rather than critically sampled. We apply these digital
	transforms to the denoising of some standard images embedded in white
	noise. In the tests reported here, simple thresholding of the curvelet
	coefficients is very competitive with "state of the art" techniques
	based on wavelets, including thresholding of decimated or undecimated
	wavelet transforms and also including tree-based Bayesian posterior
	mean methods. Moreover, the curvelet reconstructions exhibit higher
	perceptual quality than wavelet-based reconstructions, offering visually
	sharper images and, in particular, higher quality recovery of edges
	and of faint linear and curvilinear features. Existing theory for
	curvelet and ridgelet transforms suggests that these new approaches
	can outperform wavelet methods in certain image reconstruction problems.
	The empirical results reported here are in encouraging agreement},
  timestamp = {2016-09-29T16:09:32Z},
  number = {6},
  journal = {Image Processing, IEEE Transactions on},
  author = {Starck, J.-L. and Candes, E.J. and Donoho, D.L.},
  month = jun,
  year = {2002},
  keywords = {approximate digital implementations,approximate digital Radon transform,Cartesian
	samples,channel bank filters,computational complexity,concentric
	squares geometry,curvelet coefficients,curvelet transform,decimated
	wavelet transforms,exact reconstruction,Filter bank,filtering theory,filtering
	theory,Fourier-domain,Fourier space,Fourier
	space,Fourier transforms,frequency domain,frequency
	domain,image denoising,Image reconstruction,interpolation,low
	computational complexity,overcomplete wavelet pyramid,pseudo-polar
	sampling set,Radon transforms,rectopolar grid,ridgelet transform,Sampling
	methods,Stability,tree-based Bayesian posterior mean methods,trous
	wavelet filters,undecimated wavelet transforms,visual performance,wavelet-based
	image reconstruction,Wavelet domain,wavelet transforms,White noise,White
	noise},
  pages = {670--684},
  owner = {afdidehf}
}

@article{Starck2005,
  title = {Image decomposition via the combination of sparse representations 	and a variational approach},
  volume = {14},
  issn = {1057-7149},
  doi = {10.1109/TIP.2005.852206},
  abstract = {The separation of image content into semantic parts plays a vital
	role in applications such as compression, enhancement, restoration,
	and more. In recent years, several pioneering works suggested such
	a separation be based on variational formulation and others using
	independent component analysis and sparsity. This paper presents
	a novel method for separating images into texture and piecewise smooth
	(cartoon) parts, exploiting both the variational and the sparsity
	mechanisms. The method combines the basis pursuit denoising (BPDN)
	algorithm and the total-variation (TV) regularization scheme. The
	basic idea presented in this paper is the use of two appropriate
	dictionaries, one for the representation of textures and the other
	for the natural scene parts assumed to be piecewise smooth. Both
	dictionaries are chosen such that they lead to sparse representations
	over one type of image-content (either texture or piecewise smooth).
	The use of the BPDN with the two amalgamed dictionaries leads to
	the desired separation, along with noise removal as a by-product.
	As the need to choose proper dictionaries is generally hard, a TV
	regularization is employed to better direct the separation process
	and reduce ringing artifacts. We present a highly efficient numerical
	scheme to solve the combined optimization problem posed by our model
	and to show several experimental results that validate the algorithm's
	performance.},
  timestamp = {2016-07-08T12:46:35Z},
  number = {10},
  journal = {Image Processing, IEEE Transactions on},
  author = {Starck, J.-L. and Elad, M. and Donoho, D.L.},
  month = oct,
  year = {2005},
  keywords = {Algorithms,Artificial Intelligence,Automated,Basis pursuit denoising,Basis
	pursuit denoising (BPDN),BPDN algorithm,Computer-Assisted,curvelet,DCT,Dictionaries,discrete
	cosine transform,discrete cosine transforms,Discrete wavelet transforms,Image
	coding,image content separation,Image decomposition,image denoising,Image
	Enhancement,Image Interpretation,image representation,image restoration,image
	restoration,image texture,image
	texture,Independent component analysis,Independent
	component analysis,Information Storage and Retrieval,Information
	Storage and Retrieval,Layout,local discrete cosine transform (DCT),local
	discrete cosine transform (DCT),Models,Noise reduction,Noise
	reduction,optimisation,optimization problem,optimization
	problem,Pattern Recognition,piecewise smooth,piecewise
	smooth,piecewise smooth part,Pursuit algorithms,Pursuit
	algorithms,ridgelet,Separation processes,Separation
	processes,source separation,sparse representations,sparse
	representations,Statistical,Texture,total variation,total
	variation,total-variation regularization scheme,total-variation
	regularization scheme,TV,wavelet,wavelet transform,wavelet
	transform},
  pages = {1570-1582},
  owner = {afdidehf}
}

@article{Stenroos2014,
  title = {MEG forward modelling},
  timestamp = {2016-07-09T20:11:23Z},
  author = {Stenroos, Matti},
  year = {2014},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Aalto University, BECS},
  owner = {afdidehf}
}

@article{Stojnic2010,
  title = {$\ell_2/\ell_1$-Optimization in Block-Sparse Compressed Sensing and 	Its Strong Thresholds},
  volume = {4},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2009.2039172},
  abstract = {It has been known for a while that l1-norm relaxation can in certain
	cases solve an under-determined system of linear equations. Recently,
	E. Candes ("Robust uncertainty principles: Exact signal reconstruction
	from highly incomplete frequency information," IEEE Trans. Information
	Theory, vol. 52, no. 12, pp. 489-509, Dec. 2006) and D. Donoho ("High-dimensional
	centrally symmetric polytopes with neighborlines proportional to
	dimension," Disc. Comput. Geometry, vol. 35, no. 4, pp. 617-652,
	2006) proved (in a large dimensional and statistical context) that
	if the number of equations (measurements in the compressed sensing
	terminology) in the system is proportional to the length of the unknown
	vector then there is a sparsity (number of nonzero elements of the
	unknown vector) also proportional to the length of the unknown vector
	such that l1-norm relaxation succeeds in solving the system. In this
	paper, in a large dimensional and statistical context, we determine
	sharp lower bounds on the values of allowable sparsity for any given
	number (proportional to the length of the unknown vector) of equations
	for the case of the so-called block-sparse unknown vectors considered
	in "On the reconstruction of block-sparse signals with an optimal
	number of measurements," (M. Stojnic et al., IEEE Trans, Signal Processing,
	submitted for publication.},
  timestamp = {2016-07-08T09:36:22Z},
  number = {2},
  journal = {Selected Topics in Signal Processing, IEEE Journal of},
  author = {Stojnic, M.},
  month = apr,
  year = {2010},
  keywords = {$ell_{2}/ell_{1}$ -optimization,block codes,Block-sparse,block-sparse
	compressed sensing,block-sparse signal,block-sparse unknown vector,compressed
	sensing,data compression,Equations,Frequency,Geometry,Information
	theory,l1-norm relaxation,l1-optimization,l2-optimization,Length
	measurement,optimisation,sharp lower bounds,signal processing,signal
	reconstruction,statistical analysis,statistical context,Terminology,Uncertainty},
  pages = {350-357},
  owner = {afdidehf}
}

@inproceedings{Stojnic2009,
  title = {$\ell_2/\ell_1$-optimization and its strong thresholds in approximately 	block-sparse compressed sensing},
  doi = {10.1109/ISIT.2009.5205714},
  abstract = {It has been known for a while that lscr1-norm relaxation can in certain
	cases solve an under-determined system of linear equations. Recently,
	proved (in a large dimensional and statistical context) that if the
	number of equations (measurements in the compressed sensing terminology)
	in the system is proportional to the length of the unknown vector
	then there is a sparsity (number of non-zero elements of the unknown
	vector) also proportional to the length of the unknown vector such
	that lscr1-norm relaxation succeeds in solving the system. In this
	paper we consider a modification of this standard setup, namely the
	case of so-called approximately block-sparse unknown vectors. We
	determine sharp lower bounds on the values of allowable approximate
	block-sparsity for any given number (proportional to the length of
	the unknown vector) of equations. Obtained lower bounds on the allowable
	sparsity are as expected functions of a parameter used to describe
	how close the approximately block-sparse unknown vectors are to the
	ideally block-sparse ones.},
  timestamp = {2016-07-08T09:35:36Z},
  booktitle = {Information Theory, 2009. ISIT 2009. IEEE International Symposium 	on},
  author = {Stojnic, M.},
  month = jun,
  year = {2009},
  keywords = {?2/?1-optimization,approximately block-sparse signal,approximately
	block-sparse unknown vector,approximation theory,Block-sparse,compressed sensing,compressed
	sensing,compressed sensing terminology,Differential
	equations,Industrial engineering,Length measurement,linear equation,lscr1-norm
	relaxation,lscr2/lscr1-optimization,Measurement standards,optimisation,Particle
	measurements,Probability distribution,Signal analysis,signal processing,sparse
	matrices,sparse matrix,statistical analysis,statistical context,Terminology,under-determined
	system,Vectors},
  pages = {473-477},
  owner = {afdidehf}
}

@article{Stojnic2009a,
  title = {On the Reconstruction of Block-Sparse Signals With an Optimal Number 	of Measurements},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2020754},
  abstract = {Let A be an M by N matrix (M < N) which is an instance of a real random
	Gaussian ensemble. In compressed sensing we are interested in finding
	the sparsest solution to the system of equations A x = y for a given
	y. In general, whenever the sparsity of x is smaller than half the
	dimension of y then with overwhelming probability over A the sparsest
	solution is unique and can be found by an exhaustive search over
	x with an exponential time complexity for any y. The recent work
	of Candes, Donoho, and Tao shows that minimization of the lscr 1
	norm of x subject to Ax = y results in the sparsest solution provided
	the sparsity of x, say K, is smaller than a certain threshold for
	a given number of measurements. Specifically, if the dimension of
	y approaches the dimension of x , the sparsity of x should be K <
	0.239 N. Here, we consider the case where x is block sparse, i.e.,
	x consists of n = N /d blocks where each block is of length d and
	is either a zero vector or a nonzero vector (under nonzero vector
	we consider a vector that can have both, zero and nonzero components).
	Instead of lscr1 -norm relaxation, we consider the following relaxation:
	times min ||X 1||2 + ||X 2||2 + ldrldrldr + ||X n ||2, subject to
	A x = y (*) where X i = (x ( i-1)d+1, x ( i-1)d+2, ldrldrldr , x
	i d)T for i = 1, 2, ldrldrldr , N. Our main result is that as n rarr
	infin, (*) finds the sparsest solution to A=x = y, with overwhelming
	probability in A, for any x whose sparsity is k/n < (1/2) - O (isi-
	- n), provided m /n > 1 - 1/d, and d = Omega(log(1/isin)/isin3) .
	The relaxation given in (*) can be solved in polynomial time using
	semi-definite programming.},
  timestamp = {2017-04-19T15:31:08Z},
  number = {8},
  journal = {IEEE Trans. Signal Process.},
  author = {Stojnic, M. and Parvaresh, F. and Hassibi, B.},
  month = aug,
  year = {2009},
  keywords = {block-sparse signals,block-sparse signals reconstruction,compressed
	sensing,mathematical programming,Matrix,nonzero vector,overwhelming
	probability,polynomial matrices,polynomial time,probability,real
	random Gaussian ensemble,relaxation,relaxation theory,semi-definite
	programming,signal reconstruction},
  pages = {3075--3085},
  annote = {read},
  owner = {afdidehf}
}

@inproceedings{Strohmeier2015,
  title = {MEG/EEG Source Imaging with a Non-Convex Penalty in the Time-Frequency 	Domain},
  doi = {10.1109/PRNI.2015.14},
  abstract = {Due to the excellent temporal resolution, MEG/EEG source imaging is
	an important measurement modality to study dynamic processes in the
	brain. As the bio electromagnetic inverse problem is ill-posed, constraints
	have to be imposed on the source estimates to find a unique solution.
	These constraints can be applied either in the standard or a transformed
	domain. The Time-Frequency Mixed Norm Estimate applies a composite
	convex regularization functional promoting structured sparsity in
	the time-frequency domain by combining an l2,1-mixed-norm and an
	l1-norm penalty on the coefficients of the Gabor TF decomposition
	of the source signals, to improve the reconstruction of spatially
	sparse neural activations with non-stationary and transient signals.
	Due to the l1-norm based constraints, the resulting source estimates
	are however biased in amplitude and often suboptimal in terms of
	source selection. In this work, we present the iterative reweighted
	Time-Frequency Mixed Norm Estimate, which employs a composite non-convex
	penalty formed by the sum of an l2,0.5-quasinorm and an l0.5-quasinorm
	penalty. The resulting non-convex problem is solved with a reweighted
	convex optimization scheme, in which each iteration is equivalent
	to a weighted Time-Frequency Mixed-Norm Estimate solved efficiently
	using a block coordinate descent scheme and an active set strategy.
	We compare our approach to alternative solvers using simulations
	and analysis of MEG data and demonstrate the benefit of the iterative
	reweighted Time-Frequency Mixed Norm Estimate with regard to active
	source identification, amplitude bias correction, and temporal unmixing
	of activations.},
  timestamp = {2016-07-09T20:10:45Z},
  booktitle = {Pattern Recognition in NeuroImaging (PRNI), 2015 International Workshop 	on},
  author = {Strohmeier, Daniel and Gramfort, Alexandre and Haueisen, Jens},
  month = jun,
  year = {2015},
  keywords = {Brain modeling,Computational modeling,EEG,electroencephalography,Gabor
	transform,imaging,inverse problem,inverse problems,iterative reweighted
	optimization algorithm,MEG,Optimization,Structured Sparsity,time-frequency
	analysis},
  pages = {21-24},
  owner = {afdidehf}
}

@inproceedings{Strohmeier2011,
  title = {MEG/EEG source reconstruction based on Gabor thresholding in the 	source space},
  doi = {10.1109/NFSI.2011.5936829},
  abstract = {Thanks to their high temporal resolution, source reconstruction based
	on Magnetoencephalography (MEG) and/or Electroencephalography (EEG)
	is an important tool for noninvasive functional brain imaging. Since
	the MEG/EEG inverse problem is ill-posed, inverse solvers employ
	priors on the sources. While priors are generally applied in the
	time domain, the time-frequency (TF) characteristics of brain signals
	are rarely employed as a spatio-temporal prior. In this work, we
	present an inverse solver which employs a structured sparse prior
	formed by the sum of ?21 and ?1 norms on the coefficients of the
	Gabor TF decomposition of the source activations. The resulting convex
	optimization problem is solved using a first-order scheme based on
	proximal operators. We provide empirical evidence based on EEG simulations
	that the proposed method is able to recover neural activations that
	are spatially sparse, temporally smooth and non-stationary. We compare
	our approach to alternative solvers based also on convex sparse priors,
	and demonstrate the benefit of promoting sparse Gabor decompositions
	via a mathematically principled iterative thresholding procedure.},
  timestamp = {2016-07-09T20:10:49Z},
  booktitle = {Noninvasive Functional Source Imaging of the Brain and Heart 2011 	8th International Conference on Bioelectromagnetism (NFSI ICBEM), 	2011 8th International Symposium on},
  author = {Strohmeier, D. and Gramfort, A. and Haueisen, J. and Hamalainen, M. and Kowalski, M.},
  month = may,
  year = {2011},
  keywords = {Brain,Brain modeling,brain signals,convex optimization problem,Dictionaries,electroencephalography,Gabor
	TF decomposition,Gabor thresholding,Image reconstruction,inverse
	problem,inverse problems,iterative methods,iterative thresholding,iterative
	thresholding,magnetoencephalography,medical image processing,medical
	image processing,MEG-EEG source reconstruction,MEG-EEG
	source reconstruction,noninvasive functional brain imaging,noninvasive functional
	brain imaging,optimisation,Signal to noise ratio,Signal
	to noise ratio,source separation,Time domain analysis,Time
	domain analysis,time-frequency analysis,Time frequency analysis,time-frequency
	analysis,time-frequency characteristics,time-frequency
	characteristics},
  pages = {103-108},
  owner = {afdidehf}
}

@inproceedings{Strohmeier2014,
  title = {Improved MEG/EEG source localization with reweighted mixed-norms},
  doi = {10.1109/PRNI.2014.6858545},
  abstract = {MEG/EEG source imaging allows for the noninvasive analysis of brain
	activity with high temporal and good spatial resolution. As the bioelectromagnetic
	inverse problem is ill-posed, a priori information is required to
	find a unique source estimate. For the analysis of evoked brain activity,
	spatial sparsity of the neuronal activation can be assumed. Due to
	the convexity, ?-norm based constraints are often used for this,
	which however lead to source estimates biased in amplitude and often
	suboptimal in terms of source selection. As an alternative, non-convex
	regularization functionals such as ? p-quasinorms with 0 <; p <;
	1 can be used. In this work, we present a MEG/EEG inverse solver
	based on a ? 2,0.5-quasinorm penalty promoting spatial sparsity as
	well as temporal stationarity of the brain activity. For solving
	the resulting non-convex optimization problem, we propose the iterative
	reweighted Mixed Norm Estimate, which is based on reweighted convex
	optimization and combines a block coordinate descent scheme and an
	active set strategy to solve each surrogate problem efficiently.
	We provide empirical evidence based on simulations and analysis of
	MEG data that the proposed method outperforms the standard Mixed
	Norm Estimate in terms of active source identification and amplitude
	bias.},
  timestamp = {2016-07-08T12:48:32Z},
  booktitle = {Pattern Recognition in Neuroimaging, 2014 International Workshop 	on},
  author = {Strohmeier, D. and Haueisen, J. and Gramfort, A.},
  month = jun,
  year = {2014},
  keywords = {? p-quasinorms,active set strategy,active source identification,alternative
	nonconvex regularization functionals,auditory evoked potentials,bioelectromagnetic inverse problem,bioelectromagnetic
	inverse problem,block coordinate
	descent scheme,brain activity,EEG,electroencephalography,evoked brain
	activity,ill-posed a priori information,inverse problems,iterative
	methods,iterative reweighted mixed norm estimate,iterative reweighted
	optimization algorithm,magnetoencephalography,medical signal processing,MEG,MEG-EEG
	inverse solver,MEG-EEG source imaging,MEG-EEG source localization,neuronal
	activation,neurophysiology,nonconvex optimization problem,noninvasive
	analysis,optimisation,reweighted convex optimization,reweighted mixed-norms,Signal
	resolution,source estimate,Spatial resolution,spatial sparsity,structured
	sparsity,temporal resolution,temporal stationarity},
  pages = {1-4},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@article{Strohmer2003,
  title = {Grassmannian frames with applications to coding and communication},
  volume = {14},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/S1063-5203(03)00023-X},
  abstract = {For a given class F of unit norm frames of fixed redundancy we define
	a Grassmannian frame as one that minimizes the maximal correlation
	|〈fk,fl〉| among all frames {fk}k∈I∈F. We first analyze finite-dimensional
	Grassmannian frames. Using links to packings in Grassmannian spaces
	and antipodal spherical codes we derive bounds on the minimal achievable
	correlation for Grassmannian frames. These bounds yield a simple
	condition under which Grassmannian frames coincide with unit norm
	tight frames. We exploit connections to graph theory, equiangular
	line sets, and coding theory in order to derive explicit constructions
	of Grassmannian frames. Our findings extend recent results on unit
	norm tight frames. We then introduce infinite-dimensional Grassmannian
	frames and analyze their connection to unit norm tight frames for
	frames which are generated by group-like unitary systems. We derive
	an example of a Grassmannian Gabor frame by using connections to
	sphere packing theory. Finally we discuss the application of Grassmannian
	frames to wireless communication and to multiple description coding.},
  timestamp = {2016-09-29T16:10:09Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Strohmer, Thomas and Jr, Robert W. Heath},
  year = {2003},
  keywords = {Conference matrix,Equiangular line sets,frame,Gabor frame,Grassmannian
	spaces,Multiple description coding,Spherical codes,Unitary system,Unit
	norm tight frame},
  pages = {257--275},
  owner = {afdidehf}
}

@inproceedings{Su2012,
  title = {Spatiotemporal Searchlight Representational Similarity Analysis in 	EMEG Source Space},
  doi = {10.1109/PRNI.2012.26},
  abstract = {Time resolved imaging techniques, such as MEG and EEG, are unique
	in their ability to reveal the rich dynamic spatiotemporal patterning
	of neural activities. Here we propose a technique based on spatiotemporal
	searchlight Representational Similarity Analysis (RSA) of combined
	MEG and EEG (EMEG) data to directly analyze the multivariate pattern
	of information flow across the brain. This novel technique can recognize
	fine-grained dynamic neural computations both in space and in time.
	A prime example of such neural computations is our ability to understand
	spoken words in real time. A computational approach to these processes
	is suggested by the Cohort Model of spoken-word recognition. Here
	we show how spatiotemporal searchlight RSA applied to source estimations
	of EMEG data can provide insights into the neural correlates of the
	cohort model within bilateral front temporal brain regions.},
  timestamp = {2016-10-21T13:48:31Z},
  booktitle = {Pattern Recognition in NeuroImaging (PRNI), International Workshop 	on},
  author = {Su, Li and Fonteneau, E. and Marslen-Wilson, W. and Kriegeskorte, N.},
  month = jul,
  year = {2012},
  keywords = {bilateral front temporal brain regions,Brain,Brain modeling,cohort
	model,Computational modeling,Correlation,Data models,EEG,electroencephalography,EMEG
	source space,Estimation,fine-grained dynamic neural computations,information
	flow multivariate pattern,magnetoencephalography,medical image processing,MEG,MNE,MVPA,neural
	activities,neural nets,rich dynamic spatiotemporal patterning,RSA,spatiotemporal
	phenomena,spatiotemporal searchlight representational similarity
	analysis,spoken-word recognition,time resolved imaging techniques},
  pages = {97--100},
  owner = {Fardin}
}

@article{Sui2011,
  title = {Discriminating schizophrenia and bipolar disorder by fusing fMRI 	and DTI in a multimodal CCA+ joint ICA model},
  volume = {57},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2011.05.055},
  abstract = {Diverse structural and functional brain alterations have been identified
	in both schizophrenia and bipolar disorder, but with variable replicability,
	significant overlap and often in limited number of subjects. In this
	paper, we aimed to clarify differences between bipolar disorder and
	schizophrenia by combining fMRI (collected during an auditory oddball
	task) and diffusion tensor imaging (DTI) data. We proposed a fusion
	method, “multimodal CCA+ joint ICA�?, which increases flexibility
	in statistical assumptions beyond existing approaches and can achieve
	higher estimation accuracy. The data collected from 164 participants
	(62 healthy controls, 54 schizophrenia and 48 bipolar) were extracted
	into “features�? (contrast maps for fMRI and fractional anisotropy
	(FA) for DTI) and analyzed in multiple facets to investigate the
	group differences for each pair-wised groups and each modality. Specifically,
	both patient groups shared significant dysfunction in dorsolateral
	prefrontal cortex and thalamus, as well as reduced white matter (WM)
	integrity in anterior thalamic radiation and uncinate fasciculus.
	Schizophrenia and bipolar subjects were separated by functional differences
	in medial frontal and visual cortex, as well as \{WM\} tracts associated
	with occipital and frontal lobes. Both patients and controls showed
	similar spatial distributions in motor and parietal regions, but
	exhibited significant variations in temporal lobe. Furthermore, there
	were different group trends for age effects on loading parameters
	in motor cortex and multiple \{WM\} regions, suggesting that brain
	dysfunction and \{WM\} disruptions occurred in identified regions
	for both disorders. Most importantly, we can visualize an underlying
	function–structure network by evaluating the joint components with
	strong links between \{DTI\} and fMRI. Our findings suggest that
	although the two patient groups showed several distinct brain patterns
	from each other and healthy controls, they also shared common abnormalities
	in prefrontal thalamic \{WM\} integrity and in frontal brain mechanisms.},
  timestamp = {2016-07-08T12:12:08Z},
  number = {3},
  journal = {NeuroImage},
  author = {Sui, Jing and Pearlson, Godfrey and Caprihan, Arvind and Adali, T{\"u}lay and Kiehl, Kent A. and Liu, Jingyu and Yamamoto, Jeremy and D. Calhoun, Vince},
  year = {2011},
  note = {Special Issue: Educational Neuroscience},
  keywords = {Diffusion tensor imaging (DTI),Fractional anisotropy (FA),Functional
	MRI (fMRI),Independent component analysis (ICA),Multimodal canonical
	correlation analysis (mCCA),Schizophrenia Bipolar disorder},
  pages = {839 - 855},
  owner = {afdidehf}
}

@article{Sui2012a,
  title = {A selective review of multimodal fusion methods in schizophrenia},
  volume = {6},
  abstract = {Schizophrenia(SZ)isoneofthemostcrypticandcostlymentaldisordersintermsofhumansufferingandsocietalexpenditure(vanOsandKapur,2009).Thoughstrongevidenceforfunctional,structural,andgeneticabnormalitiesassociatedwiththisdiseaseexists,thereisyetnoreplicablefindingwhichhasprovenaccurateenoughtobeusefulinclinicaldecisionmaking(Fornitoetal.,2009),anditsdiagnosisreliesprimarilyuponsymptomassessment(Williamsetal.,2010a).Itislikelyinpartthatthelackofconsistentneuroimagingfindingsisbecausemostmodelsfavoronlyonedatatypeordonotcombinedatafromdifferentimagingmodalitieseffectively,thusmissingpotentiallyimportantdifferenceswhichareonlypartiallydetectedbyeachmodality(Calhounetal.,2006a).Itisbecomingincreas-inglyclearthatmultimodalfusion,atechniquewhichtakesadvantageofthefactthateachmodalityprovidesalimitedviewofthebrain/geneandmayuncoverhiddenrelationships,isanimportanttooltohelpunraveltheblackboxofschizophrenia.Inthisreviewpaper,wesurveyanumberofmultimodalfusionapplicationswhichenableustostudytheschizo-phreniamacro-connectome,includingbrainfunctional,structural,andgeneticaspectsandmayhelpusunderstandthedisorderinamorecomprehensiveandintegratedmanner.Wealsoprovideatablethatcharacterizestheseapplicationsbythemethodsusedandcomparethesemethodsindetail,especiallyformultivariatemodels,whichmayserveasavaluablereferencethathelpsreadersselectanappropriatemethodbasedonagivenresearchquestion.},
  timestamp = {2016-07-08T11:28:23Z},
  journal = {Frontiers in human neuroscience},
  author = {Sui, Jing and Yu, Qingbao and He, Hao and Pearlson, Godfrey D. and Calhoun, Vince D.},
  month = feb,
  year = {2012},
  keywords = {CCA,DTI,EEG,ICA,MRI,multimodalfusion,Schizophrenia,SNP},
  owner = {Fardin}
}

@article{Sun2013,
  title = {Sparse block circulant matrices for compressed sensing},
  volume = {7},
  issn = {1751-8628},
  doi = {10.1049/iet-com.2013.0030},
  abstract = {An undetermined measurement matrix can capture sparse signals losslessly
	if the matrix satisfies the restricted isometry property (RIP) in
	compressed sensing (CS) framework. However, existing measurement
	matrices suffer from high computational burden because of their completely
	unstructured nature. In this study, the authors propose to construct
	a novel measurement matrix with a specific structure, called sparse
	block circulant matrix (SBCM), to reduce the computational burden.
	The RIP of the proposed SBCM is also guaranteed with overwhelming
	probability. The simulation results validate that SBCM reduces the
	computational burden significantly whereas keeps similar signal recovery
	accuracy as Gaussian random matrices.},
  timestamp = {2016-07-11T16:40:04Z},
  number = {13},
  journal = {Communications, IET},
  author = {Sun, Jingming and Wang, Shu and Dong, Yan},
  month = sep,
  year = {2013},
  keywords = {compressed sensing,CS framework,Gaussian processes,Gaussian random
	matrices,matrix measurement,restricted isometry property,RIP,SBCM,signal
	recovery,sparse block circulant matrices,sparse matrices,sparse signals},
  pages = {1412-1418},
  owner = {afdidehf}
}

@inproceedings{Sun2009,
  title = {Efficient Recovery of Jointly Sparse Vectors},
  abstract = {We consider the reconstruction of sparse signals in the multiple measurement
	vector (MMV) model, in which the signal, represented as a matrix,
	consists of a set of jointly sparse vectors. MMV is an extension
	of the single measurement vector (SMV) model employed in standard
	compressive sensing (CS). Recent theoretical studies focus on the
	convex relaxation of the MMV problem based on the (2; 1)-norm minimization,
	which is an extension of the well-known 1-norm minimization employed
	in SMV. However, the resulting convex optimization problem in MMV
	is significantly much more difficult to solve than the one in SMV.
	Existing algorithms reformulate it as a second-order cone programming
	(SOCP) or semidefinite programming (SDP) problem, which is computationally
	expensive to solve for problems of moderate size. In this paper,
	we propose a new (dual) reformulation of the convex optimization
	problem in MMV and develop an efficient algorithm based on the prox-method.
	Interestingly, our theoretical analysis reveals the close connection
	between the proposed reformulation and multiple kernel learning.
	Our simulation studies demonstrate the scalability of the proposed
	algorithm.},
  timestamp = {2016-07-08T12:19:51Z},
  booktitle = {Neural Information Processing Systems},
  author = {Sun, L. and Liu, J. and Chen, J. and Ye, J.},
  year = {2009},
  owner = {Fardin}
}

@article{Sun2015,
  title = {Band Selection Using Improved Sparse Subspace Clustering for Hyperspectral 	Imagery Classification},
  volume = {8},
  issn = {1939-1404},
  doi = {10.1109/JSTARS.2015.2417156},
  abstract = {An improved sparse subspace clustering (ISSC) method is proposed to
	select an appropriate band subset for hyperspectral imagery (HSI)
	classification. The ISSC assumes that band vectors are sampled from
	a union of low-dimensional orthogonal subspaces and each band can
	be sparsely represented as a linear or affine combination of other
	bands within its subspace. First, the ISSC represents band vectors
	with sparse coefficient vectors by solving the L2-norm optimization
	problem using the least square regression (LSR) algorithm. The sparse
	and block diagonal structure of the coefficient matrix from LSR leads
	to correct segmentation of band vectors. Second, the angular similarity
	measurement is presented and utilized to construct the similarity
	matrix. Third, the distribution compactness (DC) plot algorithm is
	used to estimate an appropriate size of the band subset. Finally,
	spectral clustering is implemented to segment the similarity matrix
	and the desired ISSC band subset is found. Four groups of experiments
	on three widely used HSI datasets are performed to test the performance
	of ISSC for selecting bands in classification. In addition, the following
	six state-of-the-art band selection methods are used to make comparisons:
	linear constrained minimum variance-based band correlation constraint
	(LCMV-BCC), affinity propagation (AP), spectral information divergence
	(SID), maximum-variance principal component analysis (MVPCA), sparse
	representation-based band selection (SpaBS), and sparse nonnegative
	matrix factorization (SNMF). Experimental results show that the ISSC
	has the second shortest computational time and also outperforms the
	other six methods in classification accuracy when using an appropriate
	band number obtained by the DC plot algorithm.},
  timestamp = {2016-07-08T11:36:33Z},
  number = {6},
  journal = {Selected Topics in Applied Earth Observations and Remote Sensing, 	IEEE Journal of},
  author = {Sun, Weiwei and Zhang, Liangpei and Du, Bo and Li, Weiyue and Lai, Y.M.},
  month = jun,
  year = {2015},
  keywords = {affinity propagation,angular similarity measurement,Band selection,Band
	selection,band vectors segmentation,classification,Clustering algorithms,coefficient
	matrix,Correlation,DC plot algorithm,distribution compactness plot
	algorithm,geophysical image processing,HSI classification,hyperspectral
	imagery classification,hyperspectral imagery (HSI),image classification,improved
	sparse subspace clustering,improved sparse subspace clustering (ISSC),ISSC
	method,L2-norm optimization problem,LCMV-BCC,least square regression
	algorithm,least squares approximations,linear constrained minimum
	variance-based band correlation constraint,low-dimensional orthogonal
	subspaces,LSR algorithm,matrix decomposition,maximum-variance principal
	component analysis,MVPCA,optimisation,Optimization,principal component
	analysis,regression analysis,remote sensing,similarity matrix,SNMF,SpaBS,sparse
	coefficient vectors,sparse matrices,sparse nonnegative matrix factorization,sparse
	representation-based band selection,spectral information divergence,Sun},
  pages = {2784-2797},
  owner = {afdidehf}
}

@article{Sun2015a,
  title = {Task-Driven Dictionary Learning for Hyperspectral Image Classification 	With Structured Sparsity Constraints},
  volume = {53},
  issn = {0196-2892},
  doi = {10.1109/TGRS.2015.2399978},
  abstract = {Sparse representation models a signal as a linear combination of a
	small number of dictionary atoms. As a generative model, it requires
	the dictionary to be highly redundant in order to ensure both a stable
	high sparsity level and a low reconstruction error for the signal.
	However, in practice, this requirement is usually impaired by the
	lack of labeled training samples. Fortunately, previous research
	has shown that the requirement for a redundant dictionary can be
	less rigorous if simultaneous sparse approximation is employed, which
	can be carried out by enforcing various structured sparsity constraints
	on the sparse codes of the neighboring pixels. In addition, numerous
	works have shown that applying a variety of dictionary learning methods
	for the sparse representation model can also improve the classification
	performance. In this paper, we highlight the task-driven dictionary
	learning (TDDL) algorithm, which is a general framework for the supervised
	dictionary learning method. We propose to enforce structured sparsity
	priors on the TDDL method in order to improve the performance of
	the hyperspectral classification. Our approach is able to benefit
	from both the advantages of the simultaneous sparse representation
	and those of the supervised dictionary learning. We enforce two different
	structured sparsity priors, the joint and Laplacian sparsities, on
	the TDDL method and provide the details of the corresponding optimization
	algorithms. Experiments on numerous popular hyperspectral images
	demonstrate that the classification performance of our approach is
	superior to that of the sparse representation classifier with structured
	priors or the TDDL method.},
  timestamp = {2016-07-11T16:59:19Z},
  number = {8},
  journal = {Geoscience and Remote Sensing, IEEE Transactions on},
  author = {Sun, Xiaoxia and Nasrabadi, N.M. and Tran, T.D.},
  month = aug,
  year = {2015},
  keywords = {Dictionaries,dictionary learning method,hyperspectral image classification
	performance,Hyperspectral imagery (HSI) classification,Hyperspectral
	imaging,image classification,Image Processing,Image reconstruction,joint
	sparsity,joint sparsity (JS),Laplace equations,Laplacian sparsity,Laplacian
	sparsity,Learning systems,neighboring pixel sparse code,optimisation,Optimization,optimization
	algorithm,sparse representation,sparse representation classifier,sparse
	representation model,structured sparsity constraint,supervised dictionary
	learning,task-driven dictionary learning,task-driven dictionary learning
	(TDDL),TDDL method,Training},
  pages = {4457-4471},
  owner = {afdidehf}
}

@article{Sun2015b,
  title = {Robust 2D Principal Component Analysis: A Structured Sparsity Regularized 	Approach},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2419075},
  abstract = {Principal component analysis (PCA) is widely used to extract features
	and reduce dimensionality in various computer vision and image/video
	processing tasks. Conventional approaches either lack robustness
	to outliers and corrupted data or are designed for one-dimensional
	signals. To address this problem, we propose a robust PCA model for
	two-dimensional images incorporating structured sparse priors, referred
	to as structured sparse 2D-PCA. This robust model considers the prior
	of structured and grouped pixel values in two dimensions. As the
	proposed formulation is jointly nonconvex and nonsmooth, which is
	difficult to tackle by joint optimization, we develop a two-stage
	alternating minimization approach to solve the problem. This approach
	iteratively learns the projection matrices by bidirectional decomposition
	and utilizes the proximal method to obtain the structured sparse
	outliers. By considering the structured sparsity prior, the proposed
	model becomes less sensitive to noisy data and outliers in two dimensions.
	Moreover, the computational cost indicates that the robust two-dimensional
	model is capable of processing quarter common intermediate format
	video in real time, as well as handling large-size images and videos,
	which is often intractable with other robust PCA approaches that
	involve image-to-vector conversion. Experimental results on robust
	face reconstruction, video background subtraction data set, and real-world
	videos show the effectiveness of the proposed model compared with
	conventional 2D-PCA and other robust PCA algorithms.},
  timestamp = {2016-07-10T08:04:25Z},
  number = {8},
  journal = {Image Processing, IEEE Transactions on},
  author = {Sun, Yipeng and Tao, Xiaoming and Li, Yang and Lu, Jianhua},
  month = aug,
  year = {2015},
  keywords = {2D principal component analysis,bidirectional decomposition,Computational
	modeling,computer vision,concave programming,decomposition,Feature
	Extraction,group sparse,Image reconstruction,image-to-vector conversion,image-video
	processing,iterative approach,iterative methods,matrix algebra,matrix
	factorization,minimisation,Minimization,nonconvex formulation,nonsmooth
	formulation,Optimization,Principal component analysis,projection
	matrix,proximal method,robust face reconstruction,Robustness,Robust
	Principal Component Analysis,sparse matrices,Streaming media,structured
	sparse 2D-PCA,structured sparse outlier,Structured Sparsity,structured
	sparsity regularized approach,two dimensions,two-stage alternating
	minimization approach,Vectors,video background subtraction data set},
  pages = {2515-2526},
  owner = {afdidehf}
}

@inproceedings{Suo2014,
  title = {Group structured dirty dictionary learning for classification},
  doi = {10.1109/ICIP.2014.7025029},
  abstract = {Dictionary learning techniques have gained tremendous success in many
	classification problems. Inspired by the dirty model for multi-task
	regression problems, we proposed a novel method called group-structured
	dirty dictionary learning (GDDL) that incorporates the group structure
	(for each task) with the dirty model (across tasks) in the dictionary
	training process. Its benefits are two-fold: 1) the group structure
	enforces implicitly the label consistency needed between dictionary
	atoms and training data for classification; and 2) for each class,
	the dirty model separates the sparse coefficients into ones with
	shared support and unique support, with the first set being more
	discriminative. We use proximal operators and block coordinate decent
	to solve the optimization problem. GDDL has been shown to give state-of-art
	result on both synthetic simulation and two face recognition datasets.},
  timestamp = {2016-07-08T12:41:14Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Suo, Yuanming and Dao, Minh and Tran, Trac and Mousavi, H. and Srinivas, U. and Monga, V.},
  month = oct,
  year = {2014},
  keywords = {block coordinate decent,data classification,Dictionaries,dictionary
	learning,dirty model,encoding,Face,face recognition,face recognition
	dataset,GDDL,group structured dirty dictionary learning,Indexes,label
	consistency,multitask regression problem,optimisation,optimization
	problem,pattern classification,proximal operator,regression analysis,signal
	processing,Structured Sparsity,synthetic simulation,Training,Training
	data},
  pages = {150-154},
  owner = {afdidehf}
}

@article{Tadel2008,
  title = {Free software solutions for MEG/EEG source imaging},
  timestamp = {2016-07-08T12:33:28Z},
  author = {Tadel, Fran�ois},
  year = {2008},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Cognitive Neuroscience \& Brain Imaging Lab., CNRS � University
	of Paris - H�pital de la Salp�tri�re, Cognitive Neuroimaging
	Unit, Inserm U562 � NeuroSpin - CEA},
  owner = {Fardin}
}

@article{Tadel2011,
  title = {Brainstorm: A User-Friendly Application for MEG/EEG Analysis},
  volume = {2011},
  doi = {10.1155/2011/879716},
  abstract = {Brainstorm is a collaborative open-source application dedicated to
	magnetoencephalography (MEG) and electroencephalography (EEG) data
	visualization and processing, with an emphasis on cortical source
	estimation techniques and their integration with anatomical magnetic
	resonance imaging (MRI) data. The primary objective of the software
	is to connect MEG/EEG neuroscience investigators with both the best-established
	and cutting-edge methods through a simple and intuitive graphical
	user interface (GUI).},
  timestamp = {2016-07-08T11:49:29Z},
  journal = {Computational Intelligence and Neuroscience},
  author = {Tadel, Fran�ois and Baillet, Sylvain and Mosher, John C. and Pantazis, Dimitrios and Leahy, Richard M.},
  year = {2011},
  owner = {afdidehf}
}

@inproceedings{Tan2014,
  title = {Riemannian pursuit for big matrix recovery},
  volume = {32},
  abstract = {Low rank matrix recovery is a fundamental task in many real-world
	applications. The performance of existing methods, however, deteriorates
	significantly when applied to ill-conditioned or large-scale matrices.
	In this paper, we therefore propose an efficient method, called Riemannian
	Pursuit (RP), that aims to address these two problems simultaneously.
	Our method consists of a sequence of fixed-rank optimization problems.
	Each subproblem, solved by a nonlinear Riemannian conjugate gradient
	method, aims to correct the solution in the most important subspace
	of increasing size. Theoretically, RP converges linearly under mild
	conditions and experimental results show that it substantially outperforms
	existing methods when applied to large-scale and ill-conditioned
	matrices.},
  timestamp = {2016-07-10T08:04:15Z},
  booktitle = {Proceedings of the 31 st International Conference on Machine Learning 	(ICML 2014)},
  author = {Tan, M. and Tsang, I. and Wang, L. and Vandereycken, B. and Pan, S.},
  year = {2014},
  pages = {1539-1547},
  owner = {Fardin}
}

@inproceedings{Tan2014a,
  title = {Adaptive image sequence reduction in surveillance using region enhancement 	block compressive sensing},
  doi = {10.1109/WCICA.2014.7053275},
  abstract = {Due to huge amount of visual surveillance data, network congestion
	and latency is serious problem for surveillance applications. To
	reduce the data amount, data compression must be done before transmission.
	Compressive sensing is a new theory that sample and compress signals
	at the same time with little cost of bandwidth and energy for transmission.
	While conventional compressive sensing requires huge memory to compress
	whole image at a time, block compressive sensing is introduced which
	compress image in block manners. Based on that, region enhancement
	block compressive sensing is proposed to adaptive compress image
	sequence and enhance the quality of region of interest with reduced
	data transmission. In proposed scheme, a target detecting algorithm
	is used to select the important blocks and more number of measurements
	is allocated on them. Thus the transmission amount is adaptively
	adjusted with the target in or out. The proposed scheme is evaluated
	in practical surveillance data and the result demonstrates that it
	adjusts the measurement ratio as needed and the quality of region
	of interest is enhanced at reduced measurement ratio.},
  timestamp = {2016-07-08T10:11:02Z},
  booktitle = {Intelligent Control and Automation (WCICA), 2014 11th World Congress 	on},
  author = {Tan, Yuqi and Wang, Xue and Lin, Kuicheng},
  month = jun,
  year = {2014},
  keywords = {adaptive compress image sequence,adaptive image sequence reduction,block
	compressed sensing,compressed sensing,compressive sensing,Current
	measurement,data communication,data compression,data transmission,image coding,Image
	coding,image compression,Image reconstruction,network
	congestion,region enhancement block compressive sensing,sparse matrices,Surveillance,telecommunication
	power management,telecommunication traffic,video surveillance,Visualization,visual
	surveillance,visual surveillance data},
  pages = {3375-3380},
  owner = {afdidehf}
}

@inproceedings{Tanaka2011,
  title = {A new method for localizing the sources of correlated cross-frequency 	oscillations in human brains},
  doi = {10.1109/IEMBS.2011.6091774},
  abstract = {Anatomically distributed areas are dynamically linked to form functional
	networks for processing and integrating the different modalities
	of information in the human brain. A part of such networks is considered
	to be realized with synchronization of neuronal activities, which
	can generate correlated neural oscillation at the same and/or different
	frequency bands. To investigate the networks with the synchronization,
	analysis of connectivity between not only same frequency oscillation
	but also different frequency (i.e. cross-frequency) is needed. For
	source estimation with electroencephalogram (EEG) or magneto-encephalogram
	(MEG) signals, a spatial filtering technique is recently applied
	as an alternative method for equivalent current dipole (ECD) estimation
	technique. Non-adaptive type of spatial filtering technique, such
	as the Standardized low-resolution brain electromagnetic tomography
	(sLORETA), is reported to discriminate correlated sources. However,
	it may lead to inaccurate results due to its low spatial resolution.
	In the present study, we proposed a new systematic approach for localizing
	the sources of correlated cross-frequency oscillations. The method
	we propose can overcome the limitation of the non-adaptive spatial
	filtering technique by proactively using identified information in
	sensor level analysis (e.g. cross-correlation map and correlation
	topography), which allow us to focus on target sources. The performance
	of our proposed method is evaluated with simulated EEG signals, and
	is compared with traditional method.},
  timestamp = {2016-07-08T10:32:06Z},
  booktitle = {Engineering in Medicine and Biology Society, EMBC, 2011 Annual International 	Conference of the IEEE},
  author = {Tanaka, H. and Hayashida, Y. and Igasaki, T. and Murayama, N.},
  month = aug,
  year = {2011},
  keywords = {Adult,Brain,Brain Mapping,Cerebral Cortex,Computer-Assisted,Computer
	Simulation,correlated cross-frequency oscillations,correlated neural
	oscillation,Correlation,correlation topography,EEG,Electroencephalogram,electroencephalography,equivalent
	current dipole,Estimation,estimation technique,Filtering,filtering
	theory,frequency estimation,frequency modulation,human brains,Humans,Image
	Processing,Magnetic resonance imaging,Magnetic Resonance Spectroscopy,magneto-encephalogram,magnetoencephalography,Male,medical
	signal processing,MEG,Models,neuronal activities,neurophysiology,Oscillometry,Reproducibility
	of Results,sensor level analysis,Sensors,signal processing,Software,source
	estimation,source localization,spatial filtering technique,Spatial
	resolution,standardized low-resolution brain electromagnetic tomography,Statistical,Surfaces,Time
	Factors,Tomography},
  pages = {7017-7020},
  owner = {afdidehf}
}

@inproceedings{Tang2011,
  title = {Robust principal component analysis based on low-rank and block-sparse 	matrix decomposition},
  doi = {10.1109/CISS.2011.5766144},
  abstract = {In this paper, we propose a convex program for low-rank and block-sparse
	matrix decomposition. Potential applications include outlier detection
	when certain columns of the data matrix are outliers. We design an
	algorithm based on the augmented Lagrange multiplier method to solve
	the convex program. We solve the subproblems involved in the augmented
	Lagrange multiplier method using the Douglas/Peaceman-Rachford (DR)
	monotone operator splitting method. Numerical simulations demonstrate
	the accuracy of our method compared with the robust principal component
	analysis based on low-rank and sparse matrix decomposition.},
  timestamp = {2016-07-10T08:08:25Z},
  booktitle = {Information Sciences and Systems (CISS), 2011 45th Annual Conference 	on},
  author = {Tang, Gongguo and Nehorai, Arye},
  month = mar,
  year = {2011},
  keywords = {Algorithm design and analysis,augmented Lagrange multiplier method,augmented
	Lagrange multiplier method,block-sparse matrix decomposition,convex
	program,convex programming,data matrix,Douglas-Peaceman-Rachford
	monotone operator splitting method,low-rank and block-sparse matrix
	decomposition,low-rank matrix decomposition,matrix decomposition,Matrix
	decomposition,Numerical simulation,operator splitting method,Optimization,outlier
	detection,Principal component analysis,Robustness,robust principal component analysis,Robust
	Principal Component Analysis,robust principal
	component analysis,sparse matrices,sparse
	matrices},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Tang2011a,
  title = {Computable performance analysis of block-sparsity recovery},
  doi = {10.1109/CAMSAP.2011.6136000},
  abstract = {In this paper, we employ fixed-point iteration and semidefinite programming
	to compute performance bounds on the basis pursuit algorithm for
	block-sparsity recovery. As a prerequisite for optimal sensing matrix
	design, computable performance bounds would open doors for wide applications
	in sensor arrays, MIMO radar, DNA microarrays, and many other areas
	where block-sparsity arises naturally.},
  timestamp = {2016-07-08T12:01:20Z},
  booktitle = {Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 	2011 4th IEEE International Workshop on},
  author = {Tang, Gongguo and Nehorai, Arye},
  month = dec,
  year = {2011},
  keywords = {basis pursuit algorithm,Block-sparse signal recovery,Block-sparsity
	recovery,compressive sensing,computable performance analysis,computable
	performance analysis,DNA microarrays,fixed-point iteration,fixed-point
	iteration,Indexes,mathematical programming,MIMO communication,MIMO
	radar,optimal sensing matrix design,Optimization,programming,semidefinite programming,semidefinite
	programming,Sensor arrays,Sensors,Vectors,verifiable
	sufficient condition},
  pages = {265-268},
  owner = {afdidehf}
}

@inproceedings{Tang2013,
  title = {Image denoising via Graph regularized K-SVD},
  doi = {10.1109/ISCAS.2013.6572465},
  abstract = {Sparse representation theory has been well developed in recent years.
	In this paper, we consider an image denoising problem which can be
	efficiently solved under the framework of the sparse representation
	theory. The traditional image denoising methods based on the sparse
	representation seldom take into account the special structure of
	the data. As an attempt to overcome such problem, the Graph regularized
	K-means singular value decomposition (Graph K-SVD) algorithm is proposed
	with the manifold learning. The local geometrical structure of the
	image is considered in the sparse optimization model with the graph
	Laplacian. This manifold-based optimization problem is well solved
	in the framework of the traditional K-SVD algorithm. Since the novel
	strategy adds a graph regularizer to the sparse representation model
	in order to emphasize the correlations among image blocks, the Graph
	K-SVD can achieve better denoising performance than the traditional
	K-SVD.},
  timestamp = {2016-07-08T12:46:39Z},
  booktitle = {Circuits and Systems (ISCAS), 2013 IEEE International Symposium on},
  author = {Tang, Yibin and Shen, Yuan and Jiang, Aimin and Xu, Ning and Zhu, Changping},
  month = may,
  year = {2013},
  keywords = {Approximation algorithms,Dictionaries,Geometry,graph Laplacian,graph
	regularized K-means singular value decomposition algorithm,graph
	regularized K-SVD,Graph theory,image blocks,image denoising,image
	representation,Laplace equations,local geometrical structure,manifold-based
	optimization problem,Manifold learning,Manifolds,optimisation,Optimization,singular
	value decomposition,sparse optimization model,sparse representation
	theory,Vectors},
  pages = {2820-2823},
  owner = {afdidehf}
}

@inproceedings{Tang2010,
  title = {Sparse Representations of Image via Overcomplete Dictionary Learned 	by Adaptive Non-orthogonal Sparsifying Transform},
  doi = {10.1109/ICINIS.2010.151},
  abstract = {How to learn an over complete dictionary for sparse representations
	of image is an important topic in machine learning, sparse coding,
	blind source separation, etc. The so-called K-singular value decomposition
	(K-SVD) method [3] is powerful for this purpose, however, it is too
	time-consuming to apply. Recently, an adaptive orthogonal sparsifying
	transform (AOST) method has been developed to learn the dictionary
	that is faster. However, the corresponding coefficient matrix may
	not be as sparse as that of K-SVD. For solving this problem, in this
	paper, a non-orthogonal iterative match method is proposed to learn
	the dictionary. By using the approach of sequentially extracting
	columns of the stacked image blocks, the non-orthogonal atoms of
	the dictionary are learned adaptively, and the resultant coefficient
	matrix is sparser. Experiment results show that the proposed method
	can yield effective dictionaries and the resulting image representation
	is sparser than AOST.},
  timestamp = {2016-07-11T16:53:02Z},
  booktitle = {Intelligent Networks and Intelligent Systems (ICINIS), 2010 3rd International 	Conference on},
  author = {Tang, Zunyi and Yang, Zuyuan and Ding, Shuxue},
  month = nov,
  year = {2010},
  keywords = {adaptive orthogonal sparsifying transform method,Dictionaries,image
	representation,iterative methods,K-SVD,learning (artificial intelligence),nonorthogonal
	atoms,nonorthogonal iterative match method,non-orthogonal sparsifying
	transform,overcomplete dictionary,resultant coefficient matrix,resultant
	coefficient matrix,sequentially extracting columns,sequentially
	extracting columns,sparse image representations,sparse matrices,sparse
	matrices,sparse representations,sparse
	representations,stacked image blocks},
  pages = {120-123},
  owner = {afdidehf}
}

@book{Tarantola2005,
  title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
  timestamp = {2016-07-09T19:45:08Z},
  publisher = {{\{Society for Industrial and Applied Mathematics (SIAM)\}}},
  author = {Tarantola, Albert},
  year = {2005},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@article{Taylor1979,
  title = {Deconvolution with the $l_1$ norm},
  volume = {44},
  doi = {10.1190/1.1440921},
  abstract = {Given a wavelet w and a noisy trace t = s * w + n, an approximation
	s of the spike train s can be obtained using the l 1 norm. This extraction
	has the advantage of preserving isolated spikes in s. On some types
	of data the spike train s can represent s as a sparse series of spikes,
	which may be sampled at a rate higher than the sample rate of the
	data trace t. The extracted spike train s may be qualitatively much
	different than those commonly extracted using the l 2 norm.The l
	1 norm can also be used to extract a wavelet w from a trace t when
	a spike train s is known. This wavelet extraction can be constrained
	to give a smooth wavelet which integrates to zero and goes to zero
	at the ends.Given a trace t and an initial approximation for either
	s or w, it is possible to alternately extract spike trains and wavelets
	to improve the representation of trace t.Although special algorithms
	have been developed to solve l 1 problems, all of the calculations
	can be performed using a general linear programming system. Proper
	weighting procedures allow these methods to be used on ungained data.},
  timestamp = {2016-07-08T12:10:12Z},
  number = {1},
  journal = {Geophysics},
  author = {Taylor, H. L. and Banks, S. C. and McCoy, J. F.},
  year = {1979},
  pages = {39-52},
  owner = {afdidehf}
}

@incollection{Taylor2001,
  series = {Lecture Notes in Computer Science},
  title = {Images of the Mind: Brain Images and Neural Networks},
  volume = {2036},
  isbn = {978-3-540-42363-8},
  language = {English},
  timestamp = {2016-07-08T12:47:05Z},
  booktitle = {Emergent Neural Computational Architectures Based on Neuroscience},
  publisher = {{\{Springer Berlin Heidelberg\}}},
  author = {Taylor, JohnG.},
  editor = {Wermter, Stefan and Austin, Jim and Willshaw, David},
  year = {2001},
  keywords = {Brain imaging,intelligent systems,internal models,neural networks,structural
	models,working memory},
  pages = {20-38},
  owner = {Fardin}
}

@inproceedings{Tehrani2013,
  title = {Optimal deterministic compressed sensing matrices},
  doi = {10.1109/ICASSP.2013.6638795},
  abstract = {We present the first deterministic measurement matrix construction
	with an order-optimal number of rows for sparse signal reconstruction.
	This improves the measurements required in prior constructions and
	addresses a known open problem in the theory of sparse signal recovery.
	Our construction uses adjacency matrices of bipartite graphs that
	have large girth. The main result is that girth (the length of the
	shortest cycle in the graph) can be used as a certificate that a
	measurement matrix can recover almost all sparse signals. Specifically,
	our matrices guarantee recovery �for-each� sparse signal under
	basis pursuit. Our techniques are coding theoretic and rely on a
	recent connection of compressed sensing to LP relaxations for channel
	decoding.},
  timestamp = {2016-07-10T07:19:37Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Tehrani, A.S. and Dimakis, A.G. and Caire, G.},
  month = may,
  year = {2013},
  keywords = {adjacency matrices,bipartite graphs,channel coding,channel decoding,compressed
	sensing,Decoding,deterministic measurement matrix construction,Graph
	theory,LP relaxations,matrix algebra,Minimization,optimal deterministic
	compressed sensing matrices,order-optimal number,parity check codes,signal
	reconstruction,sparse matrices,sparse signal reconstruction,sparse
	signal recovery,Vectors},
  pages = {5895-5899},
  owner = {Fardin}
}

@mastersthesis{Tellen2013,
  title = {Sparse Reconstruction and Realistic Head Modeling in EEG/MEG},
  abstract = {The brain is one of the most interesting organs of the human body.
	Especially the understanding of the human brain functions has aroused
	researchers� interest even from various science fields. The magneto-
	and the electroencephalography are imaging techniques that record
	external electromagnetic fields resulting from neural activity. A
	widely discussed problem in the mathematical area is the ill-posed
	inverse problem of reconstructing the neuronal sources that produce
	these measurable fields. One topic in this investigation is the modeling
	of the human brain as a conductor. A mass of headmodels with varying
	number of compartments, different structures and conductivities have
	been investigated from several points of view. In this thesis we
	examine six headmodels with different complexity with regard to their
	interaction between concepts of the compressed sensing (CS) and the
	regularization theory in order to establish significant differences.
	We show that the conditions or properties provided by the CStheory
	are only satisfied under strict constraints on the sparsity level.
	On the other hand, the (Strong) Source Condition, a concept of the
	regularization theory, leads to more significant trends towards more
	complex models. This is expressed in terms of a higher percentage
	of randomly generated samples satisfying a Source Condition in case
	of more realistic conductor models.},
  timestamp = {2016-10-24T16:34:34Z},
  school = {Westfälische Wilhelms-Universität Münster},
  author = {Tellen, Sina},
  month = sep,
  year = {2013},
  owner = {afdidehf}
}

@article{Temlyakov2000,
  title = {Weak greedy algorithms},
  volume = {12},
  issn = {1019-7168},
  doi = {10.1023/A:1018917218956},
  language = {English},
  timestamp = {2016-09-29T16:05:13Z},
  number = {2-3},
  journal = {Advances in Computational Mathematics},
  author = {Temlyakov, V.N.},
  year = {2000},
  pages = {213--227},
  owner = {Fardin}
}

@article{Temlyakov1998,
  title = {The best m-term approximation and greedy algorithms},
  volume = {8},
  issn = {1019-7168},
  doi = {10.1023/A:1018900431309},
  abstract = {Two theorems on nonlinear m?term approximation in Lp,1<p<?, are proved
	in this paper. The first one (theorem 2.1) says that if a basis ?:={?I}I
	is Lp?equivalent to the Haar basis then a near best m>?term approximation
	to any f?Lp can be realized by the following simple greedy type algorithm.
	Take the expansion f=?IcI?I and form a sum of m terms with the largest
	?cI?I?p out of this expansion. The second one (theorem 3.3) states
	that nonlinear m?term approximations with regard to two dictionaries:
	the Haar basis and the set of all characteristic functions of intervals
	are equivalent in a very strong sense.},
  language = {English},
  timestamp = {2016-09-29T16:04:11Z},
  number = {3},
  journal = {Advances in Computational Mathematics},
  author = {Temlyakov, V.N.},
  year = {1998},
  pages = {249--265},
  owner = {Fardin}
}

@article{Temlyakov2011,
  title = {On performance of greedy algorithms},
  volume = {163},
  issn = {0021-9045},
  doi = {http://dx.doi.org/10.1016/j.jat.2011.03.009},
  abstract = {We show that the Orthogonal Greedy Algorithm (OGA) for dictionaries
	in a Hilbert space with small coherence M performs almost as well
	as the best m -term approximation for all signals with sparsity close
	to the best theoretically possible threshold m = 1 2 ( M − 1 +
	1 ) by proving a Lebesgue-type inequality for arbitrary signals.
	Additionally, we present a dictionary with coherence M and a 1 2
	( M − 1 + 1 ) -sparse signal for which \{OGA\} fails to pick up
	any atoms from the support, showing that the above threshold is sharp.
	We also show that the Pure Greedy Algorithm (PGA) matches the rate
	of convergence of the best m -term approximation beyond the saturation
	limit of m − 1 2 .},
  timestamp = {2016-07-10T07:13:13Z},
  number = {9},
  journal = {Journal of Approximation Theory},
  author = {Temlyakov, Vladimir N. and Zheltov, Pavel},
  year = {2011},
  keywords = {Additive-type Lebesgue inequality,Coherence,greedy algorithm,incoherent
	dictionary,m-term approximation,orthogonal greedy algorithm,orthogonal
	matching pursuit,sparse representation},
  pages = {1134 - 1145},
  owner = {Fardin}
}

@inproceedings{Thai2015,
  title = {A multi-convex approach to latency inference and control in traffic 	equilibria from sparse data},
  doi = {10.1109/ACC.2015.7170815},
  abstract = {A common behavioral assumption in the modeling of traffic networks
	is the user equilibrium. Since traffic volumes, resulting from the
	rational behavior of agents, are easily but sparsely observable,
	and delay functions are not directly observable, we present a mathematical
	program with equilibrium constraint (MPEC) framework to impute the
	delay functions and centrally control the system from partial observations
	of equilibria. We also develop a novel method for solving MPECs using
	multi-convex optimization. Our block descent method has an intuitive
	interpretation, and numerical experiments demonstrate its accuracy
	for structural estimation, and highlight the importance of sensor
	placement for toll pricing.},
  timestamp = {2016-07-08T10:28:20Z},
  booktitle = {American Control Conference (ACC), 2015},
  author = {Thai, J. and Hariss, R. and Bayen, A.},
  month = jul,
  year = {2015},
  keywords = {Aggregates,Biological system modeling,centralised control,Convex functions,convex
	programming,delay functions,delays,Estimation,latency inference,mathematical
	program with equilibrium constraint framework,MPEC framework,multiconvex
	optimization approach,Optimization,rational agent behavior,sensor
	placement,sparse data,Standards,structural estimation,toll pricing,traffic
	control,traffic equilibria,traffic network modeling},
  pages = {689-695},
  owner = {afdidehf}
}

@article{Tian2012,
  title = {A Two-way Regularization Method For MEG Source Reconstruction},
  abstract = {The MEG inverse problem refers to the reconstruction of the neural
	activity of the brain from magnetoencephalography (MEG) measurements.
	We propose a two-way regularization (TWR) method to solve the MEG
	inverse problem under the assumptions that only a small number of
	locations in space are responsible for the measured signals (focality),
	and each source time course is smooth in time (smoothness). The focality
	and smoothness of the reconstructed signals are ensured respectively
	by imposing a sparsity-inducing penalty and a roughness penalty in
	the data fitting criterion. A two-stage algorithm is developed for
	fast computation, where a raw estimate of the source time course
	is obtained in the first stage and then refined in the second stage
	by the two-way regularization. The proposed method is shown to be
	effective on both synthetic and real-world examples.},
  timestamp = {2016-07-08T11:34:49Z},
  journal = {the Annals of Applied Statistics},
  author = {Tian, Tian Siva and Huang, Jianhua Z. and Shen, Haipeng and Li, Zhimin},
  year = {2012},
  keywords = {inverse problem,MEG,Spatiotemporal,Two-way regularization},
  owner = {afdidehf}
}

@article{Tibshirani2011,
  title = {Regression shrinkage and selection via the lasso: a retrospective},
  volume = {73},
  timestamp = {2016-07-10T07:50:36Z},
  number = {3},
  journal = {Journal of The Royal Statistical Society},
  author = {Tibshirani, Robert},
  year = {2011},
  pages = {273�282},
  owner = {Fardin}
}

@article{Tibshirani1994,
  title = {Regression Shrinkage and Selection Via the Lasso},
  volume = {58},
  abstract = {We propose a new method for estimation in linear models. The "lasso"
	minimizes the residual sum of squares subject to the sum of the absolute
	value of the coefficients being less than a constant. Because of
	the nature of this constraint it tends to produce some coefficients
	that are exactly zero and hence gives interpretable models. Our simulation
	studies suggest that the lasso enjoys some of the favourable properties
	of both subset selection and ridge regression. It produces interpretable
	models like subset selection and exhibits the stability of ridge
	regression. There is also an interesting relationship with recent
	work in adaptive function estimation by Donoho and Johnstone. The
	lasso idea is quite general and can be applied in a variety of statistical
	models: extensions to generalized regression models and tree-based
	models are briefly described},
  timestamp = {2016-09-30T10:50:42Z},
  journal = {Journal of the Royal Statistical Society, Series B},
  author = {Tibshirani, Robert},
  year = {1994},
  keywords = {quadratic programming,regression,shrinkage,subset selection},
  pages = {267--288},
  owner = {Fardin}
}

@article{Tillmann2014,
  title = {The Computational Complexity of the Restricted Isometry Property, 	the Nullspace Property, and Related Concepts in Compressed Sensing},
  volume = {60},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2290112},
  abstract = {This paper deals with the computational complexity of conditions which
	guarantee that the NP-hard problem of finding the sparsest solution
	to an underdetermined linear system can be solved by efficient algorithms.
	In the literature, several such conditions have been introduced.
	The most well-known ones are the mutual coherence, the restricted
	isometry property (RIP), and the nullspace property (NSP). While
	evaluating the mutual coherence of a given matrix is easy, it has
	been suspected for some time that evaluating RIP and NSP is computationally
	intractable in general. We confirm these conjectures by showing that
	for a given matrix A and positive integer k, computing the best constants
	for which the RIP or NSP hold is, in general, NP-hard. These results
	are based on the fact that determining the spark of a matrix is NP-hard,
	which is also established in this paper. Furthermore, we also give
	several complexity statements about problems related to the above
	concepts.},
  timestamp = {2016-09-30T13:53:37Z},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Tillmann, A.M. and Pfetsch, M.E.},
  month = feb,
  year = {2014},
  keywords = {compressed sensing,computational complexity,information theory,Information
	theory,mutual coherence,mutual
	coherence,NP hard problem,nullspace property,Polynomials,restricted
	isometry property,Sparks,sparse matrices,sparse recovery conditions,underdetermined
	linear system,Vectors},
  pages = {1248--1259},
  owner = {afdidehf}
}

@article{Tillmann2013,
  title = {The Computational Complexity of Spark, RIP, and NSP},
  timestamp = {2016-07-11T17:00:17Z},
  journal = {Signal Processing with Adaptive Sparse Structured Representations 	(SPARS'13 )},
  author = {Tillmann, Andreas M. and Pfetsch, Marc E.},
  year = {2013},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {afdidehf}
}

@inproceedings{Torturela2015,
  title = {A novel sparse system estimation method based on least squares, $l_1$-norm 	minimization and shrinkage},
  doi = {10.1109/ICCNC.2015.7069465},
  abstract = {In this article, a novel low-complexity block-processing sparse system
	estimation method, based on least squares (LS), ?1-norm minimization
	and support shrinkage, is proposed. The proposed method can be seen
	as a counterpart for the Least Absolute Shrinkage and Selection Operator
	(LASSO), in the sense that the proposed method aims to find the vector
	that minimizes its ?1-norm subject to a maximum arbitrary value Jmax
	for the LS cost function. Thus, it is suitable to be used when there
	is no a priori knowledge of the maximum ?1-norm value of the system
	impulse response. In addition, making Jmax directly proportional
	to the minimum LS cost function grants the proposed method low sensitivity
	to wide ranges of signal to noise-plus-interference ratio. Simulation
	results show that the proposed method has better convergence performance
	than the ordinary Full-support Least Squares (LS), the Recursive
	Least Squares with ?1-norm regularization (?1-RLS), the Relaxations
	and the Basis Pursuit Denoising (BPDN) estimation methods.},
  timestamp = {2016-07-08T11:25:46Z},
  booktitle = {Computing, Networking and Communications (ICNC), 2015 International 	Conference on},
  author = {Torturela, A.D.M. and de Lamare, R.C. and Medina, C.A. and Sampaio-Neto, R.},
  month = feb,
  year = {2015},
  keywords = {?1-norm minimization,Basis pursuit denoising,block-processing,BPDN
	estimation method,compressed sensing,compressive sensing,Convergence,Convex
	functions,convex optimization,Cost function,Estimation,Lasso,least
	absolute shrinkage and selection operator,least mean squares methods,Least
	Squares,least squares approximations,low complexity block processing
	sparse system estimation method,LS cost function,minimisation,recursive
	estimation,recursive least square,relaxations estimation method,shrinkage,signal
	denoising,signal to noise-plus-interference ratio,Simulation,sparse
	channel estimation,Sparse system identification,system impulse response,transient
	response,ultra-wideband,Vectors},
  pages = {895-899},
  owner = {afdidehf}
}

@inproceedings{Tosic2009,
  title = {Conditions for recovery of sparse signals correlated by local transforms},
  doi = {10.1109/ISIT.2009.5205669},
  abstract = {This paper addresses the problem of correct recovery of multiple sparse
	correlated signals using distributed thresholding. We consider the
	scenario where multiple sensors capture the same event, but observe
	different signals that are correlated by local transforms of their
	sparse components. In this context, the signals do not necessarily
	have the same sparse support, but instead the support of one signal
	is built on local transforms of the atoms in the sparse support of
	another signal. We establish the sufficient condition for the correct
	recovery of such correlated signals using independent thresholding
	of the multiple signals. The condition is relevant in scenarios where
	low complexity processing such as thresholding is needed, for example
	in sensor networks. The validity of the derived recovery condition
	is confirmed by experimental results in noiseless and noisy scenarios.},
  timestamp = {2016-07-08T12:02:30Z},
  booktitle = {Information Theory, 2009. ISIT 2009. IEEE International Symposium 	on},
  author = {Tosic, I. and Frossard, P.},
  month = jun,
  year = {2009},
  keywords = {Approximation algorithms,Dictionaries,distributed thresholding,Laboratories,local
	transforms,Matching pursuit algorithms,multiple sensor,multiple sparse
	correlated signal,sensor fusion,Signal analysis,Signal design,signal
	processing,Signal processing algorithms,signal representations,sparse
	approximations,sparse signal recovery,Sufficient conditions,thresholding,transforms},
  pages = {684-688},
  owner = {Fardin}
}

@article{Townsend2008,
  title = {Dual-Modality Imaging: Combining Anatomy and Function},
  volume = {49},
  abstract = {The extensive development of image fusion techniques over the past
	20 y has shown that the fusion of images from complementary modalities
	offers a more complete and accurate assessment of disease than do
	images from a single modality. Although software techniques have
	been successful in fusing images of the brain from different modalities,
	they have achieved rather limited success for other parts of the
	body. The recent introduction of technology that can acquire both
	anatomic and functional images in a single scan has addressed many
	of the limitations of software fusion. The combination of CT and
	PET was introduced commercially in 2001, followed by CT and SPECT
	in 2004. Clinical adoption of PET/CT has been surprisingly rapid,
	and despite continuing debate, the new technology has advanced the
	use of clinical molecular imaging, particularly for oncology.},
  timestamp = {2016-07-08T12:16:06Z},
  number = {6},
  journal = {The Journal Of Nuclear Medicine},
  author = {Townsend, David W.},
  month = jun,
  year = {2008},
  keywords = {dual-modality,image fusion,molecular imaging,PET/CT},
  pages = {938 - 955},
  owner = {afdidehf}
}

@inproceedings{Trinh2013,
  title = {Compressive Sensing recovery with improved hybrid filter},
  volume = {01},
  doi = {10.1109/CISP.2013.6743983},
  abstract = {Compressive Sensing (CS) is a novel sampling framework which is more
	efficient than the Nyquist sampling for sparse signals. A major challenge
	in CS is its quality improvement of recovered signal when noise exists.
	To reduce noise in the recovered images, filters are usually employed.
	This paper focuses on improving the quality of CS recoveries by applying
	a hybrid filter which pursues smoothness and preserves edge at the
	same time. Considering desirability of the block-based recovery in
	practical usages, the proposed hybrid filter is investigated not
	only for the frame-based recovery but also for the block-based recovery.
	Experimental results demonstrate that the proposed hybrid filter
	attains much better performance in CS recovery than the conventional
	ones in term of both subjective and objective qualities.},
  timestamp = {2016-07-08T12:01:07Z},
  booktitle = {Image and Signal Processing (CISP), 2013 6th International Congress 	on},
  author = {Trinh, Chien Van and Dinh, Khanh Quoc and Nguyen, Viet Anh and Jeon, Byeungwoo and Sim, Donggyu},
  month = dec,
  year = {2013},
  keywords = {Augmented Lagrangian Method,block-based recovery,compressed sensing,Compressive
	sensing,compressive sensing recovery,Filtering algorithms,Filters,frame-based
	recovery,hybrid filter,Image edge detection,Image Processing,Image
	reconstruction,image recovery,Information filters,Nyquist sampling,signal
	recovery,Smooth Projected Landweber,sparse signals,total variation,Wiener
	filters},
  pages = {186-191},
  owner = {afdidehf}
}

@article{Tropp2006,
  title = {Just relax: convex programming methods for identifying sparse signals 	in noise},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.864420},
  abstract = {This paper studies a difficult and fundamental problem that arises
	throughout electrical engineering, applied mathematics, and statistics.
	Suppose that one forms a short linear combination of elementary signals
	drawn from a large, fixed collection. Given an observation of the
	linear combination that has been contaminated with additive noise,
	the goal is to identify which elementary signals participated and
	to approximate their coefficients. Although many algorithms have
	been proposed, there is little theory which guarantees that these
	algorithms can accurately and efficiently solve the problem. This
	paper studies a method called convex relaxation, which attempts to
	recover the ideal sparse signal by solving a convex program. This
	approach is powerful because the optimization can be completed in
	polynomial time with standard scientific software. The paper provides
	general conditions which ensure that convex relaxation succeeds.
	As evidence of the broad impact of these results, the paper describes
	how convex relaxation can be used for several concrete signal recovery
	problems. It also describes applications to channel coding, linear
	regression, and numerical analysis},
  timestamp = {2016-09-29T16:43:14Z},
  number = {3},
  journal = {Information Theory, IEEE Transactions on},
  author = {Tropp, J.A.},
  month = mar,
  year = {2006},
  keywords = {Additive noise,Algorithms,Application software,Approximation methods,Basis
	Pursuit,channel coding,Concrete,convex program,convex programming,convex
	programming method,Electrical engineering,iterative methods,linear
	codes,Linear regression,Mathematics,numerical analysis,Optimization
	methods,orthogonal matching pursuit,Polynomials,polynomial time,regression
	analysis,short linear signal combination,signal denoising,signal
	detection,signal processing,signal representation,Software standards,sparse
	representations,sparse signal identification,standard scientific
	software,Statistics,time-frequency analysis},
  pages = {1030--1051},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@article{Tropp2004,
  title = {Greed is good: algorithmic results for sparse approximation},
  volume = {50},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.834793},
  abstract = {This article presents new results on using a greedy algorithm, orthogonal
	matching pursuit (OMP), to solve the sparse approximation problem
	over redundant dictionaries. It provides a sufficient condition under
	which both OMP and Donoho's basis pursuit (BP) paradigm can recover
	the optimal representation of an exactly sparse signal. It leverages
	this theory to show that both OMP and BP succeed for every sparse
	input signal from a wide class of dictionaries. These quasi-incoherent
	dictionaries offer a natural generalization of incoherent dictionaries,
	and the cumulative coherence function is introduced to quantify the
	level of incoherence. This analysis unifies all the recent results
	on BP and extends them to OMP. Furthermore, the paper develops a
	sufficient condition under which OMP can identify atoms from an optimal
	approximation of a nonsparse signal. From there, it argues that OMP
	is an approximation algorithm for the sparse problem over a quasi-incoherent
	dictionary. That is, for every input signal, OMP calculates a sparse
	approximant whose error is only a small factor worse than the minimal
	error that can be attained with the same number of terms.},
  timestamp = {2017-04-19T14:26:20Z},
  number = {10},
  journal = {IEEE Trans. Inf. Theory},
  author = {Tropp, J.A.},
  month = oct,
  year = {2004},
  keywords = {Algorithms,algorithm theory,Approximation algorithms,Approximation methods,Approximation
	methods,approximation theory,atoms identification,Basis
	Pursuit,BP,BP paradigm,cumulative coherence function,Dictionaries,Donoho's
	basis pursuit,greedy algorithm,Greedy algorithms,Iterative algorithms,iterative
	method,iterative methods,linear programming,Matching pursuit algorithms,nonsparse
	signal,OMP,optimal approximation,orthogonal matching pursuit,quasiincoherent
	dictionary,redundant dictionary,redundant number systems,signal processing,sparse
	approximation problem,sparse matrices,Sufficient conditions},
  pages = {2231--2242},
  owner = {Fardin}
}

@article{Tropp2007,
  title = {Sparse Representations},
  timestamp = {2016-07-11T16:52:20Z},
  author = {Tropp, Joel A.},
  month = apr,
  year = {2007},
  howpublished = {Department of Mathematics, The University of Michigan},
  owner = {afdidehf}
}

@article{Tropp2005a,
  title = {Designing structured tight frames via an alternating projection method},
  volume = {51},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.839492},
  abstract = {Tight frames, also known as general Welch-bound- equality sequences,
	generalize orthonormal systems. Numerous applications - including
	communications, coding, and sparse approximation- require finite-dimensional
	tight frames that possess additional structural properties. This
	paper proposes an alternating projection method that is versatile
	enough to solve a huge class of inverse eigenvalue problems (IEPs),
	which includes the frame design problem. To apply this method, one
	needs only to solve a matrix nearness problem that arises naturally
	from the design specifications. Therefore, it is the fast and easy
	to develop versions of the algorithm that target new design problems.
	Alternating projection will often succeed even if algebraic constructions
	are unavailable. To demonstrate that alternating projection is an
	effective tool for frame design, the paper studies some important
	structural properties in detail. First, it addresses the most basic
	design problem: constructing tight frames with prescribed vector
	norms. Then, it discusses equiangular tight frames, which are natural
	dictionaries for sparse approximation. Finally, it examines tight
	frames whose individual vectors have low peak-to-average-power ratio
	(PAR), which is a valuable property for code-division multiple-access
	(CDMA) applications. Numerical experiments show that the proposed
	algorithm succeeds in each of these three cases. The appendices investigate
	the convergence properties of the algorithm.},
  timestamp = {2016-07-08T12:11:04Z},
  number = {1},
  journal = {Information Theory, IEEE Transactions on},
  author = {Tropp, J.A. and Dhillon, I.S. and Heath, R.W. and Strohmer, T.},
  month = jan,
  year = {2005},
  keywords = {algebraic construction,Algorithm design and analysis,Algorithms,alternating
	projection method,CDMA,code division multiple access,code-division
	multiple-access,code-division multiple access (CDMA),Convergence,convergence
	property,Dictionaries,Eigenvalues and eigenfunctions,Eigenvalues
	and eigenfunctions,extremal problems,frames,generalize orthonormal
	system,general Welch-bound-equality sequences,Geometry,inverse eigenvalue
	problem,inverse problem,inverse problems,lEP,Mathematics,matrix nearness
	problem,matrix
	nearness problem,Multiaccess communication,Peak to average power ratio,Peak to average power
	ratio,sequences,sparse approximation,sparse
	approximation,sparse matrices,spread spectrum communication,spread spectrum
	communication,structured tight frames,structured
	tight frames,vector norms,Vectors},
  pages = {188-209},
  owner = {Fardin}
}

@article{Tropp2007a,
  title = {Signal Recovery From Random Measurements Via Orthogonal Matching 	Pursuit},
  volume = {53},
  issn = {0018-9448},
  doi = {10.1109/TIT.2007.909108},
  abstract = {This paper demonstrates theoretically and empirically that a greedy
	algorithm called orthogonal matching pursuit (OMP) can reliably recover
	a signal with m nonzero entries in dimension d given O(m ln d) random
	linear measurements of that signal. This is a massive improvement
	over previous results, which require O(m2) measurements. The new
	results for OMP are comparable with recent results for another approach
	called basis pursuit (BP). In some settings, the OMP algorithm is
	faster and easier to implement, so it is an attractive alternative
	to BP for signal recovery problems.},
  timestamp = {2016-07-10T08:17:53Z},
  number = {12},
  journal = {Information Theory, IEEE Transactions on},
  author = {Tropp, J.A. and Gilbert, A.C.},
  month = dec,
  year = {2007},
  keywords = {Algorithms,Approximation,Basis pursuit,Blood,compressed sensing,compressed
	sensing,greedy algorithm,greedy
	algorithm,Greedy algorithms,group testing,group
	testing,iterative methods,Matching pursuit algorithms,Matching
	pursuit algorithms,Mathematics,orthogonal matching pursuit,orthogonal
	matching pursuit,performance evaluation,performance
	evaluation,random linear measurements,Reliability theory,Reliability
	theory,signal processing,signal recovery,signal
	recovery,sparse approximation,Testing,time-frequency analysis,time-frequency
	analysis,Vectors},
  pages = {4655-4666},
  owner = {afdidehf}
}

@inproceedings{Tropp2003,
  title = {Improved sparse approximation over quasiincoherent dictionaries},
  volume = {1},
  doi = {10.1109/ICIP.2003.1246892},
  abstract = {This paper discusses a new greedy algorithm for solving the sparse
	approximation problem over quasiincoherent dictionaries. These dictionaries
	consist of waveforms that are uncorrelated "on average," and they
	provide a natural generalization of incoherent dictionaries. The
	algorithm provides strong guarantees on the quality of the approximations
	it produces, unlike most other methods for sparse approximation.
	Moreover, very efficient implementations are possible via approximate
	nearest-neighbor data structures.},
  timestamp = {2016-07-08T12:48:40Z},
  booktitle = {Image Processing, 2003. ICIP 2003. Proceedings. 2003 International 	Conference on},
  author = {Tropp, J.A. and Gilbert, A.C. and Muthukrishnan, S. and Strauss, M.J.},
  month = sep,
  year = {2003},
  keywords = {Algorithm design and analysis,algorithm theory,Approximation algorithms,Costs,data
	structures,Dictionaries,greedy algorithm,Image Processing,iterative
	methods,matching pursuit,Matching pursuit algorithms,nearest-neighbor
	data structures,quasiincoherent dictionary,signal resolution,sparse
	approximation,sparse matrices,wavelet packets},
  pages = {I-37-40 vol.1},
  owner = {Fardin}
}

@article{Tropp2010,
  title = {Computational Methods for Sparse Solution of Linear Inverse Problems},
  volume = {98},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2010.2044010},
  abstract = {The goal of the sparse approximation problem is to approximate a target
	signal using a linear combination of a few elementary signals drawn
	from a fixed collection. This paper surveys the major practical algorithms
	for sparse approximation. Specific attention is paid to computational
	issues, to the circumstances in which individual methods tend to
	perform well, and to the theoretical guarantees available. Many fundamental
	questions in electrical engineering, statistics, and applied mathematics
	can be posed as sparse approximation problems, making these algorithms
	versatile and relevant to a plethora of applications.},
  timestamp = {2016-07-08T12:01:25Z},
  number = {6},
  journal = {Proceedings of the IEEE},
  author = {Tropp, J.A. and Wright, S.J.},
  month = jun,
  year = {2010},
  keywords = {Approximation algorithms,approximation theory,compressed sensing,computational
	methods,convex optimization,Dictionaries,Electrical engineering,Inverse
	problems,Least squares approximation,Linear inverse problems,matching
	pursuit,Matching pursuit algorithms,Mathematics,Signal Approximation,signal
	processing,Signal processing algorithms,sparse approximation,sparse
	approximation problem,Statistics},
  pages = {948-958},
  owner = {Fardin}
}

@article{Trujillo-Barreto2001,
  title = {Bayesian model for EEG/MEG and fMRI data fusion},
  volume = {13},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/S1053-8119(01)91613-1},
  timestamp = {2016-07-08T11:37:19Z},
  number = {6, Supplement},
  journal = {NeuroImage},
  author = {Trujillo-Barreto, N. J. and Martinez-Montes, E. and Vald�s-Sosa, P. A. and Melie-Garcia, L.},
  year = {2001},
  note = {Originally published as Volume 13, Number 6, Part 2},
  pages = {270 -},
  owner = {afdidehf}
}

@article{Tsaig2006,
  title = {Extensions of compressed sensing},
  volume = {86},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2005.05.029},
  abstract = {We study the notion of compressed sensing (CS) as put forward by Donoho,
	Candes, Tao and others. The notion proposes a signal or image, unknown
	but supposed to be compressible by a known transform, (e.g. wavelet
	or Fourier), can be subjected to fewer measurements than the nominal
	number of data points, and yet be accurately reconstructed. The samples
	are nonadaptive and measure ‘random’ linear combinations of the
	transform coefficients. Approximate reconstruction is obtained by
	solving for the transform coefficients consistent with measured data
	and having the smallest possible l 1 norm. We present initial ‘proof-of-concept’
	examples in the favorable case where the vast majority of the transform
	coefficients are zero. We continue with a series of numerical experiments,
	for the setting of l p -sparsity, in which the object has all coefficients
	nonzero, but the coefficients obey an l p bound, for some p ∈ (
	0 , 1 ] . The reconstruction errors obey the inequalities paralleling
	the theory, seemingly with well-behaved constants. We report that
	several workable families of ‘random’ linear combinations all
	behave equivalently, including random spherical, random signs, partial
	Fourier and partial Hadamard. We next consider how these ideas can
	be used to model problems in spectroscopy and image processing, and
	in synthetic examples see that the reconstructions from \{CS\} are
	often visually “noisy�?. To suppress this noise we post-process
	using translation-invariant denoising, and find the visual appearance
	considerably improved. We also consider a multiscale deployment of
	compressed sensing, in which various scales are segregated and \{CS\}
	applied separately to each; this gives much better quality reconstructions
	than a literal deployment of the \{CS\} methodology. These results
	show that, when appropriately deployed in a favorable setting, the
	\{CS\} framework is able to save significantly over traditional sampling,
	and there are many useful extensions of the basic idea.},
  timestamp = {2016-07-08T12:27:39Z},
  number = {3},
  journal = {Signal Processing},
  author = {Tsaig, Yaakov and Donoho, David L.},
  year = {2006},
  note = {Sparse Approximations in Signal and Image ProcessingSparse Approximations
	in Signal and Image Processing},
  keywords = {Basis,Pursuit},
  pages = {549 - 571},
  owner = {afdidehf}
}

@article{Tseng2009,
  title = {Further Results on Stable Recovery of Sparse Overcomplete Representations 	in the Presence of Noise},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2008.2009812},
  abstract = {Sparse over complete representations have attracted much interest
	recently for their applications to signal processing. In a recent
	work, Donoho, Elad, and Temlyakov (2006) showed that, assuming sufficient
	sparsity of the ideal underlying signal and approximate orthogonality
	of the over complete dictionary, the sparsest representation can
	be found, at least approximately if not exactly, by either an orthogonal
	greedy algorithm or by lscr1-norm minimization subject to a noise
	tolerance constraint. In this paper, we sharpen the approximation
	bounds under more relaxed conditions. We also derive analogous results
	for a stepwise projection algorithm.},
  timestamp = {2016-07-08T12:35:08Z},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Tseng, P.},
  month = feb,
  year = {2009},
  note = {read},
  keywords = {$ell _{1}$-norm minimization,approximation theory,Basis pursuit,Dictionaries,greedy
	algorithm,Greedy algorithms,l1-norm minimization,Least squares approximation,Least
	squares approximation,Least squares methods,Least
	squares methods,matching pursuit,Matching pursuit algorithms,Matching
	pursuit algorithms,Mathematics,minimisation,Minimization methods,Minimization
	methods,mutual coherence,mutual
	coherence,noise presence,noise tolerance constraint,orthogonal
	greedy algorithm,orthogonal greedy
	algorithm,orthogonality approximation,overcomplete representation,Projection
	algorithms,signal processing,Signal processing algorithms,Signal processing
	algorithms,signal representation,signal
	representation,sparse matrices,sparse overcomplete representation,sparse overcomplete
	representation,sparse representation,sparse
	representation,stable recovery,stepwise projection algorithm,stepwise projection
	algorithm},
  pages = {888-899},
  owner = {Fardin}
}

@techreport{TU-Ilmenau2007,
  title = {Solving Optimization Problems using the Matlab Optimization Toolbox 	- a Tutorial},
  timestamp = {2016-07-10T08:20:08Z},
  author = {{{TU-Ilmenau}} and Mathematik, Fakult�t f�r and {{Naturwissenschaften}} and Geletu, Abebe},
  month = dec,
  year = {2007},
  owner = {afdidehf}
}

@inproceedings{Tzimiropoulos2011,
  title = {Sparse representations of image gradient orientations for visual 	recognition and tracking},
  doi = {10.1109/CVPRW.2011.5981809},
  abstract = {Recent results have shown that sparse linear representations of a
	query object with respect to an overcomplete basis formed by the
	entire gallery of objects of interest can result in powerful image-based
	object recognition schemes. In this paper, we propose a framework
	for visual recognition and tracking based on sparse representations
	of image gradient orientations. We show that minimal ?1 solutions
	to problems formulated with gradient orientations can be used for
	fast and robust object recognition even for probe objects corrupted
	by outliers. These solutions are obtained without the need for solving
	the extended problem considered in. We further show that low-dimensional
	embeddings generated from gradient orientations perform equally well
	even when probe objects are corrupted by outliers, which, in turn,
	results in huge computational savings. We demonstrate experimentally
	that, compared to the baseline method in, our formulation results
	in better recognition rates without the need for block processing
	and even with smaller number of training samples. Finally, based
	on our results, we also propose a robust and efficient ?1-based �tracking
	by detection� algorithm. We show experimentally that our tracker
	outperforms a recently proposed ?1-based tracking algorithm in terms
	of robustness, accuracy and speed.},
  timestamp = {2016-07-11T16:53:00Z},
  booktitle = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE 	Computer Society Conference on},
  author = {Tzimiropoulos, G. and Zafeiriou, S. and Pantic, M.},
  month = jun,
  year = {2011},
  keywords = {Dictionaries,gradient methods,image based object recognition,image
	gradient orientations,image representation,object gallery,object
	queries,Object Recognition,Probes,Robustness,sparse linear representations,sparse
	linear representations,Testing,Training,Visualization,visual recognition,visual
	recognition,visual tracking,Visual
	tracking},
  pages = {26-33},
  owner = {afdidehf}
}

@article{Uludaug2014,
  title = {General overview on the merits of multimodal neuroimaging data fusion},
  volume = {102, Part 1},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.05.018},
  abstract = {Abstract Multimodal neuroimaging has become a mainstay of basic and
	cognitive neuroscience in humans and animals, despite challenges
	to consider when acquiring and combining non-redundant imaging data.
	Multimodal data integration can yield important insights into brain
	processes and structures in addition to spatiotemporal resolution
	complementarity, including: a comprehensive physiological view on
	brain processes and structures, quantification, generalization and
	normalization, and availability of biomarkers. In this review, we
	discuss data acquisition and fusion in multimodal neuroimaging in
	the context of each of these potential merits. However, limitations
	– due to differences in the neuronal and structural underpinnings
	of each method – have to be taken into account when modeling and
	interpreting multimodal data using generative models. We conclude
	that when these challenges are adequately met, multimodal data fusion
	can create substantial added value for neuroscience applications
	making it an indispensable approach for studying the brain.},
  timestamp = {2016-07-08T12:35:30Z},
  number = {0},
  journal = {NeuroImage},
  author = {Uluda\u{g}, K{\^a}mil and Roebroeck, Alard},
  year = {2014},
  note = {Multimodal Data Fusion},
  pages = {3 - 10},
  owner = {Fardin}
}

@article{VanVeen1988,
  title = {Beamforming: a versatile approach to spatial filtering},
  volume = {5},
  issn = {0740-7467},
  doi = {10.1109/53.665},
  abstract = {An overview of beamforming from a signal-processing perspective is
	provided, with an emphasis on recent research. Data-independent,
	statistically optimum, adaptive, and partially adaptive beamforming
	are discussed. Basic notation, terminology, and concepts are included.
	Several beamformer implementations are briefly described.<>},
  timestamp = {2016-07-08T11:37:24Z},
  number = {2},
  journal = {ASSP Magazine, IEEE},
  author = {Van Veen, B.D. and Buckley, K.M.},
  month = apr,
  year = {1988},
  keywords = {adaptive beamforming,Apertures,array signal processing,beamformer
	implementations,data independent beamforming,Feeds,Filtering,filtering
	and prediction theory,Frequency,Interference,Microwave filters,notation,overview,partially
	adaptive beamforming,reviews,Sampling methods,Sensor arrays,signal-processing,signal
	processing,spatial filtering,spatial filters,statistically
	optimum beamforming,statistically optimum
	beamforming,Terminology},
  pages = {4-24},
  owner = {Fardin}
}

@article{Varadarajan2011,
  title = {Stepwise Optimal Subspace Pursuit for Improving Sparse Recovery},
  volume = {18},
  issn = {1070-9908},
  doi = {10.1109/LSP.2010.2090143},
  abstract = {We propose a new iterative algorithm to reconstruct an unknown sparse
	signal x from a set of projected measurements y = ?x . Unlike existing
	methods, which rely crucially on the near orthogonality of the sampling
	matrix ? , our approach makes stepwise optimal updates even when
	the columns of ? are not orthogonal. We invoke a block-wise matrix
	inversion formula to obtain a closed-form expression for the increase
	(reduction) in the L2-norm of the residue obtained by removing (adding)
	a single element from (to) the presumed support of x . We then use
	this expression to design a computationally tractable algorithm to
	search for the nonzero components of x . We show that compared to
	currently popular sparsity seeking matching pursuit algorithms, each
	step of the proposed algorithm is locally optimal with respect to
	the actual objective function. We demonstrate experimentally that
	the algorithm significantly outperforms conventional techniques in
	recovering sparse signals whose nonzero values have exponentially
	decaying magnitudes or are distributed N(0,1) .},
  timestamp = {2016-07-11T16:57:11Z},
  number = {1},
  journal = {Signal Processing Letters, IEEE},
  author = {Varadarajan, B. and Khudanpur, Sanjeev and Tran, T.D.},
  month = jan,
  year = {2011},
  keywords = {Algorithm design and analysis,Approximation algorithms,block-wise
	matrix inversion formula,closed-form expression,Complexity theory,compressed
	sensing,Greedy algorithms,Indexes,iterative algorithm,iterative methods,L2-norm,Least
	Squares,Matching pursuit algorithms,matrix algebra,sampling matrix,signal
	reconstruction,Silicon,sparse matrices,sparse recovery,sparse signal
	reconstruction,sparsity seeking matching pursuit algorithms,stepwise
	optimal subspace pursuit},
  pages = {27-30},
  owner = {afdidehf}
}

@inproceedings{Vehkapera2011,
  title = {Analysis of MMSE estimation for compressive sensing of block sparse 	signals},
  doi = {10.1109/ITW.2011.6089563},
  abstract = {Minimum mean square error (MMSE) estimation of block sparse signals
	from noisy linear measurements is considered. Unlike in the standard
	compressive sensing setup where the non-zero entries of the signal
	are independently and uniformly distributed across the vector of
	interest, the information bearing components appear here in large
	mutually dependent clusters. Using the replica method from statistical
	physics, we derive a simple closed-form solution for the MMSE obtained
	by the optimum estimator. We show that the MMSE is a version of the
	Tse-Hanly formula with system load and MSE scaled by a parameter
	that depends on the sparsity pattern of the source. It turns out
	that this is equal to the MSE obtained by a genie-aided MMSE estimator
	which is informed in advance about the exact locations of the non-zero
	blocks. The asymptotic results obtained by the non-rigorous replica
	method are found to have an excellent agreement with finite sized
	numerical simulations.},
  timestamp = {2016-07-08T10:28:49Z},
  booktitle = {Information Theory Workshop (ITW), 2011 IEEE},
  author = {Vehkapera, M. and Chatterjee, S. and Skoglund, M.},
  month = oct,
  year = {2011},
  keywords = {block sparse signal,closed form solution,compressed sensing,Compressive
	sensing,Conferences,Estimation,least mean squares methods,minimum
	mean square error estimation,MMSE estimation,Noise,Noise measurement,noisy
	linear measurement,nonrigorous replica method,nonzero blocks,optimum
	estimator,Physics,Tse-Hanly formula,Vectors},
  pages = {553-557},
  owner = {afdidehf}
}

@article{Venkataramanan2014,
  title = {Lossy Compression via Sparse Linear Regression: Performance Under 	Minimum-Distance Encoding},
  volume = {60},
  issn = {0018-9448},
  doi = {10.1109/TIT.2014.2313085},
  abstract = {We study a new class of codes for lossy compression with the squared-error
	distortion criterion, designed using the statistical framework of
	high-dimensional linear regression. Codewords are linear combinations
	of subsets of columns of a design matrix. Called a sparse superposition
	or sparse regression codebook, this structure is motivated by an
	analogous construction proposed recently by Barron and Joseph for
	communication over an Additive White Gaussian Noise channel. For
	independent identically distributed (i.i.d) Gaussian sources and
	minimum-distance encoding, we show that such a code can attain the
	Shannon rate-distortion function with the optimal error exponent,
	for all distortions below a specified value. It is also shown that
	sparse regression codes are robust in the following sense: a codebook
	designed to compress an i.i.d Gaussian source of variance ?2 with
	(squared-error) distortion D can compress any ergodic source of variance
	less than ?2 to within distortion D. Thus, the sparse regression
	ensemble retains many of the good covering properties of the i.i.d
	random Gaussian ensemble, while having a compact representation in
	terms of a matrix whose size is a low-order polynomial in the block-length.},
  timestamp = {2016-07-09T19:58:17Z},
  number = {6},
  journal = {Information Theory, IEEE Transactions on},
  author = {Venkataramanan, R. and Joseph, A. and Tatikonda, S.},
  month = jun,
  year = {2014},
  keywords = {additive white Gaussian noise channel,AWGN channels,channel coding,Complexity
	theory,Decoding,error exponent,Gaussian distribution,Gaussian sources,lossy compression,lossy
	compression,minimum distance encoding,optimal error
	exponent,polynomial matrices,Random variables,Rate-distortion,rate-distortion
	function,rate distortion theory,regression analysis,Shannon rate
	distortion function,source coding,sparse linear regression,sparse
	regression,sparse regression codebook,sparse superposition,squared error distortion,squared
	error distortion,Vectors},
  pages = {3254-3264},
  owner = {afdidehf}
}

@book{Vetterli2014,
  title = {Foundations of Signal Processing},
  abstract = {The aim of this book is to provide a set of tools for users of state-of-the-art
	signal processing technology and a solid foundation for those hoping
	to advance the theory and practice of signal processing. Many of
	the results and techniques presented here, while rooted in classic
	Fourier techniques for signal representation, first appeared during
	a flurry of activity in the 1980s and 1990s. New constructions for
	local Fourier transforms and orthonormal wavelet bases during that
	period were motivated both by theoretical interest and by applications,
	in particular in multimedia communications. New bases with specified
	time�frequency behavior were found, with impact well beyond the
	original fields of application. Areas as diverse as computer graphics
	and numerical analysis embraced some of the new constructions�no
	surprise given the pervasive role of Fourier analysis in science
	and engineering. Now that the dust has settled, some of what was
	new and esoteric is now fundamental. Our motivation is to bring these
	new fundamentals to a broader audience to further expand their impact.
	We thus provide an integrated view of classical Fourier analysis
	of signals and systems alongside structured representations with
	time�frequency locality and their myriad of applications. - See
	more at: http://freecomputerbooks.com/Foundations-of-Signal-Processing.html#sthash.4qVop5hF.dpuf},
  timestamp = {2016-07-08T12:33:10Z},
  author = {Vetterli, Martin and Kovacevic, Jelena and Goyal, Vivek K.},
  month = may,
  year = {2014},
  owner = {Fardin}
}

@article{Vidal2011,
  title = {Subspace Clustering},
  volume = {28},
  issn = {1053-5888},
  doi = {10.1109/MSP.2010.939739},
  abstract = {Over the past few decades, significant progress has been made in clustering
	high-dimensional data sets distributed around a collection of linear
	and affine subspaces. This article presented a review of such progress,
	which included a number of existing subspace clustering algorithms
	together with an experimental evaluation on the motion segmentation
	and face clustering problems in computer vision.},
  timestamp = {2016-07-11T16:58:56Z},
  number = {2},
  journal = {Signal Processing Magazine, IEEE},
  author = {Vidal, R.},
  month = mar,
  year = {2011},
  keywords = {affine subspace,Clustering algorithms,computer vision,Data models,face
	clustering problem,face recognition,high-dimensional data set clustering,image
	motion analysis,Image segmentation,linear subspace,motion segmentation,Noise,pattern
	clustering,Polynomials,Principal component analysis,Signal processing
	algorithms,subspace clustering,Subspace constraints},
  pages = {52-68},
  owner = {afdidehf}
}

@article{Vigario2000,
  title = {Independent component approach to the analysis of EEG and MEG recordings},
  volume = {47},
  issn = {0018-9294},
  doi = {10.1109/10.841330},
  abstract = {Multichannel recordings of the electromagnetic fields emerging from
	neural currents in the brain generate large amounts of data. Suitable
	feature extraction methods are, therefore, useful to facilitate the
	representation and interpretation of the data. Recently developed
	independent component analysis (ICA) has been shown to be an efficient
	tool for artifact identification and extraction from electroencephalographic
	(EEG) and magnetoencephalographic (MEG) recordings. In addition,
	ICA has been applied to the analysis of brain signals evoked by sensory
	stimuli. This paper reviews our recent results in this field.},
  timestamp = {2016-07-08T12:49:12Z},
  number = {5},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Vigario, R. and Sarela, J. and Jousmiki, V. and Ha?ma?la?inen, M. and Oja, E.},
  month = may,
  year = {2000},
  keywords = {Algorithms,artifact identification,Artifacts,Auditory,auditory evoked
	potentials,averaged auditory evoked fields,blind source separation,Brain,brain
	signals,Computer-Assisted,Data mining,EEG recordings,electroencephalographic
	recording,electroencephalography,Electromagnetic Fields,Evoked Potentials,Evoked
	Potentials,feature extraction,Humans,Independent component analysis,Independent
	component analysis,independent component approach,independent
	component approach,Magnetic analysis,Magnetic separation,Magnetic
	separation,magnetoencephalographic recording,magnetoencephalographic
	recording,magnetoencephalography,medical signal processing,medical
	signal processing,MEG recordings,multichannel recordings,multichannel
	recordings,neural currents,sensory stimuli,sensory
	stimuli,Signal analysis,signal processing,signal
	processing,Signal processing algorithms,Somatosensory,source separation,source
	separation,statistical analysis,statistical
	analysis},
  pages = {589-593},
  owner = {Fardin}
}

@article{Villmann2010,
  title = {Sparse representation of data},
  abstract = {The amount of electronic data available today as well as its dimensionality
	and complexity increases rapidly in many scientific areas including
	biology, (bio-)chemistry, medicine, physics and its application fields
	like robotics, bioinformatics or multimedia technologies. Many of
	these data sets are very complex but have also a simple inherent
	structure which allows an appropriate sparse representation and modeling
	of such data with less or no information loss. Advanced methods are
	needed to extract these inherent but hidden information. Sparsity
	can be observed at different levels: sparse representation of data
	points using e.g. dimensionality reduction for efficient data storage,
	sparse representation of full data sets using e.g. prototypes to
	achieve compact models for lifelong learning and sparse models of
	the underlying data structure using sparse encoding techniques. One
	main goal is to achieve a human-interpretable representation of the
	essential information. Sparse representations account for the ubiquitous
	problem that humans have to deal with ever increasing and inherently
	unlimited information by means of limited resources such as limited
	time, memory, or perception abilities. Starting with the seminal
	paper of Olshausen&Field [40] researchers recognized that sparsity
	can be used as a fundamental principle to arrive at very efficient
	information processing models for huge and complex data such as observed
	e.g. in the visual cortex. Nowadays, sparse models include diverse
	methods such as relevance learning in prototype based representations,
	sparse coding neural gas, factor analysis methods, latent semantic
	indexing, sparse Bayesian networks, relevance vector machines and
	other. This tutorial paper reviews recent developments in the field.},
  timestamp = {2016-07-11T16:52:05Z},
  journal = {ESANN 2010 proceedings, European Symposium on Artificial Neural Networks 	- Computational Intelligence and Machine Learning},
  author = {Villmann, Thomas and Schleif, Frank-Michael and Hammer, Barbara},
  month = apr,
  year = {2010},
  pages = {225-234},
  annote = {ISBN 2-930307-10-2.},
  annote = {ISBN 2-930307-10-2.},
  annote = {ISBN 2-930307-10-2.},
  annote = {ISBN 2-930307-10-2.},
  annote = {ISBN 2-930307-10-2.},
  owner = {afdidehf}
}

@inproceedings{Vivekanand2014,
  title = {Analysis of RBF cascade network for sparse signal recovery and application 	in telemetry},
  doi = {10.1109/SPCOM.2014.6983938},
  abstract = {Analysis of cascade network consisting of RBF nodes and least square
	error minimization block for compressed sensing recovery of sparse
	signals is presented in this paper. The proposed algorithm radial
	basis function cascade network for sparse signal recovery uses the
	L0 norm optimization, L2 least square method and feedback network
	model to improve the signal recovery performance and computational
	time over the existing ANN based and SL0 algorithms. The recovery
	of spike and pulse current measurement from simulated compressed
	sensed data for Telemetry application is demonstrated using the new
	algorithm. The Simulink model for compressed sensing data acquisition
	process, sparse signal recovery results and algorithm performance
	evaluation are presented.},
  timestamp = {2016-07-08T10:29:00Z},
  booktitle = {Signal Processing and Communications (SPCOM), 2014 International 	Conference on},
  author = {Vivekanand, V. and Vidya, L.},
  month = jul,
  year = {2014},
  keywords = {Algorithm design and analysis,ANN,Approximation algorithms,Artificial
	neural networks,compressed sensing,compressed sensing recovery,Convergence,CS
	measurement,data acquisition,feedback network model,L0 norm optimization,L2
	least square method,least square error minimization block,least squares
	approximations,Minimization,Noise,Noise measurement,radial basis
	function cascade network,radial basis function networks,RASR,RBF,RBF
	cascade network,sparse signal recovery,telemetry,telemetry application},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Vivekanand2014a,
  title = {Radial basis function cascade network for Sparse signal Recovery 	(RASR)},
  doi = {10.1109/NCC.2014.6811251},
  abstract = {The use of cascade network consisting of RBF nodes and least square
	error minimization block to Compressed Sensing for recovery of sparse
	signals is explored in this paper to improve the computation time
	and convergence. The proposed algorithm Radial basis function cascade
	network for Sparse signal Recovery (RASR) uses the L0 norm optimization,
	L2 least square method and feedback network model to improve the
	signal recovery performance and computational time over the existing
	ANN based CSIANN and relaxation based SL0 algorithms. The simulation
	results and experimental evluation of algorithm performance are presented
	here.},
  timestamp = {2016-07-10T07:40:03Z},
  booktitle = {Communications (NCC), 2014 Twentieth National Conference on},
  author = {Vivekanand, V. and Vidya, L. and Kumar, U.S. and Mishra, D.},
  month = feb,
  year = {2014},
  keywords = {Algorithm design and analysis,Approximation algorithms,Artificial
	neural networks,cascade networks,compressed sensing,computational
	time,Convergence,CSIANN,feedback network model,least square error
	minimization block,least square method,least squares approximations,Minimization,neural
	nets,radial basis function cascade network,radial basis function
	networks,RASR,sparse matrices,sparse signal recovery},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Voronin2013,
  title = {A new generalized thresholding algorithm for inverse problems with 	sparsity constraints},
  doi = {10.1109/ICASSP.2013.6637929},
  abstract = {We propose a new generalized thresholding algorithm useful for inverse
	problems with sparsity constraints. The algorithm uses a thresholding
	function with a parameter p, first mentioned in [1]. When p = 1,
	the thresholding function is equivalent to classical soft thresholding.
	For values of p below 1, the thresholding penalizes small coefficients
	over a wider range and applies less bias to the larger coefficients,
	much like hard thresholding but without discontinuities. The functional
	that the new thresholding minimizes is non-convex for p <; 1. We
	state an algorithm similar to the Iterative Soft Thresholding Algorithm
	(ISTA) [2].We show that the new thresholding performs better in numerical
	examples than soft thresholding.},
  timestamp = {2016-07-08T10:31:57Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 	Conference on},
  author = {Voronin, S. and Chartrand, R.},
  month = may,
  year = {2013},
  keywords = {compressed sensing,compressive sensing,concave programming,concave
	programming,generalized thresholding algorithm,generalized
	thresholding algorithm,Hard thresholding,image denoising,image
	denoising,Image segmentation,inverse problems,Inverse
	problems,ISTA,iterative soft thresholding algorithm,iterative
	soft thresholding algorithm,Nonconvex,Sparsity,sparsity constraints,sparsity
	constraints,thresholding,thresholding function,thresholding
	function},
  pages = {1636-1640},
  owner = {Fardin}
}

@article{Wagner2013,
  title = {Model-based EEG and MEG Source Reconstruction Methods},
  abstract = {Neurophysiological activity, as measured with EEG or MEG, has its
	origin in the brain. For most EEG and MEG experiments it is known
	beforehand, that the folded gray matter of the cortex is the location
	of the neurons whose activity is measured. These neurons, the pyramidal
	cells, are oriented perpendicular to the cortical surface. Triangulation
	of the segmented cortical surface yields a triangle net holding all
	anatomical information which is necessary to guide the reconstruction
	procedure. We have developed dedicated 3D region-growing, surface
	subsampling, normal estimation, and surface triangulation algorithms
	for generating the cortical triangle net. Especially, triangle nets
	encode neighborhood information of the potential source locations.
	Neighborhood information provides additional source constraints that
	model the correlation of neuronal activation which is necessary to
	generate detectable fields or potentials at all. In a surface-based
	3D visualization environment, reconstruction results are displayed
	as color-coded overlays together with the cortical surface, the measurement
	setup, and arbitrary MR data slices. The algorithms described are
	part of the source reconstruction software package CURRY.},
  timestamp = {2016-07-09T20:14:13Z},
  author = {Wagner, Michael and Fuchs, Manfred and Wischmann, Hans-Aloys and Drenckhahn, Ralf and K�hler, Thomas},
  month = sep,
  year = {2013},
  keywords = {cortical constraints,CURRY,EEG,MEG,source reconstruction,surface triangulation},
  owner = {Fardin}
}

@article{Wainwright2009,
  title = {Sharp Thresholds for High-Dimensional and Noisy Sparsity Recovery 	Using $\ell _{1}$-Constrained Quadratic Programming (Lasso)},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2016018},
  abstract = {The problem of consistently estimating the sparsity pattern of a vector
	beta* isin Rp based on observations contaminated by noise arises
	in various contexts, including signal denoising, sparse approximation,
	compressed sensing, and model selection. We analyze the behavior
	of l1-constrained quadratic programming (QP), also referred to as
	the Lasso, for recovering the sparsity pattern. Our main result is
	to establish precise conditions on the problem dimension p, the number
	k of nonzero elements in beta*, and the number of observations n
	that are necessary and sufficient for sparsity pattern recovery using
	the Lasso. We first analyze the case of observations made using deterministic
	design matrices and sub-Gaussian additive noise, and provide sufficient
	conditions for support recovery and linfin-error bounds, as well
	as results showing the necessity of incoherence and bounds on the
	minimum value. We then turn to the case of random designs, in which
	each row of the design is drawn from a N (0, Sigma) ensemble. For
	a broad class of Gaussian ensembles satisfying mutual incoherence
	conditions, we compute explicit values of thresholds 0 < thetasl(Sigma)
	les thetasu(Sigma) < +infin with the following properties: for any
	delta > 0, if n > 2 (thetasu + delta) klog (p- k), then the Lasso
	succeeds in recovering the sparsity pattern with probability converging
	to one for large problems, whereas for n < 2 (thetasl - delta)klog
	(p - k), then the probability of successful recovery converges to
	zero. For the special case of the uniform Gaussian ensemble (Sigma
	= Iptimesp), we show that thetasl = thetas<u = 1, so that the precise
	threshold n = 2 klog(p- k) is exactly determined.},
  timestamp = {2016-07-10T08:11:23Z},
  number = {5},
  journal = {Information Theory, IEEE Transactions on},
  author = {Wainwright, M.J.},
  month = may,
  year = {2009},
  note = {read},
  keywords = {$ell _{1}$-constraints,Additive noise,compressed sensing,compressed
	sensing,Context modeling,convex relaxation,deterministic design matrices,Graphical
	models,high-dimensional inference,high-dimensional recovery,l1 -constrained
	quadratic programming,Lasso,Model selection,noisy sparsity recovery,Pattern
	analysis,phase transitions,Polynomials,quadratic programming,sharp
	thresholds,signal denoising,sparse approximation,sparse matrices,sparse
	matrices,sparsity pattern recovery,sparsity
	pattern recovery,Statistics,subGaussian additive noise,subGaussian additive
	noise,subset selection,Sufficient conditions,Sufficient
	conditions,uniform Gaussian ensemble},
  pages = {2183-2202},
  owner = {Fardin}
}

@article{Wainwright2009a,
  title = {Information-Theoretic Limits on Sparsity Recovery in the High-Dimensional 	and Noisy Setting},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2032816},
  abstract = {The problem of sparsity pattern or support set recovery refers to
	estimating the set of nonzero coefficients of an unknown vector beta*
	isin Ropfp based on a set of n noisy observations. It arises in a
	variety of settings, including subset selection in regression, graphical
	model selection, signal denoising, compressive sensing, and constructive
	approximation. The sample complexity of a given method for subset
	recovery refers to the scaling of the required sample size n as a
	function of the signal dimension p, sparsity index k (number of non-zeroes
	in beta*), as well as the minimum value betamin of beta* over its
	support and other parameters of measurement matrix. This paper studies
	the information-theoretic limits of sparsity recovery: in particular,
	for a noisy linear observation model based on random measurement
	matrices drawn from general Gaussian measurement matrices, we derive
	both a set of sufficient conditions for exact support recovery using
	an exhaustive search decoder, as well as a set of necessary conditions
	that any decoder, regardless of its computational complexity, must
	satisfy for exact support recovery. This analysis of fundamental
	limits complements our previous work on sharp thresholds for support
	set recovery over the same set of random measurement ensembles using
	the polynomial-time Lasso method (lscr1-constrained quadratic programming).},
  timestamp = {2016-07-08T12:49:43Z},
  number = {12},
  journal = {Information Theory, IEEE Transactions on},
  author = {Wainwright, M.J.},
  month = dec,
  year = {2009},
  keywords = {$ell_1$-relaxation,compressed sensing,compressive sensing,Computational
	complexity,constructive approximation,Decoding,exhaustive search
	decoder,Fano's method,Gaussian measurement matrices,Gaussian noise,Graphical
	models,graphical model selection,high-dimensional setting,high-dimensional
	statistical inference,information-theoretic bounds,information-theoretic
	limits,Lasso,lscr1-constrained quadratic programming,Model selection,noisy
	linear observation model,noisy setting,nonzero coefficients,Particle
	measurements,Polynomials,polynomial-time Lasso method,quadratic programming,random
	measurement matrix,search problems,set theory,signal denoising,signal
	denoising,Size measurement,sparse matrices,sparsity pattern,sparsity recovery,sparsity
	recovery,subset selection,Sufficient conditions,support
	recovery,support set recovery,vector,Vectors},
  pages = {5728-5741},
  owner = {Fardin}
}

@article{Wainwright2006,
  title = {Sharp thresholds for high-dimensional and noisy recovery of sparsity},
  abstract = {The problem of consistently estimating the sparsity pattern of a vector
	$\betastar \in \real^\mdim$ based on observations contaminated by
	noise arises in various contexts, including subset selection in regression,
	structure estimation in graphical models, sparse approximation, and
	signal denoising. We analyze the behavior of $\ell_1$-constrained
	quadratic programming (QP), also referred to as the Lasso, for recovering
	the sparsity pattern. Our main result is to establish a sharp relation
	between the problem dimension $\mdim$, the number $\spindex$ of non-zero
	elements in $\betastar$, and the number of observations $\numobs$
	that are required for reliable recovery. For a broad class of Gaussian
	ensembles satisfying mutual incoherence conditions, we establish
	existence and compute explicit values of thresholds $\ThreshLow$
	and $\ThreshUp$ with the following properties: for any $\epsilon
	> 0$, if $\numobs > 2 (\ThreshUp + \epsilon) \log (\mdim - \spindex)
	+ \spindex + 1$, then the Lasso succeeds in recovering the sparsity
	pattern with probability converging to one for large problems, whereas
	for $\numobs < 2 (\ThreshLow - \epsilon) \log (\mdim - \spindex)
	+ \spindex + 1$, then the probability of successful recovery converges
	to zero. For the special case of the uniform Gaussian ensemble, we
	show that $\ThreshLow = \ThreshUp = 1$, so that the threshold is
	sharp and exactly determined.},
  timestamp = {2016-07-10T08:11:20Z},
  journal = {ArXiv Mathematics e-prints},
  author = {Wainwright, Martin J.},
  month = may,
  year = {2006},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2006math......5740W},
  owner = {afdidehf}
}

@article{Waldmann2013,
  title = {Evaluation of the lasso and the elastic net in genome-wide association 	studies},
  volume = {4},
  doi = {10.3389/fgene.2013.00270},
  abstract = {Thenumberofpublicationsperforminggenome-wideassociationstudies(GWAS)hasincreaseddramatically.Penalizedregressionapproacheshavebeendevelopedtoovercomethechallengescausedbythehighdimensionaldata,butthesemethodsarerelativelynewintheGWASfield.Inthisstudywehavecomparedthestatisticalperformanceoftwomethods(theleastabsoluteshrinkageandselectionoperator�lassoandtheelasticnet)ontwosimulateddatasetsandonerealdatasetfroma50Kgenome-widesinglenucleotidepolymorphism(SNP)panelof5570Fleckviehbulls.ThefirstsimulateddatasetdisplaysmoderatetohighlinkagedisequilibriumbetweenSNPs,whereasthesecondsimulateddatasetfromtheQTLMAS2010workshopisbiologicallymorecomplex.Weusedcross-validationtofindtheoptimalvalueofregularizationparameter?withbothminimumMSEandminimumMSE+1SEofminimumMSE.Theoptimal?valueswereusedforvariableselection.Basedonthefirstsimulateddata,wefoundthattheminMSEingeneralpickeduptoomanySNPs.AtminMSE+1SE,thelassodidn�tacquireanyfalsepositives,butselectedtoofewcorrectSNPs.Theelasticnetprovidedthebestcompromisebetweenfewfalsepositivesandmanycorrectselectionswhenthepenaltyweight?wasaround0.1.However,inoursimulationsetting,this?valuedidn�tresultinthelowestminMSE+1SE.ThenumberofselectedSNPsfromtheQTLMAS2010datawasaftercorrectionforpopulationstructure82and161forthelassoandtheelasticnet,respectively.IntheFleckviehdatasetafterpopulationstructurecorrectionlassoandtheelasticnetidentifiedfrom1291to1966importantSNPsformilkfatcontent,withmajorpeaksonchromosomes5,14,15,and20.Hence,wecanconcludethatitisimportanttoanalyzeGWASdatawithboththelassoandtheelasticnetandanalternativetuningcriteriontominimumMSEisneededforvariableselection.},
  timestamp = {2016-07-08T12:26:33Z},
  journal = {Frontiers in Genetics},
  author = {Waldmann, Patrik and M�sz�ros, G�bor and Gredler, Birgit and Fuerst, Christian and S�lkner, Johann},
  year = {2013},
  keywords = {cattle,elasticnet,GWAS,Lasso,populationstructure,Simulation},
  owner = {Fardin}
}

@inproceedings{Wang2014,
  title = {A Modified Image Reconstruction Algorithm Based on Compressed Sensing},
  doi = {10.1109/IMCCC.2014.133},
  abstract = {Compressed sensing theory is a new kind of making full use of signal
	sparsity or compressible sampling theory. The theory suggests that
	collecting a small amount of signal values can realize accurate reconstruction
	of sparse or compressed signal. Through the research and summary
	of the existing reconstruction algorithm, the paper proposes a new
	adaptive matching pursuit algorithm based on regularization Regularized
	Adaptive Matching Pursuit (RAMP) for compressed sensing signal reconstruction,
	called blocking sparsity adaptive regularized matching pursuit (BSARMP)
	algorithms. In order to reduce the scale of a single observation
	matrix processing and the single processing speed, a novel method
	based on image blocking is presented in this paper, thereby improving
	the overall running time.},
  timestamp = {2016-07-08T10:28:16Z},
  booktitle = {Instrumentation and Measurement, Computer, Communication and Control 	(IMCCC), 2014 Fourth International Conference on},
  author = {Wang, Aili and Gao, Xue and Gao, Yue},
  month = sep,
  year = {2014},
  keywords = {blocking sparsity adaptive regularized matching pursuit,BSARMP algorithm,compressed
	sensing,compressed sensing signal reconstruction,compressed sensing
	theory,compressible sampling theory,image blocking,image coding,image
	matching,Image reconstruction,image sampling,matching pursuit,Matching
	pursuit algorithms,modified image reconstruction algorithm,overall
	running time improvement,Partitioning algorithms,PSNR,RAMP algorithm,reconstruction
	algorithm,regularization regularized adaptive matching pursuit algorithm,Signal
	processing algorithms,signal sparsity,signal values,single-observation
	matrix processing scale reduction,single-processing speed reduction,sparse
	matrices,sparse representation,sparse signal reconstruction},
  pages = {624-627},
  owner = {afdidehf}
}

@article{Wang2008,
  title = {A note on adaptive group lasso},
  volume = {52},
  issn = {0167-9473},
  doi = {http://dx.doi.org/10.1016/j.csda.2008.05.006},
  abstract = {Group lasso is a natural extension of lasso and selects variables
	in a grouped manner. However, group lasso suffers from estimation
	inefficiency and selection inconsistency. To remedy these problems,
	we propose the adaptive group lasso method. We show theoretically
	that the new method is able to identify the true model consistently,
	and the resulting estimator can be as efficient as oracle. Numerical
	studies confirmed our theoretical findings.},
  timestamp = {2016-07-08T11:24:49Z},
  number = {12},
  journal = {Computational Statistics \& Data Analysis},
  author = {Wang, Hansheng and Leng, Chenlei},
  year = {2008},
  pages = {5277 - 5286},
  owner = {Fardin}
}

@inproceedings{Wang2013,
  title = {Block compressed sensing based on human visual for image reconstruction},
  doi = {10.1109/IMSNA.2013.6743436},
  abstract = {Block Compressed Sensing (BCS) is one of the fundamental theories
	for image reconstruction. Compared with the traditional Compressed
	Sensing (CS) technique, it reduces the computational complexity and
	improves the efficiency of the reconstruction. However, the reconstruction
	quality of BCS is deteriorated to some degree. In order to improve
	the reconstruction quality of BCS, a new method based on human visual
	characteristics is proposed following the analysis of the DCT coefficients
	of an image. In the new method is introduced the contrast sensitivity
	in Watson visual model, indicating that the human eyes have different
	sensitivity to different DCT coefficients. To each element of the
	observation matrix in the same image block is assigned different
	weights based on visual characteristics. Finally, the experimental
	results demonstrate that the proposed approach can not only effectively
	improve the image reconstruction quality, but also have better subjective
	visual effect.},
  timestamp = {2016-07-08T11:41:04Z},
  booktitle = {Instrumentation and Measurement, Sensor Network and Automation (IMSNA), 	2013 2nd International Symposium on},
  author = {Wang, Jie and Bo, Hua and Sun, Qiang},
  month = dec,
  year = {2013},
  keywords = {BCS,block compressed sensing,compressed sensing,computational complexity,Computational
	complexity,contrast sensitivity,contrast
	sensitivity,DCT coefficients,DCT Sparse Coefficient,DCT
	Sparse Coefficient,discrete cosine transforms,discrete cosine
	transforms,human visual characteristics,Image reconstruction,Image
	reconstruction,matrix algebra,PSNR,Sensitivity,sparse matrices,sparse
	matrices,Visualization,Watson visual model,Watson
	visual model,Weighted Observation Matrix,Weighted
	Observation Matrix},
  pages = {951-954},
  owner = {afdidehf}
}

@article{Wang2015a,
  title = {A perturbation analysis of nonconvex block-sparse compressed sensing},
  volume = {29},
  issn = {1007-5704},
  doi = {http://dx.doi.org/10.1016/j.cnsns.2015.05.022},
  abstract = {Abstract This paper proposes a completely perturbed mixed l2/lp minimization
	to deal with a model of completely perturbed block-sparse compressed
	sensing. Based on the block restricted isometry property (BRIP),
	the paper extends the study to a complete perturbation model which
	considers not only noise but also perturbation, establishes a sufficient
	condition for efficiently recovering the block-sparse signal under
	the complete perturbation case, and offers eventually a superior
	approximation precision. The precision, in this paper, can be characterized
	in terms of the total noise and the best K-term approximation. The
	adopted mixed l2/lp minimization also gains better robustness and
	stability than ever that on recovering the block-sparse signal with
	the presence of total noise. Especially, the analysis of this study
	shows the condition is the best sufficient condition δ2K &lt; 1
	[20] when p tends to zero and a &gt; 1 for the complete perturbation
	and block-sparse signal. The numerical experiments carried out confirm
	excellently the assessed performance.},
  timestamp = {2016-07-08T11:26:39Z},
  number = {1–3},
  journal = {Communications in Nonlinear Science and Numerical Simulation},
  author = {Wang, Jianjun and Zhang, Jing and Wang, Wendong and Yang, Chanyun},
  year = {2015},
  keywords = {2/p(0<p<1),Block-sparse,Compressed,Matrix,measurement,method,Minimization,of,perturbation,sensing,Signal},
  pages = {416 - 426},
  owner = {afdidehf}
}

@inproceedings{Wang2013a,
  title = {Harmonic signal recovery and order estimation based on cascaded sparse 	processing},
  doi = {10.1109/ISCAS.2013.6572305},
  abstract = {The detection and estimation of harmonic sinusoidal signals with multiple
	unknown fundamental frequencies are of great importance in many applications.
	In this paper, a cascaded sparse processing method is proposed for
	joint recovery and order estimation of harmonic sinusoidal signals.
	The cascaded sparse processing is performed by the following two
	steps. Firstly, group Lasso method is applied to estimate and recover
	the fundamental frequencies based on the characteristics of the block-sparsity
	of the harmonics. Then, Lasso estimator is used for each block corresponding
	to its fundamental frequency for further noise suppression. The theoretical
	conditions under which the proposed method can give a correct estimate
	of the signal support is derived under the Fourier basis. Simulation
	results of the proposed method are given to show the desirable performance.},
  timestamp = {2016-07-08T12:41:33Z},
  booktitle = {Circuits and Systems (ISCAS), 2013 IEEE International Symposium on},
  author = {Wang, Lu and Bi, Guoan},
  month = may,
  year = {2013},
  keywords = {block-sparsity harmonics,cascaded sparse processing method,Dictionaries,Estimation,Fourier
	analysis,Fourier basis,frequency estimation,fundamental
	frequency estimation,fundamental frequency
	estimation,fundamental frequency recovery,group Lasso estimator method,group Lasso estimator
	method,Harmonic analysis,Harmonic
	analysis,harmonic signal recovery,harmonic sinusoidal signal detection,harmonic sinusoidal
	signal detection,harmonic sinusoidal signal estimation,harmonic
	sinusoidal signal estimation,Noise,noise abatement,noise
	abatement,noise suppression,order estimation,order
	estimation,Radar,signal detection,sonar},
  pages = {2171-2174},
  owner = {afdidehf}
}

@article{Wang2015b,
  title = {Harmonic tonal detectors based on the BOGA},
  volume = {106},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.08.008},
  abstract = {Abstract Tonals generated by machineries with rotating elements typically
	have a harmonic structure with unknown fundamental frequencies, amplitude,
	harmonic order and phase. Detecting this type of signals is of great
	importance to numerous engineering applications. In the frequency
	domain, tonals are represented by a few harmonic frequencies, which
	appear in blocks, related to one or more fundamental frequencies.
	This block-sparsity property of the frequency content suggests alternative
	ways to recover and detect tonals by using sparse signal processing
	techniques. Motivated by the success of the block orthonormal greedy
	algorithm (BOGA), new detection architectures, which require no prior
	information about the number of the fundamental frequencies, are
	proposed for robust tonal detection in low signal to noise ratio
	(SNR) environments. The distributions of the test statistics of detection
	architectures are firstly analyzed theoretically and comprehensively
	based on the theory of order statistics. Detection performances are
	also analyzed and compared theoretically and experimentally. Significant
	improvements on detection performance in low \{SNR\} environments
	are shown over the conventional detectors that do not consider the
	harmonic structure and the sparsity of the tonals.},
  timestamp = {2016-07-08T12:41:37Z},
  journal = {Signal Processing},
  author = {Wang, Lu and Wan, Chunru and Li, Shenghong and Bi, Guoan},
  year = {2015},
  keywords = {Block-sparsity,BOGA,detection,Order,Signal,statistic},
  pages = {215 - 230},
  owner = {afdidehf}
}

@article{Wang2014a,
  title = {Hierarchical Sparse Signal Recovery by Variational Bayesian Inference},
  volume = {21},
  issn = {1070-9908},
  doi = {10.1109/LSP.2013.2292589},
  abstract = {This letter addresses the recovery of hierarchical sparse signals
	in a Bayesian framework. Hierarchical sparse signals exhibit two
	levels of sparsity, i.e., block-sparsity among different blocks and
	internal sparsity within each individual block. As in sparse Bayesian
	learning, each component of the coefficient vector is firstly modeled
	as a Gaussian distributed variable with zero mean. To enforce the
	two-level hierarchical sparsity, the variance is further modeled
	by two classes of hidden variables controlling the block-sparsity
	and the internal sparsity, respectively. Finally, variational Bayesian
	inference is used to recover the coefficient vector from the noise
	corrupted data. Numerical simulation and experimental results show
	that the proposed method outperforms those recently reported recovery
	methods.},
  timestamp = {2016-07-08T12:41:43Z},
  number = {1},
  journal = {Signal Processing Letters, IEEE},
  author = {Wang, Lu and Zhao, Lifan and Bi, Guoan and Wan, Chunru},
  month = jan,
  year = {2014},
  keywords = {Bayesian framework,Bayes methods,coefficient vector,Dictionaries,Gaussian
	distributed variable,hidden variables,Hierarchical sparse signal,hierarchical
	sparse signal recovery,Lasso,Noise,noise corrupted data,Noise measurement,Numerical
	Analysis,Numerical simulation,Probabilistic logic,signal processing,Signal
	processing algorithms,sparse Bayesian learning,variational Bayesian
	inference,Vectors},
  pages = {110-113},
  owner = {afdidehf}
}

@inproceedings{Wang2014b,
  title = {Terahertz radar imaging based on block sparse Bayesian learning framework},
  doi = {10.1109/SSP.2014.6884645},
  abstract = {There is an increasing interest in high-resolution radar imaging of
	objects, and recent developments of terahertz sensing techniques
	provide the depiction ability of objects in detail. In this paper,
	the compressed sensing theory is introduced to terahertz radar imaging.
	A terahertz radar azimuth-elevation imaging scheme based on block
	sparse Bayesian learning framework is proposed. By exploiting block
	sparse structures of the terahertz azimuth-elevation imagery, the
	reconstruction performance can be improved significantly. Simulation
	results based on electromagnetic calculation data show that the block
	sparse Bayesian learning algorithm keeps a better balance between
	the computation load and the accuracy of the reconstruction signal
	than the existing algorithms.},
  timestamp = {2016-07-11T16:59:21Z},
  booktitle = {Statistical Signal Processing (SSP), 2014 IEEE Workshop on},
  author = {Wang, Ruijun and Deng, Bin and Qin, Yuliang and Cheng, Yongqiang and Su, Wuge},
  month = jun,
  year = {2014},
  keywords = {Azimuth,block sparse Bayesian learning,block sparse Bayesian learning
	framework,block sparse structures,compressed sensing,compressed sensing
	theory,electromagnetic calculation data,high-resolution radar object
	imaging,Image reconstruction,Image resolution,imaging,object depiction
	ability,radar imaging,radar resolution,reconstruction performance,reconstruction
	performance,reconstruction signal accuracy,reconstruction
	signal accuracy,Signal processing algorithms,Terahertz,terahertz
	azimuth-elevation imagery,terahertz radar azimuth-elevation imaging
	scheme,terahertz sensing technique,terahertz wave imaging,Vectors},
  pages = {340-343},
  owner = {afdidehf}
}

@inproceedings{Wang2011,
  title = {Distributed compressed sensing for block-sparse signals},
  doi = {10.1109/PIMRC.2011.6140053},
  abstract = {To address the problems of high sampling rates, shadow fading and
	additive noise from the receiver, in this paper, a distributed compressed
	sampling (DCS) and centralized reconstruction approach which utilize
	the spatial diversity against fading channels is proposed. Unlike
	traditional centralized reconstruction, in this paper, we centralized
	recover the spectrums by exploiting the block-sparsity which is rather
	prevalent in multi-band signals. In DCS, first each CR samples the
	signals with a sub-Nyquist sampling rate independently, then the
	sampled data are uploaded to the fusion center (FC), finally FC reconstructs
	these data simultaneously. To exploit the block-sparsity, two new
	centralized recovery algorithms simultaneous block orthogonal matching
	pursuit (S-BOMP) and simultaneous binary tree based block orthogonal
	matching pursuit (S-BTBOMP) are developed. Simulation results show
	they outperform existing simultaneous recovery algorithms which don't
	take block-sparsity into consideration.},
  timestamp = {2016-07-08T12:14:52Z},
  booktitle = {Personal Indoor and Mobile Radio Communications (PIMRC), 2011 IEEE 	22nd International Symposium on},
  author = {Wang, Xing and Guo, Wenbin and Lu, Yang and Wang, Wenbo},
  month = sep,
  year = {2011},
  keywords = {Additive noise,Binary trees,block-sparse signal,Block-sparsity,Cognitive
	radio,compressed sensing,CR sample,DCS,distributed compressed sampling,distributed
	compressed sensing,Fading,fading channel,fading channels,FC,fusion
	center,iterative methods,Matching pursuit algorithms,multiband signal,radio
	receivers,S-BOMP,S-BTBOMP,Sensors,shadow fading,signal reconstruction,Signal
	to noise ratio,simultaneous binary tree based block orthogonal matching
	pursuit,simultaneous block orthogonal matching pursuit,simultaneous
	recovery,spatial diversity,subNyquist sampling rate,time-frequency
	analysis,Wideband},
  pages = {695-699},
  owner = {afdidehf}
}

@inproceedings{Wang2013b,
  title = {Exploiting hidden block sparsity: Interdependent matching pursuit 	for cyclic feature detection},
  doi = {10.1109/GLOCOM.2013.6831224},
  abstract = {In this paper, we propose a novel Compressive Sensing (CS)-enhanced
	spectrum sensing approach for Cognitive Radio (CR) systems. The new
	framework enables cyclic feature detection with a significantly reduced
	sampling rate. We associate the new framework with a novel model-based
	greedy reconstruction algorithm: interdependent matching pursuit
	(IMP). For IMP, the hidden block sparsity owing to the symmetry present
	in the cyclic spectrum is exploited which effectively reduces the
	degree of freedom of problem. Compared with conventional CS with
	independent support selection, a remarkable spectrum reconstruction
	improvement is achieved by IMP.},
  timestamp = {2016-07-08T12:27:25Z},
  booktitle = {Global Communications Conference (GLOBECOM), 2013 IEEE},
  author = {Wang, Yu and Chen, Wei and Wassell, I.J.},
  month = dec,
  year = {2013},
  keywords = {cognitive radio,cognitive radio systems,compressed sensing,compressed
	sensing,compressive sensing,Compressive
	sensing,Correlation,CR systems,CS-enhanced spectrum sensing,CS-enhanced spectrum
	sensing,cyclic feature detection,cyclic
	feature detection,cyclic spectrum,feature extraction,Greedy algorithms,Greedy
	algorithms,hidden block sparsity,hidden
	block sparsity,IMP,independent support selection,Indexes,interdependent
	matching pursuit,iterative methods,Matching pursuit algorithms,model-based
	greedy reconstruction algorithm,radio spectrum management,Sensors,signal
	detection,sparse matrices,spectrum reconstruction,time-frequency
	analysis,Vectors},
  pages = {1119-1124},
  owner = {afdidehf}
}

@article{Wang2013c,
  title = {On recovery of block-sparse signals via mixed $\ell_2/\ell_q (0 < 	q \leq 1)$ norm minimization},
  volume = {2013},
  doi = {10.1186/1687-6180-2013-76},
  abstract = {Compressed sensing (CS) states that a sparse signal can exactly be
	recovered from very few linear measurements. While in many applications,
	real-world signals also exhibit additional structures aside from
	standard sparsity. The typical example is the so-called block-sparse
	signals whose non-zero coefficients occur in a few blocks. In this
	article, we investigate the mixed l2/lq(0 < q - 1) norm minimization
	method for the exact and robust recovery of such block-sparse signals.
	We mainly show that the non-convex l2/lq(0 < q < 1) minimization
	method has stronger sparsity promoting ability than the commonly
	used l2/l1 minimization method both practically and theoretically.
	In terms of a block variant of the restricted isometry property of
	measurement matrix, we present weaker sufficient conditions for exact
	and robust block-sparse signal recovery than those known for l2/l1
	minimization. We also propose an efficient Iteratively Reweighted
	Least-Squares (IRLS) algorithm for the induced non-convex optimization
	problem. The obtained weaker conditions and the proposed IRLS algorithm
	are tested and compared with the mixed l2/l1 minimization method
	and the standard lq minimization method on a series of noiseless
	and noisy block-sparse signals. All the comparisons demonstrate the
	outperformance of the mixed l2/lq(0 < q < 1) method for block-sparse
	signal recovery applications, and meaningfulness in the development
	of new CS technology.},
  language = {English},
  timestamp = {2016-07-10T07:13:20Z},
  number = {1},
  journal = {EURASIP Journal on Advances in Signal Processing},
  author = {Wang, Yao and Wang, Jianjun and Xu, Zongben},
  year = {2013},
  keywords = {Block-RIP,block-sparse recovery,compressed sensing,IRLS,l2/l q minimization},
  annote = {asp.eurasipjournals.com/content/pdf/1687-6180-2013-76.pdf},
  annote = {asp.eurasipjournals.com/content/pdf/1687-6180-2013-76.pdf},
  annote = {asp.eurasipjournals.com/content/pdf/1687-6180-2013-76.pdf},
  annote = {asp.eurasipjournals.com/content/pdf/1687-6180-2013-76.pdf},
  annote = {asp.eurasipjournals.com/content/pdf/1687-6180-2013-76.pdf},
  eid = {76},
  owner = {Fardin}
}

@inproceedings{Wang2014d,
  title = {Block-sparse signal recovery with synthesized multitask compressive 	sensing},
  doi = {10.1109/ICASSP.2014.6853753},
  abstract = {The paper considers the problem of reconstructing blocks-sparse signals.
	A new algorithm, called synthesized multitask compressive sensing
	(SMCS), is proposed. In contrast to existing methods that rely on
	the availability of the sparsity structure information, the SMCS
	algorithm resorts to the multitask compressive sensing (MCS) technique
	for signal recovery. The SMCS algorithm synthesizes new compressive
	sensing (CS) tasks via circular-shifting operations and utilizes
	the minimum description length (MDL) principle to determine the proper
	set of the synthesized CS tasks for signal reconstruction. An outstanding
	advantage of SMCS is that it can achieve good signal reconstruction
	performance without using prior information on the block-sparsity
	structure. Simulations corroborate the theoretical developments.},
  timestamp = {2016-07-08T11:43:07Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Wang, Ying-Gui and Liu, Zheng and Jiang, Wen-Li and Yang, Le},
  month = may,
  year = {2014},
  keywords = {Bayesian learning,Bayes methods,block sparse signal recovery,Block-sparsity,circular
	shifting operations,compressed sensing,Educational institutions,minimum
	description length,minimum description length principle,Partitioning
	algorithms,Signal processing algorithms,signal reconstruction,sparsity
	structure information,synthesized multitask compressive sensing,synthesized
	multitask compressive sensing,Vectors},
  pages = {1030-1034},
  owner = {afdidehf}
}

@inproceedings{Wang2013d,
  title = {A multitask recovery algorithm for block-sparse signals},
  doi = {10.1109/WCSP.2013.6677216},
  abstract = {The paper considers the problem of jointly reconstructing multiple
	block-sparse signals with block partition unknown. Based on the framework
	of block sparse Bayesian learning (BSBL), we develop a new multitask
	recovery algorithm, called the extension algorithm of multitask block
	sparse Bayesian learning (EMBSBL). In contrast to existing methods,
	EMBSBL exploits not only the statistical interrelationships of signals
	(i.e., a degree of overlap of nonzero elements' positions among different
	signals), but also signals' intra-block correlation, and does not
	need a priori information on block partition. Simulations corroborate
	the theoretical developments.},
  timestamp = {2016-07-08T10:28:27Z},
  booktitle = {Wireless Communications Signal Processing (WCSP), 2013 International 	Conference on},
  author = {Wang, Ying-Gui and Qu, Jian-Sheng and Liu, Zheng and Jiang, Wen-Li},
  month = oct,
  year = {2013},
  keywords = {Bayes methods,block partition,Block-sparse,EMBSBL,extension algorithm
	of multitask block sparse Bayesian learning,intra-block correlation,learning
	(artificial intelligence),multiple block-sparse signal reconstruction,Multitask,multitask
	recovery algorithm,nonzero elements positions,signal reconstruction,sparse
	Bayesian learning,statistical interrelationships},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Wang2012,
  title = {Medical Image Super-resolution Analysis with Sparse Representation},
  doi = {10.1109/IIH-MSP.2012.31},
  abstract = {In this paper, we propose a novel method for Super-Resolution Medical
	image based sparse representation, with the aim to solve the problem
	of MR image resolution owing to the limitations of hardware and acquisitions.
	With two coupled dictionaries the sparse representation of a low
	resolution medical image blocks is used to generate a high resolution.
	Some evaluations are implemented to compare with previous method,
	and the proposed algorithm has its advantage on super-resolution.},
  timestamp = {2016-07-09T20:10:08Z},
  booktitle = {Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), 	2012 Eighth International Conference on},
  author = {Wang, Yun-Heng and Li, Jun-Bao and Fu, Ping},
  month = jul,
  year = {2012},
  keywords = {biomedical imaging,biomedical MRI,coupled dictionaries,Dictionaries,Image
	reconstruction,image representation,Image resolution,low resolution
	medical image blocks,Magnetic resonance imaging,medical image,medical
	image processing,medical image super-resolution analysis,MR image
	resolution,Optimization,signal resolution,sparese representation,sparse
	representation,super-resolution analysis,super-resolution medical
	image},
  pages = {106-109},
  owner = {afdidehf}
}

@inproceedings{Wei2014,
  title = {A compressive sensing recovery algorithm based on sparse Bayesian 	learning for block sparse signal},
  doi = {10.1109/WPMC.2014.7014878},
  abstract = {Compressive sensing offers a new wideband spectrum sensing scheme
	in cognitive radio. In this paper, a sparse signal recovery algorithm
	based on sparse Bayesian learning (SBL) framework is proposed. By
	exploiting intrablock correlation in a block sparse model and using
	Expectation-Maximization (EM) method, this algorithm achieves superior
	performance. The results of experiments show that this algorithm
	is robust to noise and has better performance than other algorithms
	in signal recovery. Then we apply it to wideband spectrum sensing,
	we find that proposed algorithm not only guarantees accurate signal
	estimation, but also obtains higher correct detection probability.},
  timestamp = {2016-07-08T10:08:46Z},
  booktitle = {Wireless Personal Multimedia Communications (WPMC), 2014 International 	Symposium on},
  author = {Wei, Wang and Min, Jia and Qing, Guo},
  month = sep,
  year = {2014},
  keywords = {Bayes methods,block sparse signal,cognitive radio,compressed sensing,compressed
	sensing,compressive sensing,compressive sensing recovery algorithm,Correlation,detection
	probability,EM method,expectation-maximisation algorithm,expectation
	maximization method,intrablock correlation,intra-block correlation,learning
	(artificial intelligence),radio spectrum management,SBL framework,Sensors,signal
	detection,signal estimation,Signal processing algorithms,signal recovery,signal
	recovery algorithm,Signal to noise ratio,sparse Bayesian learning,sparse
	Bayesian learning (SBL),sparse signal recovery algorithm,wideband spectrum sensing,wideband
	spectrum sensing,wideband spectrum sensing
	scheme,Wireless communication},
  pages = {547-551},
  owner = {afdidehf}
}

@article{Wei2012,
  title = {DOA Estimation Using a Greedy Block Coordinate Descent Algorithm},
  volume = {60},
  issn = {1053-587X},
  doi = {10.1109/TSP.2012.2218812},
  abstract = {This paper presents a novel jointly sparse signal reconstruction algorithm
	for the DOA estimation problem, aiming to achieve faster convergence
	rate and better estimation accuracy compared to existing l2,1-norm
	minimization approaches. The proposed greedy block coordinate descent
	(GBCD) algorithm shares similarity with the standard block coordinate
	descent method for l2,1-norm minimization, but adopts a greedy block
	selection rule which gives preference to sparsity. Although greedy,
	the proposed algorithm is proved to also have global convergence
	in this paper. Through theoretical analysis we demonstrate its stability
	in the sense that all nonzero supports found by the proposed algorithm
	are the actual ones under certain conditions. Last, we move forward
	to propose a weighted form of the block selection rule based on the
	MUSIC prior. The refinement greatly improves the estimation accuracy
	especially when two point sources are closely spaced. Numerical experiments
	show that the proposed GBCD algorithm has several notable advantages
	over the existing DOA estimation methods, such as fast convergence
	rate, accurate reconstruction, and noise resistance.},
  timestamp = {2016-07-08T12:15:33Z},
  number = {12},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Wei, Xiaohan and Yuan, Yabo and Ling, Qing},
  month = dec,
  year = {2012},
  keywords = {1-norm minimization approach,Algorithm design and analysis,block coordinate
	descent,Convergence,direction-of-arrival estimation,Direction of
	arrival estimation,DOA estimation,DOA estimation problem,Estimation,GBCD
	algorithm,Greedy algorithms,greedy block coordinate descent algorithm,greedy
	block selection rule,joint sparsity,l2,minimisation,Minimization,MUSIC,noise
	resistance,Signal processing algorithms,signal reconstruction,sparse
	signal reconstruction algorithm,Vectors},
  pages = {6382-6394},
  owner = {afdidehf}
}

@phdthesis{Weiss2009,
  title = {Underdetermined Source Separation Using Speaker Subspace Models},
  abstract = {Sounds rarely occur in isolation. Despite this, significant effort
	has been dedicated to the design of computer audition systems, such
	as speech recognizers, that can only analyze isolated sound sources.
	In fact, there are a variety of applications in both human and computer
	audition for which it is desirable to understand more complex auditory
	scenes. In order to extend such systems to operate on mixtures of
	many sources, the ability to recover the source signals from the
	mixture is required. This process is known as source separation.
	In this thesis we focus on the problem of underdetermined source
	separation where the number of sources is greater than the number
	of channels in the observed mixture. In the worst case, when the
	observations are derived from a single microphone, it is often necessary
	for a separation algorithm to utilize prior information about the
	sources present in the mixture to constrain possible source reconstructions.
	A common approach for separating such signals is based on the use
	of source-specific statistical models. In most cases this approach
	requires that significant training data be available to train models
	for the sources known in advance to be present in the mixed signal.
	We propose a speaker subspace model for source adaptation that alleviates
	this requirement. We report a series of experiments on monaural mixtures
	of speech signals and demonstrate that the use of the proposed speaker
	subspace model can separate sources far better than the use of unadapted,
	source-independent models. The proposed method also outperforms other
	state of the art approaches when training data is not available for
	the exact speakers present in the mixed signal. Finally, we describe
	an system for binaural speech separation that combines constraints
	based on interaural localization cues with constraints derived from
	source models. Although a simpler system based only on localization
	cues is sometimes able to adequately isolate sources, the incorporation
	of a source-independent model is shown to significantly improve performance.
	Further improvements are obtained by using the proposed speaker subspace
	model to adapt to match the sources present in the signal.},
  timestamp = {2016-07-11T17:09:51Z},
  school = {COLUMBIA UNIVERSITY},
  author = {Weiss, Ron J.},
  year = {2009},
  owner = {Fardin}
}

@inproceedings{Wen2014,
  title = {Learning overcomplete sparsifying transforms with block cosparsity},
  doi = {10.1109/ICIP.2014.7025161},
  abstract = {The sparsity of images in a transform domain or dictionary has been
	widely exploited in image processing. Compared to the synthesis dictionary
	model, sparse coding in the (single) transform model is cheap. However,
	natural images typically contain diverse textures that cannot be
	sparsified well by a single transform. Hence, we propose a union
	of sparsifying transforms model, which is equivalent to an overcomplete
	transform model with block cosparsity (OC-TOBOS). Our alternating
	algorithm for transform learning involves simple closed-form updates.
	When applied to images, our algorithm learns a collection of well-conditioned
	transforms, and a good clustering of the patches or textures. Our
	learnt transforms provide better image representations than learned
	square transforms. We also show the promising denoising performance
	and speedups provided by the proposed method compared to synthesis
	dictionary-based denoising.},
  timestamp = {2016-07-09T19:55:37Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Wen, B. and Ravishankar, S. and Bresler, Y.},
  month = oct,
  year = {2014},
  keywords = {Analytical models,block cosparsity,closed-form updates,clustering,Clustering
	algorithms,Dictionaries,discrete cosine transforms,diverse textures,encoding,image
	denoising,Image Processing,image representation,image representations,image
	texture,natural images,Noise reduction,OC-TOBOS,overcomplete representation,overcomplete
	sparsifying transform learning,pattern clustering,single transform
	model,sparse representation,Sparsifying transform learning,speedups,synthesis
	dictionary-based denoising,transforms},
  pages = {803-807},
  owner = {afdidehf}
}

@article{Wendel2009,
  title = {EEG/MEG Source Imaging: Methods, Challenges, and Open Issues},
  doi = {10.1155/2009/656092},
  abstract = {We present the four key areas of research�preprocessing, the volume
	conductor, the forward problem, and the inverse problem� that affect
	the performance of EEG and MEG source imaging. In each key area we
	identify prominent approaches and methodologies that have open issues
	warranting further investigation within the community, challenges
	associated with certain techniques, and algorithms necessitating
	clarification of their implications. More than providing definitive
	answers we aim to identify important open issues in the quest of
	source localization.},
  timestamp = {2016-07-08T12:17:37Z},
  journal = {Computational Intelligence and Neuroscience},
  author = {Wendel, Katrina and Vaisanen, Outi and Malmivuo, Jaakko and G. Gencer, Nevzat and Vanrumste, Bart and Durka, Piotr and Magjarevic, Ratko and Supek, Selma and Pascu, Mihail Lucian and Fontenelle, Hugues and PeraltaMenendez, Rolando Grave de},
  year = {2009},
  owner = {Fardin}
}

@article{Wimalajeewa2015,
  title = {Subspace Recovery From Structured Union of Subspaces},
  volume = {61},
  issn = {0018-9448},
  doi = {10.1109/TIT.2015.2403260},
  abstract = {Lower dimensional signal representation schemes frequently assume
	that the signal of interest lies in a single vector space. In the
	context of the recently developed theory of compressive sensing,
	it is often assumed that the signal of interest is sparse in an orthonormal
	basis. However, in many practical applications, this requirement
	may be too restrictive. A generalization of the standard sparsity
	assumption is that the signal lies in a union of subspaces. Recovery
	of such signals from a small number of samples has been studied recently
	in several works. Here, we consider the problem of only subspace
	recovery in which our goal is to identify the subspace (from the
	union) in which the signal lies using a small number of samples,
	in the presence of noise. More specifically, we derive performance
	bounds and conditions under which reliable subspace recovery is guaranteed
	using maximum likelihood (ML) estimation. We begin by treating general
	unions and then obtain the results for the special case in which
	the subspaces have structure leading to block sparsity. In our analysis,
	we treat both general sampling operators and random sampling matrices.
	With general unions, we show that under certain conditions, the number
	of measurements required for reliable subspace recovery in the presence
	of noise via ML is less than that implied using the restricted isometry
	property, which guarantees complete signal recovery. In the special
	case of block sparse signals, we quantify the gain achievable over
	standard sparsity in subspace recovery. Our results also strengthen
	existing results on sparse support recovery in the presence of noise
	under the standard sparsity model.},
  timestamp = {2016-07-11T16:58:58Z},
  number = {4},
  journal = {Information Theory, IEEE Transactions on},
  author = {Wimalajeewa, T. and Eldar, Y.C. and Varshney, P.K.},
  month = apr,
  year = {2015},
  keywords = {block sparse signal,block sparsity,complete signal recovery,complete signal
	recovery,compressed sensing,compressed
	sensing,compressive sensing,general sampling operator,general sampling
	operator,matrix algebra,Maximum likelihood estimation,maximum
	likelihood estimation,ML estimation,random sampling matrix,random
	sampling matrix,Reliability,restricted isometry property,restricted
	isometry property,signal sampling,Signal to noise ratio,Signal
	to noise ratio,Silicon,Standards,standard sparsity model,standard
	sparsity model,structured subspace union,structured
	subspace union,subspace recovery,union of linear subspaces,union
	of linear subspaces,Vectors},
  pages = {2101-2114},
  owner = {afdidehf}
}

@inproceedings{Wipf2007,
  title = {EM Algorithms for Generalizing MCE and Focuss},
  doi = {10.1109/NFSI-ICFBI.2007.4387678},
  abstract = {The focal underdetermined system solver (FOCUSS) algorithm and the
	minimum current estimation (MCE) paradigm represent two ways of computing
	tomographic source reconstructions from MEG or EEG data. Both methods
	produce source estimates characterized by very compact, localized
	activity regions consistent with some neurophysiological evidence.
	While these methods have been studied extensively in the statistics
	and signal processing communities, they remain somewhat underutilized
	in the context of MEG and EEG source imaging. This paper presents
	simple extensions of both FOCUSS and MCE with the potential for handling
	flexible orientation constraints, estimation of distributed sources,
	and spatio-temporal smoothing. The resulting algorithms are easily
	implemented via the EM algorithm.},
  timestamp = {2016-07-08T12:20:50Z},
  booktitle = {Noninvasive Functional Source Imaging of the Brain and Heart and 	the International Conference on Functional Biomedical Imaging, 2007. 	NFSI-ICFBI 2007. Joint Meeting of the 6th International Symposium 	on},
  author = {Wipf, D. and Sekihara, K. and Nagarajan, S.},
  month = oct,
  year = {2007},
  keywords = {Bayesian model,Bayes methods,Biomagnetics,EEG source imaging,electroencephalography,Electromagnetic
	measurements,EM algorithms,focal underdetermined system solver,Focusing,FOCUSS
	algorithm,inverse problems,magnetic field measurement,magnetoencephalography,medical
	signal processing,MEG source imaging,minimum current estimation paradigm,neurophysiological
	evidence,neurophysiology,orientation constraints,Scalp,Sensor arrays,Signal
	processing algorithms,signal processing communities,smoothing methods,Spatial
	resolution,spatio-temporal smoothing,tomographic source reconstructions},
  pages = {24-27},
  owner = {afdidehf}
}

@article{Wohlberg2003,
  title = {Noise sensitivity of sparse signal representations: reconstruction 	error bounds for the inverse problem},
  volume = {51},
  issn = {1053-587X},
  doi = {10.1109/TSP.2003.819006},
  abstract = {Certain sparse signal reconstruction problems have been shown to have
	unique solutions when the signal is known to have an exact sparse
	representation. This result is extended to provide bounds on the
	reconstruction error when the signal has been corrupted by noise
	or is not exactly sparse for some other reason. Uniqueness is found
	to be extremely unstable for a number of common dictionaries.},
  timestamp = {2016-09-29T16:16:24Z},
  number = {12},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Wohlberg, B.},
  month = dec,
  year = {2003},
  keywords = {adaptive signal decomposition,adaptive signal processing,basis selection,Dictionaries,electroencephalography,Fourier
	transforms,inverse problem,inverse problems,Matching pursuit algorithms,Noise
	generators,noise sensitivity,random noise,signal dictionary,signal reconstruction,signal
	reconstruction,signal reconstruction error
	bounds,signal representation,signal representations,signal resolution,sparse
	signal representations,wavelet transforms},
  pages = {3053--3060},
  owner = {Fardin}
}

@article{Wright2010,
  title = {Sparse and Structured Optimization},
  timestamp = {2016-07-11T16:37:40Z},
  author = {Wright, Stephen},
  month = may,
  year = {2010},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {CSMP, Shanghai},
  owner = {afdidehf}
}

@article{Wright2009,
  title = {Sparse Reconstruction by Separable Approximation},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2016892},
  abstract = {Finding sparse approximate solutions to large underdetermined linear
	systems of equations is a common problem in signal/image processing
	and statistics. Basis pursuit, the least absolute shrinkage and selection
	operator (LASSO), wavelet-based deconvolution and reconstruction,
	and compressed sensing (CS) are a few well-known areas in which problems
	of this type appear. One standard approach is to minimize an objective
	function that includes a quadratic (lscr 2) error term added to a
	sparsity-inducing (usually lscr1) regularizater. We present an algorithmic
	framework for the more general problem of minimizing the sum of a
	smooth convex function and a nonsmooth, possibly nonconvex regularizer.
	We propose iterative methods in which each step is obtained by solving
	an optimization subproblem involving a quadratic term with diagonal
	Hessian (i.e., separable in the unknowns) plus the original sparsity-inducing
	regularizer; our approach is suitable for cases in which this subproblem
	can be solved much more rapidly than the original problem. Under
	mild conditions (namely convexity of the regularizer), we prove convergence
	of the proposed iterative algorithm to a minimum of the objective
	function. In addition to solving the standard lscr2-lscr1 case, our
	framework yields efficient solution techniques for other regularizers,
	such as an lscrinfin norm and group-separable regularizers. It also
	generalizes immediately to the case in which the data is complex
	rather than real. Experiments with CS problems show that our approach
	is competitive with the fastest known methods for the standard lscr2-lscr1
	problem, as well as being efficient on problems with other separable
	regularization terms.},
  timestamp = {2016-07-11T16:50:00Z},
  number = {7},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Wright, S.J. and Nowak, R.D. and Figueiredo, M.A.T.},
  month = jul,
  year = {2009},
  keywords = {compressed sensing,Deconvolution,iterative methods,Lasso,least
	absolute shrinkage and selection operator,least absolute
	shrinkage and selection operator,optimisation,Optimization,optimization
	subproblem,reconstruction,regularization,separable approximation,signal
	reconstruction,smooth convex function,sparse approximation,sparse
	matrices,Sparse reconstruction,standard lscr2-lscr1 problem,wavelet
	transform,wavelet transforms},
  pages = {2479-2493},
  owner = {Fardin}
}

@inproceedings{Wu2014,
  title = {BI-CosampSE: Block identification based compressive sampling matching 	pursuit for speech enhancement},
  doi = {10.1109/ACSSC.2014.7094690},
  abstract = {The conventional compressive sampling matching pursuit (CoSaMP) algorithm
	often produces isolated components (IFC) in the frequency domain,
	when it is used to recover sparse or compressible signals from their
	downsampled data. The IFCs in the frequency domain are perceived
	as musical noise, when CoSaMP is used for speech enhancement (SE).
	In this paper, we propose a novel method to tackle this problem by
	using a block based identification strategy (BIS) to seek the most
	prominent components in the observed data to update the sparse estimate
	of CoSaMP. The proposed method has been found to be very effective
	to reduce musical noise in speech enhancement, in combination with
	some time-frequency smoothing techniques (TFSTs). The perceptual
	quality of the enhanced signals is significantly improved.},
  timestamp = {2016-07-08T11:37:56Z},
  booktitle = {Signals, Systems and Computers, 2014 48th Asilomar Conference on},
  author = {Wu, Dalei and Zhu, Wei-Ping and Swamy, M.N.S.},
  month = nov,
  year = {2014},
  keywords = {BI-CosampSE,block based identification strategy,block identification,compressed
	sensing,compressive sampling matching pursuit,musical noise,Noise,Noise
	measurement,Noise reduction,smoothing methods,sparse estimate,Speech,speech enhancement,Speech
	enhancement,Speech processing,time-frequency smoothing
	techniques},
  pages = {1396-1399},
  owner = {afdidehf}
}

@inproceedings{Wu2011,
  title = {Compressive sensing of digital sparse signals},
  doi = {10.1109/WCNC.2011.5779350},
  abstract = {This paper discusses compressive sensing with digital sparse signals.
	The motivation is that most existing sparse signal recovery algorithms,
	like matching pursuit, convex relaxation and Bayesian framework,
	do not fully exploit the digital nature of signals when dealing with
	digital sparse signals, which result in certain performance losses.
	In this paper, we solve this problem via a permutation-based multi-dimensional
	sensing matrix and an iterative recovery algorithm with maximum likelihood
	(ML) local detectors. The sensing matrix considered consists of several
	sub-matrices, each composed of a random permutation matrix and a
	block-diagonal matrix. The measurements generated from the same permutation
	matrix are referred to as a dimension. The block-diagonal matrices
	allow the use of the low-complexity ML detector in each dimension,
	which best utilizes the digital nature of signals. The multi-dimensional
	structure of the sensing matrix enables information exchange between
	dimensions through an iterative process to achieve a near global-optimal
	estimation. Numerical results are used to show the rate-distortion
	performance of the proposed technique. It is shown that it can achieve
	much better rate-distortion than the existing approaches based on
	convex relaxation and Bayesian framework with digital source signals.},
  timestamp = {2016-07-08T12:01:02Z},
  booktitle = {Wireless Communications and Networking Conference (WCNC), 2011 IEEE},
  author = {Wu, Keying and Guo, Xiaoyong},
  month = mar,
  year = {2011},
  keywords = {Bayesian framework,Bayesian methods,Bayes methods,block-diagonal matrix,compressed
	sensing,compressive sensing,convex relaxation,Detectors,digital sparse
	signal,Distortion,iterative detection,iterative methods,iterative
	recovery algorithm,matching pursuit,matrix algebra,maximum likelihood,maximum
	likelihood detection,maximum likelihood local detector,near global-optimal
	estimation,permutation,permutation-based multi-dimensional sensing
	matrix,Quantization,Rate-distortion,rate-distortion performance,rate
	distortion theory,recovery,signal reconstruction,sparse matrices},
  pages = {1488-1492},
  owner = {afdidehf}
}

@inproceedings{Wu2011a,
  title = {Compressive Sensing with Sparse Measurement Matrices},
  doi = {10.1109/VETECS.2011.5956294},
  abstract = {This paper discusses compressive sensing with sparse measurement matrices.
	Sparse matrices have several attractive properties, like low computational
	complexity in both encoding and recovery, easy incremental updates
	to signals, and low storage requirement, etc. Typical examples of
	existing algorithms for sparse signal recovery with sparse measurement
	matrices include convex relaxation, matching pursuit, and Bayesian
	framework based approaches. In this paper, we propose an alternative
	technique to this problem. The proposed technique has a linear recovery
	complexity and relatively good empirical behavior. In this technique,
	we employ a permutation-based multi-dimensional measurement matrix,
	which is composed of several sub-matrices, each consisting of a block-diagonal
	matrix and a random permutation matrix. The measurement symbols generated
	from the same permutation matrix are referred to as a dimension.
	Such a measurement matrix brings some useful features to the measurement
	symbols. Fully exploiting these features enables us to design a simple
	and effective recovery algorithm. The proposed recovery algorithm
	employs an iterative process. In each iteration, the algorithm looks
	for measurement symbols with certain features, and uses such features
	to recover the source symbols related to these measurements. An iterative
	process is then applied, along with an interference cancellation
	operation, to reconstruct all source symbols gradually. The complexity
	of the proposed algorithm is relatively low, which grows linearly
	with the source signal length. Numerical results show that the proposed
	technique empirically offers a much lower sketch length than ?1-minimization-based
	convex relaxation and Bayesian framework based algorithms. It achieves
	the empirical lower bound of sketch length and linear recovery complexity
	at the same time.},
  timestamp = {2016-07-08T12:01:12Z},
  booktitle = {Vehicular Technology Conference (VTC Spring), 2011 IEEE 73rd},
  author = {Wu, Keying and Guo, Xiaoyong},
  month = may,
  year = {2011},
  keywords = {Bayesian framework,Bayesian methods,block-diagonal matrix,Cognitive
	radio,communication complexity,Complexity theory,compressed sensing,Compressive
	sensing,computational complexity,convex programming,encoding,interference
	cancellation,interference suppression,iterative methods,iterative
	process,linear recovery complexity,Matching pursuit algorithms,minimization-based
	convex relaxation,permutation-based multidimensional measurement
	matrix,random permutation matrix,Signal processing algorithms,sparse
	matrices,sparse measurement matrices,sparse signal recovery},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Wu2012,
  title = {Improved Image Reconstruction Based on Block Compressed Sensing},
  doi = {10.1109/SCET.2012.6342118},
  abstract = {Constrained by traditional sampling theory,it's difficult to obtain
	high resolution image and the sampling data is great. The theory
	compressed sensing combines the sampling and compressing together
	under the assumption that the signal is compressible or sparse in
	a certain sparse transform domain.Compressed sensing needs fewer
	measurements and it can successfully recover original signal using
	an optimization process,which will greatly reduce the complexity
	of sampling and calculation.Since traditional algorithm to sample
	the whole image is time-consuming and it requires huge storage space,we
	study block compressed sensing.According to the properties of coefficients,
	only the high-pass coefficients are measured,then the original image
	is reconstructed using the orthogonal matching pursuit method.Compared
	with the original algorithm, simulation result demonstrates that
	high resolution image can be obtained with the proposed algorithm,which
	reduces the sampling and storage data.The quality of the reconstruction
	image is greatly improved.},
  timestamp = {2016-07-08T12:48:15Z},
  booktitle = {Engineering and Technology (S-CET), 2012 Spring Congress on},
  author = {Wu, Qiaoling and Ni, Lin and He, Delong},
  month = may,
  year = {2012},
  keywords = {block compressed sensing,compressed sensing,Frequency measurement,high-pass
	coefficients,high resolution image,image matching,Image reconstruction,Image
	reconstruction,Image resolution,image sampling,improved image reconstruction,Matching
	pursuit algorithms,optimisation,optimization process,orthogonal matching
	pursuit method,sampling complexity reduction,sampling data,sampling
	theory,Sensors,Signal processing algorithms,sparse matrices,sparse
	transform domain,storage data,transforms},
  pages = {1-4},
  owner = {afdidehf}
}

@inproceedings{Wu2014a,
  title = {An adaptive transfer scheme based on sparse representation for figure-ground 	segmentation},
  doi = {10.1109/ICIP.2014.7025673},
  abstract = {Figure-ground segmentation benefits lots of tasks in the field of
	computer vision. Exemplar-based approaches are capable of performing
	segmenting automatically without user interaction. However, most
	of them adopt fixed parameters for all the target images, which blocks
	their segmentation performances. We present a novel sparse representation
	based transfer scheme to gain adaptive parameters automatically.
	The proposed scheme transfers the segmentation masks of some windows
	from training images to obtain the soft mask of the target window
	from any given test image, when the target window can be represented
	by the linear combination of those windows. On the challenging PASCAL
	VOC 2010 segmentation dataset, experimental results and comparisons
	with the state-of-the-art methods show the effectiveness of the proposed
	scheme.},
  timestamp = {2016-07-08T10:28:30Z},
  booktitle = {Image Processing (ICIP), 2014 IEEE International Conference on},
  author = {Wu, Xianyan and Han, Qi and Niu, Xiamu},
  month = oct,
  year = {2014},
  keywords = {adaptive transfer scheme,Computational modeling,computer vision,Conferences,figure-ground
	segmentation,Image segmentation,Labeling,PASCAL
	VOC 2010 segmentation dataset,PASCAL VOC 2010 segmentation
	dataset,Shape,sparse representation,Training,transfer scheme,transfer
	scheme},
  pages = {3327-3331},
  owner = {afdidehf}
}

@article{Wyner1979,
  title = {An analog scrambling scheme which does not expand bandwidth, Part 	II: Continuous time},
  volume = {25},
  issn = {0018-9448},
  doi = {10.1109/TIT.1979.1056071},
  abstract = {The techniques developed in Part I[1] for discrete-time analog scrambling
	are applied to the problem of scrambling band-limited continuous-time
	signals or waveforms. The idea behind the waveform scrambler is to
	sample the waveform (which is assumed to be band-limited) at a rate
	exceeding the Nyquist rate. The resulting sequence of samples is
	band-limited in the sense of Part I. The discrete-time scrambler
	described in Part I is applied to this sequence to produce a nearly
	band-limited scrambled sequence. A scrambled waveform is formed by
	modulating the amplitudes of a chain of pulses. This scrambled waveform
	can be transmitted over a band-limited channel, and the original
	unscrambled waveform can be recovered at the receiver.},
  timestamp = {2016-09-30T10:47:08Z},
  number = {4},
  journal = {IEEE Trans. Inf. Theory},
  author = {Wyner, A.D.},
  month = jul,
  year = {1979},
  keywords = {Amplitude modulation,Bandwidth,cryptography,Fourier transforms,Frequency,Privacy,Pulse
	modulation,Sampling methods,Signal sampling/reconstruction},
  pages = {415--425},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  annote = {Information Theory, IEEE Transactions on},
  owner = {afdidehf}
}

@article{Wyner1979a,
  title = {An analog scrambling scheme which does not expand bandwidth, Part 	I: Discrete time},
  volume = {25},
  issn = {0018-9448},
  doi = {10.1109/TIT.1979.1056050},
  abstract = {The problem of scrambling a discrete-time analog sequence for the
	purpose of privacy encoding is studied. A large family of linear
	orthogonal invertible scrambling transformations is described that
	result in a negligible expansion of bandwidth and can therefore serve
	as building blocks in a secure communication system.},
  timestamp = {2016-09-30T10:47:16Z},
  number = {3},
  journal = {Information Theory, IEEE Transactions on},
  author = {Wyner, A.D.},
  month = may,
  year = {1979},
  keywords = {Bandwidth,Communication systems,Convergence,Convolution,cryptography,encoding,Fourier
	transforms,Privacy,Stochastic processes},
  pages = {261--274},
  owner = {afdidehf}
}

@inproceedings{Xiao2015,
  title = {Object tracking algorithm based on HSV color histogram and block-sparse 	representation},
  doi = {10.1109/ChiCC.2015.7260229},
  abstract = {Sparse representation has been applied in tracking rapidly. However,
	most of these methods use the holistic model and grayscale image
	which result in insensitiveness to color information and ineffectiveness
	to deformation of non-rigid object. In this paper, we propose a robust
	and effective tracking method (SRH) based on the block-sparse representation
	(local information) and HSV color histogram (spatial information).
	This method not only keeps the merits of block-sparse representation
	in handling illumination changes and occlusions, but also adds the
	object color resolution and is not subjected to similar colors interference.
	It improves the accuracy and efficiency of tracking result. In addition,
	we calculate the weight of block-sparse representation and HSV color
	histogram and introduce a fusion method to fuse these two data. Furthermore,
	the occlusion handling reduces the influence of heavy block, the
	update strategy guarantees the tracker to adapt to the complex background
	and morphological changes of target which enhances the reliability
	of this method. Experimental results compared with several state-of-the-art
	trackers on challenging sequences demonstrate the robustness and
	accuracy of the proposed tracking algorithm.},
  timestamp = {2016-07-10T07:10:37Z},
  booktitle = {Control Conference (CCC), 2015 34th Chinese},
  author = {Xiao, Chi and Chen, Wenjie and Gao, Huilin},
  month = jul,
  year = {2015},
  keywords = {background changes,block-sparse representation,color information,color
	information,color interference,color
	interference,data fusion method,Dictionaries,Fasteners,feature fusion,feature
	fusion,grayscale image,grayscale
	image,Histograms,holistic model,HSV color histogram,illumination
	change handling,Image color analysis,image colour analysis,image
	fusion,image representation,Image resolution,lighting,morphological
	changes,nonrigid object deformation,object color resolution,object
	tracking,object tracking algorithm,occlusion handling,robust
	effective tracking method,robust effective
	tracking method,spatial information,SRH,target tracking,update strategy,update
	strategy},
  pages = {3826-3831},
  owner = {afdidehf}
}

@article{Xiong2013,
  title = {Sparse Spatio-Temporal Representation With Adaptive Regularized Dictionary 	Learning for Low Bit-Rate Video Coding},
  volume = {23},
  issn = {1051-8215},
  doi = {10.1109/TCSVT.2012.2221271},
  abstract = {For promising vision-based video coding on low-quality data, this
	paper proposes a sparse spatio-temporal representation with adaptive
	regularized dictionary learning and develops a low bit-rate video
	coding scheme. In a reversed-complexity Wyner-Ziv coding manner,
	it selects a subset of key frames to code at original resolution,
	while the rest are down sampled and reconstructed by a sparse spatio-temporal
	approximation using key frames as a training dataset. Since primitive
	patches (geometry) are of low dimensionality and can be well learned
	from the primitive patches across frames in a scale space, a video
	frame is divided into three layers: a primitive layer, a nonprimitive
	coarse layer, and a nonprimitive smooth layer. The multiscale differential
	feature representations are invertible to facilitate reconstruction
	with dictionary learning, and the target is formulated as an optimization
	problem by constructing a sparse representation of 2-D patches and
	3-D volumes over adaptive regularized dictionaries, a set of 2-D
	subdictionary pairs trained from primitive patches, and a 3-D dictionary
	trained from nonprimitive volumes. Specifically, the nonprimitive
	layer is constructed as volumes in to order keep it consistent along
	the motion trajectory, which enables sparse representations over
	a learned 3-D spatio-temporal dictionary. Through hierarchical bidirectional
	motion estimation and adaptive overlapped block motion compensation,
	the 3-D low-frequency and high-frequency dictionary pair is designed
	by the K-SVD algorithm to update the atoms for optimal sparse representation
	and convergence. In reconstruction, the lost high-frequency information
	of the down-sampled frames can be synthesized from the sparse spatio-temporal
	representation over the adaptive regularized dictionaries. Extensive
	experiments validate the compression efficiency of the proposed scheme
	versus H.264/AVC in terms of both objective and subjective comparisons.},
  timestamp = {2016-07-11T16:54:28Z},
  number = {4},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  author = {Xiong, Hongkai and Pan, Zhiming and Ye, Xinwei and Chen, Chang Wen},
  month = apr,
  year = {2013},
  keywords = {2-D patches,3-D dictionary,3-D volumes,adaptive overlapped block motion
	compensation,adaptive regularized dictionaries,adaptive regularized
	dictionary learning,Atom decomposition,bit-rate video coding scheme,Dictionaries,dictionary
	learning,H.264/AVC,Hafnium,hierarchical bidirectional motion estimation,high-frequency
	dictionary pair,Image edge detection,Image reconstruction,image representation,Image
	resolution,K-SVD algorithm,low bit-rate video coding,motion compensation,Motion
	estimation,multiscale differential feature representations,optimal
	sparse representation,primitive patch,primitive patches,reversed-complexity
	Wyner-Ziv coding,sparse representation,sparse spatio-temporal approximation,sparse
	spatio-temporal approximation,sparse spatio-temporal representation,sparse
	spatio-temporal representation,Training,video coding,video
	coding,vision-based video coding,vision-based
	video coding},
  pages = {710-728},
  owner = {afdidehf}
}

@inproceedings{Xu2013,
  title = {GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity},
  doi = {10.1109/ICCV.2013.419},
  abstract = {We study the problem of online subspace learning in the context of
	sequential observations involving structured perturbations. In online
	subspace learning, the observations are an unknown mixture of two
	components presented to the model sequentially - the main effect
	which pertains to the subspace and a residual/error term. If no additional
	requirement is imposed on the residual, it often corresponds to noise
	terms in the signal which were unaccounted for by the main effect.
	To remedy this, one may impose "structural" contiguity, which has
	the intended effect of leveraging the secondary terms as a covariate
	that helps the estimation of the subspace itself, instead of merely
	serving as a noise residual. We show that the corresponding online
	estimation procedure can be written as an approximate optimization
	process on a Grassmannian. We propose an efficient numerical solution,
	GOSUS, Grassmannian Online Subspace Updates with Structured-sparsity,
	for this problem. GOSUS is expressive enough in modeling both homogeneous
	perturbations of the subspace and structural contiguities of outliers,
	and after certain manipulations, solvable via an alternating direction
	method of multipliers (ADMM). We evaluate the empirical performance
	of this algorithm on two problems of interest: online background
	subtraction and online multiple face tracking, and demonstrate that
	it achieves competitive performance with the state-of-the-art in
	near real time.},
  timestamp = {2016-07-08T12:38:35Z},
  booktitle = {Computer Vision (ICCV), 2013 IEEE International Conference on},
  author = {Xu, Jia and Ithapu, V.K. and Mukherjee, L. and Rehg, J.M. and Singh, V.},
  month = dec,
  year = {2013},
  keywords = {ADMM,alternating direction method of multipliers,approximate optimization
	process,Estimation,Face,face recognition,GOSUS,Grassmannian online
	subspace updates,learning (artificial intelligence),lighting,Manifold
	optimization,object tracking,online background subtraction,online
	estimation procedure,online multiple face tracking,online multiple
	face tracking,online subspace learning,Optimization,Principal
	component analysis,principal component
	analysis,Robustness,sequential observations,Structured Sparsity,structured
	sparsity,Vectors},
  pages = {3376-3383},
  owner = {afdidehf}
}

@article{Xu2007,
  title = {Lp Norm Iterative Sparse Solution for EEG Source Localization},
  volume = {54},
  issn = {0018-9294},
  doi = {10.1109/TBME.2006.886640},
  abstract = {How to localize the neural electric activities effectively and precisely
	from the scalp EEG recordings is a critical issue for clinical neurology
	and cognitive neuroscience. In this paper, based on the spatial sparse
	assumption of brain activities, proposed is a novel iterative EEG
	source imaging algorithm, Lp norm iterative sparse solution (LPISS).
	In LPISS, the lp(ples1) norm constraint for sparse solution is integrated
	into the iterative weighted minimum norm solution of the underdetermined
	EEG inverse problem, and it is the constraint and the iteratively
	renewed weight that forces the inverse problem to converge to a sparse
	solution effectively. The conducted simulation studies with comparison
	to LORETA and FOCUSS for various dipoles configurations confirmed
	the validation of LPISS for sparse EEG source localization. Finally,
	LPISS was applied to a real evoked potential collected in a study
	of inhibition of return (IOR), and the result was consistent with
	the previously suggested activated areas involved in an IOR process},
  timestamp = {2016-07-09T19:59:27Z},
  number = {3},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Xu, Peng and Tian, Y. and Chen, Huafu and Yao, Dezhong},
  month = mar,
  year = {2007},
  keywords = {Algorithms,Brain,Brain Mapping,Brain modeling,clinical neurology,Cognition,cognitive
	neuroscience,Computer-Assisted,Computer Simulation,Diagnosis,EEG
	source imaging,EEG source localization,electroencephalography,evoked
	potential,Evoked Potentials,Focusing,FOCUSS,Humans,inhibition of
	return,inverse problem,inverse problems,Iterative algorithms,Iterative
	algorithms,iterative EEG source imaging algorithm,iterative
	EEG source imaging algorithm,iterative methods,iterative
	weighted minimum norm solution,iterative weighted
	minimum norm solution,LORETA,Lp norm iterative sparse solution,medical
	computing,Models,Nervous system,neural electric activities,Neural
	Inhibition,Neurological,neurophysiology,Neuroscience,Scalp,Space
	technology,sparse constraint,Spatial resolution,underdetermined system,Visual,Visual
	Perception,weighted minimum norm solution},
  pages = {400-409},
  owner = {Fardin}
}

@inproceedings{Xu2010,
  title = {A block-based compressed sensing method for underdetermined blind 	speech separation incorporating binary mask},
  doi = {10.1109/ICASSP.2010.5494935},
  abstract = {A block-based compressed sensing approach coupled with binary time-frequency
	masking is presented for the underdetermined speech separation problem.
	The proposed algorithm consists of multiple steps. First, the mixed
	signals are segmented to a number of blocks. For each block, the
	unknown mixing matrix is estimated in the transform domain by a clustering
	algorithm. Using the estimated mixing matrix, the sources are recovered
	by a compressed sensing approach. The coarsely separated sources
	are then used to estimate the time-frequency binary masks which are
	further applied to enhance the separation performance. The separated
	source components from all the blocks are concatenated to reconstruct
	the whole signal. Numerical experiments are provided to show the
	improved separation performance of the proposed algorithm, as compared
	with two recent approaches. The block-based operation has the advantage
	in improving considerably the computational efficiency of the compressed
	sensing algorithm without degrading its separation performance.},
  timestamp = {2016-07-08T10:06:25Z},
  booktitle = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International 	Conference on},
  author = {Xu, Tao and Wang, Wenwu},
  month = mar,
  year = {2010},
  keywords = {binary time-frequency mask,binary time-frequency masking,block-based
	compressed sensing method,block-based processing,clustering algorithm,coarsely
	separated sources,compressed sensing,Compressed Sensing (CS),matrix
	algebra,mixing matrix,pattern clustering,sparse representation,Speech
	coding,Underdetermined blind source separation (BSS),underdetermined
	blind speech separation},
  pages = {2022-2025},
  owner = {afdidehf}
}

@article{XXXxxxx,
  title = {Critical Issues in EEG/MEG Research: Source Localization},
  timestamp = {2016-07-08T12:06:19Z},
  author = {{{XXX}}},
  year = {xxxx},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{XXXxxxxc,
  title = {What is the lasso in regression analysis?},
  timestamp = {2016-07-11T17:11:43Z},
  author = {{{XXX}}},
  year = {xxxx},
  owner = {Fardin}
}

@article{XXXxxxxd,
  title = {The Lasso Page: L1-constrained fitting for statistics and data mining},
  timestamp = {2016-07-11T17:01:21Z},
  author = {{{XXX}}},
  year = {xxxx},
  owner = {Fardin}
}

@article{XXXxxxxe,
  title = {Vector and Matrix Norms},
  timestamp = {2016-07-11T17:10:53Z},
  author = {{{XXX}}},
  year = {xxxx},
  owner = {Fardin}
}

@article{XXXxxxxf,
  title = {Vector Norms and Matrix Norms},
  timestamp = {2016-07-11T17:11:11Z},
  author = {{{XXX}}},
  year = {xxxx},
  owner = {Fardin}
}

@article{XXXxxxxg,
  title = {Norms and Condition Numbers},
  timestamp = {2016-07-10T07:00:38Z},
  author = {{{XXX}}},
  year = {xxxx},
  owner = {Fardin}
}

@article{XXXxxxxh,
  title = {Introduction to EEG and MEG},
  timestamp = {2016-07-08T12:50:31Z},
  author = {{{XXX}}},
  year = {xxxx},
  owner = {Fardin}
}

@book{XXX2014a,
  title = {Curry 7 User guide: Multi-modal Neuroimaging},
  timestamp = {2016-07-08T12:06:37Z},
  author = {{{XXX}}},
  year = {2014},
  owner = {Fardin}
}

@article{Yaghoobi2009,
  title = {Dictionary Learning for Sparse Approximations With the Majorization 	Method},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2009.2016257},
  abstract = {In order to find sparse approximations of signals, an appropriate
	generative model for the signal class has to be known. If the model
	is unknown, it can be adapted using a set of training samples. This
	paper presents a novel method for dictionary learning and extends
	the learning problem by introducing different constraints on the
	dictionary. The convergence of the proposed method to a fixed point
	is guaranteed, unless the accumulation points form a continuum. This
	holds for different sparsity measures. The majorization method is
	an optimization method that substitutes the original objective function
	with a surrogate function that is updated in each optimization step.
	This method has been used successfully in sparse approximation and
	statistical estimation [ e.g., expectation-maximization (EM)] problems.
	This paper shows that the majorization method can be used for the
	dictionary learning problem too. The proposed method is compared
	with other methods on both synthetic and real data and different
	constraints on the dictionary are compared. Simulations show the
	advantages of the proposed method over other currently available
	dictionary learning methods not only in terms of average performance
	but also in terms of computation time.},
  timestamp = {2016-07-08T12:11:23Z},
  number = {6},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Yaghoobi, M. and Blumensath, T. and Davies, M.E.},
  month = jun,
  year = {2009},
  keywords = {approximation theory,Block relaxation methods,Constrained optimization,convergence
	method,convergence of numerical methods,Dictionaries,dictionary learning,dictionary
	learning,majorization method,majorization methods,optimisation,optimization
	method,signal processing,sparse approximation,sparse
	approximation,sparse matrices,sparse
	matrices,statistical analysis,statistical
	analysis,statistical estimation,surrogate function optimization method,surrogate
	function optimization method,surrogate function
	optimization method},
  pages = {2178-2191},
  owner = {afdidehf}
}

@inproceedings{Yamazaki2014,
  title = {Access-averse framework for computing low-rank matrix approximations},
  doi = {10.1109/BigData.2014.7004374},
  abstract = {Low-rank matrix approximations play important roles in many statistical,
	scientific, and engineering applications. To compute such approximations,
	different algorithms have been developed by researchers from a wide
	range of areas including theoretical computer science, numerical
	linear algebra, statistics, applied mathematics, data analysis, machine
	learning, and physical and biological sciences. In this paper, to
	combine these efforts, we present an �access-averse� framework
	which encapsulates some of the existing algorithms for computing
	a truncated singular value decomposition (SVD). This framework not
	only allows us to develop software whose performance can be tuned
	based on domain specific knowledge, but it also allows a user from
	one discipline to test an algorithm from another, or to combine the
	techniques from different algorithms. To demonstrate this potential,
	we implement the framework on multicore CPUs with multiple GPUs and
	compare the performance of two representative algorithms, blocked
	variants of matrix power and Lanczos methods. Our performance studies
	with large-scale graphs from real applications demonstrate that,
	when combined with communication-avoiding and thick-restarting techniques,
	the Lanczos method can be competitive with the power method, which
	is one of the most popular methods currently used for these applications.
	InIn addition, though we only focus on the truncated SVDs, the two
	computational kernels used in our studies, the sparse-matrix dense-matrix
	multiply and tall-skinny QR factorization, are fundamental building
	blocks for computing low-rank approximations with other objectives.
	Hence, our studies may have a greater impact beyond the truncated
	SVDs.},
  timestamp = {2016-07-08T10:07:52Z},
  booktitle = {Big Data (Big Data), 2014 IEEE International Conference on},
  author = {Yamazaki, I. and Mary, T. and Kurzak, J. and Tomov, S. and Dongarra, J.},
  month = oct,
  year = {2014},
  keywords = {access-averse framework,Approximation algorithms,Approximation methods,approximation
	theory,communication-avoiding techniques,computational kernels,Convergence,domain
	specific knowledge,GPU,graphics processing units,Kernel,Lanczos method,low-rank
	matrix approximation computation,mathematics computing,matrix multiplication,matrix
	power blocked variants,multicore CPU,multiprocessing systems,power
	method,Singular Value Decomposition,Software algorithms,sparse matrices,sparse
	matrices,sparse-matrix dense-matrix multiplication,SVD,tall-skinny
	QR factorization,thick-restarting techniques,truncated singular value
	decomposition,Vectors},
  pages = {70-77},
  owner = {afdidehf}
}

@article{Yang2015,
  title = {Estimation of Signal-Dependent Noise Level Function in Transform 	Domain via a Sparse Recovery Model},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2405417},
  abstract = {This paper proposes a novel algorithm to estimate the noise level
	function (NLF) of signal-dependent noise (SDN) from a single image
	based on the sparse representation of NLFs. Noise level samples are
	estimated from the high-frequency discrete cosine transform (DCT)
	coefficients of nonlocal-grouped low-variation image patches. Then,
	an NLF recovery model based on the sparse representation of NLFs
	under a trained basis is constructed to recover NLF from the incomplete
	noise level samples. Confidence levels of the NLF samples are incorporated
	into the proposed model to promote reliable samples and weaken unreliable
	ones. We investigate the behavior of the estimation performance with
	respect to the block size, sampling rate, and confidence weighting.
	Simulation results on synthetic noisy images show that our method
	outperforms existing state-of-the-art schemes. The proposed method
	is evaluated on real noisy images captured by three types of commodity
	imaging devices, and shows consistently excellent SDN estimation
	performance. The estimated NLFs are incorporated into two well-known
	denoising schemes, nonlocal means and BM3D, and show significant
	improvements in denoising SDN-polluted images.},
  timestamp = {2016-07-08T12:24:52Z},
  number = {5},
  journal = {Image Processing, IEEE Transactions on},
  author = {Yang, Jingyu and Gan, Ziqiao and Wu, Zhaoyang and Hou, Chunping},
  month = may,
  year = {2015},
  keywords = {block size,BM3D,commodity imaging devices,confidence levels,confidence
	weighting,DCT coefficients,discrete cosine transforms,Estimation,high-frequency
	discrete cosine transform,image denoising,image representation,image
	sampling,NLF recovery model,Noise,noise estimation,Noise level,noise
	level function,Noise reduction,nonlocal-grouped low-variation image
	patches,nonlocal means,sampling rate,SDN estimation performance,SDN-polluted
	image denoising scheme,signal-dependent noise,signal-dependent noise
	level function estimation,Silicon compounds,sparse NLF representation,sparse
	recovery model,sparse representation,synthetic noisy images,transform
	domain},
  pages = {1561-1572},
  owner = {afdidehf}
}

@article{Yang2009,
  title = {Ways to sparse representation: An overview},
  volume = {52},
  issn = {1009-2757},
  doi = {10.1007/s11432-009-0045-5},
  abstract = {Many algorithms have been proposed to find sparse representations
	over redundant dictionaries or transforms. This paper gives an overview
	of these algorithms by classifying them into three categories: greedy
	pursuit algorithms, p norm regularization based algorithms, and
	iterative shrinkage algorithms. We summarize their pros and cons
	as well as their connections. Based on recent evidence, we conclude
	that the algorithms of the three categories share the same root:
	p norm regularized inverse problem. Finally, several topics that
	deserve further investigation are also discussed.},
  language = {English},
  timestamp = {2016-07-11T17:11:40Z},
  number = {4},
  journal = {Science in China Series F: Information Sciences},
  author = {Yang, JingYu and Peng, YiGang and Xu, WenLi and Dai, QiongHai},
  year = {2009},
  keywords = {Basis pursuit,iterative shrinkage,matching pursuit,nonlinear approximation,redundant
	dictionary,redundant transform,sparse representation},
  pages = {695-703},
  owner = {Fardin}
}

@inproceedings{Yang2012,
  title = {Estimation of signal-dependent sensor noise via sparse representation 	of noise level functions},
  doi = {10.1109/ICIP.2012.6466949},
  abstract = {This paper proposes a noise estimation method for signal-dependent
	sensor noise based on sparse representation of noise level functions
	(NLFs). Homogeneous blocks are detected by an image structure analyzer,
	and grouped to estimate noise levels for various image intensities
	with confidences. The noise level function is recovered from the
	incomplete and noisy estimated samples by solving its sparse representation
	under a trained basis. Experimental results show that our proposed
	method accurately estimates NLFs for both smooth and highly-textured
	images over various noise levels.},
  timestamp = {2016-07-08T12:25:04Z},
  booktitle = {Image Processing (ICIP), 2012 19th IEEE International Conference 	on},
  author = {Yang, Jingyu and Wu, Zhaoyang and Hou, Chunping},
  month = sep,
  year = {2012},
  keywords = {Charge coupled devices,Dictionaries,Estimation,homogeneous block detection,Image
	color analysis,image representation,image sensors,image structure
	analyzer,image texture,Noise,noise estimation,Noise level,noise level
	function,noise level functions,Noise measurement,signal-dependent
	noise,signal-dependent sensor noise estimation,sparse representation},
  pages = {673-676},
  owner = {afdidehf}
}

@inproceedings{Yang2015a,
  title = {Network reconstruction based on compressive sensing},
  doi = {10.1109/ChiCC.2015.7259961},
  abstract = {To identify the structure of networks is essential for analysis of
	complex networks. This paper transforms network reconstruction to
	be a signal recovery problem by means of compressive sensing. In
	the literature, the sensing matrix is determined by the network dynamic
	and measured states of nodes, which might violates the restriction
	on the coherence of the sensing matrix for exact recovery. This paper
	proposes random projection and zero component analysis to preprocess
	the sensing matrix in order to reduce the coherence of the sensing
	matrix. These two data whitening techniques are implemented in three
	different ways with different space complexity required, performing
	transformation on diagonal blocks, on multiple diagonal blocks and
	on the whole of the sensing matrix. Numerical simulations suggest
	that the latter method are effective to improve the quality of the
	reconstructed networks and comparisons are made among these methods
	and the ways they are implemented.},
  timestamp = {2016-07-10T06:49:19Z},
  booktitle = {Control Conference (CCC), 2015 34th Chinese},
  author = {Yang, Jiajun and Yang, Guanxue},
  month = jul,
  year = {2015},
  keywords = {Accuracy,Coherence,Complex network,complex networks,compressed sensing,compressed
	sensing,compressive sensing,covariance matrices,Data whitening,Data
	whitening,Image reconstruction,Image
	reconstruction,matrix algebra,network dynamic,Network reconstruction,Network
	reconstruction,network reconstruction quality,network
	reconstruction quality,random projection,Random
	projection,sensing matrix,sensing
	matrix,Sensors,signal reconstruction,signal
	reconstruction,signal recovery problem,signal
	recovery problem,sparse matrices,sparse
	matrices,zero component
	analysis,zero component analysis},
  pages = {2123-2128},
  owner = {afdidehf}
}

@inproceedings{Yang2015b,
  title = {Parameter estimation of incoherently distributed source based on 	block sparse Bayesian learning},
  doi = {10.1109/ICDSP.2015.7251951},
  abstract = {In practical array signal processing applications, the performance
	of DOA (direction-of-arrival) estimation methods is known to degrade
	severely in the presence of angular spread. In this paper, a new
	approach of estimating parameter via block sparse Bayesian learning
	is proposed for multiple incoherently distributed sources. Unlike
	traditional subspace based methods, the new technique makes use of
	a sparse representation of the received data with a perturbed overcomplete
	dictionary. Specifically, after using the temporal correlation between
	snapshots, the central DOA is estimated by using a Bayesian learning
	algorithm. The new method is able to mitigate the influence of angular
	spread, and its performance is demonstrated from numerical simulations.},
  timestamp = {2016-07-10T07:25:29Z},
  booktitle = {Digital Signal Processing (DSP), 2015 IEEE International Conference 	on},
  author = {Yang, Xuemin and Li, Guangjun and Zheng, Zhi and Ko, Chi Chung and Yeo, Tat Soon},
  month = jul,
  year = {2015},
  keywords = {angular spread,Arrays,array signal processing,Bayes methods,Bayes
	methods,block sparse Bayesian learning,central DOA,Dictionaries,direction-of-arrival,Direction-of-arrival
	estimation,direction-of-arrival estimation methods,DOA estimation
	methods,Estimation,incoherently distributed source,learning (artificial
	intelligence),multiple incoherently distributed sources,parameter
	estimation,perturbed overcomplete dictionary,practical array signal
	processing applications,Sensors,Signal processing algorithms,signal
	representation,sparse Bayesian learning,sparse representation,temporal
	correlation},
  pages = {633-637},
  owner = {afdidehf}
}

@article{Yang2015c,
  title = {Robust Low-Rank Tensor Recovery With Regularized Redescending M-Estimator},
  volume = {PP},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2015.2465178},
  abstract = {This paper addresses the robust low-rank tensor recovery problems.
	Tensor recovery aims at reconstructing a low-rank tensor from some
	linear measurements, which finds applications in image processing,
	pattern recognition, multitask learning, and so on. In real-world
	applications, data might be contaminated by sparse gross errors.
	However, the existing approaches may not be very robust to outliers.
	To resolve this problem, this paper proposes approaches based on
	the regularized redescending M-estimators, which have been introduced
	in robust statistics. The robustness of the proposed approaches is
	achieved by the regularized redescending M-estimators. However, the
	nonconvexity also leads to a computational difficulty. To handle
	this problem, we develop algorithms based on proximal and linearized
	block coordinate descent methods. By explicitly deriving the Lipschitz
	constant of the gradient of the data-fitting risk, the descent property
	of the algorithms is present. Moreover, we verify that the objective
	functions of the proposed approaches satisfy the Kurdyka-?ojasiewicz
	property, which establishes the global convergence of the algorithms.
	The numerical experiments on synthetic data as well as real data
	verify that our approaches are robust in the presence of outliers
	and still effective in the absence of outliers.},
  timestamp = {2016-07-10T08:07:38Z},
  number = {99},
  journal = {Neural Networks and Learning Systems, IEEE Transactions on},
  author = {Yang, Y. and Feng, Y. and Suykens, J.A.K.},
  year = {2015},
  keywords = {block coordinate descent,Convergence,global convergence,least squares
	approximations,matrix decomposition,Noise,nonconvexity,Principal
	component analysis,redescending M-estimator,Robustness,robust tensor
	recovery.,Tensile stress},
  pages = {1-1},
  owner = {afdidehf}
}

@article{Yao2005,
  title = {Evaluation of different cortical source localization methods using 	simulated and experimental EEG data},
  volume = {25},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2004.11.036},
  abstract = {Different cortical source localization methods have been developed
	to directly link the scalp potentials with the cortical activities.
	Up to now, these methods are the only possible solution to noninvasively
	investigate cortical activities with both high spatial and time resolutions.
	However, the application of these methods is hindered by the fact
	that they have not been rigorously evaluated nor compared. In this
	paper, the performances of several source localization methods (moving
	dipoles, minimum Lp norm, and low resolution tomography (LRT) with
	Lp norm, p equal to 1, 1.5, and 2) were evaluated by using simulated
	scalp \{EEG\} data, scalp somatosensory evoked potentials (SEPs),
	and upper limb motor-related potentials (MRPs) obtained on human
	subjects (all with 163 scalp electrodes). By using simulated \{EEG\}
	data, we first evaluated the source localization ability of the above
	methods quantitatively. Subsequently, the performance of the various
	methods was evaluated qualitatively by using experimental \{SEPs\}
	and MRPs. Our results show that the overall \{LRT\} Lp norm method
	with p equal to 1 has a better source localization ability than any
	of the other investigated methods and provides physiologically meaningful
	reconstruction results. Our evaluation results provide useful information
	for choosing cortical source localization approaches for future EEG/MEG
	studies.},
  timestamp = {2016-07-08T12:26:14Z},
  number = {2},
  journal = {NeuroImage},
  author = {Yao, Jun and Dewald, Julius P. A.},
  year = {2005},
  keywords = {Brain,imaging},
  pages = {369 - 382},
  owner = {Fardin}
}

@inproceedings{Yap2011,
  title = {The Restricted Isometry Property for block diagonal matrices},
  doi = {10.1109/CISS.2011.5766142},
  abstract = {In compressive sensing (CS), the Restricted Isometry Property (RIP)
	is a powerful condition on measurement operators which ensures robust
	recovery of sparse vectors is possible from noisy, undersampled measurements
	via computationally tractable algorithms. Early papers in CS showed
	that Gaussian random matrices satisfy the RIP with high probability,
	but such matrices are usually undesirable in practical applications
	due to storage limitations, computational considerations, or the
	mismatch of such matrices with certain measurement architectures.
	To alleviate some or all of these difficulties, recent research efforts
	have focused on structured random matrices. In this paper, we study
	block diagonal measurement matrices where each block on the main
	diagonal is itself a Gaussian random matrix. The main result of this
	paper shows that such matrices can indeed satisfy the RIP but that
	the requisite number of measurements depends on the coherence of
	the basis in which the signals are sparse. In the best case-for signals
	that are sparse in the frequency domain-these matrices perform nearly
	as well as dense Gaussian random matrices despite having many fewer
	nonzero entries.},
  timestamp = {2016-07-11T17:04:23Z},
  booktitle = {Information Sciences and Systems (CISS), 2011 45th Annual Conference 	on},
  author = {Yap, Han Lun and Eftekhari, Armin and Wakin, M.B. and Rozell, C.J.},
  month = mar,
  year = {2011},
  keywords = {block diagonal matrices,block diagonal measurement matrices,Coherence,Compressive
	sensing,frequency domain,frequency-domain analysis,Frequency domain
	analysis,Gaussian processes,Gaussian random matrices,matrix algebra,measurement
	architectures,Noise measurement,random processes,Random variables,restricted
	isometry property,Sensors,signal reconstruction,sparse matrices,sparse
	vectors recovery,structured random matrices,Vectors},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Ye2014,
  title = {Sparse representation in electrical resistance tomography based on 	extended sensitivity matrix},
  doi = {10.1109/IST.2014.6958443},
  abstract = {Electrical resistance tomography is a soft-field tomography technique,
	i.e. the electrical field is changed everywhere in the sensing area
	with the change of conductivity in any pixel. To improve the image
	quality, an extended sensitivity matrix is designed in this paper.
	The base conductivity elements in the extended sensitivity matrix
	are consisted of a series of blocks with different number of pixels
	at all possible locations in the sensing region. Based on the new
	sensitivity matrix, a sparse representation method is implemented
	to reconstruct the conductivity distribution of cross-sectional area.
	Simulation results show that the proposed method based on the extended
	sensitivity matrix can reconstruct the image with a high quality.},
  timestamp = {2016-07-11T16:51:57Z},
  booktitle = {Imaging Systems and Techniques (IST), 2014 IEEE International Conference 	on},
  author = {Ye, Jiamin and Wang, Haigang and Qiu, Guizhi and Yang, Wuqiang},
  month = oct,
  year = {2014},
  keywords = {base conductivity elements,Conductivity,conductivity distribution
	reconstruction,cross-sectional area,electrical resistance tomography,Electrodes,extended
	sensitivity matrix,image quality improvement,Image reconstruction,image
	representation,matrix algebra,resistance sensor,Sensitivity,Sensors,soft-field
	tomography technique,sparse matrices,sparse representation method,Tomography},
  pages = {43-47},
  owner = {afdidehf}
}

@article{Yeh2013,
  title = {Sparse Solution of Fiber Orientation Distribution Function by Diffusion 	Decomposition},
  volume = {8},
  doi = {10.1371/journal.pone.0075747},
  abstract = {Fiber orientation is the key information in diffusion tractography.
	Several deconvolution methods have been proposed to obtain fiber
	orientations by estimating a fiber orientation distribution function
	(ODF). However, the L2 regularization used in deconvolution often
	leads to false fibers that compromise the specificity of the results.
	To address this problem, we propose a method called diffusion decomposition,
	which obtains a sparse solution of fiber ODF by decomposing the diffusion
	ODF obtained from q-ball imaging (QBI), diffusion spectrum imaging
	(DSI), or generalized q-sampling imaging (GQI). A simulation study,
	a phantom study, and an in-vivo study were conducted to examine the
	performance of diffusion decomposition. The simulation study showed
	that diffusion decomposition was more accurate than both constrained
	spherical deconvolution and ball-and-sticks model. The phantom study
	showed that the angular error of diffusion decomposition was significantly
	lower than those of constrained spherical deconvolution at 30u crossing
	and ball-and-sticks model at 60u crossing. The invivo study showed
	that diffusion decomposition can be applied to QBI, DSI, or GQI,
	and the resolved fiber orientations were consistent regardless of
	the diffusion sampling schemes and diffusion reconstruction methods.
	The performance of diffusion decomposition was further demonstrated
	by resolving crossing fibers on a 30-direction QBI dataset and a
	40- direction DSI dataset. In conclusion, diffusion decomposition
	can improve angular resolution and resolve crossing fibers in datasets
	with low SNR and substantially reduced number of diffusion encoding
	directions. These advantages may be valuable for human connectome
	studies and clinical research.},
  timestamp = {2016-07-11T16:53:23Z},
  number = {10},
  journal = {PLoS One},
  author = {Yeh, Fang-Cheng and Tseng, Wen-Yih Isaac},
  month = oct,
  year = {2013},
  owner = {Fardin}
}

@article{Yi2015,
  title = {Invariance of the spark, NSP order and RIP order under elementary 	transformations of matrices},
  volume = {abs/1502.02874},
  abstract = {The theory of compressed sensing tells us that recovering all k-sparse
	signals requires a sensing matrix to satisfy that its spark is greater
	than 2k, or its order of the null space property (NSP) or the restricted
	isometry property (RIP) is 2k or above. If we perform elementary
	row or column operations on the sensing matrix, what are the changes
	of its spark, NSP order and RIP order? In this paper, we study this
	problem and discover that these three quantitative indexes of sensing
	matrices all possess invariance under all matrix elementary transformations
	except column-addition ones. Putting this result in form of matrix
	products, we get the types of matrices which multiply a sensing matrix
	and make the products still have the same properties of sparse recovery
	as the sensing matrix. According to these types of matrices, we made
	an interesting discovery that sensing matrices with deterministic
	constructions do not possess the property universality which belongs
	to sensing matrices with random constructions.},
  timestamp = {2016-07-08T12:51:06Z},
  journal = {CoRR},
  author = {Yi, Jiawang and Tan, Guanzheng},
  year = {2015},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/YiT15},
  owner = {afdidehf}
}

@inproceedings{Yin2012,
  title = {Application of improved BOMP algorithm in face recognition},
  doi = {10.1109/ICCSNT.2012.6525937},
  abstract = {When the group sparse representation is used to face recognition,
	the same samples take participate in representation the test sample
	at the same time. The original method ignored the correlation between
	the samples. To solve this problem, an improved block orthogonal
	matching pursuit algorithm is presented. The proposed algorithm uses
	the coherent coefficient of the samples as a parameter, setting the
	proper threshold value to select sample discrimination. Therefore,
	the reconstruction of the algorithm is optimized. Experiments on
	the Yale B database show that the recognition rate of improved algorithm
	is higher than the original one. The experiment results verify the
	validity of the proposed algorithm.},
  timestamp = {2016-07-08T11:26:58Z},
  booktitle = {Computer Science and Network Technology (ICCSNT), 2012 2nd International 	Conference on},
  author = {Yin, Aihan and Jiang, Huiming and Zhang, Qingmiao},
  month = dec,
  year = {2012},
  keywords = {algorithm reconstruction,block othogonal matching pursuit,coherent
	coefficient,face recognition,group sparse representation,Image reconstruction,Image
	reconstruction,image representation,image
	representation,improved block orthogonal matching pursuit algorithm,improved block orthogonal matching
	pursuit algorithm,improved BOMP algorithm,improved
	BOMP algorithm,iterative methods,sample discrimination,sample
	discrimination,threshold value,threshold
	value,visual databases,Yale B database},
  pages = {275-278},
  owner = {afdidehf}
}

@article{Yin2013,
  title = {Sparse Optimization - Lecture: Sparse Recovery Guarantees},
  timestamp = {2016-07-11T16:49:52Z},
  author = {Yin, Wotao},
  month = jul,
  year = {2013},
  annote = {PPT, read},
  annote = {PPT, read},
  annote = {PPT, read},
  annote = {PPT, read},
  annote = {PPT, read},
  owner = {Fardin}
}

@article{You2015,
  title = {Geometric Conditions for Subspace-Sparse Recover},
  volume = {37},
  abstract = {Given a dictionary  and a signal  = x generated by a few linearly
	independent columns of , classical sparse recovery theory deals
	with the problem of uniquely recovering the sparse representation
	x of . In this work, we consider the more general case where  lies
	in a lowdimensional subspace spanned by a few columns of , which
	are possibly linearly dependent. In this case, x may not unique,
	and the goal is to recover any subset of the columns of  that spans
	the subspace containing . We call such a representation x subspace-sparse.
	We study conditions under which existing pursuit methods recover
	a subspace-sparse representation. Such conditions reveal important
	geometric insights and have implications for the theory of classical
	sparse recovery as well as subspace clustering.},
  timestamp = {2016-09-29T15:32:10Z},
  journal = {Proceedings of the 32 nd International Conference on Machine Learning},
  author = {You, Chong and Vidal, Ren\'{e}},
  year = {2015},
  owner = {Fardin}
}

@inproceedings{Yu2014,
  title = {Blurred license plate recognition via sparse representations},
  doi = {10.1109/ICIEA.2014.6931433},
  abstract = {Blurred license plate recognition is one of the most challenging problems
	in computer vision. A novel license plate recognition framework based
	on no-negative sparse representation is proposed. In the proposed
	algorithm, the testing image is represented as a no-negative sparse
	linear combination of the dictionary, which is preprocessed from
	standard templates. The proposed framework include two stage: singe
	character estimation via no-negative block correlation and multiple-characters
	no-negative sparse representations optimization for refining the
	recognition results, which simultaneously fulfil the license plate
	de-blurring and recognition. The experimental results show that it
	can achieve a high recognition rate.},
  timestamp = {2016-07-08T11:48:52Z},
  booktitle = {Industrial Electronics and Applications (ICIEA), 2014 IEEE 9th Conference 	on},
  author = {Yu, A.H. and Bai, H. and Jiang, Q.R. and Zhu, Z.H. and Huang, C.G. and Hou, B.P.},
  month = jun,
  year = {2014},
  keywords = {blur kernel,blurred license plate recognition,Character recognition,computer vision,Computer
	vision,Correlation,de-blurring,Estimation,Image recognition,image
	representation,image restoration,Kernel,license plate deblurring,license
	plate recognition (LPR),Licenses,Noise,no-negative block correlation,no-negative
	sparse representation,No-negative sparse representations,Object Recognition,single
	character estimation,traffic engineering computing},
  pages = {1657-1661},
  owner = {afdidehf}
}

@article{Yu2010,
  title = {Fast Source Reconstruction for Bioluminescence Tomography Based on 	Sparse Regularization},
  volume = {57},
  issn = {0018-9294},
  doi = {10.1109/TBME.2010.2059024},
  abstract = {Bioluminescence tomography (BLT) is an inherent ill-posed inverse
	problem to reconstruct the internal source in 3-D with limited measurements
	on the external surface. In most BLT studies so far, a relatively
	small permissible source region or multispectral approach is typically
	used to enhance the stability or quality of the solution. In this
	letter, considering the sparsity characteristic of the light source,
	BLT is reformulated as a least absolute shrinkage and selection operator
	(LASSO) problem with l1 regularization, and then, a fast reconstruction
	algorithm named as stagewise fast LASSO is proposed for solving this
	problem. Numerical simulations of a 3-D mouse atlas under different
	noise levels demonstrate that the proposed algorithm is robust against
	measurement noise, and it can achieve high computational efficiency
	and accurate localization of source even without any permissible
	region constraint.},
  timestamp = {2016-07-08T12:29:44Z},
  number = {10},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Yu, Jingjing and Liu, Fang and Wu, Jiao and Jiao, Licheng and He, Xiaowei},
  month = oct,
  year = {2010},
  keywords = {3-D mouse atlas,Abdomen,Algorithms,Animals,bioluminescence,bioluminescence
	tomography,Bioluminescence tomography (BLT),Computer-Assisted,Computer
	Simulation,fast reconstruction algorithm,finite element analysis,Image
	Processing,Image reconstruction,inherent ill-posed inverse problem,least
	absolute shrinkage and selection operator (LASSO),least absolute
	shrinkage and selection operator problem,light source,Luminescent
	Measurements,medical image processing,Mice,multispectral approach,optical
	tomography,physiological models,reconstruction algorithm,Sparse regularization,Tomography},
  pages = {2583-2586}
}

@article{Yu2015,
  title = {Adaptive Bayesian Estimation with Cluster Structured Sparsity},
  volume = {22},
  issn = {1070-9908},
  doi = {10.1109/LSP.2015.2477440},
  abstract = {Armed with structures, group sparsity can be exploited to extraordinarily
	improve the performance of adaptive estimation. In this letter, the
	adaptive estimation algorithm for cluster structured sparse signals,
	called A-CluSS, is proposed. In particular, a hierarchical Bayesian
	model is built, where both sparse prior and cluster structured prior
	are exploited simultaneously. The adaptive updating formulas for
	statistical variables are obtained via the variational Bayesian inference
	and the resulted algorithms can adaptively estimate the cluster structured
	sparse signals without knowledge of block size, block numbers and
	block locations. Superiority of proposed A-CluSS is demonstrated
	via various simulations.},
  timestamp = {2016-07-08T10:10:21Z},
  number = {12},
  journal = {Signal Processing Letters, IEEE},
  author = {Yu, Lei and Wei, Chen and Zheng, Gang},
  month = dec,
  year = {2015},
  keywords = {A-CluSS,Adaptation models,adaptive Bayesian estimation,adaptive estimation,Bayesian inference,Bayesian
	inference,Bayes methods,block sparsity,Clustering
	algorithms,cluster structured sparsity,compressed sensing,Inference
	algorithms,Signal processing algorithms,sparse signals},
  pages = {2309-2313},
  owner = {afdidehf}
}

@inproceedings{Yu2011,
  title = {Group sparsity based classification for cervigram segmentation},
  doi = {10.1109/ISBI.2011.5872667},
  abstract = {This paper presents an algorithm to classify pixels in uterine cervix
	images into two classes, namely normal and abnormal tissues, and
	simultaneously select relevant features, using group sparsity. Because
	of the large variations in image appearance due to changes of illumination,
	specular reflections and other visual noise, the two classes have
	a strong overlap in feature space, whether features are obtained
	from color or texture information. Using more features makes the
	classes more separable and increases the segmentation's quality,
	but also its complexity. However, the properties of these features
	have not been well investigated. In most cases, a group of features
	is selected prior to the segmentation process; features with minor
	contributions to the results are kept and add to the computational
	cost. We propose feature selection as a significant improvement in
	this problem. It provides a robust trade-off between segmentation
	quality and computational complexity. In this work we formulate the
	cervigram segmentation problem as a feature-selection-based classification
	method, and we introduce a regularization-based feature-selection
	algorithm to leverage both the sparsity and clustering properties
	of the features used. We implemented our method to automatically
	segment the biomarker AcetoWhite (AW) regions in a dataset of 200
	images of the uterine cervix, for which manual segmentation is available.
	We compare the performance of several regularization-based feature-selection
	methods. The experimental results demonstrate that on this dataset,
	our proposed group-sparsity-based method gives overall better results
	in terms of sensitivity, specificity and sparsity.},
  timestamp = {2016-07-08T12:40:45Z},
  booktitle = {Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium 	on},
  author = {Yu, Yang and Huang, Junzhou and Zhang, Shaoting and Restif, C. and Huang, Xiaolei and Metaxas, D.},
  month = mar,
  year = {2011},
  keywords = {abnormal tissues,biomarker AcetoWhite,biomarker AcetoWhite regions,Biomedical
	imaging,biomedical optical imaging,cancer,cervigram,cervix image,classification,clustering
	properties,color information,computational complexity,feature extraction,feature
	selection,group sparsity,gynaecology,illumination,image classification,Image
	classification,Image color analysis,image colour analysis,Image segmentation,Image
	segmentation,image texture,medical image processing,pattern clustering,Pixel,regularization,segmentation,Shape,specular
	reflections,texture information,Training,tumours,uterine cervix images,visual
	noise},
  pages = {1425-1429},
  owner = {afdidehf}
}

@inproceedings{Yu2014a,
  title = {Distributed compressed sensing for image signals},
  doi = {10.1109/ICMEW.2014.6890579},
  abstract = {Distributed compressed sensing (DCS) is able to exploit both intra-and
	inter-signal correlation structures of multi-signal ensemble. This
	paper proposes a DCS scheme for image signal compression and reconstruction.
	The key idea is to exploit the inter-correlation of the blocks that
	split from the image. Significantly, joint sparse model was employed
	to compress the intra- and inter-redundancy of the image signal.
	Moreover, our scheme allocates more sensing resources to common component
	while fewer measurements for innovation component. In order to improve
	the performance, we also utilize variable sizes method to replace
	the uniform size approach for image split. Experimental results on
	natural images validate that the proposed DCS scheme validly improves
	the reconstructed image quality with fewer measurements compared
	to the existing CS schemes.},
  timestamp = {2016-07-08T12:14:57Z},
  booktitle = {Multimedia and Expo Workshops (ICMEW), 2014 IEEE International Conference 	on},
  author = {Yu, Zongxin and Wang, Rui and Zhang, Haiyan and Jin, Yanliang and Fu, Yixing},
  month = jul,
  year = {2014},
  keywords = {block inter-correlation,compressed sensing,Correlation,correlation
	theory,data compression,DCS scheme,distributed compressed sensing,distributed
	compressed sensing,image coding,image compression,Image reconstruction,Image
	reconstruction,image signal compression,image signal reconstruction,image
	split,inter-signal correlation structures,intra-redundancy,Joints,joint
	sparse model,Joint Sparse Signal Model,multisignal ensemble,reconstructed
	image quality,redundancy,Sensors,Vectors},
  pages = {1-5},
  owner = {afdidehf}
}

@article{Yuan2006,
  title = {Model selection and estimation in regression with grouped variables},
  volume = {68},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2005.00532.x},
  abstract = {{Summary. We consider the problem of selecting grouped variables (factors)
	for accurate prediction in regression. Such a problem arises naturally
	in many practical situations with the multifactor analysis-of-variance
	problem as the most important and well-known example. Instead of
	selecting factors by stepwise backward elimination, we focus on the
	accuracy of estimation and consider extensions of the lasso, the
	LARS algorithm and the non-negative garrotte for factor selection.
	The lasso, the LARS algorithm and the non-negative garrotte are recently
	proposed regression methods that can be used to select individual
	variables. We study and propose efficient algorithms for the extensions
	of these methods for factor selection and show that these extensions
	give superior performance to the traditional stepwise backward elimination
	method in factor selection problems. We study the similarities and
	the differences between these methods. Simulations and real examples
	are used to illustrate the methods.}},
  timestamp = {2016-07-09T20:14:26Z},
  number = {1},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author = {Yuan, Ming and Lin, Yi},
  month = feb,
  year = {2006},
  keywords = {Analysis of variance,feature-selection,Lasso,least angle regression,Non-negative
	garrotte,Piecewise linear solution path},
  pages = {49-67},
  annote = {read},
  citeulike-article-id = {448082},
  citeulike-linkout-0 = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
  citeulike-linkout-1 = {http://dx.doi.org/10.1111/j.1467-9868.2005.00532.x},
  citeulike-linkout-2 = {http://www.ingentaconnect.com/content/bpl/rssb/2006/00000068/00000001/art00004},
  citeulike-linkout-3 = {http://www3.interscience.wiley.com/cgi-bin/abstract/118566968/ABSTRACT},
  day = {1},
  owner = {afdidehf},
  posted-at = {2008-02-27 06:49:54},
  priority = {2}
}

@inproceedings{Yuan2014,
  title = {Image compression via sparse reconstruction},
  doi = {10.1109/ICASSP.2014.6853954},
  abstract = {The traditional compression system only considers the statistical
	redundancy of images. Recent compression works exploit the visual
	redundancy of images to further improve the coding efficiency. However,
	the existing works only provide suboptimal visual redundancy removal
	schemes. In this paper, we propose an efficient image compression
	scheme based on the selection and reconstruction of the visual redundancy.
	The visual redundancy in an image is defined by some images blocks,
	named redundant blocks, which can be well reconstructed by the others
	in the image. At the encoder, we design an effective optimization
	strategy to elaborately select redundant blocks and intentionally
	remove them. At the decoder, we propose an image restoration method
	to reconstruct the removed redundant blocks with minimum reconstructed
	error. Encouraging experimental results show that our compression
	scheme achieves up to 13.67% bit rate reduction with a comparable
	visual quality compared to traditional High Efficiency Video Coding
	(HEVC).},
  timestamp = {2016-07-08T12:46:01Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Yuan, Yuan and Au, O.C. and Zheng, Amin and Yang, Haitao and Tang, Ketan and Sun, Wenxiu},
  month = may,
  year = {2014},
  keywords = {bit rate reduction,codecs,coding efficiency,compressed sensing,compression
	system,data compression,decoder,Decoding,Dictionaries,dictionary
	learning,encoder,HEVC,high efficiency video coding,image coding,Image
	coding,image compression,image compression scheme,Image reconstruction,image
	restoration,image restoration method,images blocks,optimisation,optimization
	strategy,reconstructed error,redundancy,redundant blocks,sparse model,Sparse
	reconstruction,statistical redundancy,Statistics,suboptimal visual
	redundancy removal schemes,Visualization,visual quality,visual redundancy},
  pages = {2025-2029},
  owner = {afdidehf}
}

@article{Zanko2012,
  title = {Analog Product Codes Decodable by Linear Programming},
  volume = {58},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2173709},
  abstract = {In this paper, we present a new analog error correcting coding scheme
	for real valued signals that are corrupted by impulsive noise. This
	product code improves Donoho's deterministic construction by using
	a probabilistic approach. More specifically, our construction corrects
	more errors than the Donoho matrices by allowing a vanishingly small
	probability of error (with the increase in block size). The problem
	of decoding the long block code is decoupled into two sets of parallel
	Linear Programming problems. This leads to a significant reduction
	in decoding complexity as compared to one-step Linear Programming
	decoding.},
  timestamp = {2016-07-08T10:28:38Z},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Zanko, A. and Leshem, A. and Zehavi, E.},
  month = feb,
  year = {2012},
  keywords = {Analog codes,analog error correcting coding scheme,analog product
	code,communication complexity,compressed sensing,Decoding,Donoho
	deterministic construction,Donoho matrices,encoding,error correction
	codes,Error probability,error statistics,impulse noise,impulsive
	noise,Iterative decoding,linear codes,linear programming,long block
	code,matrix algebra,Noise,one-step linear programming decoding complexity,parallel
	linear programming problem,parallel programming,product codes,product
	codes,real valued signal,sparse matrices,turbo decoding,Vectors},
  pages = {509-518},
  owner = {afdidehf}
}

@article{Zdunek2008,
  title = {Improved M-FOCUSS Algorithm With Overlapping Blocks for Locally Smooth 	Sparse Signals},
  volume = {56},
  issn = {1053-587X},
  doi = {10.1109/TSP.2008.928160},
  abstract = {The focal underdetermined system solver (FOCUSS) algorithm has already
	found many applications in signal processing and data analysis, whereas
	the regularized M-FOCUSS algorithm has been recently proposed by
	Cotter for finding sparse solutions to an underdetermined system
	of linear equations with multiple measurement vectors. In this paper,
	we propose three modifications to the M-FOCUSS algorithm to make
	it more efficient for sparse and locally smooth solutions. First,
	motivated by the simultaneously autoregressive (SAR) model, we incorporate
	an additional weighting (smoothing) matrix into the Tikhonov regularization
	term. Next, the entire set of measurement vectors is divided into
	blocks, and the solution is updated sequentially, based on the overlapping
	of data blocks. The last modification is based on an alternating
	minimization technique to provide data-driven (simultaneous) estimation
	of the regularization parameter with the generalized cross-validation
	(GCV) approach. Finally, the simulation results demonstrating the
	benefits of the proposed modifications support the analysis.},
  timestamp = {2016-07-08T12:48:36Z},
  number = {10},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Zdunek, R. and Cichocki, A.},
  month = oct,
  year = {2008},
  keywords = {additional weighting matrix,autoregressive model,autoregressive processes,Data
	analysis,data-driven estimation,focal underdetermined system solver,FOCal
	Underdetermined System Solver (FOCUSS),FOCUSS,GCV,generalized cross-validation
	approach,generalized cross-validation (GCV),linear equation,locally
	smooth sparse signal,matrix algebra,minimization technique,multiple
	measurement vectors,regularized M-FOCUSS algorithm,signal processing,smoothing
	method,smoothing methods,smooth signals,sparse solutions,Tikhonov
	regularization,underdetermined systems,Vectors},
  pages = {4752-4761},
  owner = {afdidehf}
}

@article{Zeinalkhani2015,
  title = {Iterative Reweighted $\ell_2/\ell_1$ Recovery Algorithms for Compressed 	Sensing of Block Sparse Signals},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2441032},
  abstract = {In many applications of compressed sensing the signal is block sparse,
	i.e., the non-zero elements of the sparse signal are clustered in
	blocks. Here, we propose a family of iterative algorithms for the
	recovery of block sparse signals. These algorithms, referred to as
	iterative reweighted ?2/?1 minimization algorithms (IR-?2/?1), solve
	a weighted ?2/?1 minimization in each iteration. Our simulation and
	analytical results on the recovery of both ideally and approximately
	block sparse signals show that the proposed iterative algorithms
	have significant advantages in terms of accuracy and the number of
	required measurements over non-iterative approaches as well as existing
	iterative methods. In particular, we demonstrate that, by increasing
	the block length, the performance of the proposed algorithms approaches
	the Wu-Verdu? theoretical limit. The improvement in performance comes
	at a rather small cost in complexity increase. Further improvement
	in performance is achieved by using a priori information about the
	location of non-zero blocks, even if such a priori information is
	not perfectly reliable.},
  timestamp = {2016-07-09T19:45:38Z},
  number = {17},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Zeinalkhani, Z. and Banihashemi, A.H.},
  month = sep,
  year = {2015},
  keywords = {?2/?1 minimization algorithms,$ell_{2}/ell_{1}$ minimization,block
	sparse signal recovery,block sparsity,compressed sensing,compressed
	sensing,Indexes,Iterative algorithms,iterative methods,iterative
	methods,iterative recovery algorithms,iterative reweighted $ell_{2}/ell_{1}$
	minimization,iterative reweighted ?2/?1 recovery algorithms,minimisation,Minimization,Noise
	measurement,Sensors,Signal processing algorithms,signal reconstruction,Standards,weighted
	?2/?1 minimization,Wu-Verdu theoretical limit},
  pages = {4516-4531},
  owner = {afdidehf}
}

@inproceedings{Zeinalkhani2012,
  title = {Iterative recovery algorithms for compressed sensing of wideband 	block sparse spectrums},
  doi = {10.1109/ICC.2012.6364377},
  abstract = {A major task in cognitive radios (CRs) is spectrum sensing. In a wide-band
	regime, this is a challenging task requiring very high-speed analog-to-digital
	converters (ADCs), operating at or above the Nyquist rate. Compressed
	sensing is recognized as an effective technique to significantly
	reduce the sampling rate in wideband spectrum sensing, taking advantage
	of the sparsity of the spectrum. The recovery of the spectrum from
	the samples at sub-Nyquist rates is usually achieved through the
	so-called ?1-norm minimization. A more effective recovery technique
	for block sparse signals, called ?2/?1-norm minimization, can be
	used as a replacement for ?1-norm minimization to reduce the sampling
	rate and consequently simplify the implementation of ADCs even further.
	In this paper, we propose two iterative ?2/ ?1-norm minimization
	algorithms for the recovery of block sparse spectrums. Similar to
	the standard ?2/?1-norm minimization, the proposed algorithms require
	the side information about the boundaries of the spectral blocks.
	We evaluate the performance of the proposed algorithms both in the
	absence and in the presence of noise, and demonstrate that for both
	cases, the proposed algorithms significantly outperform the existing
	?1-minimization-based and standard ?2/?1 minimization recovery algorithms.
	The improvement in performance comes at a small cost in complexity
	increase.},
  timestamp = {2016-07-09T19:45:32Z},
  booktitle = {Communications (ICC), 2012 IEEE International Conference on},
  author = {Zeinalkhani, Z. and Banihashemi, A.H.},
  month = jun,
  year = {2012},
  keywords = {ADC,analog-to-digital converter,analogue-digital conversion,block
	sparse signal spectum,broadband networks,cognitive radio,Cognitive
	radio,compressed sensing,CR,iterative methods,iterative
	recovery algorithm,iterative recovery
	algorithm,l2-l1-norm minimization algorithm,minimisation,sampling
	rate reduction,side information,signal reconstruction,signal sampling,spectral
	block boundary,subNyquist rate,wideband block sparse spectrum sensing},
  pages = {1630-1634},
  owner = {afdidehf}
}

@article{Zelinski2010,
  title = {Simultaneously Sparse Solutions to Linear Inverse Problems with Multiple 	System Matrices and a Single Observation Vector},
  volume = {31},
  doi = {10.1137/080730822},
  abstract = {A problem that arises in slice-selective magnetic resonance imaging
	(MRI) radiofrequency (RF) excitation pulse design is abstracted as
	a novel linear inverse problem with a simultaneous sparsity constraint.
	Multiple unknown signal vectors are to be determined, where each
	passes through a different system matrix and the results are added
	to yield a single observation vector. Given the matrices and lone
	observation, the objective is to find a simultaneously sparse set
	of unknown vectors that approximately solves the system. We refer
	to this as the multiple-system single-output (MSSO) simultaneous
	sparse approximation problem. This manuscript contrasts the MSSO
	problem with other simultaneous sparsity problems and conducts an
	initial exploration of algorithms with which to solve it. Greedy
	algorithms and techniques based on convex relaxation are derived
	and compared empirically. Experiments involve sparsity pattern recovery
	in noiseless and noisy settings and MRI RF pulse design.},
  timestamp = {2016-07-10T08:18:36Z},
  number = {6},
  journal = {Siam Journal on Scientific Computing},
  author = {Zelinski, Adam C. and Goyal, Vivek K. and Adalsteinsson, Elfar},
  year = {2010},
  keywords = {iteratively reweighted least squares,iterative shrinkage,magnetic
	resonance imaging excitation pulse design,matching pursuit,multiple
	measurement vectors,second-order cone programming,simultaneous sparse
	approximation,sparse approximation},
  pages = {4533-4579},
  masid = {6069660},
  owner = {Fardin}
}

@article{Zelnik-Manor2012,
  title = {Dictionary Optimization for Block-Sparse Representations},
  volume = {60},
  issn = {1053-587X},
  doi = {10.1109/TSP.2012.2187642},
  abstract = {Recent work has demonstrated that using a carefully designed dictionary
	instead of a predefined one, can improve the sparsity in jointly
	representing a class of signals. This has motivated the derivation
	of learning methods for designing a dictionary which leads to the
	sparsest representation for a given set of signals. In some applications,
	the signals of interest can have further structure, so that they
	can be well approximated by a union of a small number of subspaces
	(e.g., face recognition and motion segmentation). This implies the
	existence of a dictionary which enables block-sparse representations
	of the input signals once its atoms are properly sorted into blocks.
	In this paper, we propose an algorithm for learning a block-sparsifying
	dictionary of a given set of signals. We do not require prior knowledge
	on the association of signals into groups (subspaces). Instead, we
	develop a method that automatically detects the underlying block
	structure given the maximal size of those groups. This is achieved
	by iteratively alternating between updating the block structure of
	the dictionary and updating the dictionary atoms to better fit the
	data. Our experiments show that for block-sparse data the proposed
	algorithm significantly improves the dictionary recovery ability
	and lowers the representation error compared to dictionary learning
	methods that do not employ block structure.},
  timestamp = {2016-07-08T12:11:27Z},
  number = {5},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Zelnik-Manor, L. and Rosenblum, K. and Eldar, Y.C.},
  month = may,
  year = {2012},
  keywords = {Algorithm design and analysis,block-sparse representation,block-sparsifying
	dictionary,block sparsity,Cost function,Dictionaries,dictionary design,dictionary
	learning method,dictionary optimization,Learning systems,Matching
	pursuit algorithms,optimisation,signal reconstruction,signal representation,Sparse
	coding,Vectors},
  pages = {2386-2395},
  owner = {afdidehf}
}

@article{Zelnik-Manor2011,
  title = {Sensing Matrix Optimization for Block-Sparse Decoding},
  volume = {59},
  issn = {1053-587X},
  doi = {10.1109/TSP.2011.2159211},
  abstract = {Recent work has demonstrated that using a carefully designed sensing
	matrix rather than a random one, can improve the performance of compressed
	sensing. In particular, a well-designed sensing matrix can reduce
	the coherence between the atoms of the equivalent dictionary, and
	as a consequence, reduce the reconstruction error. In some applications,
	the signals of interest can be well approximated by a union of a
	small number of subspaces (e.g., face recognition and motion segmentation).
	This implies the existence of a dictionary which leads to block-sparse
	representations. In this work, we propose a framework for sensing
	matrix design that improves the ability of block-sparse approximation
	techniques to reconstruct and classify signals. This method is based
	on minimizing a weighted sum of the interblock coherence and the
	subblock coherence of the equivalent dictionary. Our experiments
	show that the proposed algorithm significantly improves signal recovery
	and classification ability of the Block-OMP algorithm compared to
	sensing matrix optimization methods that do not employ block structure.},
  timestamp = {2016-07-10T08:10:34Z},
  number = {9},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Zelnik-Manor, L. and Rosenblum, K. and Eldar, Y.C.},
  month = sep,
  year = {2011},
  keywords = {Algorithm design and analysis,Approximation methods,block codes,block-sparse
	approximation,block-sparse decoding,Block-sparsity,Coherence,compressed
	sensing,Decoding,Dictionaries,Dictionary,interblock coherence,matrix
	algebra,Minimization,optimisation,reconstruction error,sensing matrix
	design,sensing matrix optimization,Sensors,signal classification,signal
	reconstruction,sparse matrices,subblock coherence},
  pages = {4300-4312},
  owner = {afdidehf}
}

@article{Zeng1989,
  title = {A block coding technique for encoding sparse binary patterns},
  volume = {37},
  issn = {0096-3518},
  doi = {10.1109/29.17576},
  abstract = {An efficient method is presented for encoding sparse binary patterns.
	This method is very simple to implement and performs in a near-optimum
	way. The sparse pattern is assumed to be a memoryless binary source.
	This kind of pattern is found in a three-dimensional authentication
	scheme. In the data compression area, the patterns are usually not
	memoryless sources. However, when LPC (linear prediction coding)
	is applied, the resulting error pattern is very close to a memoryless
	model.<>},
  timestamp = {2016-07-08T10:06:47Z},
  number = {5},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  author = {Zeng, G. and Ahmed, N.},
  month = may,
  year = {1989},
  keywords = {block codes,block coding,Chaos,data compression,Electrons,encoding,error
	pattern,linear prediction coding,LPC,memoryless binary source,memoryless
	model,Noise level,Radar detection,sensor fusion,Sensor systems,Signal
	to noise ratio,sparse binary patterns coding,Testing,three-dimensional
	authentication scheme},
  pages = {778-780},
  owner = {afdidehf}
}

@article{Zepeda2011,
  title = {Image Compression Using Sparse Representations and the Iteration-Tuned 	and Aligned Dictionary},
  volume = {5},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2011.2135332},
  abstract = {We introduce a new image coder which uses the Iteration Tuned and
	Aligned Dictionary (ITAD) as a transform to code image blocks taken
	over a regular grid. We establish experimentally that the ITAD structure
	results in lower-complexity representations that enjoy greater sparsity
	when compared to other recent dictionary structures. We show that
	this superior sparsity can be exploited successfully for compressing
	images belonging to specific classes of images (e.g., facial images).
	We further propose a global rate-distortion criterion that distributes
	the code bits across the various image blocks. Our evaluation shows
	that the proposed ITAD codec can outperform JPEG2000 by more than
	2 dB at 0.25 bpp and by 0.5 dB at 0.45 bpp, accordingly producing
	qualitatively better reconstructions.},
  timestamp = {2016-07-08T12:45:57Z},
  number = {5},
  journal = {Selected Topics in Signal Processing, IEEE Journal of},
  author = {Zepeda, J. and Guillemot, C. and Kijak, E.},
  month = sep,
  year = {2011},
  keywords = {Atomic layer deposition,code bit,codecs,data compression,Dictionaries,dictionary
	structure,facial image,global rate-distortion criterion,image block,image
	coder,image coding,image compression,image representation,ITAD codec,iteration-tuned
	and aligned dictionary,iterative methods,learned dictionaries,Matching pursuit algorithms,Matching
	pursuit algorithms,rate distortion theory,sparse
	representation,sparse representations,Training,Transform coding,transforms},
  pages = {1061-1073},
  owner = {afdidehf}
}

@article{Zhang2014,
  title = {Multi-Observation Blind Deconvolution with an Adaptive Sparse Prior},
  volume = {36},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.241},
  abstract = {This paper describes a robust algorithm for estimating a single latent
	sharp image given multiple blurry and/or noisy observations. The
	underlying multi-image blind deconvolution problem is solved by linking
	all of the observations together via a Bayesian-inspired penalty
	function, which couples the unknown latent image along with a separate
	blur kernel and noise variance associated with each observation,
	all of which are estimated jointly from the data. This coupled penalty
	function enjoys a number of desirable properties, including a mechanism
	whereby the relative-concavity or sparsity is adapted as a function
	of the intrinsic quality of each corrupted observation. In this way,
	higher quality observations may automatically contribute more to
	the final estimate than heavily degraded ones, while troublesome
	local minima can largely be avoided. The resulting algorithm, which
	requires no essential tuning parameters, can recover a sharp image
	from a set of observations containing potentially both blurry and
	noisy examples, without knowing a priori the degradation type of
	each observation. Experimental results on both synthetic and real-world
	test images clearly demonstrate the efficacy of the proposed method.},
  timestamp = {2016-07-09T20:16:40Z},
  number = {8},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  author = {Zhang, Haichao and Wipf, D. and Zhang, Yanning},
  month = aug,
  year = {2014},
  keywords = {adaptive sparse prior,Algorithm design and analysis,Bayesian-inspired
	penalty function,blind image deblurring,blur kernel,blurry observations,Cost
	function,Deconvolution,Estimation,image denoising,image restoration,Kernel,latent
	image,multiimage blind deconvolution problem,multiobservation blind deconvolution,Multi-observation blind deconvolution,multiobservation blind
	deconvolution,Multi-observation blind
	deconvolution,Noise level,Noise
	measurement,noise variance,noisy observations,relative-concavity,relative-sparsity,single
	latent sharp image estimation,sparse estimation,Sparse priors,tuning
	parameters},
  pages = {1628-1643},
  owner = {Fardin}
}

@inproceedings{Zhang2013,
  title = {Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior},
  doi = {10.1109/CVPR.2013.140},
  abstract = {This paper presents a robust algorithm for estimating a single latent
	sharp image given multiple blurry and/or noisy observations. The
	underlying multi-image blind deconvolution problem is solved by linking
	all of the observations together via a Bayesian-inspired penalty
	function which couples the unknown latent image, blur kernels, and
	noise levels together in a unique way. This coupled penalty function
	enjoys a number of desirable properties, including a mechanism whereby
	the relative-concavity or shape is adapted as a function of the intrinsic
	quality of each blurry observation. In this way, higher quality observations
	may automatically contribute more to the final estimate than heavily
	degraded ones. The resulting algorithm, which requires no essential
	tuning parameters, can recover a high quality image from a set of
	observations containing potentially both blurry and noisy examples,
	without knowing a priori the degradation type of each observation.
	Experimental results on both synthetic and real-world test images
	clearly demonstrate the efficacy of the proposed method.},
  timestamp = {2016-11-16T10:04:44Z},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference 	on},
  author = {Zhang, Haichao and Wipf, D. and Zhang, Yanning},
  month = jun,
  year = {2013},
  keywords = {adaptive coupled sparsity,Algorithm design and analysis,Bayesian-inspired
	penalty function,Bayes methods,blind image deblurring,Blind Source
	Separation,blur kernels,blurry observations,coupled adaptive sparse
	prior,Deconvolution,Estimation,image denoising,image quality,image
	restoration,intrinsic quality,Kernel,multiimage blind deblurring,multi-image
	blind deconvolution,multiimage blind deconvolution problem,Noise,Noise
	level,noise levels,Noise measurement,noisy observations,quality observations,real-world
	test images,relative-concavity,robust algorithm,single latent sharp
	image,sparse recovery,synthetic test images,tuning parameters},
  pages = {1051--1058},
  owner = {Fardin}
}

@article{Zhang2012,
  title = {Joint-Structured-Sparsity-Based Classification for Multiple-Measurement 	Transient Acoustic Signals},
  volume = {42},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2012.2196038},
  abstract = {This paper investigates the joint-structured-sparsity-based methods
	for transient acoustic signal classification with multiple measurements.
	By joint structured sparsity, we not only use the sparsity prior
	for each measurement but we also exploit the structural information
	across the sparse representation vectors of multiple measurements.
	Several different sparse prior models are investigated in this paper
	to exploit the correlations among the multiple measurements with
	the notion of the joint structured sparsity for improving the classification
	accuracy. Specifically, we propose models with the joint structured
	sparsity under different assumptions: same sparse code model, common
	sparse pattern model, and a newly proposed joint dynamic sparse model.
	For the joint dynamic sparse model, we also develop an efficient
	greedy algorithm to solve it. Extensive experiments are carried out
	on real acoustic data sets, and the results are compared with the
	conventional discriminative classifiers in order to verify the effectiveness
	of the proposed method.},
  timestamp = {2016-07-09T19:50:03Z},
  number = {6},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions 	on},
  author = {Zhang, Haichao and Zhang, Yanning and Nasrabadi, N.M. and Huang, T.S.},
  month = dec,
  year = {2012},
  keywords = {acoustic data set,Acoustic measurements,Acoustics,acoustic signal
	processing,Atomic measurements,codes,common sparse pattern model,conventional
	discriminative classifier,correlation methods,Dictionaries,dynamic
	sparse code model,greedy algorithm,Greedy algorithms,joint sparse
	representation,joint structured sparsity,joint-structured-sparsity-based
	classification method,multiple-measurement transient acoustic signal classification,multiple-measurement transient acoustic signal
	classification,pattern
	classification,signal classification,signal representation,sparse
	representation vector,structural information,Vectors},
  pages = {1586-1598},
  owner = {afdidehf}
}

@inproceedings{Zhang2013a,
  title = {Real time tracking via sparse representation},
  doi = {10.1109/ChinaCom.2013.6694688},
  abstract = {The L1 tracker gains robustness by casting tracking as a problem of
	sparse approximation in a particle filter framework. Unfortunately,
	the particle filter and ? norm minimization lead to a large amount
	of calculation as a result that the L1 tracker cannot achieve real-time
	tracking. The aim of this paper is to develop a new tracker which
	not only runs in real time but also has a better robustness than
	L1 tracker via sparse representation. In our proposed algorithm,
	candidate targets are sampled in the region of interest (ROI) to
	increase the tracking speed. Moreover, based on the block orthogonal
	matching pursuit (BOMP), a very fast solver is developed to solve
	the problem of ? norm minimization to improve tracking speed and
	accuracy. We conduct extensive experiment to validate and compare
	the performance of the BOMP algorithms against six popular ?-minimization
	solvers in different challenging sequences. We also implement great
	experiment to validate the high computational efficiency and tracking
	accuracy of our proposed tracker compare with four alternative state-of-the-art
	trackers in six challenging sequences.},
  timestamp = {2016-07-10T07:41:03Z},
  booktitle = {Communications and Networking in China (CHINACOM), 2013 8th International 	ICST Conference on},
  author = {Zhang, Hong-Mei and Wei, Xian-Sui and Huang, Tao and He, Yan and Zhang, Xiang-Li and Jin, Ye},
  month = aug,
  year = {2013},
  keywords = {Algorithm design and analysis,approximation theory,block orthogonal
	matching pursuit,BOMP,casting tracking,computer vision,Face,L1 tracker,Matching
	pursuit algorithms,Minimization,object tracking,particle filter,particle
	filter framework,particle filtering (numerical methods),Particle
	filters,Real-time systems,real time tracking,region of interest,ROI,Spares
	Representation,sparse approximation,sparse representation,Target
	tracking},
  pages = {724-729},
  owner = {afdidehf}
}

@article{Zhang2014a,
  title = {Image compressive sensing recovery using adaptively learned sparsifying 	basis via L0 minimization},
  volume = {103},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2013.09.025},
  abstract = {Abstract From many fewer acquired measurements than suggested by the
	Nyquist sampling theory, compressive sensing (CS) theory demonstrates
	that, a signal can be reconstructed with high probability when it
	exhibits sparsity in some domain. Most of the conventional \{CS\}
	recovery approaches, however, exploited a set of fixed bases (e.g.
	DCT, wavelet and gradient domain) for the entirety of a signal, which
	are irrespective of the non-stationarity of natural signals and cannot
	achieve high enough degree of sparsity, thus resulting in poor \{CS\}
	recovery performance. In this paper, we propose a new framework for
	image compressive sensing recovery using adaptively learned sparsifying
	basis via \{L0\} minimization. The intrinsic sparsity of natural
	images is enforced substantially by sparsely representing overlapped
	image patches using the adaptively learned sparsifying basis in the
	form of \{L0\} norm, greatly reducing blocking artifacts and confining
	the \{CS\} solution space. To make our proposed scheme tractable
	and robust, a split Bregman iteration based technique is developed
	to solve the non-convex \{L0\} minimization problem efficiently.
	Experimental results on a wide range of natural images for \{CS\}
	recovery have shown that our proposed algorithm achieves significant
	performance improvements over many current state-of-the-art schemes
	and exhibits good convergence property.},
  timestamp = {2016-07-08T12:46:05Z},
  journal = {Signal Processing},
  author = {Zhang, Jian and Zhao, Chen and Zhao, Debin and Gao, Wen},
  year = {2014},
  note = {Image Restoration and Enhancement: Recent Advances and Applications},
  keywords = {Basis,Compressive,Image,Optimization,recovery,sensing,Sparsifying,Sparsity},
  pages = {114 - 126},
  owner = {afdidehf}
}

@inproceedings{Zhang2010,
  title = {Sparse representation for weed seeds classification},
  doi = {10.1109/ICGCS.2010.5542988},
  abstract = {In agricultural industry, there is a longing for highly efficient
	and reliable seeds classification methods. Fast implementation of
	the existing methods is of great economical importance. Almost all
	categories of weed seeds have different size, shape and texture,
	and even the same species are quantitatively diverse in feature.
	Therefore, feature extraction is a tough, time consuming and labor-intensive
	task. In this paper, we use the compressive sensing theory, which
	has been applied to the field of machine learning, to do some dimension
	reduction treatment such as principle component analysis, downsampling
	and random projection to avoid careful selection of the feature set.
	As long as the dimension of the extracted features is beyond the
	theoretical threshold, we can achieve the desired classification
	results. It is worth mentioning that on account of humidity, bacteria
	and many other factors,the seeds are prone to have mould blocks or
	scabs. Thus, it is extremely necessary to do some simulations like
	contiguous occlusion. According to the experimental results, we can
	see that this algorithm is fit for solving this problem.},
  timestamp = {2016-07-11T16:51:52Z},
  booktitle = {Green Circuits and Systems (ICGCS), 2010 International Conference 	on},
  author = {Zhang, Ming and Cai, Cheng and Zhu, Junping},
  month = jun,
  year = {2010},
  keywords = {Agricultural engineering,agricultural industry,agriculture,bacteria,compressive
	sensing theory,dimension reduction treatment,downsampling,economical
	importance,Educational institutions,face recognition,feature extraction,Feature
	Extraction,feature set,humidity,image classification,image representation,learning
	(artificial intelligence),Machine learning,microorganisms,mould blocks,mould
	blocks,Principal component analysis,Principal
	component analysis,principle component analysis,random projection,Random
	projection,Reliability engineering,Reliability
	engineering,scabs,Shape,sparse representation,theoretical threshold,theoretical
	threshold,Vectors,weed seeds classification,weed
	seeds classification},
  pages = {626-631},
  owner = {afdidehf}
}

@inproceedings{Zhang2010a,
  title = {Block-based compressive sampling for digital pixel sensor array},
  doi = {10.1109/ASQED.2010.5548164},
  abstract = {In this paper, a block-based online compressive sampling scheme for
	digital pixel sensor (DPS) is proposed. The overall sensor array
	is divided into blocks whereby one randomly selected pixel within
	each block is sampled using a random access control circuit. The
	latter is performed using off-array horizontal and vertical control
	logic. The random access addresses are updated during readout phase
	using low complexity logic operations performed on the readout pixel
	values. A sparse matrix, consisting of all the sampled pixel values,
	is buildup to reconstruct the image by solving the l1-norm minimization
	as the linear programming problem in the framework of Convex Optimization.
	The proposed system features reduced on-chip compression processing
	complexity and significant reduced memory requirement. System level
	simulation results show that a 25dB reconstructed image quality in
	terms of PSNR is achieved enabling a compression ratio of 4. In addition,
	a pixel-level memory requirement reduction of 75% is achieved when
	compared to a standard PWM DPS architecture.},
  timestamp = {2016-07-08T11:40:48Z},
  booktitle = {Quality Electronic Design (ASQED), 2010 2nd Asia Symposium on},
  author = {Zhang, Milin and Wang, Yan and Bermak, A.},
  month = aug,
  year = {2010},
  keywords = {Access control,block-based compressive sampling,Circuits,convex optimization,convex
	programming,digital pixel sensor array,image coding,Image reconstruction,Image
	reconstruction,image sampling,image sensors,l1-norm minimization,linear
	programming,linear programming problem,logic circuits,Logic programming,low
	complexity logic operation,off-array horizontal control logic,off-array
	vertical control logic,Pixel,Pulse width modulation,random access
	control circuit,random-access storage,readout phase,reduced on-chip
	compression processing complexity,Sampling methods,Sensor arrays,sparse matrices,sparse
	matrices,sparse matrix,system level simulation},
  pages = {9-12},
  owner = {afdidehf}
}

@article{Zhang2015,
  title = {Modulated Unit-Norm Tight Frames for Compressed Sensing},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2425809},
  abstract = {In this paper, we propose a compressed sensing (CS) framework that
	consists of three parts: a unit-norm tight frame (UTF), a random
	diagonal matrix and a column-wise orthonormal matrix. We prove that
	this structure satisfies the restricted isometry property (RIP) with
	high probability if the number of measurements m=O(slog2slog2n) for
	s-sparse signals of length n and if the column-wise orthonormal matrix
	is bounded. Some existing structured sensing models can be studied
	under this framework, which then gives tighter bounds on the required
	number of measurements to satisfy the RIP. More importantly, we propose
	several structured sensing models by appealing to this unified framework,
	such as a general sensing model with arbitrary/determinisic subsamplers,
	a fast and efficient block compressed sensing scheme, and structured
	sensing matrices with deterministic phase modulations, all of which
	can lead to improvements on practical applications. In particular,
	one of the constructions is applied to simplify the transceiver design
	of CS-based channel estimation for orthogonal frequency division
	multiplexing (OFDM) systems.},
  timestamp = {2016-07-09T20:14:41Z},
  number = {15},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Zhang, Peng and Gan, Lu and Sun, Sumei and Ling, Cong},
  month = aug,
  year = {2015},
  keywords = {Arbitrary/deterministic subsampling,block compressed sensing scheme,Channel
	estimation,coherence analysis,column-wise orthonormal matrix,compressed
	sensing,Compressed Sensing (CS),compressed sensing framework,Computational
	modeling,CS-based channel estimation,CS framework,general sensing
	model-arbitrary-determinisic subsamplers,Golay sequence,matrix algebra,measurement
	number,modulated unit-norm tight frames,OFDM,OFDM modulation,OFDM
	systems,orthogonal frequency division multiplexing systems,phase modulation,phase
	modulation,random diagonal matrix,restricted isometry
	property,Sensors,sparse matrices,s-sparse signals,structured sensing
	matrix,structured sensing matrix-deterministic phase modulations,structured
	sensing model,transceiver design,transforms,unit-norm tight frame
	(UTF),UTF},
  pages = {3974-3985},
  owner = {afdidehf}
}

@article{Zhang2012a,
  title = {Automatic Image Annotation and Retrieval Using Group Sparsity},
  volume = {42},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2011.2179533},
  abstract = {Automatically assigning relevant text keywords to images is an important
	problem. Many algorithms have been proposed in the past decade and
	achieved good performance. Efforts have focused upon model representations
	of keywords, whereas properties of features have not been well investigated.
	In most cases, a group of features is preselected, yet important
	feature properties are not well used to select features. In this
	paper, we introduce a regularization-based feature selection algorithm
	to leverage both the sparsity and clustering properties of features,
	and incorporate it into the image annotation task. Using this group-sparsity-based
	method, the whole group of features [e.g., red green blue (RGB) or
	hue, saturation, and value (HSV)] is either selected or removed.
	Thus, we do not need to extract this group of features when new data
	comes. A novel approach is also proposed to iteratively obtain similar
	and dissimilar pairs from both the keyword similarity and the relevance
	feedback. Thus, keyword similarity is modeled in the annotation framework.
	We also show that our framework can be employed in image retrieval
	tasks by selecting different image pairs. Extensive experiments are
	designed to compare the performance between features, feature combinations,
	and regularization-based feature selection methods applied on the
	image annotation task, which gives insight into the properties of
	features in the image annotation task. The experimental results demonstrate
	that the group-sparsity-based method is more accurate and stable
	than others.},
  timestamp = {2016-07-08T11:35:41Z},
  number = {3},
  journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions 	on},
  author = {Zhang, Shaoting and Huang, Junzhou and Li, Hongsheng and Metaxas, D.N.},
  month = jun,
  year = {2012},
  keywords = {Algorithms,Artificial Intelligence,Automated,automatic image annotation,automatic
	image retrieval,Corel5K,Decision Support Techniques,Documentation,Feature
	Extraction,feature properties,feature selection,group sparsity,group
	sparsity based method,group theory,Histograms,HSV,hue saturation
	and value,IAPR TC12,image annotation,Image color analysis,image retrieval,Information
	Storage and Retrieval,Natural Language Processing,Pattern Recognition,Radiology
	Information Systems,regularization,regularization based feature selection
	algorithm,relevance feedback,RGB,Testing,text analysis,text keywords,Training,Training
	data,Visualization},
  pages = {838-849},
  owner = {afdidehf}
}

@article{Zhang2013b,
  title = {Sparse Regression with Non-Convex Regularization},
  timestamp = {2016-07-11T16:51:25Z},
  author = {Zhang, Tong},
  month = aug,
  year = {2013},
  howpublished = {Rutgers University},
  owner = {afdidehf}
}

@article{Zhang2010b,
  title = {Analysis of Multi-stage Convex Relaxation for Sparse Regularization},
  abstract = {We consider learning formulations with non-convex objective functions
	that often occur in practical applications. There are two approaches
	to this problem: � Heuristic methods such as gradient descent that
	only find a local minimum. A drawback of this approach is the lack
	of theoretical guarantee showing that the local minimum gives a good
	solution. � Convex relaxation such as L1-regularization that solves
	the problem under some conditions. However it often leads to a sub-optimal
	solution in reality. This paper tries to remedy the above gap between
	theory and practice. In particular, we present a multi-stage convex
	relaxation scheme for solving problems with non-convex objective
	functions. For learning formulations with sparse regularization,
	we analyze the behavior of a specific multistage relaxation scheme.
	Under appropriate conditions, we show that the local solution obtained
	by this procedure is superior to the global solution of the standard
	L1 convex relaxation for learning sparse targets.},
  timestamp = {2016-07-08T10:28:54Z},
  journal = {Journal of Machine Learning Research},
  author = {Zhang, Tong},
  year = {2010},
  keywords = {convex relaxation,multi-stage convex relaxation,Non-convex optimization,Sparsity},
  pages = {1081-1107},
  owner = {Fardin}
}

@inproceedings{Zhang2013c,
  title = {Joint segmentation and classification of hyperspectral image using 	meanshift and sparse representation classifier},
  doi = {10.1109/IGARSS.2013.6723194},
  abstract = {A novel spectral-spatial classification method based on mean shift
	and sparse representation classifier (SRC) for hyperspectral images
	is proposed in this paper. Firstly, the nonnegative matrix factorization,
	is used as a preprocessing for mean shift. Then, the mean shift algorithm
	is adopted to partition an image into amount of blocks and get the
	segmentation map. Through this way, many size-variable and close
	regions can be got while the boundary information is remained. Secondly,
	the classification map is obtained by using the SRC. Finally, the
	fusion of the segmentation map and the classification map is done
	by using the majority vote rule. Experimental results on two real
	hyperspectral images demonstrate the effectiveness and good performance
	of the proposed method.},
  timestamp = {2016-07-09T19:49:26Z},
  booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International},
  author = {Zhang, Xiangrong and Li, Yufang and Zheng, Yaoguo and Hou, Biao and Hou, Xiaojin},
  month = jul,
  year = {2013},
  keywords = {boundary information,classification map,Educational institutions,geophysical
	image processing,hyperspectral image classification,hyperspectral
	image segmentation,Hyperspectral imaging,image classification,Image
	classification,image fusion,image
	fusion,image representation,Image segmentation,majority vote rule,majority
	vote rule,matrix decomposition,Matrix
	decomposition,meanshift,mean shift algorithm,nonnegative matrix factorization,nonnegative
	matrix factorization,segmentation map fusion,segmentation
	map fusion,size variable,sparse matrices,sparse
	matrices,sparse representation classification,sparse representation
	classifier,sparse
	representation classifier,spectral spatial classification method,SRC,Training},
  pages = {1971-1974},
  owner = {afdidehf}
}

@inproceedings{Zhang2015a,
  title = {Visual Object Clustering via Mixed-Norm Regularization},
  doi = {10.1109/WACV.2015.142},
  abstract = {Many vision problems deal with high-dimensional data, such as motion
	segmentation and face clustering. However, these high-dimensional
	data usually lie in a low-dimensional structure. Sparse representation
	is a powerful principle for solving a number of clustering problems
	with high-dimensional data. This principle is motivated from an ideal
	modeling of data points according to linear algebra theory. However,
	real data in computer vision are unlikely to follow the ideal model
	perfectly. In this paper, we exploit the mixed norm regularization
	for sparse subspace clustering. This regularization term is a convex
	combination of the ?1 norm, which promotes sparsity at the individual
	level and the block norm ?2/1 which promotes group sparsity. Combining
	these powerful regularization terms will provide a more accurate
	modeling, subsequently leading to a better solution for the affinity
	matrix used in sparse subspace clustering. This could help us achieve
	better performance on motion segmentation and face clustering problems.
	This formulation also caters for different types of data corruptions.
	We derive a provably convergent algorithm based on the alternating
	direction method of multipliers (ADMM) framework, which is computationally
	efficient, to solve the formulation. We demonstrate that this formulation
	outperforms other state-of-arts on both motion segmentation and face
	clustering.},
  timestamp = {2016-07-11T17:11:36Z},
  booktitle = {Applications of Computer Vision (WACV), 2015 IEEE Winter Conference 	on},
  author = {Zhang, Xin and Pham, Duc-Son and Phung, Dinh and Liu, Wanquan and Saha, B. and Venkatesh, S.},
  month = jan,
  year = {2015},
  keywords = {alternating direction method of multipliers framework,Clustering algorithms,Computer
	vision,Data models,Educational institutions,Face,face clustering
	problems,Image segmentation,linear algebra theory,matrix algebra,mixed-norm
	regularization,motion segmentation,pattern clustering,sparse matrices,sparse
	representation,sparse subspace clustering,visual object clustering
	problem},
  pages = {1030-1037},
  owner = {afdidehf}
}

@article{Zhang2010c,
  title = {Regularization Parameter Selections via Generalized Information Criterion},
  volume = {105},
  issn = {0162-1459},
  doi = {10.1198/jasa.2009.tm08013},
  abstract = {{We apply the nonconcave penalized likelihood approach to obtain variable
	selections as well as shrinkage estimators. This approach relies
	heavily on the choice of regularization parameter, which controls
	the model complexity. In this paper, we propose employing the generalized
	information criterion, encompassing the commonly used Akaike information
	criterion (AIC) and Bayesian information criterion (BIC), for selecting
	the regularization parameter. Our proposal makes a connection between
	the classical variable selection criteria and the regularization
	parameter selections for the nonconcave penalized likelihood approaches.
	We show that the BIC-type selector enables identification of the
	true model consistently, and the resulting estimator possesses the
	oracle property in the terminology of Fan and Li (2001). In contrast,
	however, the AIC-type selector tends to overfit with positive probability.
	We further show that the AIC-type selector is asymptotically loss
	efficient, while the BIC-type selector is not. Our simulation results
	confirm these theoretical findings, and an empirical example is presented.
	Some technical proofs are given in the online supplementary material.}},
  timestamp = {2016-07-10T07:50:39Z},
  number = {489},
  journal = {Journal of the American Statistical Association},
  author = {Zhang, Yiyun},
  month = mar,
  year = {2010},
  keywords = {Lasso,shrinkage,Statistics,variable-selection},
  pages = {312-323},
  citeulike-article-id = {7988979},
  citeulike-linkout-0 = {http://dx.doi.org/10.1198/jasa.2009.tm08013},
  owner = {Fardin},
  posted-at = {2010-10-11 13:53:39},
  priority = {0}
}

@inproceedings{Zhang2015b,
  title = {Short-term wind power forecasting using nonnegative sparse coding},
  doi = {10.1109/CISS.2015.7086873},
  abstract = {State-of-the-art statistical learning techniques are adapted in this
	contribution for real-time wind power forecasting. Spatio-temporal
	wind power outputs are modeled as a linear combination of �few�
	atoms in a dictionary. By exploiting geographical information of
	wind farms, a graph Laplacian-based regularizer encourages positive
	correlation of wind power levels of adjacent farms. Real-time forecasting
	is achieved by online nonnegative sparse coding with elastic net
	regularization. The resultant convex optimization problems are efficiently
	solved using a block coordinate descent solver. Numerical tests on
	real data corroborate the merits of the proposed approach, which
	outperforms competitive alternatives in forecasting accuracy.},
  timestamp = {2016-07-10T08:11:30Z},
  booktitle = {Information Sciences and Systems (CISS), 2015 49th Annual Conference 	on},
  author = {Zhang, Yu and Kim, Seung-Jun and Giannakis, G.B.},
  month = mar,
  year = {2015},
  keywords = {Artificial neural networks,Atomic measurements,block coordinate descent
	solver,Dictionaries,elastic net regularization,geographical information,graph
	Laplacian-based regularizer,linear combination,load forecasting,nonnegative
	sparse coding,numerical analysis,numerical tests,Prediction algorithms,spatio-temporal
	wind power outputs,wind farms,Wind forecasting,wind power,wind power
	forecasting,Wind power generation,wind power plants,Wind turbines},
  pages = {1-5},
  owner = {afdidehf}
}

@inproceedings{Zhang2014b,
  title = {BSIK-SVD: A dictionary-learning algorithm for block-sparse representations},
  doi = {10.1109/ICASSP.2014.6854257},
  abstract = {Sparse dictionary learning has attracted enormous interest in image
	processing and data representation in recent years. To improve the
	performance of dictionary learning, we propose an efficient block-structured
	incoherent K-SVD algorithm for the sparse representation of signals.
	Without relying on any prior knowledge of the group structure for
	the input data, we develop a two-stage agglomerative hierarchical
	clustering method for block sparse representations. This clustering
	method adaptively identifies the underlying block structure of the
	dictionary under the restricted conditions of both a maximal block
	size and a minimal distance between the blocks. Furthermore, to meet
	the constraints of both the upper bound and the lower bound of the
	mutual coherence of dictionary atoms, we introduce a regularization
	term for the objective function to suppress the block coherence of
	the overcomplete dictionary. The experiments on synthetic data and
	real images demonstrate that the proposed algorithm has lower representation
	error, higher visual quality and better reconstructed results than
	other state-of-the-art methods.},
  timestamp = {2016-07-08T11:49:46Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International 	Conference on},
  author = {Zhang, Yongqin and Liu, Jiaying and Li, Mading and Guo, Zongming},
  month = may,
  year = {2014},
  keywords = {block coherence suppression,block-sparse representations,block sparsity,block-structured
	incoherent K-SVD algorithm,BSIK-SVD,Clustering algorithms,Coherence,data
	representation,data structures,Dictionaries,dictionary atoms,dictionary
	learning,encoding,image coding,Image Processing,Image reconstruction,image
	representation,maximal block size,objective function,overcomplete
	dictionary,pattern clustering,performance improvement,regularization
	term,Signal processing algorithms,Signal to noise ratio,Sparse Coding,sparse
	dictionary learning,sparse representation,sparse signal representation,two-stage
	agglomerative hierarchical clustering method},
  pages = {3528-3532},
  owner = {afdidehf}
}

@inproceedings{Zhang2007,
  title = {Solution to Linear Inverse Problem with MMV having Linearly Varying 	Sparsity Structure},
  doi = {10.1109/RADAR.2007.374287},
  abstract = {In this paper, an extension to current algorithms dealing with inverse
	problem is considered. In stead of that invariant sparse profile
	of the solution vectors is concerned, we mainly focus on the problem
	with linearly varying sparse structures. Two methods are proposed
	to solve the linear inverse problem with the unknown linearly varying
	sparse structure by using MMV. In order to adapt to the linearly
	varying sparse profile of the solutions, one method, named LMMV (Linearly-MMV),
	introduces a new parameter and makes use of circular shift matrix
	to convert the new problem to the one with invariant sparse profile
	and new iterative algorithm is derived in principle of existing methods.
	Another method, named WMMV (Wide-MMV), attributes the change of the
	sparse structure of the solution vectors to the inaccuracy caused
	by the chosen dictionary and combines several rows of the dictionary
	together, which is equivalent to find a lower dimensional sparse
	solution and in turn gives a more robust algorithm. Numerical experiments
	with random dictionaries and applications to direction-of-arrival
	(DOA) estimation verify the validation of the proposed two methods
	and their superiority to some existing methods is illustrated.},
  timestamp = {2016-07-10T08:19:55Z},
  booktitle = {Radar Conference, 2007 IEEE},
  author = {Zhang, Y. and Wan, Q. and Yang, W.L.},
  month = apr,
  year = {2007},
  keywords = {circular shift matrix,Dictionaries,direction-of-arrival estimation,Direction
	of arrival estimation,invariant sparse profile,inverse problems,iterative
	algorithm,Iterative algorithms,iterative methods,linear Inverse,linear
	inverse problem,linearly varying,linearly varying sparsity structure,Matching
	pursuit algorithms,Matrix converters,MMV,multiple measurement vectors,Robustness,Sparse,sparse matrices,sparse
	matrices,target tracking,Vectors},
  pages = {602-607},
  owner = {Fardin}
}

@article{Zhang2014c,
  title = {Off-grid DOA estimation using array covariance matrix and block-sparse 	Bayesian learning},
  volume = {98},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2013.11.022},
  abstract = {Abstract A new method based on a novel model for off-grid direction-of-arrival
	(DOA) estimation is presented. The novel model is based on the sample
	covariance matrix and the off-grid representation of the steering
	vector. Based on this model, its equivalent signals are assumed to
	satisfy independent Gaussian distribution and its noise variance
	can be normalized to 1. The off-grid \{DOAs\} are estimated by the
	block sparse Bayesian algorithm. The advantages of the proposed method
	are that it considers the temporal correlation existed in each row
	of the equivalent signal sample matrix and the normalized noise variance
	does not need to be estimated. Moreover, this algorithm can work
	without the knowledge of the number of signals. Numerical simulations
	demonstrate the superior performance of the proposed method.},
  timestamp = {2016-07-10T07:10:48Z},
  journal = {Signal Processing},
  author = {Zhang, Yi and Ye, Zhongfu and Xu, Xu and Hu, Nan},
  year = {2014},
  keywords = {Bayesian,direction-of-arrival,learning,model,Off-grid,recovery,Sparse},
  pages = {197 - 201},
  owner = {afdidehf}
}

@inproceedings{Zhang2014d,
  title = {Multi-source motion images fusion based on 3D sparse representation},
  doi = {10.1109/CCIS.2014.7175810},
  abstract = {In order to effectively fusing multi-source images of the same scene,
	this paper proposes a novel multi-source motion images' fusion framework
	based on 3d sparse representation. Compared to the single-frame image
	fusion technology, the temporal dimension of the motion image is
	needed to be considered by using sparse coefficient across the adjacent
	front and rear frames. This helps to obtain a more efficient algorithm
	and better fusion quality. In addition, the proposed algorithm uses
	3D atomic block, make full use of the space-time motion image sequence
	information, removes redundant dictionary atoms and improves dictionary
	generation rules to reduce the number of iterations. The proposed
	fusion framework consists of four steps, i.e. training and updating
	the dictionary, finding the sparse coefficient, coefficient fusion,
	image reconstruction. The experimental results demonstrate that,
	the proposed based on 3D sparse representation fusion method has
	superior performance to the traditional methods (Dual-Tree Complex
	Wavelet transform, discrete wavelet transform and Ordinary sparse
	representation) on objective and subjective metrics.},
  timestamp = {2016-07-10T06:47:55Z},
  booktitle = {Cloud Computing and Intelligence Systems (CCIS), 2014 IEEE 3rd International 	Conference on},
  author = {Zhang, Zhenhong and Du, Junping and Xu, Liang and Li, Qingping},
  month = nov,
  year = {2014},
  keywords = {3D atomic block,3D sparse representation,coefficient fusion,coefficient
	fusion,dictionary generation rule improvement,dictionary
	generation rule improvement,dictionary training,dictionary update,dictionary
	update,Discrete wavelet transforms,Discrete
	wavelet transforms,front frames,image fusion,Image reconstruction,Image
	reconstruction,image representation,image
	representation,image sequences,iteration reduction,learning
	(artificial intelligence),learning (artificial
	intelligence),Matrix converters,motion estimation,multisource motion
	image fusion,multisource
	motion image fusion,objective metrics,optimisation,Random access memory,Random access
	memory,rear frames,rear
	frames,redundant dictionary atom removal,space-time motion
	image sequence information,space-time motion image
	sequence information,sparse coefficient,sparse coefficients,sparse
	matrices,subjective metrics,temporal dimension,Three-dimensional
	displays},
  pages = {624-629},
  owner = {afdidehf}
}

@article{Zhang2014e,
  title = {Spatiotemporal Sparse Bayesian Learning With Applications to Compressed 	Sensing of Multichannel Physiological Signals},
  volume = {22},
  issn = {1534-4320},
  doi = {10.1109/TNSRE.2014.2319334},
  abstract = {Energy consumption is an important issue in continuous wireless telemonitoring
	of physiological signals. Compressed sensing (CS) is a promising
	framework to address it, due to its energy-efficient data compression
	procedure. However, most CS algorithms have difficulty in data recovery
	due to nonsparsity characteristic of many physiological signals.
	Block sparse Bayesian learning (BSBL) is an effective approach to
	recover such signals with satisfactory recovery quality. However,
	it is time-consuming in recovering multichannel signals, since its
	computational load almost linearly increases with the number of channels.
	This work proposes a spatiotemporal sparse Bayesian learning algorithm
	to recover multichannel signals simultaneously. It not only exploits
	temporal correlation within each channel signal, but also exploits
	inter-channel correlation among different channel signals. Furthermore,
	its computational load is not significantly affected by the number
	of channels. The proposed algorithm was applied to brain computer
	interface (BCI) and EEG-based driver's drowsiness estimation. Results
	showed that the algorithm had both better recovery performance and
	much higher speed than BSBL. Particularly, the proposed algorithm
	ensured that the BCI classification and the drowsiness estimation
	had little degradation even when data were compressed by 80%, making
	it very suitable for continuous wireless telemonitoring of multichannel
	signals.},
  timestamp = {2016-07-11T16:55:18Z},
  number = {6},
  journal = {Neural Systems and Rehabilitation Engineering, IEEE Transactions 	on},
  author = {Zhang, Zhilin and Jung, Tzyy-Ping and Makeig, S. and Pi, Zhouyue and Rao, B.D.},
  month = nov,
  year = {2014},
  keywords = {Bayes methods,block sparse Bayesian learning,body area networks,Brain�computer
	interface (BCI),Brain-computer interfaces,compressed sensing,compressed
	sensing,Compressed Sensing (CS),computational load,continuous wireless
	telemonitoring,data compression,EEG based driver's drowsiness estimation,electroencephalography,electroencephalography
	(EEG),energy consumption,learning (artificial intelligence),medical
	signal processing,multichannel physiological signals,recovery quality,sparse
	Bayesian learning (SBL),spatiotemporal correlation,spatiotemporal
	phenomena,spatiotemporal sparse Bayesian learning,telemedicine,telemonitoring,wireless
	body-area network (WBAN),Wireless communication},
  pages = {1186-1197},
  owner = {afdidehf}
}

@article{Zhang2013d,
  title = {Compressed Sensing for Energy-Efficient Wireless Telemonitoring of 	Noninvasive Fetal ECG Via Block Sparse Bayesian Learning},
  volume = {60},
  issn = {0018-9294},
  doi = {10.1109/TBME.2012.2226175},
  abstract = {Fetal ECG (FECG) telemonitoring is an important branch in telemedicine.
	The design of a telemonitoring system via a wireless body area network
	with low energy consumption for ambulatory use is highly desirable.
	As an emerging technique, compressed sensing (CS) shows great promise
	in compressing/reconstructing data with low energy consumption. However,
	due to some specific characteristics of raw FECG recordings such
	as nonsparsity and strong noise contamination, current CS algorithms
	generally fail in this application. This paper proposes to use the
	block sparse Bayesian learning framework to compress/reconstruct
	nonsparse raw FECG recordings. Experimental results show that the
	framework can reconstruct the raw recordings with high quality. Especially,
	the reconstruction does not destroy the interdependence relation
	among the multichannel recordings. This ensures that the independent
	component analysis decomposition of the reconstructed recordings
	has high fidelity. Furthermore, the framework allows the use of a
	sparse binary sensing matrix with much fewer nonzero entries to compress
	recordings. Particularly, each column of the matrix can contain only
	two nonzero entries. This shows that the framework, compared to other
	algorithms such as current CS algorithms and wavelet algorithms,
	can greatly reduce code execution in CPU in the data compression
	stage.},
  timestamp = {2016-07-08T11:55:52Z},
  number = {2},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Zhang, Zhilin and Jung, Tzyy-Ping and Makeig, S. and Rao, B.D.},
  month = feb,
  year = {2013},
  keywords = {Algorithms,Bayes methods,Bayes Theorem,block sparse Bayesian learning,Block
	sparse Bayesian learning (BSBL),body area networks,compressed sensing,Compressed
	Sensing (CS),Computer-Assisted,Correlation,CPU,current compressed
	sensing algorithms,Databases,data compressing-reconstructing,Data
	Compression,electrocardiography,energy consumption,energy-efficient
	wireless telemonitoring,Factual,Female,fetal ECG (FECG),Fetal Monitoring,healthcare,Humans,Independent
	component analysis,independent component analysis decomposition,Independent
	component analysis (ICA),low energy consumption,medical signal processing,multichannel
	recordings,Noise,noise contamination,noninvasive fetal ECG,Partitioning
	algorithms,Pregnancy,Sensors,signal denoising,signal processing,Signal
	processing algorithms,signal reconstruction,Signal-To-Noise Ratio,sparse
	binary sensing matrix,sparse matrices,telemedicine,telemonitoring,wavelet
	algorithms,wireless body area network,Wireless Technology},
  pages = {300-309},
  owner = {afdidehf}
}

@article{Zhang2013e,
  title = {Compressed Sensing of EEG for Wireless Telemonitoring With Low Energy 	Consumption and Inexpensive Hardware},
  volume = {60},
  issn = {0018-9294},
  doi = {10.1109/TBME.2012.2217959},
  abstract = {Telemonitoring of electroencephalogram (EEG) through wireless body-area
	networks is an evolving direction in personalized medicine. Among
	various constraints in designing such a system, three important constraints
	are energy consumption, data compression, and device cost. Conventional
	data compression methodologies, although effective in data compression,
	consumes significant energy and cannot reduce device cost. Compressed
	sensing (CS), as an emerging data compression methodology, is promising
	in catering to these constraints. However, EEG is nonsparse in the
	time domain and also nonsparse in transformed domains (such as the
	wavelet domain). Therefore, it is extremely difficult for current
	CS algorithms to recover EEG with the quality that satisfies the
	requirements of clinical diagnosis and engineering applications.
	Recently, block sparse Bayesian learning (BSBL) was proposed as a
	new method to the CS problem. This study introduces the technique
	to the telemonitoring of EEG. Experimental results show that its
	recovery quality is better than state-of-the-art CS algorithms, and
	sufficient for practical use. These results suggest that BSBL is
	very promising for telemonitoring of EEG and other nonsparse physiological
	signals.},
  timestamp = {2016-07-08T11:56:57Z},
  number = {1},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Zhang, Zhilin and Jung, Tzyy-Ping and Makeig, S. and Rao, B.D.},
  month = jan,
  year = {2013},
  keywords = {Algorithms,Bayes methods,Bayes Theorem,block sparse Bayesian learning,Block
	sparse Bayesian learning (BSBL),body area networks,BSBL,compressed
	sensing,Compressed Sensing (CS),Computer-Assisted,Databases,data
	compression methodologies,device cost,Dictionaries,EEG compressed
	sensing,Electroencephalogram,electroencephalogram (EEG),electroencephalography,energy
	consumption,Factual,healthcare,Humans,learning (artificial intelligence),medical
	signal processing,nonsparse physiological signals,nonsparse time
	domain EEG data,nonsparse wavelet domain EEG data,patient monitoring,personalized
	medicine,Remote Sensing Technology,Sensors,signal processing,sparse
	matrices,telemedicine,telemonitoring,wavelet transforms,wireless
	body area networks,wireless body-area network (WBAN),wireless sensor
	networks,Wireless Technology,wireless telemonitoring},
  pages = {221-224},
  owner = {afdidehf}
}

@article{Zhang2013f,
  title = {Extension of SBL Algorithms for the Recovery of Block Sparse Signals 	With Intra-Block Correlation},
  volume = {61},
  issn = {1053-587X},
  doi = {10.1109/TSP.2013.2241055},
  abstract = {We examine the recovery of block sparse signals and extend the recovery
	framework in two important directions; one by exploiting the signals'
	intra-block correlation and the other by generalizing the signals'
	block structure. We propose two families of algorithms based on the
	framework of block sparse Bayesian learning (BSBL). One family, directly
	derived from the BSBL framework, require knowledge of the block structure.
	Another family, derived from an expanded BSBL framework, are based
	on a weaker assumption on the block structure, and can be used when
	the block structure is completely unknown. Using these algorithms,
	we show that exploiting intra-block correlation is very helpful in
	improving recovery performance. These algorithms also shed light
	on how to modify existing algorithms or design new ones to exploit
	such correlation and improve performance.},
  timestamp = {2016-07-08T12:27:34Z},
  number = {8},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Zhang, Z. and Rao, B.D.},
  month = apr,
  year = {2013},
  keywords = {Bayesian methods,Bismuth,Block sparse model,compressed sensing,Correlation,Cost
	function,intra-block correlation,Partitioning algorithms,sparse Bayesian
	learning (SBL),sparse matrices,sparse signal recovery,Vectors},
  pages = {2009-2015},
  owner = {afdidehf}
}

@article{Zhang2011,
  title = {Sparse Signal Recovery With Temporally Correlated Source Vectors 	Using Sparse Bayesian Learning},
  volume = {5},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2011.2159773},
  abstract = {We address the sparse signal recovery problem in the context of multiple
	measurement vectors (MMV) when elements in each nonzero row of the
	solution matrix are temporally correlated. Existing algorithms do
	not consider such temporal correlation and thus their performance
	degrades significantly with the correlation. In this paper, we propose
	a block sparse Bayesian learning framework which models the temporal
	correlation. We derive two sparse Bayesian learning (SBL) algorithms,
	which have superior recovery performance compared to existing algorithms,
	especially in the presence of high temporal correlation. Furthermore,
	our algorithms are better at handling highly underdetermined problems
	and require less row-sparsity on the solution matrix. We also provide
	analysis of the global and local minima of their cost function, and
	show that the SBL cost function has the very desirable property that
	the global minimum is at the sparsest solution to the MMV problem.
	Extensive experiments also provide some interesting results that
	motivate future theoretical research on the MMV model.},
  timestamp = {2016-07-11T16:53:22Z},
  number = {5},
  journal = {Selected Topics in Signal Processing, IEEE Journal of},
  author = {Zhang, Zhilin and Rao, B.D.},
  month = sep,
  year = {2011},
  keywords = {Algorithm design and analysis,Bayesian methods,Bayes methods,block
	sparse Bayesian learning framework,brain models,compressed sensing,Correlation,correlation
	methods,Cost function,Covariance matrix,global minima,learning (artificial
	intelligence),Local minima,multiple measurement vectors,multiple
	measurement vectors (MMV),recovery performance,SBL algorithm,Signal
	processing algorithms,source vector,sparse Bayesian learning (SBL),sparse
	matrices,sparse signal recovery,sparse signal recovery problem,temporal correlation,temporal
	correlation},
  pages = {912-926},
  owner = {afdidehf}
}

@inproceedings{Zhao2015,
  title = {Block-wise constrained sparse graph for face image representation},
  volume = {1},
  doi = {10.1109/FG.2015.7163087},
  abstract = {Subspace segmentation is one of the hottest issues in computer vision
	and machine learning fields. Generally, data (e.g. face images) are
	lying in a union of multiple linear subspaces, therefore, it is the
	key to find a block diagonal affinity matrix, which would result
	in segmenting data into different clusters correctly. Recently, graph
	construction based segmentation methods attract lots of attention.
	Following this line, we propose a novel approach to construct a Sparse
	Graph with Block-wise constraint for face representation, named SGB.
	Inspired by the recent study of least square regression coefficients,
	SGB firstly generates a compact block-diagonal coefficient matrix.
	Meanwhile, graph regularizer brings in a sparse graph, which focuses
	on the local structure and benefits multiple subspaces segmentation.
	By introducing different graph regularizers, our graph would be more
	balanced with b-matching constraint for balanced data. By using k-nearest
	neighbor regularizer, more manifold information can be preserved
	for unbalanced data. To solve our model, we come up with a joint
	optimization strategy to learn block-wise and sparse graph simultaneously.
	To demonstrate the effectiveness of our method, we consider two application
	scenarios, i.e., face clustering and kinship verification. Extensive
	results on Extended YaleB, ORL and kinship dataset Family101 demonstrate
	that our graph consistently outperforms several state-of-the-art
	graphs. Particularly, our method raises the performance bar by around
	14% in kinship verification application.},
  timestamp = {2016-07-08T11:48:25Z},
  booktitle = {Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International 	Conference and Workshops on},
  author = {Zhao, Handong and Ding, Zhengming and Fu, Yun},
  month = may,
  year = {2015},
  keywords = {Accuracy,block-diagonal coefficient matrix,block-wise constrained
	sparse graph,b-matching constraint,Convergence,Face,face clustering,face
	image representation,face recognition,face representation,graph regularizer,Graph
	theory,image representation,Image segmentation,kinship verification,kinship
	verification,k-nearest neighbor regularizer,k-nearest
	neighbor regularizer,linear programming,matrix algebra,matrix
	algebra,multiple subspaces segmentation,multiple subspaces
	segmentation,Optimization,SGB,sparse matrices},
  pages = {1-6},
  owner = {afdidehf}
}

@inproceedings{Zhao2012,
  title = {Compressive sensing of block-sparse signals recovery based on sparsity 	adaptive regularized orthogonal matching pursuit algorithm},
  doi = {10.1109/ICACI.2012.6463352},
  abstract = {The structure of block sparsity in multi-band signals is prevalent.
	Among the block-sparse signal problems for compressive sensing, the
	most presenting recovery algorithms require block sparsity as prior
	information, whereas it may not be available in many practical applications.
	In this paper, a block sparsity adaptive regularized orthogonal matching
	pursuit algorithm (BAROMP) for compressive sensing is presented.
	The proposed algorithm could guarantee the accuracy of recovery by
	both the adaptive process which chooses the candidate set automatically
	and the regularization process which decides the atoms in the final
	support set. The simulation results show that the recovery probability
	of BAROMP which does not require sparsity as prior information is
	near to BROMP.},
  timestamp = {2016-07-08T12:00:58Z},
  booktitle = {Advanced Computational Intelligence (ICACI), 2012 IEEE Fifth International 	Conference on},
  author = {Zhao, Qiang and Wang, Jinkuan and Han, Yinghua and Han, Peng},
  month = oct,
  year = {2012},
  keywords = {BAROMP,block sparse signal problems,block sparse signals recovery,block
	sparsity adaptive regularized orthogonal matching pursuit algorithm,block
	sparsity structure,compressed sensing,compressive sensing,Educational
	institutions,Indexes,iterative methods,Matching pursuit algorithms,multiband
	signals,Signal processing algorithms,sparse matrices,sparsity adaptive
	orthogonal matching pursuit algorithm,time-frequency analysis,Vectors},
  pages = {1141-1144},
  owner = {afdidehf}
}

@inproceedings{Zheng2011,
  title = {Persistently active block sparsity with application to direction-of-arrival 	estimation of moving sources},
  doi = {10.1109/CAMSAP.2011.6136035},
  abstract = {In this paper, the problem of recovering inconsistent sparse models
	from multiple observations is considered. A new method is developed
	by introducing a novel objective function, which exploits both block-level
	and element-level sparsities and promotes persistence in activity
	within a block. Then, we use a SVD-based method to reduce its computational
	complexity. Application of the method to the Direction-Of-Arrival
	(DOA) estimation of moving sources using a sensor array is presented
	and a simulation example is shown as a demonstration of the promising
	performance of the method in a moving DOA setting, particularly when
	sources are very close to each other.},
  timestamp = {2016-07-10T07:32:15Z},
  booktitle = {Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 	2011 4th IEEE International Workshop on},
  author = {Zheng, J. and Kaveh, M.},
  month = dec,
  year = {2011},
  keywords = {active block level sparsity,Arrays,array signal processing,block sparsity,Computational
	complexity,direction-of-arrival estimation,direction of arrival estimation,Direction
	of arrival estimation,DOA,element-level sparsity,element-level
	sparsity,Estimation,inconsistent sparse model recovery,inconsistent
	sparse model recovery,Indexes,moving sources,moving
	sources,objective function,Persistent Activity,Persistent
	Activity,sensor array,Singular Value Decomposition,singular
	value decomposition,SPICE,SVD-based method,SVD-based
	method,Trajectory},
  pages = {393-396},
  owner = {afdidehf}
}

@inproceedings{Zhong2014,
  title = {Radar signal reconstruction algorithm based on Complex Block Sparse 	Bayesian Learning},
  doi = {10.1109/ICOSP.2014.7015329},
  abstract = {Sparse signal reconstruction is one core technology of Radar compressive
	sensing. Algorithms based on Sparse Bayesian Learning are the focus
	of study in recent years, because of their good reconstruction preferment.
	Signal reconstruction algorithm base Block Sparse Bayesian Learning
	(BSBL) frame is excellent for reconstructing signals of block sparse
	structure. It exploits not only the block sparse structure of signals,
	but also the correlation inside the block to improve the accuracy
	of reconstruction. But, classical BSBL algorithm is proposed for
	real signals. So, it cannot reconstruct complex signal directly.
	This paper extended the BSBL frame and reconstruction algorithm based
	on BSBL frame into complex number domain. The extended signal reconstruction
	algorithm based on Complex Block Sparse Bayesian Learning (CBSBL)
	frame can deal with complex signals directly. And then CBSBL algorithm
	was applied to reconstruct radar complex signals. Experimental results
	showed that the CBSBL algorithm is effective and efficient.},
  timestamp = {2016-07-10T07:40:00Z},
  booktitle = {Signal Processing (ICSP), 2014 12th International Conference on},
  author = {Zhong, Jinrong and Wen, Gongjian and Ma, CongHui and Ding, Boyuan},
  month = oct,
  year = {2014},
  keywords = {Accuracy,Bayesian learning,Bayes methods,block sparse,block
	sparse structure,block sparse
	structure,CBSBL algorithm,complex block sparse Bayesian learning,complex
	signal,compressed sensing,compress sensing,correlation methods,Image
	reconstruction,Radar,radar complex signals,radar compressive sensing,radar
	signal processing,radar signal reconstruction algorithm,reconstructed
	method,Signal processing algorithms,signal reconstruction,sparse
	signal reconstruction},
  pages = {1930-1933},
  owner = {afdidehf}
}

@inproceedings{Zhu2014,
  title = {Sparse representation of working memory processes based on fMRI data},
  doi = {10.1109/ISBI.2014.6867938},
  abstract = {Cognitive processes, such as working memory, are widely considered
	as dynamic, and they are believed to involve complex functional information
	flows in large-scale brain networks. However, traditional voxel-based
	fMRI time series analysis methods, which essentially assume that
	the hemodynamic responses of involved brain regions follow the block
	or event-related paradigms, are limited in recognizing and modeling
	this complex process that has been changing in both spatial and temporal
	spaces. In this paper, we propose a novel computational framework
	to explore the potential mechanisms underlying the working memory
	process. Specifically, we adopt the Transfer Entropy (TE) as the
	measure to model the directional functional interactions based on
	a series of structurally-consistent cortical landmarks. Then an effective
	and carefully-designed sparsity learning procedure was applied to
	derive the most representative interaction patterns for further analysis.
	Our results show a few cortical landmarks displaying significantly
	higher interaction strength during the whole fMRI scan and suggest
	that they might act as hubs in coordinating working memory process.
	Moreover, four prominent interaction patterns associated with the
	Default Mode Network (DMN) are found to be attenuated in the task
	performance period.},
  timestamp = {2016-07-11T16:52:16Z},
  booktitle = {Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium 	on},
  author = {Zhu, Dajiang and Li, Xiang and Liu, Tianming},
  month = apr,
  year = {2014},
  keywords = {Analytical models,biomedical MRI,Brain,brain models,Cognition,cognitive
	processes,complex functional information,cortical landmarks,default
	mode network,Dictionaries,Diffusion Tensor Imaging,directional functional
	interactions,DMN,encoding,Entropy,event-related paradigms,functional
	interaction,functional magnetic resonance imaging,haemodynamics,hemodynamic
	responses,image representation,interaction patterns,large-scale brain
	networks,medical image processing,Sparse Coding,sparse representation,sparsity
	learning procedure,structurally-consistent cortical landmarks,task
	performance,Time series,transfer entropy,voxel-based fMRI time series
	analysis methods,working memory processes},
  pages = {584-587},
  owner = {afdidehf}
}

@inproceedings{Zhu2013,
  title = {A lightweight decentralized algorithm for jointly sparse optimization},
  abstract = {This paper develops a lightweight decentralized algorithm to solve
	the convex jointly sparse optimization problem, as known as the group
	lasso. In a networked multi-agent system, each agent takes linear
	measurements from its signal, and all signals share the same sparsity
	pattern. In decentralized jointly sparse optimization, agents collaborate
	to recover their signals by taking advantage of the same sparsity
	pattern, but they are allowed to have limited information exchange
	with their one-hop neighbors. We propose to use the block coordinate
	descent algorithm to solve the convex jointly sparse optimization
	problem in a centralized manner, and adopt an inexact average consensus
	technique for its decentralized implementation. The proposed decentralized
	algorithm is lightweight; each agent neither exchanges its measurement
	matrix and measurement vector nor shares its current estimate of
	its signal with its one-hop neighbors. Simulation results demonstrate
	the effectiveness of the proposed algorithm, as well as its empirical
	global convergence to the centralized optimal solution.},
  timestamp = {2016-07-08T10:25:41Z},
  booktitle = {Control Conference (CCC), 2013 32nd Chinese},
  author = {Zhu, Jie and Li, Yongcheng and Xue, Meisheng and Ling, Qing},
  month = jul,
  year = {2013},
  keywords = {Algorithm design and analysis,average consensus technique,block coordinate
	descent,block coordinate descent algorithm,centralized manner,centralized
	optimal solution,compressed sensing,Convergence,convex jointly sparse
	optimization problem,convex programming,Current measurement,decentralized
	implementation,decentralized jointly sparse optimization,Decentralized
	optimization,global convergence,group lasso,information exchange,jointly
	sparse optimization,Joints,lightweight decentralized algorithm,linear
	measurements,matrix algebra,measurement matrix,measurement vector,Minimization,multi-agent
	systems,networked multiagent system,Networked multi-agent system,one-hop
	neighbors,Optimization,sparsity pattern,Vectors},
  pages = {4666-4670},
  owner = {afdidehf}
}

@inproceedings{Ziaei2010,
  title = {Compressed sensing of different size block-sparse signals: Efficient 	recovery},
  doi = {10.1109/ACSSC.2010.5757679},
  abstract = {This paper considers compressed sensing of different size block-sparse
	signals, i.e. signals with nonzero elements occurring in blocks with
	different lengths. A new sufficient condition for mixed l2/l1-optimization
	algorithm is derived to successfully recover k-sparse signals. We
	show that if the signal possesses k-block sparse structure, then
	via mixed l2/l1-optimization algorithm, a better reconstruction results
	can be achieved in comparison with the conventional l1-optimization
	algorithm and fixed-size mixed l2/l1-optimization algorithm. The
	significance of the results presented in this paper lies in the fact
	that making explicit use of different block-sparsity can yield better
	reconstruction properties than treating the signal as being sparse
	in the conventional sense, thereby ignoring the structure in the
	signal.},
  timestamp = {2016-07-08T11:56:47Z},
  booktitle = {Signals, Systems and Computers (ASILOMAR), 2010 Conference Record 	of the Forty Fourth Asilomar Conference on},
  author = {Ziaei, A. and Pezeshki, A. and Bahmanpour, S. and Azimi-Sadjadi, M.R.},
  month = nov,
  year = {2010},
  keywords = {block-sparse signals,Block-sparsity,Coherence,compressed sensing,compressed
	sensing,Dictionaries,Error analysis,Matching pursuit algorithms,Minimization,mixed
	l2/l1-optimization algorithm,mixed-optimization algorithm,signal
	processing,sparse matrices},
  pages = {818-821},
  annote = {read},
  owner = {afdidehf}
}

@article{Zibulevsky2001,
  title = {Blind source separation by sparse decomposition in a signal dictionary},
  volume = {13},
  abstract = {The blind source separation problem is to extract the underlying source
	signals from a set of linear mixtures, where the mixing matrix is
	unknown. This situation is common in acoustics, radio, medical signal
	and image processing, hyperspectral imaging, and other areas. We
	suggest a twostage separation process: a priori selection of a possibly
	overcomplete signal dictionary (for instance, a wavelet frame or
	a learned dictionary) in which the sources are assumed to be sparsely
	representable, followed by unmixing the sources by exploiting the
	their sparse representability. We consider the general case of more
	sources than mixtures, but also derive a more efficient algorithm
	in the case of a nonovercomplete dictionary and an equal numbers
	of sources and mixtures. Experiments with artificial signals and
	musical sounds demonstrate significantly better separation than other
	known techniques.},
  timestamp = {2016-09-29T16:10:36Z},
  number = {4},
  journal = {Neural Computation},
  author = {Zibulevsky, M. and Pearlmutter, B. A.},
  year = {2001},
  pages = {863--882},
  owner = {afdidehf}
}

@article{Zonoobi2011,
  title = {Gini Index as Sparsity Measure for Signal Reconstruction from Compressive 	Samples},
  volume = {5},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2011.2160711},
  abstract = {Sparsity is a fundamental concept in compressive sampling of signals/images,
	which is commonly measured using the l0 norm, even though, in practice,
	the l1 or the lp ( 0 <; p <; 1) (pseudo-) norm is preferred. In this
	paper, we explore the use of the Gini index (GI), of a discrete signal,
	as a more effective measure of its sparsity for a significantly improved
	performance in its reconstruction from compressive samples. We also
	successfully incorporate the GI into a stochastic optimization algorithm
	for signal reconstruction from compressive samples and illustrate
	our approach with both synthetic and real signals/images.},
  timestamp = {2016-07-08T12:37:57Z},
  number = {5},
  journal = {Selected Topics in Signal Processing, IEEE Journal of},
  author = {Zonoobi, D. and Kassim, A.A. and Venkatesh, Y.V.},
  month = sep,
  year = {2011},
  keywords = {compressive sampling,Compressive sensing (CS),discrete signal,Gini
	index,Gini index (GI),image coding,Image reconstruction,image sampling,Minimization,Noise,Noise
	measurement,Non-convex optimization,signal reconstruction,simultaneous
	perturbation stochastic approximation (SPSA),sparsity measure,sparsity
	measures,stochastic optimization algorithm,stochastic programming,transforms,TV},
  pages = {927-932},
  owner = {Fardin}
}

@inproceedings{Zoric2013,
  title = {Block-sparse out-of-core solver accelerated using GPUs for solving 	MoM problems},
  doi = {10.1109/APS.2013.6711601},
  abstract = {We outline an algorithm for block-sparse GPU accelerated out-of-core
	solver. The algorithm takes advantage of the block-sparsity that
	exists in method of moment analysis of composite electromagnetic
	systems with multiple materials. The efficiently of the proposed
	algorithm is demonstrated on the example of Luneburg lenses up to
	40 wavelengths diameter. The approach yields speedup of up to 8 times,
	compared to general-purpose GPU accelerated out-of-core solver.},
  timestamp = {2016-07-08T11:41:49Z},
  booktitle = {Antennas and Propagation Society International Symposium (APSURSI), 	2013 IEEE},
  author = {Zoric, D.P. and Olcan, D.I. and Kolundzija, B.M.},
  month = jul,
  year = {2013},
  keywords = {Acceleration,block-sparse GPU accelerated out-of-core solver,composite
	electromagnetic system,composite materials,computational electromagnetics,graphics processing units,Graphics
	processing units,lenses,Luneburg lenses,Mathematical
	model,matrix decomposition,method of moment analysis,method of moments,Method
	of moments,MoM problem,multiple material,sparse matrices},
  pages = {1886-1887},
  owner = {afdidehf}
}

@inproceedings{Zorlein2011,
  title = {Performance of error correction based on Compressed Sensing},
  doi = {10.1109/ISWCS.2011.6125372},
  abstract = {In this paper, the real-valued Compressed Sensing (CS)-based error
	correction scheme which was proposed by Candes and Tao is evaluated
	against the background of channel coding theory. Substantial differences
	to conventional linear block codes over Galois fields are reviewed
	and interpreted. A unified channel model is introduced. It enables
	performance evaluation of more realistic almost sparse channels,
	which are of interest for communication applications. It is shown
	by simulations that certain variations of the CS based error correction
	scheme are suitable for special channels. Additionally, explanations
	for the noise sensitivity of CS based error correction schemes are
	given.},
  timestamp = {2016-07-10T07:32:09Z},
  booktitle = {Wireless Communication Systems (ISWCS), 2011 8th International Symposium 	on},
  author = {Zorlein, H. and Lazich, D. and Bossert, M.},
  month = nov,
  year = {2011},
  keywords = {bit error rate,block codes,channel coding,channel coding theory,Channel
	models,compressed sensing,CS based error correction scheme,CS based error correction
	scheme,Decoding,error correction codes,Error
	correction codes,error correction performance evaluation,error correction performance
	evaluation,Galois fields,Generators,linear block code,linear
	block code,linear codes,performance evaluation,performance
	evaluation,Signal to noise ratio,unified channel model,unified
	channel model,Vectors},
  pages = {301-305},
  owner = {afdidehf}
}

@article{Zou2012,
  title = {A Block Fixed Point Continuation Algorithm for Block-Sparse Reconstruction},
  volume = {19},
  issn = {1070-9908},
  doi = {10.1109/LSP.2012.2195488},
  abstract = {Block-sparse reconstruction, which arises from the reconstruction
	of block-sparse signals in structured compressed sensing, is generally
	considered difficult to solve due to the mixed-norm structure. In
	this letter, we propose an algorithm for reconstructing block-sparse
	signals, that is an extension of fixed point continuation in block-wise
	case by incorporating block coordinate descent technique. We also
	apply our algorithm to multiple measurement vector reconstruction,
	that is a special case of block-sparse reconstruction and can be
	used in magnetic resonance imaging reconstruction. Numerical results
	show the validity of our algorithm for both synthetic and real-world
	data.},
  timestamp = {2016-07-08T10:06:53Z},
  number = {6},
  journal = {Signal Processing Letters, IEEE},
  author = {Zou, Jian and Fu, Yuli and Xie, Shengli},
  month = jun,
  year = {2012},
  keywords = {approximation theory,block coordinate descent,block coordinate descent
	technique,block fixed point continuation algorithm,block-sparse reconstruction,block-sparse
	reconstruction,block-sparse signal,block-wise case,compressed sensing,Educational
	institutions,fixed point continuation,Image reconstruction,Magnetic
	Resonance Imaging,magnetic resonance imaging reconstruction,measurement
	vector reconstruction,Minimization,mixed-norm structure,Signal processing
	algorithms,signal reconstruction,structured compressed sensing,Vectors},
  pages = {364-367},
  owner = {afdidehf}
}

@inproceedings{Zou2012a,
  title = {Split Bregman algorithms for block-sparse reconstruction},
  doi = {10.1109/ICACI.2012.6463349},
  abstract = {Block-sparse reconstruction, which arises from the reconstruction
	of block-sparse signals in structured compressed sensing, is generally
	considered to be difficult due to the mixed-norm structure. In this
	paper, we propose efficient algorithms based on split Bregman iteration
	to solve the block-sparse reconstruction problems, including the
	constrained form and unconstrained form. Numerical results show that
	the proposed algorithms are outperform the state of the art algorithms.},
  timestamp = {2016-07-11T16:55:35Z},
  booktitle = {Advanced Computational Intelligence (ICACI), 2012 IEEE Fifth International 	Conference on},
  author = {Zou, Jian and Fu, Yuli and Zhang, Qiheng and Li, Haifeng},
  month = oct,
  year = {2012},
  keywords = {block-sparse reconstruction problem,block-sparse signal reconstruction,compressed
	sensing,Conferences,constrained form,iterative methods,mixed-norm
	structure,signal reconstruction,split Bregman algorithms,split Bregman
	iteration,structured compressed sensing,unconstrained form},
  pages = {1128-1129},
  owner = {afdidehf}
}

@inproceedings{Zubair2014,
  title = {Signal classification based on block-sparse tensor representation},
  doi = {10.1109/ICDSP.2014.6900687},
  abstract = {Block sparsity was employed recently in vector/matrix based sparse
	representations to improve their performance in signal classification.
	It is known that tensor based representation has potential advantages
	over vector/matrix based representation in retaining the spatial
	distributions within the data. In this paper, we extend the concept
	of block sparsity for tensor representation, and develop a new algorithm
	for obtaining sparse tensor representations with block structure.
	We show how the proposed algorithm can be used for signal classification.
	Experiments on face recognition are provided to demonstrate the performance
	of the proposed algorithm, as compared with several sparse representation
	based classification algorithms.},
  timestamp = {2016-07-10T08:11:33Z},
  booktitle = {Digital Signal Processing (DSP), 2014 19th International Conference 	on},
  author = {Zubair, S. and Wang, Wenwu},
  month = aug,
  year = {2014},
  keywords = {block sparse representations,block sparse tensor,block sparsity,classification,Dictionaries,dictionary
	learning,Digital signal processing,face recognition,Indexes,signal
	classification,Signal processing algorithms,sparse matrices,spatial
	distribution,Tensile stress,Tensor Factorization,tensors,Vectors},
  pages = {361-365},
  owner = {afdidehf}
}

@book{Alquier2009,
  series = {Lecture Notes in Statistics},
  title = {Inverse Problems and High-Dimensional Estimation},
  timestamp = {2016-07-09T19:44:14Z},
  publisher = {{\{Springer Berlin Heidelberg\}}},
  editor = {Alquier, Pierre and Gautier, Eric and Stoltz, Gilles},
  year = {2009},
  owner = {afdidehf}
}

@book{Buntine2009,
  title = {Machine learning and knowledge discovery in databases : European 	conference, ECML PKDD 2009, Antwerp, Belgium, September 7-11, 2009 	: proceedings},
  isbn = {3-642-04179-5},
  timestamp = {2016-10-24T16:39:40Z},
  publisher = {{Springer}},
  editor = {Buntine, W. and Grobelnik, M. and Mladenic, D. and J, Shawe-Taylor},
  year = {2009},
  owner = {Fardin}
}

@book{Calders2014,
  edition = {1},
  series = {Lecture Notes in Computer Science 8724 Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, 	Part I},
  isbn = {978-3-662-44847-2 978-3-662-44848-9},
  timestamp = {2016-10-24T16:41:28Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  editor = {Calders, Toon and Esposito, Floriana and H�llermeier, Eyke and Meo, Rosa},
  year = {2014},
  owner = {Fardin}
}

@book{Calders2014a,
  edition = {1},
  series = {Lecture Notes in Computer Science 8725 Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, 	Part II},
  isbn = {978-3-662-44850-2 978-3-662-44851-9},
  timestamp = {2016-10-24T16:41:33Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  editor = {Calders, Toon and Esposito, Floriana and H�llermeier, Eyke and Meo, Rosa},
  year = {2014},
  owner = {Fardin}
}

@book{Calders2014b,
  edition = {1},
  series = {Lecture Notes in Computer Science 8726 Lecture Notes in Artificial 	Intelligence},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, 	Part III},
  isbn = {978-3-662-44844-1 978-3-662-44845-8},
  timestamp = {2016-10-24T16:41:38Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  editor = {Calders, Toon and Esposito, Floriana and H�llermeier, Eyke and Meo, Rosa},
  year = {2014},
  owner = {Fardin}
}

@book{Hanson2010,
  title = {Foundational Issues in Human Brain Mapping},
  timestamp = {2016-07-08T12:32:32Z},
  editor = {Hanson, Stephen Jos� and Bunzl, Martin},
  year = {2010},
  owner = {Fardin}
}

@article{2004,
  title = {Modeling and Analysis (2004)},
  volume = {22, Supplement 1},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/S1053-8119(05)70018-5},
  timestamp = {2016-10-24T16:30:40Z},
  journal = {NeuroImage},
  year = {2004},
  pages = {e1393 - e1900},
  key = {tagkey2004e1393},
  owner = {afdidehf}
}

@article{2002,
  title = {Modeling and analysis (2002)},
  volume = {16},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/S1053-8119(02)90013-3},
  timestamp = {2016-10-24T16:31:06Z},
  number = {2, Supplement 1},
  journal = {NeuroImage},
  year = {2002},
  note = {8th International Conference on Functional Mapping of the Human Brain},
  pages = {769--1198},
  key = {tagkey2002769},
  owner = {afdidehf}
}

@incollection{Calhoun2008,
  title = {ICA for Fusion of Brain Imaging Data},
  timestamp = {2016-07-08T12:45:02Z},
  publisher = {{\{Springer\}}},
  author = {Calhoun, Vince D. and Adal?, T�ulay},
  year = {2008},
  pages = {221-242},
  owner = {Fardin}
}

@book{Casazza2013,
  series = {Applied and Numerical Harmonic Analysis},
  title = {Finite Frames- Theory and Applications},
  isbn = {978-0-8176-8372-6},
  language = {English},
  timestamp = {2016-07-08T12:31:03Z},
  publisher = {{\{Birkhauser Boston\}}},
  author = {Casazza, Peter G. and Kutyniok, Gitta and Philipp, Friedrich},
  editor = {Casazza, Peter G. and Kutyniok, Gitta},
  year = {2013},
  keywords = {Applications of finite frames,Construction of frames,Dual frames,Frame
	operator,frames,Grammian operator,Hilbert space theory,Operator theory,Reconstruction
	algorithms,redundancy,Tight frames},
  owner = {afdidehf}
}

@book{Eldar2015,
  title = {Sampling theory. Beyond bandlimited systems},
  isbn = {978-1-107-00339-2},
  timestamp = {2016-07-10T08:10:00Z},
  publisher = {{\{CUP\}}},
  author = {Eldar, Y.C.},
  year = {2015},
  owner = {Fardin}
}

@book{Fornasier2010,
  series = {Radon Series on Computational and Applied Mathematics},
  title = {Theoretical foundations and numerical methods for sparse recovery},
  isbn = {3-11-022614-6 978-3-11-022614-0},
  timestamp = {2016-07-11T17:02:09Z},
  publisher = {{\{De Gruyter\}}},
  author = {Fornasier, M.},
  year = {2010},
  owner = {Fardin}
}

@incollection{Haufe2009,
  title = {Estimating vector fields using sparse basis field expansions},
  timestamp = {2016-07-08T12:24:20Z},
  booktitle = {Advances in Neural Information Processing Systems 21},
  publisher = {{\{Curran Associates, Inc.\}}},
  author = {Haufe, Stefan and Nikulin, Vadim V. and Ziehe, Andreas and M\"{u}ller, Klaus-Robert and Nolte, Guido},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  pages = {617-624},
  owner = {afdidehf}
}

@book{Junge2010,
  title = {Mixed-Norm Inequalities and Operator Space Lp Embedding Theory},
  timestamp = {2016-07-09T20:12:28Z},
  publisher = {{\{Amer Mathematical Society\}}},
  author = {Junge, Marius and Parcet, Javier},
  year = {2010},
  owner = {afdidehf}
}

@incollection{Karzan2011,
  title = {On the accuracy of $\ell_1$-filtering of signals with block-sparse 	structure},
  abstract = {We discuss new methods for the recovery of signals with block-sparse
	structure, based on `1-minimization. Our emphasis is on the efficiently
	computable error bounds for the recovery routines. We optimize these
	bounds with respect to the method parameters to construct the estimators
	with improved statistical properties. We justify the proposed approach
	with an oracle inequality which links the properties of the recovery
	algorithms and the best estimation performance.},
  timestamp = {2016-07-10T07:14:25Z},
  booktitle = {Advances in Neural Information Processing Systems 24},
  publisher = {{\{Curran Associates, Inc.\}}},
  author = {Karzan, Fatma K. and Nemirovski, Arkadi S. and Polyak, Boris T. and Juditsky, Anatoli},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {1260-1268},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {Fardin}
}

@book{Leman2008,
  edition = {1},
  series = {Lecture Notes in Computer Science 5212},
  title = {Machine Learning and Knowledge Discovery in Databases: European Conference, 	ECML PKDD 2008, Antwerp, Belgium, September 15-19, 2008, Proceedings, 	Part II},
  isbn = {3-540-87480-1 978-3-540-87480-5},
  timestamp = {2016-10-24T16:39:34Z},
  publisher = {{Springer-Verlag Berlin Heidelberg}},
  author = {Leman, Dennis and Feelders, Ad and Knobbe, Arno},
  editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
  year = {2008},
  owner = {Fardin}
}

@book{Meyrowitz1993,
  edition = {1},
  title = {Foundations of Knowledge Acquisition: Machine Learning},
  isbn = {0-7923-9278-7 978-0-7923-9278-1 978-0-585-27366-2},
  timestamp = {2016-07-08T12:32:42Z},
  publisher = {{\{Springer\}}},
  author = {Meyrowitz, Alan L. and Chipman, Susan},
  year = {1993},
  owner = {Fardin}
}

@book{Mohri2012,
  series = {Adaptive computation and machine learning},
  title = {Foundations of machine learning},
  isbn = {978-0-262-01825-8 978-0-262-30566-2 0-262-30566-6 0-262-01825-X},
  timestamp = {2016-07-08T12:32:51Z},
  publisher = {{\{The MIT Press\}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2012},
  owner = {Fardin}
}

@book{Nilsson2005,
  edition = {web draft},
  title = {The mathematical foundations of learning machines},
  isbn = {1-55860-123-6 978-1-55860-123-9},
  timestamp = {2016-07-11T17:01:29Z},
  publisher = {{\{Morgan Kaufmann\}}},
  author = {Nilsson, Nils J.},
  year = {2005},
  owner = {Fardin}
}

@incollection{Podani,
  title = {Distance, similarity, correlation...},
  timestamp = {2016-07-08T12:14:09Z},
  publisher = {{\{E$\backslash$dbendtv$\backslash$dbends University\}}},
  author = {Podani, J�nos},
  owner = {afdidehf}
}

@incollection{Preissl2005,
  series = {International Review of Neurobiology},
  title = {Fetal Magnetoencephalography: Viewing the Developing Brain In Utero},
  volume = {68},
  abstract = {Publisher Summary Neurological assessment is fundamental to the concept
	of comprehensive fetal monitoring. However, there are no tests to
	reliably identify the fetus with neurological impairment or determine
	the developmental trajectory of the fetus in utero. New multichannel
	Superconducting Quantum Interference Device (SQUID) sensor devices
	can now be used to effectively record fetal auditory-evoked magnetic
	fields (fAEF), visual-evoked magnetic fields (fVEF), spontaneous
	brain activity, and heart activity. This chapter discusses the current
	status in the area of fetal magneto-encephalography (fMEG). It describes
	an integrative approach to the data analysis problem, which is based
	on the fact the fMEG is recorded in the presence of several interfering
	biomagnetic signals. The main interferences are the maternal and
	fetal heart signal. These signals can be effectively removed. Regarding
	the primary sensory processing capabilities of the fetus and the
	newborn, it is presented that auditory-evoked fields could be recorded
	in a longitudinal study and that fAEF latencies show a significant
	decrease with increased gestation during normal pregnancies. In addition,
	it successfully records mismatch negativity (MMN) responses to auditory
	stimuli, which is an indicator of sound discrimination.},
  timestamp = {2016-07-08T12:30:20Z},
  booktitle = {Magnetoencephalography},
  publisher = {{\{Academic Press\}}},
  author = {Preissl, Hubert and Lowery, Curtis L. and Eswaran, Hari},
  editor = {Preissl, Hubert},
  year = {2005},
  pages = {1 - 23},
  owner = {Fardin}
}

@incollection{Ramirez2010,
  series = {Springer Optimization and Its Applications},
  title = {Neuroelectromagnetic Source Imaging of Brain Dynamics},
  volume = {38},
  isbn = {978-0-387-88629-9},
  abstract = {Neuroelectromagnetic source imaging (NSI) is the scientific field
	devoted to modeling and estimating the spatiotemporal dynamics of
	the neuronal currents that generate the electric potentials and magnetic
	fields measured with electromagnetic (EM) recording technologies.
	Unlike functional magnetic resonance imaging (fMRI), which is indirectly
	related to neuroelectrical activity through neurovascular coupling
	[e.g., the blood oxygen level-dependent (BOLD) signal], EM measurements
	directly relate to the electrical activity of neuronal populations.
	In the past few decades, researchers have developed a great variety
	of source estimation techniques that are well informed by anatomy,
	neurophysiology, and the physics of volume conduction. State-of-the-art
	approaches can resolve many simultaneously active brain regions and
	their single trial dynamics and can even reveal the spatial extent
	of local cortical current flows.},
  language = {English},
  timestamp = {2016-07-10T06:49:55Z},
  booktitle = {Computational Neuroscience},
  publisher = {{\{Springer New York\}}},
  author = {Ram{\'i}rez, ReyR. and Wipf, David and Baillet, Sylvain},
  editor = {Chaovalitwongse, Wanpracha and Pardalos, Panos M. and Xanthopoulos, Petros},
  year = {2010},
  pages = {127-155},
  owner = {afdidehf}
}

@incollection{Rutkowski2008,
  title = {Information Fusion for Perceptual Feedback: A Brain Activity Sonification 	Approach},
  timestamp = {2016-07-08T12:49:24Z},
  publisher = {{\{Springer\}}},
  author = {Rutkowski, Tomasz M. and Cichocki, Andrzej and Mandic, Danilo P.},
  year = {2008},
  pages = {261-274},
  owner = {Fardin}
}

@article{Temlyakov1999,
  title = {Greedy Algorithms and M-Term Approximation with Regard to Redundant 	Dictionaries},
  volume = {98},
  issn = {0021-9045},
  doi = {http://dx.doi.org/10.1006/jath.1998.3265},
  abstract = {We study the efficiency of greedy type algorithms with regard to redundant
	dictionaries in Hilbert space and we prove a general result which
	gives a sufficient condition on a dictionary to guarantee that the
	pure greedy algorithm is near best in the sense of power decay of
	error of approximation. We discuss also some important examples.
	It is already known (see DeVore and Temlyakov,Adv. Comput. Math.5(1996),
	173–187) that the Pure Greedy Algorithm for some dictionaries has
	a saturation property. We construct an example which shows that a
	natural generalization of the Pure Greedy Algorithm also has a saturation
	property. Next we discuss some new phenomena which occur in approximation
	by a greedy type algorithm with regards to a highly redundant dictionary.},
  timestamp = {2016-09-29T16:04:30Z},
  number = {1},
  journal = {Journal of Approximation Theory},
  author = {Temlyakov, V. N.},
  year = {1999},
  pages = {117--145},
  owner = {Fardin}
}

@book{XXX2014,
  title = {CURRY Neuroimaging Suite: Installation and Tutorials-Multi-modal 	neuroimaging for CURRY 7},
  timestamp = {2016-07-08T12:08:35Z},
  author = {{{XXX}}},
  year = {2014},
  owner = {Fardin}
}

@inproceedings{Yang2010a,
  title = {Online Learning for Group Lasso},
  abstract = {We develop a novel online learning algorithm for the group lasso in
	order to efficiently find the important explanatory factors in a
	grouped manner. Different from traditional batch-mode group lasso
	algorithms, which suffer from the inefficiency and poor scalability,
	our proposed algorithm performs in an online mode and scales well:
	at each iteration one can update the weight vector according to a
	closed-form solution based on the average of previous subgradients.
	Therefore, the proposed online algorithm can be very efficient and
	scalable. This is guaranteed by its low worst-case time complexity
	and memory cost both in the order of O(d), where d is the number
	of dimensions. Moreover, in order to achieve more sparsity in both
	the group level and the individual feature level, we successively
	extend our online system to efficiently solve a number of variants
	of sparse group lasso models. We also show that the online system
	is applicable to other group lasso models, such as the group lasso
	with overlap and graph lasso. Finally, we demonstrate the merits
	of our algorithm by experimenting with both synthetic and real-world
	datasets.},
  timestamp = {2016-07-10T07:12:19Z},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning 	(ICML-10)},
  publisher = {{\{Omnipress\}}},
  author = {Yang, Haiqin and Xu, Zenglin and King, Irwin and Lyu, Michael R.},
  editor = {F�rnkranz, Johannes and Joachims, Thorsten},
  year = {2010},
  pages = {1191-1198},
  owner = {Fardin}
}

@book{Mandic2008,
  title = {Signal Processing Techniques for Knowledge Extraction and Information 	Fusion},
  timestamp = {2016-07-10T08:11:59Z},
  publisher = {{\{Springer\}}},
  editor = {Mandic, Danilo and Golz, Martin and Kuh, Anthony and Obradovic, Dragan and Tanaka, Toshihisa},
  year = {2008},
  annote = {ISBN: 978-0-387-74366-0 (Print) 978-0-387-74367-7 (Online)},
  annote = {ISBN: 978-0-387-74366-0 (Print) 978-0-387-74367-7 (Online)},
  annote = {ISBN: 978-0-387-74366-0 (Print) 978-0-387-74367-7 (Online)},
  annote = {ISBN: 978-0-387-74366-0 (Print) 978-0-387-74367-7 (Online)},
  annote = {ISBN: 978-0-387-74366-0 (Print) 978-0-387-74367-7 (Online)},
  annote = {ISBN: 978-0-387-74366-0 (Print) 978-0-387-74367-7 (Online)},
  owner = {Fardin}
}

@article{Amaldi1998,
  title = {On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems},
  volume = {209},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(97)00115-1},
  abstract = {We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (Min ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (Min RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both Min ULR and Min RVLS the four basic types of relational operators =, ⩾, &gt; and ≠ are considered. While Min RVLS with equations was mentioned to be NP-hard in (Garey and Johnson, 1979), we established in (Amaldi; 1992; Amaldi and Kann, 1995) that min ULR with equalities and inequalities are NP-hard even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in (Arora et al., 1993). In this paper we determine strong bounds on the approximability of various variants of Min RVLS and min ULR, including constrained ones where the variables are restricted to take binary values or where some relations are mandatory while others are optional. The various NP-hard versions turn out to have different approximability properties depending on the type of relations and the additional constraints, but none of them can be approximated within any constant factor, unless P = NP. Particular attention is devoted to two interesting special cases that occur in discriminant analysis and machine learning. In particular, we disprove a conjecture of van Horn and Martinez (1992) regarding the existence of a polynomial-time algorithm to design linear classifiers (or perceptrons) that involve a close-to-minimum number of features.},
  timestamp = {2016-09-30T10:42:52Z},
  number = {1–2},
  urldate = {2016-03-30},
  journal = {Theoretical Computer Science},
  author = {Amaldi, Edoardo and Kann, Viggo},
  month = dec,
  year = {1998},
  keywords = {Approximability bounds,Approximability bounds,Designing linear classifiers,Designing linear classifiers,Linear systems,Linear systems,Nonzero variables,Nonzero
	variables,Unsatisfied relations,Unsatisfied relations},
  pages = {237--260},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Q9NIJNQ8\\S0304397597001151.html:text/html}
}

@book{Boyd2004,
  title = {Convex Optimization},
  abstract = {Convex optimization problems arise frequently in many different fields.
	A comprehensive introduction to the subject, this book shows in detail
	how such problems can be solved numerically with great efficiency.
	The focus is on recognizing convex optimization problems and then
	finding the most appropriate technique for solving them. The text
	contains many worked examples and homework exercises and will appeal
	to students, researchers and practitioners in fields such as engineering,
	computer science, mathematics, statistics, finance, and economics.
	More material can be found at the web sites for EE364A (Stanford)
	or EE236B (UCLA), and our own web pages. Source code for almost all
	examples and figures in part 2 of the book is available in CVX (in
	the examples directory), in CVXOPT (in the book examples directory),
	and in CVXPY. Source code for examples in Chapters 9, 10, and 11
	can be found here. Instructors can obtain complete solutions to exercises
	by email request to us; please give us the URL of the course you
	are teaching. If you find an error not listed in our errata list,
	please do let us know about it.},
  timestamp = {2016-07-08T12:04:41Z},
  publisher = {{Cambridge, U.K.: Cambridge Univ. Press}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2004},
  owner = {afdidehf}
}

@book{Daniel1999,
  title = {Fitting Equations to Data: Computer Analysis of Multifactor Data},
  abstract = {Helps any serious data analyst with a computer to recognize the strengths
	and limitations of data, to test the assumptions implicit in the
	least squares methods used to fit the data, to select appropriate
	forms of the variables, to judge which combinations of variables
	are most influential, and to state the conditions under which the
	fitted equations are applicable. This edition includes numerous extensions
	and new devices such as component and component-plus-residual plots,
	cross verification with a second sample, and an index of required
	x-precision; also, the search for better subset equations is enlarged
	to cover 262,144 alternatives. The methods described have been applied
	in agricultural, environmental, management, marketing, medical, physical,
	and social sciences. Mathematics is kept to the level of college
	algebra.},
  timestamp = {2016-07-08T12:31:17Z},
  publisher = {{New York: Wiley}},
  author = {Daniel, Cuthbert and Wood, Fred S.},
  year = {1999},
  owner = {Fardin}
}

@article{Davies2009,
  title = {The Restricted Isometry Property and $\ell^p$ sparse recovery failure},
  abstract = {This paper considers conditions based on the restricted isometry constant
	(RIC) under which the solution of an underdetermined linear system
	with minimal ? p norm, 0 \ensuremath{<} p ? 1, is guaranteed to be
	also the sparsest one. Specifically matrices are identified that
	have RIC, ?2m, arbitrarily close to 1/ ? 2 ? 0.707 where sparse recovery
	with p = 1 fails for at least one m-sparse vector. This indicates
	that there is limited room for improvement over the best known positive
	results of Foucart and Lai, which guarantee that ? 1 -minimisation
	recovers all m-sparse vectors for any matrix with ?2m \ensuremath{<}
	2(3 ? ? 2)/7 ? 0.4531. We also present results that show, compared
	to ? 1 minimisation, ? p minimisation recovery failure is only slightly
	delayed in terms of the RIC values. Furthermore when ? p optimisation
	is attempted using an iterative reweighted ? 1 scheme, failure can
	still occur for ?2m arbitrarily close to 1/ ? 2.},
  timestamp = {2016-07-11T17:03:48Z},
  journal = {SPARS09},
  author = {Davies, Mike and Gribonval, Remi},
  month = apr,
  year = {2009},
  keywords = {NSP,null space property},
  annote = {read https://hal.inria.fr/inria-00370402 - Signal Processing with
	Adaptive Sparse Structured Representations},
  annote = {read https://hal.inria.fr/inria-00370402 - Signal Processing with
	Adaptive Sparse Structured Representations},
  annote = {read https://hal.inria.fr/inria-00370402 - Signal Processing with
	Adaptive Sparse Structured Representations},
  annote = {read https://hal.inria.fr/inria-00370402 - Signal Processing with
	Adaptive Sparse Structured Representations},
  annote = {read https://hal.inria.fr/inria-00370402 - Signal Processing withAdaptive
	Sparse Structured Representations},
  annote = {read https://hal.inria.fr/inria-00370402 - Signal Processing with	Adaptive Sparse Structured Representations},
  owner = {Fardin}
}

@article{DeVore1992,
  title = {Compression of Wavelet Decompositions},
  volume = {114},
  abstract = {We characterize functions with a given degree of nonlinear approximation
	by linear combinations with n terms of a function cp, its dilates
	and their translates. This gives a unified viewpoint of recent results
	on nonlinear approximation by spline functions and give their extension
	to functions of several variables. Our approach is formulated in
	terms of wavelet decompositions.},
  timestamp = {2016-09-29T16:13:07Z},
  number = {4},
  journal = {American Journal of Mathematics},
  author = {DeVore, Ronald A. and Jawerth, Bj{\"o}rn and Popov, Vasil},
  month = aug,
  year = {1992},
  pages = {737--785}
}

@techreport{Efron2003,
  title = {Least Angle Regression},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward
	Selection, and Backward Elimination is to choose a linear model on
	the basis of the same set of data to which the model will be applied.
	Typically we have available a large collection of possible covariates
	from which we hope to select a parsimonious set for the efficient
	prediction of a response variable. Least Angle Regression (�LARS�),
	a new model selection algorithm, is a useful and less greedy version
	of traditional forward selection methods. Three main properties are
	derived. (1)A simple modification of the LARS algorithm implements
	the Lasso, an attractive version of Ordinary Least Squares that constrains
	the sum of the absolute regression coefficients; the LARS modification
	calculates all possible Lasso estimates for a given problem, using
	an order of magnitude less computer time than previous methods. (2)A
	different LARS modification efficiently implements Forward Stagewise
	linear regression, another promising new model selection method;
	this connection explains the similar numerical results previously
	observed for the Lasso and Stagewise, and helps understand the properties
	of both methods, which are seen as constrained versions of the simpler
	LARS algorithm. (3) A simple approximation for the degrees of freedom
	of a LARS estimate is available, from which we derive a Cp estimate
	of prediction error; this allows a principled choice among the range
	of possible LARS estimates. LARS and its variants are computationally
	efficient: the paper describes a publicly available algorithm that
	requires only the same order of magnitude of computational effort
	as Ordinary Least Squares applied to the full set of covariates.},
  timestamp = {2016-07-09T19:56:12Z},
  institution = {Stanford University},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  month = jan,
  year = {2003},
  owner = {afdidehf}
}

@book{Golub2013,
  edition = {4},
  title = {Matrix Computations},
  timestamp = {2017-04-20T09:22:14Z},
  publisher = {{The Johns Hopkins Univ. Press}},
  author = {Golub, Gene H. and Loan, Charles F. van},
  year = {2013},
  keywords = {FUNDAMENTALS,Mathematics},
  annote = {Johns Hopkins Series in the Mathematical Sciences},
  owner = {afdidehf}
}

@book{Hastie2009,
  address = {New York},
  series = {Springer series in statistics},
  title = {The elements of statistical learning : data mining, inference, and 	prediction},
  isbn = {978-0-387-84857-0},
  timestamp = {2016-07-11T17:01:01Z},
  publisher = {{Springer}},
  author = {Hastie, Trevor J. and Tibshirani, Robert John and Friedman, Jerome
	H.},
  year = {2009},
  note = {Autres impressions : 2011 (corr.), 2013 (7e corr.)},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  annote = {read},
  owner = {Fardin}
}

@book{HornR.A.2012,
  edition = {2ed.},
  title = {Matrix Analysis},
  isbn = {978-0-521-54823-6},
  timestamp = {2016-07-09T20:08:30Z},
  publisher = {{New York, NY: Cambridge Press}},
  author = {Horn R.A., Johnson C.R.},
  year = {2012},
  owner = {Fardin}
}

@article{Pascual-Marqui2007,
  title = {Discrete, {{3D}} distributed, linear imaging methods of electric 	neuronal activity. {{Part}} 1: exact, zero error localization},
  shorttitle = {Discrete, {{3D}} distributed, linear imaging methods of electric neuronal 	activity. {{Part}} 1},
  abstract = {This paper deals with the EEG/MEG neuroimaging problem: given measurements
	of scalp electric potential differences (EEG: electroencephalogram)
	and extracranial magnetic fields (MEG: magnetoencephalogram), find
	the 3D distribution of the generating electric neuronal activity.
	This problem has no unique solution. Only particular solutions with
	"good" localization properties are of interest, since neuroimaging
	is concerned with the localization of brain function. In this paper,
	a general family of linear imaging methods with exact, zero error
	localization to point-test sources is presented. One particular member
	of this family is sLORETA (standardized low resolution brain electromagnetic
	tomography; Pascual-Marqui, Methods Find. Exp. Clin. Pharmacol. 2002,
	24D:5-12; http://www.unizh.ch/keyinst/NewLORETA/sLORETA/sLORETA-Math01.pdf).
	It is shown here that sLORETA has no localization bias in the presence
	of measurement and biological noise. Another member of this family,
	denoted as eLORETA (exact low resolution brain electromagnetic tomography;
	Pascual-Marqui 2005: http://www.research-projects.unizh.ch/p6990.htm),
	is a genuine inverse solution (not merely a linear imaging method)
	with exact, zero error localization in the presence of measurement
	and structured biological noise. The general family of imaging methods
	is further extended to include data-dependent (adaptive) quasi-linear
	imaging methods, also with the exact, zero error localization property.},
  timestamp = {2016-07-08T12:12:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0710.3341},
  urldate = {2016-03-29},
  journal = {arXiv:0710.3341 {[}math-ph, physics:physics, q-bio]},
  author = {Pascual-Marqui, Roberto D.},
  month = oct,
  year = {2007},
  keywords = {Mathematical Physics,Physics - Biological Physics,Quantitative Biology
	- Neurons and Cognition},
  annote = {Comment: KEY Institute technical report},
  annote = {Comment: KEY Institute technical report},
  annote = {Comment: KEY Institute technical report},
  annote = {Comment: KEY Institute technical report},
  annote = {Comment: KEY Institute technical report}
}

@techreport{Schmidt2005,
  title = {Least Squares Optimization with L1-Norm Regularization},
  abstract = {This project surveys and examines optimization approaches proposed
	for parameter estimation in Least Squares linear regression models
	with an L1 penalty on the regression coefficients. We first review
	linear regression and regularization, and both motivate and formalize
	this problem. We then give a detailed analysis of 8 of the varied
	approaches that have been proposed for optimizing this objective,
	4 focusing on constrained formulations and 4 focusing on the unconstrained
	formulation. We then briefly survey closely related work on the orthogonal
	design case, approximate optimization, regularization parameter estimation,
	other loss functions, active application areas, and properties of
	L1 regularization. Illustrative implementations of each of these
	8 methods are included with this document as a web resource.},
  timestamp = {2016-07-09T19:56:38Z},
  institution = {CS542B Project Report},
  author = {Schmidt, Mark},
  year = {2005},
  owner = {afdidehf}
}

@article{Temlyakov2003,
  title = {Nonlinear Methods of Approximation},
  volume = {3},
  issn = {1615-3375},
  doi = {10.1007/s102080010029},
  abstract = {Our main interest in this paper is nonlinear approximation. The basic
	idea behind nonlinear approximation is that the elements used in
	the approximation do not come from a fixed linear space but are allowed
	to depend on the function being approximated. While the scope of
	this paper is mostly theoretical, we should note that this form of
	approximation appears in many numerical applications such as adaptive
	PDE solvers, compression of images and signals, statistical classification,
	and so on. The standard problem in this regard is the problem of
	m -term approximation where one fixes a basis and looks to approximate
	a target function by a linear combination of m terms of the basis.
	When the basis is a wavelet basis or a basis of other waveforms,
	then this type of approximation is the starting point for compression
	algorithms. We are interested in the quantitative aspects of this
	type of approximation. Namely, we want to understand the properties
	(usually smoothness) of the function which govern its rate of approximation
	in some given norm (or metric). We are also interested in stable
	algorithms for finding good or near best approximations using m terms.
	Some of our earlier work has introduced and analyzed such algorithms.
	More recently, there has emerged another more complicated form of
	nonlinear approximation which we call highly nonlinear approximation.
	It takes many forms but has the basic ingredient that a basis is
	replaced by a larger system of functions that is usually redundant.
	Some types of approximation that fall into this general category
	are mathematical frames, adaptive pursuit (or greedy algorithms),
	and adaptive basis selection. Redundancy on the one hand offers much
	promise for greater efficiency in terms of approximation rate, but
	on the other hand gives rise to highly nontrivial theoretical and
	practical problems. With this motivation, our recent work and the
	current activity focuses on nonlinear approximation both in the classical
	form of m -term approximation (where several important problems remain
	unsolved) and in the form of highly nonlinear approximation where
	a theory is only now emerging.},
  language = {English},
  timestamp = {2016-09-29T16:05:27Z},
  number = {1},
  journal = {Foundations of Computational Mathematics},
  author = {{Temlyakov}},
  year = {2003},
  keywords = {41A25,41A46,41A65,42A10,42C10,42C15,46A35,46C99,46E35,46N40,65D15,65J05.,AMS
	Classification. 41A17},
  pages = {33--107},
  owner = {Fardin}
}

@book{Wickerhauser1994,
  title = {Adapted Wavelet Analysis from Theory to Software},
  timestamp = {2016-07-08T10:10:15Z},
  publisher = {{Wellesley, MA: A K Peters Ltd.}},
  author = {Wickerhauser, Mladen Victor},
  year = {1994},
  owner = {afdidehf}
}

@incollection{DeVore2009,
  title = {Nonlinear approximation and its applications},
  copyright = {©2009 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-03412-1 978-3-642-03413-8},
  abstract = {I first met Wolfgang Dahmen in 1974 in Oberwolfach. He looked like a high school student to me but he impressed everyone with his talk on whether polynomial operators could produce both polynomial and spectral orders of approximation. We became the best of friends and frequent collaborators. While Wolfgang’s mathematical contributions spread across many disciplines, a major thread in his work has been the exploitation of nonlinear approximation. This article will reflect on Wolfgang’s pervasive contributions to the development of nonlinear approximation and its application. Since many of the contributions in this volume will address specific application areas in some details, my thoughts on these will be to a large extent anecdotal.},
  language = {en},
  timestamp = {2016-09-29T15:38:18Z},
  urldate = {2016-04-05},
  booktitle = {Multiscale, Nonlinear and Adaptive Approximation},
  publisher = {{Springer Berlin Heidelberg}},
  author = {DeVore, Ronald A.},
  editor = {DeVore, Ronald and Kunoth, Angela},
  year = {2009},
  keywords = {Appl.Mathematics/Computational Methods of Engineering,Appl.Mathematics/Computational Methods of Engineering,Computational Mathematics and Numerical Analysis,Computational
	Mathematics and Numerical Analysis,Computational Mathematics and Numerical Analysis,numerical analysis,Numerical Analysis,Numeric Computing,Numeric Computing,Theory of Computation,Theory
	of Computation,Theory of Computation},
  pages = {169--201},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NJCZKRVR\\978-3-642-03413-8_6.html:text/html},
  doi = {10.1007/978-3-642-03413-8_6}
}

@incollection{DeVore1998,
  series = {Acta Numerica},
  title = {Nonlinear approximation},
  abstract = {This is a survey of nonlinear approximation, especially that part of the subject
which is important in numerical computation. Nonlinear approximation
means that the approximants do not come from linear spaces but rather from
nonlinear manifolds. The central question to be studied is what, if any, are the
advantages of nonlinear approximation over the simpler, more established, linear
methods. This question is answered by studying the rate of approximation
which is the decrease in error versus the number of parameters in the approximant.
The number of parameters usually correlates well with computational
effort. It is shown that in many settings the rate of nonlinear approximation
can be characterized by certain smoothness conditions which are significantly
weaker than required in the linear theory. Emphasis in the survey will be
placed on approximation by piecewise polynomials and wavelets as well as
their numerical implementation. Results on highly nonlinear methods such
as optimal basis selection and greedy algorithms (adaptive pursuit) are also
given. Applications to image processing, statistical estimation, regularity for
PDEs, and adaptive algorithms are discussed},
  timestamp = {2016-07-10T06:55:32Z},
  publisher = {{Cambridge University Press}},
  author = {DeVore, Ronald A.},
  year = {1998},
  pages = {51–150}
}

@article{Gribonval2004,
  title = {Nonlinear Approximation with Dictionaries I. Direct Estimates},
  volume = {10},
  issn = {1069-5869, 1531-5851},
  doi = {10.1007/s00041-004-8003-5},
  abstract = {We study various approximation classes associated with m-term approximation by elements from a (possibly redundant) dictionary in a Banach space. The standard approximation class associated with the best m-term approximation is compared to new classes defined by considering m-term approximation with algorithmic constraints: thresholding and Chebychev approximation classes are studied, respectively. We consider embeddings of the Jackson type (direct estimates) of sparsity spaces into the mentioned approximation classes. General direct estimates are based on the geometry of the Banach space, and we prove that assuming a certain structure of the dictionary is sufficient and (almost) necessary to obtain stronger results. We give examples of classical dictionaries in Lp spaces and modulation spaces where our results recover some known Jackson type estimates, and discuss some new estimates they provide.},
  language = {en},
  timestamp = {2016-07-10T06:57:17Z},
  number = {1},
  urldate = {2016-04-05},
  journal = {Journal of Fourier Analysis and Applications},
  author = {Gribonval, Rémi and Nielsen, Morten},
  month = jan,
  year = {2004},
  pages = {51-71},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5TVP4VN4\\s00041-004-8003-5.html:text/html}
}

@article{Gribonval2006a,
  title = {Nonlinear Approximation with Dictionaries. II. Inverse Estimates},
  volume = {24},
  issn = {0176-4276, 1432-0940},
  doi = {10.1007/s00365-005-0621-x},
  abstract = {In this paper, which is the sequel to [16], we study inverse estimates of the Bernstein type for nonlinear approximation with structured redundant dictionaries in a Banach space. The main results are for blockwise incoherent dictionaries in Hilbert spaces, which generalize the notion of joint block-diagonal mutually incoherent bases introduced by Donoho and Huo. The Bernstein inequality obtained for such dictionaries is proved to be sharp, but it has an exponent that does not match that of the corresponding Jackson inequality.},
  language = {en},
  timestamp = {2016-09-29T15:37:58Z},
  number = {2},
  urldate = {2016-04-05},
  journal = {Constructive Approximation},
  author = {Gribonval, Remi and Nielsen, Morten},
  month = apr,
  year = {2006},
  keywords = {analysis,Analysis,Bernstein inequality,Bernstein inequality,Mutually incoherent bases,Mutually incoherent bases,nonlinear approximation,nonlinear
	approximation,Nonlinear approximation,numerical analysis,Numerical Analysis},
  pages = {157--173},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9EM7Q79S\\s00365-005-0621-x.html:text/html}
}

@article{Chunmei2005,
  title = {Signal overcomplete representation and sparse decomposition based on redundant dictionaries},
  volume = {50},
  issn = {1001-6538, 1861-9541},
  doi = {10.1007/BF02899633},
  abstract = {Decomposing a signal based upon redundant dictionaries is a new method for data representation on signal processing. It approximates a signal with an overcomplete system instead of an orthonormal basis to provide a sufficient choice for adaptive sparse decompositions. Replacing the original data with a sparse approximation can result in not only a higher compression ratio, but also greater flexibility in capturing the inherent structure of the natural signals with the redundancy of dictionaries. This paper gives an overview of a series of recent results in this field, and deals with the relationship between sparsity of signal decomposition and incoherence of dictionaries with BP and MP algorithms. The current and future challenges of the dictionary construction are discussed.},
  language = {en},
  timestamp = {2016-07-10T08:11:45Z},
  number = {23},
  urldate = {2016-04-05},
  journal = {Chinese Science Bulletin},
  author = {Chunmei, Zhang and Zhongke, Yin and Xiangdong, Chen and Mingxia, Xiao},
  month = dec,
  year = {2005},
  keywords = {Basis Pursuit (BP),Basis Pursuit (BP),Chemistry/Food Science,Chemistry/Food Science; general,engineering,Engineering; general,general,Geosciences,Geosciences; general,Life Sciences,Life
	Sciences,Life Sciences; general,Marching Pursuit (MP),Marching Pursuit (MP),overcomplete representation,overcomplete representation,Physics,Physics; general,redundant dictionary,redundant
	dictionary,redundant dictionary,Science,Science; general,sparse decomposition,sparse decomposition},
  pages = {2672-2677},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RFGDHE8A\\BF02899633.html:text/html}
}

@inproceedings{Rao1997,
  title = {Deriving algorithms for computing sparse solutions to linear inverse problems},
  volume = {1},
  doi = {10.1109/ACSSC.1997.680585},
  abstract = {A novel methodology is employed to develop algorithms for computing sparse solutions to linear inverse problems, starting from suitably defined diversity measures whose minimization promotes sparsity. These measures include p-norm-like (/spl Lscr//sub (p/spl les/1)/) diversity measures, and the Gaussian and Shannon entropies. The algorithm development methodology uses a factored representation of the gradient, and involves successive relaxation of the Lagrangian necessary condition. The general nature of the methodology provides a systematic approach for deriving a class of algorithms called FOCUSS (FOCal Underdetermined System Solver), and a natural mechanism for extending them.},
  timestamp = {2016-07-08T12:10:59Z},
  booktitle = {Conference Record of the Thirty-First Asilomar Conference on Signals, Systems amp; Computers, 1997},
  author = {Rao, B. D. and Kreutz-Delgado, K.},
  month = nov,
  year = {1997},
  keywords = {algorithm,algorithm,Algorithm design and analysis,Algorithm design and analysis,Algorithms,algorithms,Convergence,convergence,convergence of numerical methods,convergence
	of numerical methods,convergence of numerical methods,direction of arrival estimation,Direction of arrival estimation,Entropy,entropy,factored representation,factored
	representation,factored representation,focal underdetermined system solver,focal underdetermined system solver,Focusing,Focusing,FOCUSS,FOCUSS,Gaussian entrophy,Gaussian
	entrophy,Gaussian entrophy,Gaussian processes,Gaussian processes,gradient,gradient,information theory,information theory,inverse problems,inverse problems,Iterative algorithms,Iterative
	algorithms,Iterative algorithms,Lagrangian functions,Lagrangian functions,Lagrangian necessary condition,Lagrangian necessary condition,Linear inverse problems,Linear
	inverse problems,linear inverse problems,Minimization,minimization,Minimization methods,Minimization methods,p-norm-like diversity measures,p-norm-like diversity
	measures,p-norm-like diversity measures,Shannon entrophy,Shannon entrophy,signal processing,signal processing,signal representations,Signal representations,sparse
	solutions,sparse solutions,sparse solutions,successive relaxation,successive relaxation},
  pages = {955-959 vol.1},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\M3B3XVVP\\articleDetails.html:text/html}
}

@article{Lei2011,
  title = {Multimodal Functional Network Connectivity: An EEG-fMRI Fusion in 	Network Space},
  volume = {6},
  abstract = {EEG and fMRI recordings measure the functional activity of multiple
	coherent networks distributed in the cerebral cortex. Identifying
	network interaction from the complementary neuroelectric and hemodynamic
	signals may help to explain the complex relationships between different
	brain regions. In this paper, multimodal functional network connectivity
	(mFNC) is proposed for the fusion of EEG and fMRI in network space.
	First, functional networks (FNs) are extracted using spatial independent
	component analysis (ICA) in each modality separately. Then the interactions
	among FNs in each modality are explored by Granger causality analysis
	(GCA). Finally, fMRI FNs are matched to EEG FNs in the spatial domain
	using networkbased source imaging (NESOI). Investigations of both
	synthetic and real data demonstrate that mFNC has the potential to
	reveal the underlying neural networks of each modality separately
	and in their combination. With mFNC, comprehensive relationships
	among FNs might be unveiled for the deep exploration of neural activities
	and metabolic responses in a specific task or neurological state.},
  timestamp = {2016-07-09T20:16:08Z},
  number = {9},
  journal = {Plose ONE},
  author = {Lei, Xu and Ostwald, Dirk and Hu, Jiehui and Qiu, Chuan and Porcaro, Camillo and Bagshaw, Andrew P. and Yao, Dezhong},
  month = sep,
  year = {2011},
  owner = {afdidehf}
}

@article{Bhandari2016,
  title = {Introduction to Sparse Approximation},
  timestamp = {2016-07-08T12:51:02Z},
  author = {Bhandari, Ayush},
  year = {2016},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT}
}

@article{Li2009,
  title = {Novel Algorithm for Sparse Solutions to Linear Inverse Problems with 	Multiple Measurements},
  volume = {abs/0905.3},
  abstract = {In this report, a novel efficient algorithm for recovery of jointly
	sparse signals (sparse matrix) from multiple incomplete measurements
	has been presented, in particular, the NESTA-based MMV optimization
	method. In a nutshell, the jointly sparse recovery is obviously superior
	to applying standard sparse reconstruction methods to each channel
	individually. Moreover several efforts have been made to improve
	the NESTA-based MMV algorithm, in particular, (1) the NESTA-based
	MMV algorithm for partially known support to greatly improve the
	convergence rate, (2) the detection of partial (or all) locations
	of unknown jointly sparse signals by using so-called MUSIC algorithm;
	(3) the iterative NESTA-based algorithm by combing hard thresholding
	technique to decrease the numbers of measurements. It has been shown
	that by using proposed approach one can recover the unknown sparse
	matrix X with Spark (A)-sparsity fromSpark (A)measurements, predicted
	in Ref. [1], where the measurement matrix denoted by A satisfies
	the so-called restricted isometry property (RIP). Under a very mild
	condition on the sparsity of X and characteristics of the A , the
	iterative hard threshold (IHT)-based MMV method has been shown to
	be also a very good candidate.},
  timestamp = {2016-07-10T07:02:11Z},
  journal = {Computing Research Repository},
  author = {Li, Lianlin and Li, Fang},
  year = {2009},
  keywords = {compressive sensing,iterative hard threshold algorithm,iterative hard threshold
	algorithm,MMV (multiple
	measurement vector),MUSIC,Nesterov�s method,restricted isometry property (RIP),restricted isometry
	property (RIP),SMV (single measurement vector)},
  owner = {Fardin}
}

@article{Li2015e,
  title = {Towards robust subspace recovery via sparsity-constrained latent 	low-rank representation},
  issn = {1047-3203},
  doi = {http://dx.doi.org/10.1016/j.jvcir.2015.06.012},
  abstract = {Abstract Robust recovery of subspace structures from noisy data has
	received much attention in visual analysis recently. To achieve this
	goal, previous works have developed a number of low-rank based methods,
	among of which Low-Rank Representation (LRR) is a typical one. As
	a refined variant, Latent \{LRR\} constructs the dictionary using
	both observed and hidden data to relieve the insufficient sampling
	problem. However, they fail to consider the observation that each
	data point can be represented by only a small subset of atoms in
	a dictionary. Motivated by this, we present the Sparse Latent Low-rank
	representation (SLL) method, which explicitly imposes the sparsity
	constraint on Latent \{LRR\} to encourage a sparse representation.
	In this way, each data point can be represented by only selecting
	a few points from the same subspace. Its objective function is solved
	by the linearized Augmented Lagrangian Multiplier method. Favorable
	experimental results on subspace clustering, salient feature extraction
	and outlier detection have verified promising performances of our
	method.},
  timestamp = {2016-07-11T17:08:43Z},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Li, Ping and Bu, Jiajun and Yu, Jun and Chen, Chun},
  year = {2015},
  keywords = {analysis,Augmented,clustering,detection,extraction,Feature,Lagrangian,Latent,learning,Low-rank,method,Multiplier,Outlier,recovery,representation,Robust,Sparse,Subspace,Visual},
  pages = {-},
  owner = {afdidehf}
}

@article{Li2014e,
  title = {Joint $L_1/L_p$-regularized minimization in video recovery of remote 	sensing based on compressed sensing},
  volume = {125},
  issn = {0030-4026},
  doi = {http://dx.doi.org/10.1016/j.ijleo.2014.08.098},
  abstract = {Abstract \{L1\} regularization and Lp regularization are proposed
	for processing recovered images based on compressed sensing (CS).
	\{L1\} regularization can be solved as a convex optimization problem
	but is less sparse than Lp (0 &lt; p &lt; 1). Lp regularization is
	sparser than \{L1\} regularization but is more difficult to solve.
	This paper proposes joint L1/Lp (0 &lt; p &lt; 1) regularization,
	which combines Lp regularization and \{L1\} regularization. This
	joint regularization is applied to recover video of remote sensing
	based on CS. Joint regularization is sparser than \{L1\} regularization
	but is as easy to solve as \{L1\} regularization. A linearized Bregman
	reweighted iteration algorithm is proposed to solve the joint L1/Lp
	regularization problem. The performance and capabilities of the linearized
	Bregman algorithm and linearized Bregman reweighted algorithm for
	solving the joint L1/Lp regularization model are analyzed and compared
	through numerical simulations.},
  timestamp = {2016-07-09T19:46:25Z},
  number = {23},
  journal = {Optik - International Journal for Light and Electron Optics},
  author = {Li, Sheng-liang and Liu, Kun and Zhang, Feng and Xiao, Long-long and Han, Da-Peng},
  year = {2014},
  keywords = {algorithm,Bregman,Compressed,Linearized,Lp-regularized,Minimization,Remote,reweighted,sensing,video},
  pages = {7080 - 7084},
  owner = {afdidehf}
}

@article{Li2015l,
  title = {Infrared small moving target detection algorithm based on joint spatio-temporal 	sparse recovery},
  volume = {69},
  issn = {1350-4495},
  doi = {http://dx.doi.org/10.1016/j.infrared.2015.01.008},
  abstract = {Abstract A dim small moving target detection algorithm based on joint
	spatio-temporal sparse recovery is proposed in this paper. A spatio-temporal
	over-complete dictionary is firstly trained from infrared image sequence,
	and it can characterize not only motion information but also morphological
	feature. In the spatio-temporal over-complete dictionary, the spatio-temporal
	atom is then classified as target spatio-temporal atom building target
	spatio-temporal over-complete dictionary, which describes moving
	target, and background spatio-temporal atom constructing background
	spatio-temporal over-complete dictionary, which embeds background
	clutter. Infrared image sequence is decomposed on the union of target
	spatio-temporal over-complete dictionary and background spatio-temporal
	over-complete dictionary. The residual reconstructed by its homologous
	spatio-temporal over-complete dictionary is very little, yet the
	residual recovered by its heterogonous spatio-temporal over-complete
	dictionary is quite large. Therefore, their residuals after decomposing
	and reconstruction by the joint spatio-temporal sparse recovery would
	differ so distinctly that it is adopted to decide the signal is from
	target or background. Some experiments are induced and the experimental
	results show this proposed approach could not only improve the sparsity
	more efficiently, but also enhance the target detection performance
	more effectively.},
  timestamp = {2016-07-08T12:50:22Z},
  journal = {Infrared Physics \& Technology},
  author = {Li, Zhengzhou and Hou, Qian and Fu, Hongxia and Dai, Zhen and Yang, Lijiao and Jin, Gang and Li, Ruzhang},
  year = {2015},
  keywords = {detection,Dictionary,difference,Dim,Joint,over-complete,recovery,residual,Signal,Sparse,spatio-temporal,target},
  pages = {44 - 52},
  owner = {afdidehf}
}

@article{Liljestrom2005,
  title = {Neuromagnetic localization of rhythmic activity in the human brain: 	a comparison of three methods},
  volume = {25},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2004.11.034},
  abstract = {Cortical rhythmic activity is increasingly employed for characterizing
	human brain function. Using MEG, it is possible to localize the generators
	of these rhythms. Traditionally, the source locations have been estimated
	using sequential dipole modeling. Recently, two new methods for localizing
	rhythmic activity have been developed, Dynamic Imaging of Coherent
	Sources (DICS) and Frequency-Domain Minimum Current Estimation (MCEFD).
	With new analysis methods emerging, the researcher faces the problem
	of choosing an appropriate strategy. The aim of this study was to
	compare the performance and reliability of these three methods. The
	evaluation was performed using measured data from four healthy subjects,
	as well as with simulations of rhythmic activity. We found that the
	methods gave comparable results, and that all three approaches localized
	the principal sources of oscillatory activity very well. Dipole modeling
	is a very powerful tool once appropriate subsets of sensors have
	been selected. \{MCEFD\} provides simultaneous localization of sources
	and was found to give a good overview of the data. With DICS, it
	was possible to separate close-by sources that were not retrieved
	by the other two methods.},
  timestamp = {2016-07-10T06:52:55Z},
  number = {3},
  journal = {NeuroImage},
  author = {Liljestrom, M. and Kujala, J. and Jensen, O. and Salmelin, R.},
  year = {2005},
  keywords = {DICS,Dipole,MCE,MEG,modeling,Oscillations,source localization,Source
	localization,Spatial filter,Spatial
	filter},
  pages = {734 - 745},
  owner = {afdidehf}
}

@book{Lim2015,
  title = {Package 'glinternet': Learning Interactions via Hierarchical Group-Lasso 	Regularization},
  abstract = {Group-Lasso INTERaction-NET. Fits linear pairwise-interaction models
	that satisfy strong hierarchy: if an interaction coefficient is estimated
	to be nonzero, then its two associated main effects also have nonzero
	estimated coefficients. Accommodates categorical variables (factors)
	with arbitrary numbers of levels, continuous variables, and combinations
	thereof. Implements the machinery described in the paper ``Learning
	interactions via hierarchical group-lasso regularization'' (JCGS
	2014).},
  timestamp = {2016-07-10T07:21:16Z},
  author = {Lim, Michael and Hastie, Trevor},
  month = mar,
  year = {2015},
  owner = {Fardin}
}

@article{Lin2014,
  title = {Sparse models for correlative and integrative analysis of imaging 	and genetic data},
  volume = {237},
  issn = {0165-0270},
  doi = {http://dx.doi.org/10.1016/j.jneumeth.2014.09.001},
  abstract = {Abstract The development of advanced medical imaging technologies
	and high-throughput genomic measurements has enhanced our ability
	to understand their interplay as well as their relationship with
	human behavior by integrating these two types of datasets. However,
	the high dimensionality and heterogeneity of these datasets presents
	a challenge to conventional statistical methods; there is a high
	demand for the development of both correlative and integrative analysis
	approaches. Here, we review our recent work on developing sparse
	representation based approaches to address this challenge. We show
	how sparse models are applied to the correlation and integration
	of imaging and genetic data for biomarker identification. We present
	examples on how these approaches are used for the detection of risk
	genes and classification of complex diseases such as schizophrenia.
	Finally, we discuss future directions on the integration of multiple
	imaging and genomic datasets including their interactions such as
	epistasis.},
  timestamp = {2016-07-11T16:49:28Z},
  journal = {Journal of Neuroscience Methods},
  author = {Lin, Dongdong and Cao, Hongbao and Calhoun, Vince D. and Wang, Yu-Ping},
  year = {2014},
  keywords = {classification,Correspondence analysis,Imaging genetics,Integration,sparse modeling,sparse
	modeling},
  pages = {69 - 78},
  owner = {afdidehf}
}

@article{Lin2006,
  title = {Assessing and improving the spatial accuracy in MEG source localization 	by depth-weighted minimum-norm estimates},
  volume = {31},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.11.054},
  abstract = {Cerebral currents responsible for the extra-cranially recorded magnetoencephalography
	(MEG) data can be estimated by applying a suitable source model.
	A popular choice is the distributed minimum-norm estimate (MNE) which
	minimizes the l2-norm of the estimated current. Under the l2-norm
	constraint, the current estimate is related to the measurements by
	a linear inverse operator. However, the \{MNE\} has a bias towards
	superficial sources, which can be reduced by applying depth weighting.
	We studied the effect of depth weighting in \{MNE\} using a shift
	metric. We assessed the localization performance of the depth-weighted
	\{MNE\} as well as depth-weighted noise-normalized \{MNE\} solutions
	under different cortical orientation constraints, source space densities,
	and signal-to-noise ratios (SNRs) in multiple subjects. We found
	that \{MNE\} with depth weighting parameter between 0.6 and 0.8 showed
	improved localization accuracy, reducing the mean displacement error
	from 12 mm to 7 mm. The noise-normalized \{MNE\} was insensitive
	to depth weighting. A similar investigation of \{EEG\} data indicated
	that depth weighting parameter between 2.0 and 5.0 resulted in an
	improved localization accuracy. The application of depth weighting
	to auditory and somatosensory experimental data illustrated the beneficial
	effect of depth weighting on the accuracy of spatiotemporal mapping
	of neuronal sources.},
  timestamp = {2016-07-08T11:31:28Z},
  number = {1},
  journal = {NeuroImage},
  author = {Lin, Fa-Hsuan and Witzel, Thomas and Ahlfors, Seppo P. and M. Stufflebeam, Steven and Belliveau, John W. and H�m�l�inen, Matti S.},
  year = {2006},
  keywords = {Inverse,problem},
  pages = {160 - 171},
  owner = {afdidehf}
}

@article{Liu2015,
  title = {Robust sparse signal reconstructions against basis mismatch and their 	applications},
  volume = {316},
  issn = {0020-0255},
  doi = {http://dx.doi.org/10.1016/j.ins.2015.04.027},
  abstract = {Abstract Creating a proper dictionary is an essential step in sparse
	signal recovery to explore sparsity in a variety of applications.
	To utilize the sparse property of the signal of interest, a large
	and fine dictionary is usually desired to achieve high estimation
	accuracy. Unfortunately, a big dictionary requires high computational
	complexity. Furthermore, one can imagine that no matter how fine
	we grid the domain to create the dictionary, there will always be
	an off-grid problem, namely the parameters to be estimated do not
	lie on the grids. This off-grid problem is the so-called a basis
	mismatch, which will degrade the estimation performance and it can
	be formulated as the multiplicative noise to the unknown parameters
	or quantization error to the dictionary. To tackle this issue, in
	this paper, robust algorithms are developed to enhance the estimation
	accuracy by utilizing the robust optimization techniques such as
	stochastic robust and worst case optimizations. As a result, an extra
	l 2 -norm constraint on parameter of interest is introduced to increase
	the estimation robustness. In addition, both theoretical analysis
	and simulations show that the proposed robust sparsity recovery approaches
	are superior in performance to other recovery schemes.},
  timestamp = {2016-07-10T08:08:50Z},
  journal = {Information Sciences},
  author = {Liu, Hongqing and Li, Yong and Truong, Trieu-Kien},
  year = {2015},
  note = {Nature-Inspired Algorithms for Large Scale Global Optimization},
  keywords = {Basis,Dictionary,mismatch,Robustness,Sparsity},
  pages = {1 - 17},
  owner = {afdidehf}
}

@article{Liu2015a,
  title = {Optimal-correlation-based reconstruction for distributed compressed 	video sensing},
  volume = {31},
  issn = {1047-3203},
  doi = {http://dx.doi.org/10.1016/j.jvcir.2015.06.020},
  abstract = {Abstract Distributed compressed video sensing (DCVS) is a framework
	that integrates both compressed sensing and distributed video coding
	characteristics to achieve a low-complexity video coding. However,
	how to design an efficient joint reconstruction by leveraging more
	realistic signal models is still an open challenge. In this paper,
	we present a novel optimal-correlation-based reconstruction method
	for compressively sampled videos from multiple measurement vectors.
	In our method, the sparsity is mainly exploited through inter-signal
	correlations rather than the traditional frequency transform, wherein
	the optimization is not only over the signal space to satisfy data
	consistency but also over all possible linear correlation models
	to achieve minimum-l1-norm correlation noise. Additionally, a two-phase
	Bregman iterative based algorithm is outlined for solving the optimization
	problem. Simulation results show that our proposal can achieve an
	improved reconstruction performance in comparison to the conventional
	approaches, and especially, offer a 0.7–9.9 dB gain in the average
	\{PSNR\} for DCVS.},
  timestamp = {2016-07-10T07:18:32Z},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Liu, Haixiao and Song, Bin and Tian, Fang and Qin, Hao and Liu, Xiao},
  year = {2015},
  keywords = {Bregman,coding,Compressed,Correlation,Distributed,Inter-signal,iteration,Joint,model,Optimal-correlation-based,reconstruction,redundancy,sensing,Spatial/temporal,video},
  pages = {197 - 207},
  owner = {afdidehf}
}

@article{Liu2014c,
  title = {Globally sparse and locally dense signal recovery for compressed 	sensing},
  volume = {351},
  issn = {0016-0032},
  doi = {http://dx.doi.org/10.1016/j.jfranklin.2014.01.009},
  abstract = {Abstract Sparsity regularized least squares are very popular for the
	solution of the underdetermined linear inverse problem. One of the
	recent progress is that structural information is incorporated to
	the sparse signal recovery for compressed sensing. Sparse group signal
	model, which is also called block-sparse signal, is one example in
	this way. In this paper, the internal structure of each group is
	further defined to get the globally sparse and locally dense group
	signal model. It assumes that most of the entries in the active groups
	are nonzero. To estimate this newly defined signal, minimization
	of the l 1 norm of the total variation is incorporated to the group
	Lasso which is the combination of a sparsity constraint and a data
	fitting constraint. The newly proposed optimization model is called
	globally sparse and locally dense group Lasso. The added total variation
	based constraint can encourage local dense distribution in each group.
	Theoretical analysis is performed to give a class of theoretical
	sufficient conditions to guarantee successful recovery. Simulations
	demonstrate the proposed method׳s performance gains against Lasso
	and group Lasso.},
  timestamp = {2016-07-08T12:38:27Z},
  number = {5},
  journal = {Journal of the Franklin Institute},
  author = {Liu, Yipeng},
  year = {2014},
  pages = {2711 - 2727},
  owner = {afdidehf}
}

@article{Lu2015,
  title = {Compressive image sensing for fast recovery from limited samples: 	A variation on compressive sensing},
  volume = {325},
  issn = {0020-0255},
  doi = {http://dx.doi.org/10.1016/j.ins.2015.07.017},
  abstract = {Abstract In order to attain better reconstruction quality from compressive
	sensing (CS) of images, exploitation of the dependency or correlation
	patterns among the transform coefficients commonly has been employed.
	In this paper, we study a new image sensing technique, called compressive
	image sensing (CIS), with computational complexity O(m2), where m
	denotes the length of a measurement vector y = φ x , which is sampled
	from the signal x of length n via the sampling matrix φ with dimensionality
	m × n. \{CIS\} is basically a variation on compressive sampling.
	The contributions of \{CIS\} include: (i) reconstruction speed is
	extremely fast due to a closed-form solution being derived; (ii)
	certain reconstruction accuracy is preserved because significant
	components of x can be reconstructed with higher priority via an
	elaborately designed φ; and (iii) in addition to conventional 1D
	sensing, we also study 2D separate sensing to enable simultaneous
	acquisition and compression of large-sized images.},
  timestamp = {2016-07-08T11:59:44Z},
  journal = {Information Sciences},
  author = {Lu, Chun-Shien and Chen, Hung-Wei},
  year = {2015},
  keywords = {Compressed,reconstruction,Sampling,sensing,Sparsity,transform},
  pages = {33 - 47},
  owner = {afdidehf}
}

@article{Lucka2014,
  title = {Sparse Recovery Conditions and Realistic Forward Modeling in EEG/MEG 	Source Reconstruction},
  timestamp = {2016-07-11T16:51:04Z},
  author = {Lucka, Felix and Tellen, Sina and Wolters, Carsten H. and Burger, Martin},
  month = sep,
  year = {2014},
  owner = {afdidehf}
}

@inproceedings{Lustig2006,
  title = {k-t SPARSE: High frame rate dynamic MRI exploiting spatio-temporal 	sparsity},
  timestamp = {2016-07-09T19:52:14Z},
  booktitle = {Proceedings of the 13th Annual Meeting of ISMRM, Seattle},
  author = {Lustig, M. and Santos, J. M. and Donoho, D. L. and Pauly, J. M.},
  year = {2006},
  owner = {afdidehf}
}

@article{Lopez2014,
  title = {Algorithmic procedures for Bayesian MEG/EEG source reconstruction 	in SPM},
  volume = {84},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2013.09.002},
  abstract = {Abstract The MEG/EEG inverse problem is ill-posed, giving different
	source reconstructions depending on the initial assumption sets.
	Parametric Empirical Bayes allows one to implement most popular MEG/EEG
	inversion schemes (Minimum Norm, LORETA, etc.) within the same generic
	Bayesian framework. It also provides a cost-function in terms of
	the variational Free energy—an approximation to the marginal likelihood
	or evidence of the solution. In this manuscript, we revisit the algorithm
	for MEG/EEG source reconstruction with a view to providing a didactic
	and practical guide. The aim is to promote and help standardise the
	development and consolidation of other schemes within the same framework.
	We describe the implementation in the Statistical Parametric Mapping
	(SPM) software package, carefully explaining each of its stages with
	the help of a simple simulated data example. We focus on the Multiple
	Sparse Priors (MSP) model, which we compare with the well-known Minimum
	Norm and \{LORETA\} models, using the negative variational Free energy
	for model comparison. The manuscript is accompanied by Matlab scripts
	to allow the reader to test and explore the underlying algorithm.},
  timestamp = {2016-07-08T10:19:26Z},
  journal = {NeuroImage},
  author = {L{\'o}pez, J. D. and Litvak, V. and Espinosa, J. J. and Friston, K. and Barnes, G. R.},
  year = {2014},
  keywords = {Inverse,MEG/EEG,problem},
  pages = {476 - 487},
  owner = {afdidehf}
}

@article{Majumdar2015,
  title = {Energy efficient EEG sensing and transmission for wireless body area 	networks: A blind compressed sensing approach},
  volume = {20},
  issn = {1746-8094},
  doi = {http://dx.doi.org/10.1016/j.bspc.2015.03.002},
  abstract = {Abstract The problem of recovering multi-channel \{EEG\} signals from
	their randomly under-sampled measurements is addressed. The objective
	is to reduce the energy consumed by sensing, processing and transmission
	in an \{EEG\} wireless body area network. Our work is based on the
	Blind Compressed Sensing (BCS) framework, however instead of exploiting
	only the sparsity of the multi-channel ensemble in a learned basis,
	we also make use of the ensembles’ approximate rank deficiency.
	Our proposed formulation requires solving new optimization problems.
	To solve these problems, we derive algorithms based on the Split
	Bregman approach. The resulting recovery results are considerably
	better than those of previous techniques, in terms of the quantitative
	and qualitative evaluations.},
  timestamp = {2016-07-08T12:21:10Z},
  journal = {Biomedical Signal Processing and Control},
  author = {Majumdar, Angshul and Ward, Rabab K.},
  year = {2015},
  pages = {1 - 9},
  owner = {afdidehf}
}

@article{Mattout2006,
  title = {MEG source localization under multiple constraints: An extended Bayesian 	framework},
  volume = {30},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.10.037},
  abstract = {To use Electroencephalography (EEG) and Magnetoencephalography (MEG)
	as functional brain 3D imaging techniques, identifiable distributed
	source models are required. The reconstruction of EEG/MEG sources
	rests on inverting these models and is ill-posed because the solution
	does not depend continuously on the data and there is no unique solution
	in the absence of prior information or constraints. We have described
	a general framework that can account for several priors in a common
	inverse solution. An empirical Bayesian framework based on hierarchical
	linear models was proposed for the analysis of functional neuroimaging
	data [Friston, K., Penny, W., Phillips, C., Kiebel, S., Hinton, G.,
	Ashburner, J., 2002. Classical and Bayesian inference in neuroimaging:
	theory. NeuroImage 16, 465–483] and was evaluated recently in the
	context of \{EEG\} [Phillips, C., Mattout, J., Rugg, M.D., Maquet,
	P., Friston, K., 2005. An empirical Bayesian solution to the source
	reconstruction problem in EEG. NeuroImage 24, 997–1011]. The approach
	consists of estimating the expected source distribution and its conditional
	variance that is constrained by an empirically determined mixture
	of prior variance components. Estimation uses Expectation-Maximization
	(EM) to give the Restricted Maximum Likelihood (ReML) estimate of
	the variance components (in terms of hyperparameters) and the Maximum
	A Posteriori (MAP) estimate of the source parameters. In this paper,
	we extend the framework to compare different combinations of priors,
	using a second level of inference based on Bayesian model selection.
	Using Monte-Carlo simulations, ReML is first compared to a classic
	Weighted Minimum Norm (WMN) solution under a single constraint. Then,
	the ReML estimates are evaluated using various combinations of priors.
	Both standard criterion and ROC-based measures were used to assess
	localization and detection performance. The empirical Bayes approach
	proved useful as: (1) ReML was significantly better than \{WMN\}
	for single priors; (2) valid location priors improved ReML source
	localization; (3) invalid location priors did not significantly impair
	performance. Finally, we show how model selection, using the log-evidence,
	can be used to select the best combination of priors. This enables
	a global strategy for multiple prior-based regularization of the
	MEG/EEG source reconstruction.},
  timestamp = {2016-07-09T20:11:39Z},
  number = {3},
  journal = {NeuroImage},
  author = {Mattout, J�r�mie and Phillips, Christophe and Penny, William
	D. and Rugg, Michael D. and Friston, Karl J.},
  year = {2006},
  keywords = {MEG},
  pages = {753 - 767},
  owner = {afdidehf}
}

@article{Mattout2005,
  title = {Multivariate source prelocalization (MSP): Use of functionally informed 	basis functions for better conditioning the MEG inverse problem},
  volume = {26},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2005.01.026},
  abstract = {Spatially characterizing and quantifying the brain electromagnetic
	response using MEG/EEG data still remains a critical issue since
	it requires solving an ill-posed inverse problem that does not admit
	a unique solution. To overcome this lack of uniqueness, inverse methods
	have to introduce prior information about the solution. Most existing
	approaches are directly based upon extrinsic anatomical and functional
	priors and usually attempt at simultaneously localizing and quantifying
	brain activity. By contrast, this paper deals with a preprocessing
	tool which aims at better conditioning the source reconstruction
	process, by relying only upon intrinsic knowledge (a forward model
	and the MEG/EEG data itself) and focusing on the key issue of localization.
	Based on a discrete and realistic anatomical description of the cortex,
	we first define functionally Informed Basis Functions (fIBF) that
	are subject specific. We then propose a multivariate method which
	exploits these fIBF to calculate a probability-like coefficient of
	activation associated with each dipolar source of the model. This
	estimated distribution of activation coefficients may then be used
	as an intrinsic functional prior, either by taking these quantities
	into account in a subsequent inverse method, or by thresholding the
	set of probabilities in order to reduce the dimension of the solution
	space. These two ways of constraining the source reconstruction process
	may naturally be coupled. We successively describe the proposed Multivariate
	Source Prelocalization (MSP) approach and illustrate its performance
	on both simulated and real \{MEG\} data. Finally, the better conditioning
	induced by the \{MSP\} process in a classical regularization scheme
	is extensively and quantitatively evaluated.},
  timestamp = {2016-07-10T06:48:06Z},
  number = {2},
  journal = {NeuroImage},
  author = {Mattout, J. and P�l�grini-Issac, M. and Garnero, L. and Benali, H.},
  year = {2005},
  keywords = {Activation probability,Better conditioning,Functionally
	Informed Basis Functions (fIBF),Functionally Informed Basis
	Functions (fIBF),inverse problem,MEG/EEG,Multivariate
	Source Prelocalization (MSP),Multivariate Source Prelocalization
	(MSP),regularization},
  pages = {356 - 373},
  owner = {afdidehf}
}

@book{Meier2015,
  title = {Package 'grplasso': Fitting user specified models with Group Lasso 	penalty},
  timestamp = {2016-07-10T07:21:32Z},
  author = {Meier, Lukas},
  month = jan,
  year = {2015},
  owner = {afdidehf}
}

@article{Mohammed2014,
  volume = {95},
  abstract = {Schizophrenia is still considered unknown disease that needs more
	study and analysis. In this case Multimodal fusion is a good way
	to make analysis on the joint information found on the different
	imaging modalities related to this disease. This paper discusses
	Schizophrenia analysis using two approaches of the common Brian Imaging
	multimodal fusion approaches (Joint ICA and Parallel ICA). The aim
	of this study is to investigate these two approaches for more understanding
	showing their strengths, limitations, and analysis strategies.},
  timestamp = {2016-05-26T09:47:43Z},
  number = {9},
  journal = {International Journal of Computer Applications},
  author = {Mohammed, Abdullah N. and Taha, Taha E. and Faragallah, Osama S.},
  year = {2014},
  keywords = {fMRI image,image fusion,Joint ICA,Medical image fusion,Parallel ICA,Schizophrenia},
  pages = {25 - 28},
  owner = {Fardin}
}

@article{Molins2008,
  title = {Quantification of the benefit from integrating MEG and EEG data in 	minimum l2-norm estimation},
  volume = {42},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2008.05.064},
  abstract = {Source current estimation from electromagnetic (MEG and EEG) signals
	is an ill-posed problem that often produces blurry or inaccurately
	positioned estimates. The two modalities have distinct factors limiting
	the resolution, e.g., \{MEG\} cannot detect radially oriented sources,
	while \{EEG\} is sensitive to accuracy of the head model. This makes
	combined \{EEG\} + \{MEG\} estimation techniques desirable, but different
	acquisition noise statistics, complexity of the head models, and
	lack of pertinent metrics all complicate the assessment of the resulting
	improvements. We investigated analytically the effect of including
	\{EEG\} recordings in \{MEG\} studies versus the addition of new
	\{MEG\} channels when computing noise-normalized minimum l2-norm
	estimates. Three-compartment boundary-element forward models were
	constructed using structural \{MRI\} scans for four subjects. Singular
	value analysis of the resulting forward models predicted better performance
	of the \{EEG\} + \{MEG\} case in the form of higher matrix rank.
	\{MNE\} inverse operators for EEG, \{MEG\} and \{EEG\} + \{MEG\}
	were constructed using the sensor noise covariance estimated from
	data. Metrics derived from the resolution matrices predicted higher
	spatial resolution in \{EEG\} + \{MEG\} as compared to \{MEG\} due
	to decreased spread (lower spatial dispersion, higher resolution
	index) with no reduction in dipole localization error. The effect
	was apparent in all source locations, with increased magnitude for
	deep areas such as the cingulate cortex. We were also able to corroborate
	the results for the somatosensory cortex using median nerve responses.},
  timestamp = {2016-10-21T13:40:44Z},
  number = {3},
  journal = {NeuroImage},
  author = {Molins, A. and Stufflebeam, S. M. and Brown, E. N. and H{\"a}m{\"a}l{\"a}inen, M. S.},
  year = {2008},
  keywords = {EEG},
  pages = {1069--1077},
  owner = {afdidehf}
}

@phdthesis{Moller2012,
  title = {Multiscale Methods for (generalized) Sparse Recovery and Applications 	in High Dimensional Imaging},
  abstract = {This thesis presents various aspects of sparsity related regularization
	of inverse problems in high dimensional data. It is divided into
	two parts. The rst part is of theoretical nature; besides laying
	the mathematical background, it discusses the possibility of recovering
	sparse solutions exactly from highly underdetermined linear systems.
	Furthermore, we extend some of the results from the typical compressed
	sensing case, i.e., from `1 minimization, to arbitrary polyhedral
	functions. In the following chapters, we focus on the development
	and analysis of numerical methods for the `1 and polyhedral function
	minimization. We provide the theory for solving the so-called inverse
	scale space ow to any polyhedral function exactly. Furthermore, we
	show the equivalence of the popular gradient descent method to an
	inverse scale space ow on the dual functional, thus allowing us to
	apply our convergence theory to this widely used class of methods.
	For problems where the inverse scale space ow is not directly applicable,
	we provide a framework for convergence analysis of split Bregman
	type methods and show that the application of this method to the
	primal and dual problem can yield very dierent convergence properties.
	Finally, the variable splitting approach of split Bregman is taken
	to its continuous limit, leading again to a polyhedral inverse scale
	space ow. In the second part of this thesis, we explore multiple
	applications. The concepts of sparsity and `1 regularization are
	applied to the problem of hyperspectral unmixing and the inverse
	scale space ow is shown to yield superior results even in the case
	of overdetermined systems with noisy data. The second application
	is related to hyperspectral unmixing but can also be seen in the
	general context of dimension reduction or subset selection. We propose
	a new method for selecting a small subset of points from a dataset
	that lead to a non-negative sparse description of the whole dataset
	in terms of the selected points. Mathematically, we make use of the
	concept of row-sparsity with the means of `1;1 regularization. Finally,
	we present a project on automatic cell tracking in phase contrast
	microscopic videos. With the help of two topology preserving level
	set methods we extract the movement of the centroid as well as the
	change of area and perimeter of each cell. We validate our model
	against several manual tracking results.},
  timestamp = {2016-10-24T16:35:22Z},
  school = {Westfälische Wilhelms-Universität Münster},
  author = {M{\"o}ller, Michael},
  year = {2012},
  keywords = {`1 regularization,augmented Lagrangian,Bregman distance,Bregman
	distance,Bregman iteration,cell tracking,cell
	tracking,compressed sensing,dimension reduction,hyperspectral unmixing,inverse scale space,inverse
	scale space,matrix factorization,polyhedral functions,Sparsity},
  owner = {afdidehf}
}

@inproceedings{Moller1998,
  title = {Solving Set Partitioning Problems with Constraint Programming},
  abstract = {This paper investigates the potential of constraint programming for
	solving set partitioning problems occurring in crew scheduling, where
	constraint programming is restricted to not employ external solvers,
	as for instance integer linear programming solvers. We evaluate preprocessing
	steps known from the OR literature on moderately sized set partitioning
	problems. Further, we propose a new preprocessing technique which
	allows to reduce problem size more effectively than standard preprocessing
	techniques but with similar computational effort. Additionally, we
	propose a propagation algorithm for a global set partitioning constraint
	which, compared with other constraint programming approaches, finds
	and proves optimal solutions significantly faster resp. produces
	better solutions in a given time period.},
  timestamp = {2016-07-10T08:20:31Z},
  booktitle = {Sixth International Conference on the Practical Application of Prolog 	and the Fourth International Confernce on the Practical Application 	of Constraint Technology � PAPPACT98},
  author = {M{\"o}ller, Tobias},
  year = {1998},
  pages = {313�332},
  owner = {afdidehf}
}

@article{Nardi2008,
  title = {On the Asymptotic Properties of The Group Lasso Estimator for Linear 	Models},
  volume = {2},
  abstract = {We establish estimation and model selection consistency, pre- diction
	and estimation bounds and persistence for the group-lasso estimator
	and model selector proposed by Yuan and Lin (2006) for least squares
	prob- lems when the covariates have a natural grouping structure.
	We consider the case of a xed-dimensional parameter space with increasing
	sample size and the double asymptotic scenario where the model complexity
	changes with the sample size.},
  timestamp = {2017-06-23T10:02:10Z},
  journal = {Electronic Journal of Statistics},
  author = {Nardi, Yuval and Rinaldoy, Alessandro},
  year = {2008},
  keywords = {Group-Lasso,Least Squares,Model selection,oracle inequalities,Persistence.,Sparsity},
  pages = {605--633},
  owner = {afdidehf}
}

@article{Natarajan1995,
  title = {Sparse Approximate Solutions To Linear Systems},
  volume = {24},
  abstract = {The following problem is considered: given a matrix A in Rm,
	(m rows and n columns), a vector b in Rm, and 6 > 0, compute a vector
	x satisfying IIAx bl[2 <_ 6 if such exists, such that x has the fewest
	number of non-zero entries over all such vectors. It is shown that
	the problem is NP-hard, but that the well-known greedy heuristic
	is good in that it computes a solution with at most [18 Opt(6/Z)llA+
	I1 ln(llbl12/6)] non-zero entries, where Opt(6/2) is the optimum
	number of nonzero entries at error 6/2, A is the matrix obtained
	by normalizing each column of A with respect to the L2 norm, and
	A+ is its pseudo-inverse.},
  timestamp = {2017-06-23T13:18:32Z},
  number = {2},
  journal = {SIAM J. Comput.},
  author = {Natarajan, B. K.},
  month = apr,
  year = {1995},
  keywords = {Linear systems,sparse solutions},
  pages = {227--234},
  owner = {afdidehf}
}

@article{Needell2009,
  title = {CoSaMP: Iterative signal recovery from incomplete and inaccurate 	samples},
  volume = {26},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2008.07.002},
  abstract = {Compressive sampling offers a new paradigm for acquiring signals that
	are compressible with respect to an orthonormal basis. The major
	algorithmic challenge in compressive sampling is to approximate a
	compressible signal from noisy samples. This paper describes a new
	iterative recovery algorithm called CoSaMP that delivers the same
	guarantees as the best optimization-based approaches. Moreover, this
	algorithm offers rigorous bounds on computational cost and storage.
	It is likely to be extremely efficient for practical problems because
	it requires only matrix–vector multiplies with the sampling matrix.
	For compressible signals, the running time is just O ( N log 2 N
	) , where N is the length of the signal.},
  timestamp = {2016-07-08T12:06:12Z},
  number = {3},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Needell, D. and Tropp, J. A.},
  year = {2009},
  keywords = {Algorithms},
  pages = {301 - 321},
  owner = {afdidehf}
}

@article{Olshausen1997,
  title = {Sparse coding with an overcomplete basis set: A strategy employed 	by V1?},
  volume = {37},
  issn = {0042-6989},
  doi = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
  abstract = {The spatial receptive fields of simple cells in mammalian striate
	cortex have been reasonably well described physiologically and can
	be characterized as being localized, oriented, and bandpass, comparable
	with the basis functions of wavelet transforms. Previously, we have
	shown that these receptive field properties may be accounted for
	in terms of a strategy for producing a sparse distribution of output
	activity in response to natural images. Here, in addition to describing
	this work in a more expansive fashion, we examine the neurobiological
	implications of sparse coding. Of particular interest is the case
	when the code is overcomplete—i.e., when the number of code elements
	is greater than the effective dimensionality of the input space.
	Because the basis functions are non-orthogonal and not linearly independent
	of each other, sparsifying the code will recruit only those basis
	functions necessary for representing a given input, and so the input-output
	function will deviate from being purely linear. These deviations
	from linearity provide a potential explanation for the weak forms
	of non-linearity observed in the response properties of cortical
	simple cells, and they further make predictions about the expected
	interactions among units in response to naturalistic stimuli.},
  timestamp = {2016-09-29T16:27:52Z},
  number = {23},
  journal = {Vision Research},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {1997},
  keywords = {coding,Gabor-wavelet,Neural images,V1},
  pages = {3311--3325},
  annote = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  annote = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  annote = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  annote = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  annote = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  annote = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  owner = {Fardin}
}

@article{Ou2010,
  title = {Multimodal functional imaging using fMRI-informed regional EEG/MEG 	source estimation},
  volume = {52},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.03.001},
  abstract = {We propose a novel method, fMRI-Informed Regional Estimation (FIRE),
	which utilizes information from fMRI in E/MEG source reconstruction.
	\{FIRE\} takes advantage of the spatial alignment between the neural
	and the vascular activities, while allowing for substantial differences
	in their dynamics. Furthermore, with a region-based approach, \{FIRE\}
	estimates the model parameters for each region independently. Hence,
	it can be efficiently applied on a dense grid of source locations.
	The optimization procedure at the core of \{FIRE\} is related to
	the re-weighted minimum-norm algorithms. The weights in the proposed
	approach are computed from both the current source estimates and
	fMRI data, leading to robust estimates in the presence of silent
	sources in either fMRI or E/MEG measurements. We employ a Monte Carlo
	evaluation procedure to compare the proposed method to several other
	joint E/MEG-fMRI algorithms. Our results show that \{FIRE\} provides
	the best trade-off in estimation accuracy between the spatial and
	the temporal accuracy. Analysis using human E/MEG-fMRI data reveals
	that \{FIRE\} significantly reduces the ambiguities in source localization
	present in the minimum-norm estimates, and that it accurately captures
	activation timing in adjacent functional regions.},
  timestamp = {2016-07-09T20:15:38Z},
  number = {1},
  journal = {NeuroImage},
  author = {Ou, Wanmei and Nummenmaa, Aapo and Ahveninen, Jyrki and Belliveau, John W. and H�m�l�inen, Matti S. and Golland, Polina},
  year = {2010},
  keywords = {EEG},
  pages = {97 - 108},
  owner = {afdidehf}
}

@article{Ozer2015,
  title = {Extraction of primary and secondary frequency control from active 	power generation data of power plants},
  volume = {73},
  issn = {0142-0615},
  doi = {http://dx.doi.org/10.1016/j.ijepes.2015.03.007},
  abstract = {Abstract Frequency control is a vital component of a secure and robust
	power grid and it ought to be closely monitored. Frequency control
	consists of two main components; primary and secondary control and
	their contributions are usually aggregated in the active power generation
	data of a plant, which is acquired via Supervisory Control And Data
	Acquisition. In many cases, such as in Turkey, they are demanded
	to be evaluated separately due to different impacts on power system
	or different financial policies. However, this is not usually a straightforward
	process since primary and secondary response cannot be obtained distinctly.
	In this work, Extraction of Primary and Secondary Control (EPSCon)
	algorithm is introduced to extract primary and secondary response
	over active power generation data. Based on time and frequency domain
	characteristics of primary and secondary response, \{EPSCon\} is
	developed on a Expectation-Maximization type recursive scheme employing
	Generalized Cross Correlation and l 1 Trend Filtering techniques.
	Favorably, \{EPSCon\} uses a simple plant model built upon basic
	governor and plant load controller technical characteristics as an
	initial estimate of primary and secondary response.},
  timestamp = {2016-07-08T12:27:49Z},
  journal = {International Journal of Electrical Power \& Energy Systems},
  author = {Ozer, B. and Arikan, O. and Moral, G. and Altintas, A.},
  year = {2015},
  keywords = {applied,control,Frequency,monitoring,Power,processing,recovery,Signal,Sparse,system,systems,to},
  pages = {16 - 22},
  owner = {afdidehf}
}

@article{Pascual-Marqui1999a,
  title = {Reply to Comments Made by R. Grave De Peralta Menendez and S.L. Gozalez 	Andino},
  volume = {1},
  timestamp = {2016-07-10T07:54:21Z},
  number = {1},
  journal = {International Journal of Bioelectromagnetism},
  author = {Pascual-Marqui, Roberto Domingo},
  year = {1999},
  owner = {Fardin}
}

@article{Penny2008,
  title = {Physiological Basis of EEG/MEG Signals, Forward Models and Source 	Reconstruction},
  timestamp = {2016-07-10T07:32:36Z},
  author = {Penny, Will},
  month = dec,
  year = {2008},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Wellcome Trust Centre for Neuroimaging, University College London,
	UK},
  owner = {afdidehf}
}

@article{Petukhov2006,
  title = {Fast implementation of orthogonal greedy algorithm for tight wavelet 	frames},
  volume = {86},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2005.05.025},
  abstract = {We introduce and study a fast implementation of orthogonal greedy
	algorithm for wavelet frames. The results of numerical simulation
	for N-term approximations of standard images by tight wavelet frames
	showed that to provide the same \{PSNR\} the popular 9 / 7 biorthogonal
	wavelet bases require 38–42% more terms of the expansion. This
	is equivalent to 2–4 dB advantage of the wavelet frames in \{PSNR\}
	over the 9 / 7 bases for a fixed number of decomposition terms.},
  timestamp = {2016-07-08T12:28:03Z},
  number = {3},
  journal = {Signal Processing},
  author = {Petukhov, Alexander},
  year = {2006},
  note = {Sparse Approximations in Signal and Image ProcessingSparse Approximations
	in Signal and Image Processing},
  keywords = {image compression,orthogonal greedy algorithm,Wavelet frame},
  pages = {471 - 479},
  owner = {afdidehf}
}

@article{Peyre2009,
  title = {Manifold models for signals and images},
  volume = {113},
  issn = {1077-3142},
  doi = {http://dx.doi.org/10.1016/j.cviu.2008.09.003},
  abstract = {This article proposes a new class of models for natural signals and
	images. These models constrain the set of patches extracted from
	the data to analyze to be close to a low-dimensional manifold. This
	manifold structure is detailed for various ensembles suitable for
	natural signals, images and textures modeling. These manifolds provide
	a low-dimensional parameterization of the local geometry of these
	datasets. These manifold models can be used to regularize inverse
	problems in signal and image processing. The restored signal is represented
	as a smooth curve or surface traced on the manifold that matches
	the forward measurements. A manifold pursuit algorithm computes iteratively
	a solution of the manifold regularization problem. Numerical simulations
	on inpainting and compressive sensing inversion show that manifolds
	models bring an improvement for the recovery of data with geometrical
	features.},
  timestamp = {2016-07-09T20:07:07Z},
  number = {2},
  journal = {Computer Vision and Image Understanding},
  author = {Peyr{\'e}, Gabriel},
  year = {2009},
  keywords = {Image,Manifold,modeling,processing,Signal,Texture},
  pages = {249 - 260},
  __markedentry = {[afdidehf:6]},
  owner = {afdidehf}
}

@techreport{Phlypo2015,
  title = {More accurate approximations to the $\ell_0$ norm (Bye bye $\ell_1$)},
  timestamp = {2016-07-09T20:15:01Z},
  author = {Phlypo, Ronald},
  month = mar,
  year = {2015},
  owner = {afdidehf}
}

@techreport{Pong2010,
  title = {Trace Norm Regularization: Reformulations, Algorithms, and Multi-task 	Learning},
  abstract = {We consider a recently proposed optimization formulation of multi-task
	learning based on trace norm regularized least squares. While this
	problem may be formulated as a semide�nite program (SDP), its size
	is beyond general SDP solvers. Previous solution approaches apply
	proximal gradient methods to solve the primal problem. We derive
	new primal and dual reformulations of this problem, including a reduced
	dual formulation that involves minimizing a convex quadratic function
	over an operator-norm ball in matrix space. This reduced dual problem
	may be solved by gradient-projection methods, with each projection
	involving a singular value decomposition. The dual approach is com-
	pared with existing approaches and its practical e�ectiveness is
	illustrated on simulations and an application to gene expression
	pattern analysis.},
  timestamp = {2016-07-11T17:08:53Z},
  author = {Pong, Ting Kei and Tseng, Paul and Ji, Shuiwang and Ye, Jieping},
  month = mar,
  year = {2010},
  keywords = {convex optimization,duality,gene expression pattern analysis,multi-task learning,multi-task
	learning,proximal gradient method.,semide�nite programming,trace norm
	regularization,trace
	norm regularization},
  owner = {afdidehf}
}

@article{Poon2015,
  title = {Structure dependent sampling in compressed sensing: Theoretical guarantees 	for tight frames},
  issn = {1063-5203},
  doi = {http://dx.doi.org/10.1016/j.acha.2015.09.003},
  abstract = {Abstract Many of the applications of compressed sensing have been
	based on variable density sampling, where certain sections of the
	sampling coefficients are sampled more densely. Furthermore, it has
	been observed that these sampling schemes are dependent not only
	on sparsity but also on the sparsity structure of the underlying
	signal. This paper extends the result of Adcock, Hansen, Poon and
	Roman (arXiv:1302.0561, 2013) [2] to the case where the sparsifying
	system forms a tight frame. By dividing the sampling coefficients
	into levels, our main result will describe how the amount of subsampling
	in each level is determined by the local coherences between the sampling
	and sparsifying operators and the localized level sparsities –
	the sparsity in each level under the sparsifying operator.},
  timestamp = {2016-07-11T16:57:28Z},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Poon, Clarice},
  year = {2015},
  keywords = {analysis,Compressed,frames,L1,Minimization,Multilevel,Sampling,sensing,Tight},
  pages = {-},
  owner = {afdidehf}
}

@article{Ramirez2008,
  title = {Source localization},
  volume = {3},
  timestamp = {2016-07-10T08:23:01Z},
  number = {11},
  author = {Ram{\'i}rez, Rey R.},
  year = {2008},
  owner = {Fardin}
}

@article{Rao2003,
  title = {Subset selection in noise based on diversity measure minimization},
  volume = {51},
  issn = {1053-587X},
  doi = {10.1109/TSP.2002.808076},
  abstract = {We develop robust methods for subset selection based on the minimization
	of diversity measures. A Bayesian framework is used to account for
	noise in the data and a maximum a posteriori (MAP) estimation procedure
	leads to an iterative procedure which is a regularized version of
	the focal underdetermined system solver (FOCUSS) algorithm. The convergence
	of the regularized FOCUSS algorithm is established and it is shown
	that the stable fixed points of the algorithm are sparse. We investigate
	three different criteria for choosing the regularization parameter:
	quality of fit; sparsity criterion; L-curve. The L-curve method,
	as applied to the problem of subset selection, is found not to be
	robust, and we propose a novel modified L-curve procedure that solves
	this problem. Each of the regularized FOCUSS algorithms is evaluated
	through simulation of a detection problem, and the results are compared
	with those obtained using a sequential forward selection algorithm
	termed orthogonal matching pursuit (OMP). In each case, the regularized
	FOCUSS algorithm is shown to be superior to the OMP in noisy environments.},
  timestamp = {2016-07-11T16:58:50Z},
  number = {3},
  journal = {IEEE Transactions on Signal Processing},
  author = {Rao, B. D. and Engan, K. and Cotter, S. F. and Palmer, J. and Kreutz-Delgado, K.},
  month = mar,
  year = {2003},
  keywords = {Bayesian framework,Bayesian framework,Bayesian methods,Bayesian methods,Bayes methods,Bayes methods,detection problem,detection problem,Dictionaries,Dictionaries,diversity measure minimization,diversity
	measure minimization,diversity measure minimization,focal underdetermined system solver algorithm,focal underdetermined system solver algorithm,Iterative algorithms,Iterative
	algorithms,Iterative algorithms,iterative methods,iterative methods,iterative procedure,iterative procedure,L-curve method,L-curve method,MAP estimation,MAP
	estimation,MAP estimation,Matching pursuit algorithms,Matching pursuit algorithms,maximum a posteriori estimation,maximum a posteriori estimation,Maximum likelihood estimation,maximum
	likelihood estimation,maximum likelihood estimation,minimisation,minimisation,Minimization methods,Minimization methods,Noise measurement,Noise measurement,Noise robustness,Noise
	robustness,Noise robustness,orthogonal matching pursuit,orthogonal matching pursuit,Pursuit algorithms,Pursuit algorithms,quality of fit,quality
	of fit,quality of fit,random noise,random noise,regularization parameter,regularization parameter,sequential forward selection algorithm,sequential forward selection
	algorithm,sequential forward selection algorithm,set theory,set theory,signal processing,signal processing,Signal-To-Noise Ratio,Signal to noise ratio,Signal
	to noise ratio,signal-to-noise ratio,Signal to noise ratio,SNR,SNR,sparsity criterion,sparsity criterion,subset selection,subset selection,Working environment noise,Working environment
	noise,Working environment noise},
  pages = {760-770},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UGKFUAKS\\articleDetails.html:text/html}
}

@inproceedings{Rath2008a,
  address = {Cairns, Australia},
  title = {Sparse approximations for joint source-channel coding and its application 	to image transmission},
  timestamp = {2016-07-11T16:39:57Z},
  booktitle = {IEEE Intl. Workshop on Multimedia Signal Processing, IEEE-MMSP},
  author = {Rath, Gagan and Guillemot, Christine and Fuchs, Jean-Jacques},
  year = {2008},
  hal_id = {inria-00504580},
  hal_version = {v1},
  owner = {afdidehf}
}

@article{Razavi2012,
  title = {Reformulated Neural Network (ReNN): A New Alternative for Data-driven 	Modelling in Hydrology and Water Resources Engineering},
  timestamp = {2016-07-10T07:50:28Z},
  author = {Razavi, Saman and Tolson, Bryan and Burn, Donald and Seglenieks, Frank},
  year = {2012},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Romberg2013,
  title = {Solving Underdetermined Linear Equations and Overdetermined Quadratic 	Equations (using Convex Programming)},
  timestamp = {2016-07-10T08:21:43Z},
  author = {Romberg, Justin},
  month = jun,
  year = {2013},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {Caltech ROM-GR Workshop},
  owner = {Fardin}
}

@techreport{Rosenzweig2016,
  title = {$L^p$ Spaces for $0 < p < 1$},
  timestamp = {2016-07-08T09:37:44Z},
  author = {Rosenzweig, Matt},
  year = {2016},
  owner = {afdidehf}
}

@article{Rousseau2013,
  title = {Dispositifs d'imagerie m{\'e}dicale: MEG - EEG},
  timestamp = {2016-07-08T12:13:49Z},
  author = {Rousseau, F.},
  month = mar,
  year = {2013},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Sajjad2015,
  title = {Image super-resolution using sparse coding over redundant dictionary 	based on effective image representations},
  volume = {26},
  issn = {1047-3203},
  doi = {http://dx.doi.org/10.1016/j.jvcir.2014.10.012},
  abstract = {Abstract Recent years have shown a growing research interest in the
	sparse-representation of signals. Signals are described through sparse
	linear combinations of signal-atoms over a redundant-dictionary.
	Therefore, we propose a novel super-resolution framework using an
	overcomplete-dictionary based on effective image-representations
	such as edges, contours and high-order structures. This scheme recovers
	the vector of common sparse-representations between low-resolution
	and corresponding high-resolution image-patches by solving the l
	1 -regularized least-squared problem; subsequently, it reconstructs
	the \{HR\} output by multiplying it with the learned dictionary.
	The dictionary used in the proposed-technique contains more effective
	image-representations than those in previous approaches because it
	contains feature-descriptors such as edges, contours and motion-selective
	features. Therefore, the proposed-technique is more robust to various
	types of distortion. A saliency-map quickens this technique by confining
	the optimization-process to visually salient regions. Experimental
	analyses confirm the effectiveness of the proposed-scheme, and its
	quantitative and qualitative performance as compared with other state-of-the-art
	super-resolution algorithms.},
  timestamp = {2016-07-08T12:47:34Z},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Sajjad, Muhammad and Mehmood, Irfan and Baik, Sung Wook},
  year = {2015},
  keywords = {algorithm,denoising,Overcomplete-dictionary,Pursuit,reconstruction,Representation-coefficients,Sparse-coding,super-resolution,Visual-saliency},
  pages = {50 - 65},
  owner = {afdidehf}
}

@article{Sathya2013,
  title = {Comparison of Supervised and Unsupervised Learning Algorithms for 	Pattern Classification},
  volume = {2},
  abstract = {This paper presents a comparative account of unsupervised and supervised
	learning models and their pattern classification evaluations as applied
	to the higher education scenario. Classification plays a vital role
	in machine based learning algorithms and in the present study, we
	found that, though the error back-propagation learning algorithm
	as provided by supervised learning model is very efficient for a
	number of non-linear real-time problems, KSOM of unsupervised learning
	model, offers efficient solution and classification in the present
	study.},
  timestamp = {2016-07-08T11:54:56Z},
  number = {2},
  journal = {International Journal of Advanced Research in Artificial Intelligence 	(IJARAI)},
  author = {Sathya, R. and Abraham, Annamma},
  year = {2013},
  keywords = {classification,clustering,learning,MLP,SOM,Supervised learning,unsupervised learning,unsupervised
	learning},
  owner = {Fardin}
}

@techreport{Schmidt2009,
  title = {Optimization Methods for $\ell_1$-Regularization},
  abstract = {In this paper we review and compare state-of-the-art optimization
	techniques for solving the problem of minimizing a twice-dierentiable
	loss function subject to `1-regularization. The rst part of this
	work outlines a variety of the approaches that are available to solve
	this type of problem, highlighting some of their strengths and weaknesses.
	In the second part, we present numerical results comparing 14 optimization
	strategies under various scenarios.},
  timestamp = {2016-07-10T07:20:13Z},
  author = {Schmidt, Mark and Fung, Glenn and Rosaless, Romer},
  month = aug,
  year = {2009},
  owner = {afdidehf}
}

@Article{Sharon2007,
  author    = {Sharon, Dahlia and H\"{a}m\"{a}l\"{a}inen, Matti S. and Tootell, Roger B. H. and Halgren, Eric and Belliveau, John W.},
  title     = {The advantage of combining MEG and EEG: Comparison to fMRI in focally stimulated visual cortex},
  journal   = {NeuroImage},
  year      = {2007},
  volume    = {36},
  number    = {4},
  pages     = {1225--1235},
  month     = jul,
  issn      = {1053-8119},
  abstract  = {To exploit the high (millisecond) temporal resolution of magnetoencephalography
	(MEG) and electroencephalography (EEG) for measuring neuronal dynamics
	within well-defined brain regions, it is important to quantitatively
	assess their localizing ability. Previous modeling studies and empirical
	data suggest that a combination of MEG and EEG signals should
	yield the most accurate localization, due to their complementary
	sensitivities. However, these two modalities have rarely been explicitly
	combined for source estimation in studies of recorded brain activity,
	and a quantitative empirical assessment of their abilities, combined
	and separate, is currently lacking. Here we studied early visual
	responses to focal Gabor patches flashed during subject fixation.
	 MEG and EEG data were collected simultaneously and were compared
	with the functional MRI (fMRI) localization produced by identical
	stimuli in the same subjects. This allowed direct evaluation of the
	localization accuracy of separate and combined MEG/EEG inverse solutions.
	We found that the localization accuracy of the combined MEG EEG
	solution was consistently better than that of either modality alone,
	using three different source estimation approaches. Further analysis
	suggests that this improved localization is due to the different
	properties of the two imaging modalities rather than simply due to
	increased total channel number. Thus, combining MEG and EEG 
	data is important for high-resolution spatiotemporal studies of the
	human brain.},
  doi       = {http://dx.doi.org/10.1016/j.neuroimage.2007.03.066},
  owner     = {Fardin},
  timestamp = {2016-10-21T13:37:52Z},
}

@incollection{Silva2009,
  abstract = {The existence of the electrical activity of the brain (i.e. the electroencephalogram
	or EEG) was discovered more than a century ago by Caton. After the
	demonstration that the EEG could be recorded from the human scalp
	by Berger in the 1920s, it made a slow start before it became accepted
	as a method of analysis of brain functions in health and disease.
	It is interesting to note that this acceptance came only after the
	demonstration by Adrian and Mathews (1934) that the EEG, namely the
	alpha rhythm, was likely generated in the occipital lobes in man,
	and was not artefactual. However, the neuronal sources of the alpha
	rhythm remained undefined until the 1970s, when we demonstrated,
	in dog, that the alpha rhythm is generated by a dipole layer cantered
	at layers IV and V of the visual cortex (Lopes da Silva and Storm
	van Leeuwen 1977). It may be not surprising that the mechanisms of
	generation and the functional significance of the EEG remained controversial
	for a relatively long time considering the complexity of the underlying
	systems of neuronal generators on the one hand and the rather involved
	transfer of signals from the cortical surface to the scalp due to
	the topological and electrical properties of the volume conductor
	(brain, cerebrospinal fluid, skull, scalp) on the other. The EEG
	consists of the summed electrical activities of populations of neurons,
	with a modest contribution from glial cells. The neurons are excitable
	cells with characteristic intrinsic electrical properties, and their
	activity produces electrical and magnetic fields. These fields may
	be recorded by means of electrodes at a short distance from the sources
	(the local EEG or local field potentials, LFPs), or from the cortical
	surface (the electrocorticogram or ECoG), or at longer distances,
	even from the scalp (i.e. the EEG, in the most common sense). The
	associated MEG is usually recorded via sensors that are highly sensitive
	to changes in the very weak neuronal magnetic fields, which are placed
	at short distances around the scalp.},
  timestamp = {2016-05-26T09:49:26Z},
  author = {Silva, Fernando Lopes da},
  year = {2009},
  pages = {19-38},
  owner = {Fardin}
}

@article{Silva2014,
  title = {A statistically motivated framework for simulation of stochastic 	data fusion models applied to multimodal neuroimaging},
  volume = {102, Part 1},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.04.035},
  abstract = {Abstract Multimodal fusion is becoming more common as it proves to
	be a powerful approach to identify complementary information from
	multimodal datasets. However, simulation of joint information is
	not straightforward. Published approaches mostly employ limited,
	provisional designs that often break the link between the model assumptions
	and the data for the sake of demonstrating properties of fusion techniques.
	This work introduces a new approach to synthetic data generation
	which allows full-compliance between data and model while still representing
	realistic spatiotemporal features in accordance with the current
	neuroimaging literature. The focus is on the simulation of joint
	information for the verification of stochastic linear models, particularly
	those used in multimodal data fusion of brain imaging data. Our first
	goal is to obtain a benchmark ground-truth in which estimation errors
	due to model mismatch are minimal or none. Then we move on to assess
	how estimation is affected by gradually increasing model discrepancies
	toward a more realistic dataset. The key aspect of our approach is
	that it permits complete control over the type and level of model
	mismatch, allowing for more educated inferences about the limitations
	and caveats of select stochastic linear models. As a result, impartial
	comparison of models is possible based on their performance in multiple
	different scenarios. Our proposed method uses the commonly overlooked
	theory of copulas to enable full control of the type and level of
	dependence/association between modalities, with no occurrence of
	spurious multimodal associations. Moreover, our approach allows for
	arbitrary single-modality marginal distributions for any fixed choice
	of dependence/association between multimodal features. Using our
	simulation framework, we can rigorously challenge the assumptions
	of several existing multimodal fusion approaches. Our study brings
	a new perspective to the problem of simulating multimodal data that
	can be used for ground-truth verification of various stochastic multimodal
	models available in the literature, and reveals some important aspects
	that are not captured or are overlooked by ad hoc simulations that
	lack a firm statistical motivation.},
  timestamp = {2016-07-08T11:31:52Z},
  journal = {NeuroImage},
  author = {Silva, Rogers F. and Plis, Sergey M. and Adal?, T�lay and Calhoun, Vince D.},
  year = {2014},
  note = {Multimodal Data Fusion},
  keywords = {Copula,Fusion,ICA,Multidimensional,multimodal,Simulation,Stochastic},
  pages = {92 - 117},
  owner = {afdidehf}
}

@article{Starck2003,
  title = {Image Decomposition: Separation of Texture from Piecewise Smooth 	Content},
  abstract = {This paper presents a novel method for separating images into texture
	and piecewise smooth parts. The proposed approach is based on a combination
	of the Basis Pursuit Denoising (BPDN) algorithm and the Total-Variation
	(TV) regularization scheme. The basic idea promoted in this paper
	is the use of two appropriate dictionaries, one for the representation
	of textures, and the other for the natural scene parts. Each dictionary
	is designed for sparse representation of a particular type of image-content
	(either texture or piecewise smooth). The use of BPDN with the two
	augmented dictionaries leads to the desired separation, along with
	noise removal as a by-product. As the need to choose a proper dictionary
	for natural scene is very hard, a TV regularization is employed to
	better direct the separation process. Experimental results validate
	the algorithm's performance.},
  timestamp = {2016-07-08T12:46:25Z},
  journal = {SPIE Meeting},
  author = {Starck, J.-L. and Elad, M. and Donoho, D. L.},
  month = aug,
  year = {2003},
  keywords = {Basis pursuit denoising,Curvelet.,Local DCT,piecewise smooth,ridgelet,sparse representations,sparse
	representations,Texture,total variation,wavelet},
  owner = {Fardin}
}

@article{Sui2012,
  title = {A review of multivariate methods for multimodal fusion of brain imaging 	data},
  volume = {204},
  issn = {0165-0270},
  doi = {http://dx.doi.org/10.1016/j.jneumeth.2011.10.031},
  abstract = {The development of various neuroimaging techniques is rapidly improving
	the measurements of brain function/structure. However, despite improvements
	in individual modalities, it is becoming increasingly clear that
	the most effective research approaches will utilize multi-modal fusion,
	which takes advantage of the fact that each modality provides a limited
	view of the brain. The goal of multi-modal fusion is to capitalize
	on the strength of each modality in a joint analysis, rather than
	a separate analysis of each. This is a more complicated endeavor
	that must be approached more carefully and efficient methods should
	be developed to draw generalized and valid conclusions from high
	dimensional data with a limited number of subjects. Numerous research
	efforts have been reported in the field based on various statistical
	approaches, e.g. independent component analysis (ICA), canonical
	correlation analysis (CCA) and partial least squares (PLS). In this
	review paper, we survey a number of multivariate methods appearing
	in previous multimodal fusion reports, mostly fMRI with other modality,
	which were performed with or without prior information. A table for
	comparing optimization assumptions, purpose of the analysis, the
	need of priors, dimension reduction strategies and input data types
	is provided, which may serve as a valuable reference that helps readers
	understand the trade-offs of the 7 methods comprehensively. Finally,
	we evaluate 3 representative methods via simulation and give some
	suggestions on how to select an appropriate method based on a given
	research.},
  timestamp = {2016-07-08T11:27:35Z},
  number = {1},
  journal = {Journal of Neuroscience Methods},
  author = {Sui, Jing and Adali, T�lay and Yu, Qingbao and Chen, Jiayu and Calhoun, Vince D.},
  year = {2012},
  keywords = {CCA,ICA,MRI,multimodal fusion,Multivariate methods,PLS},
  pages = {68 - 81},
  owner = {Fardin}
}

@article{Sui2010,
  title = {A CCA $+$ ICA based model for multi-task brain imaging data fusion 	and its application to schizophrenia},
  volume = {51},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.01.069},
  abstract = {Collection of multiple-task brain imaging data from the same subject
	has now become common practice in medical imaging studies. In this
	paper, we propose a simple yet effective model, “CCA + ICA�?,
	as a powerful tool for multi-task data fusion. This joint blind source
	separation (BSS) model takes advantage of two multivariate methods:
	canonical correlation analysis and independent component analysis,
	to achieve both high estimation accuracy and to provide the correct
	connection between two datasets in which sources can have either
	common or distinct between-dataset correlation. In both simulated
	and real fMRI applications, we compare the proposed scheme with other
	joint \{BSS\} models and examine the different modeling assumptions.
	The contrast images of two tasks: sensorimotor (SM) and Sternberg
	working memory (SB), derived from a general linear model (GLM), were
	chosen to contribute real multi-task fMRI data, both of which were
	collected from 50 schizophrenia patients and 50 healthy controls.
	When examining the relationship with duration of illness, \{CCA\}
	+ \{ICA\} revealed a significant negative correlation with temporal
	lobe activation. Furthermore, \{CCA\} + \{ICA\} located sensorimotor
	cortex as the group-discriminative regions for both tasks and identified
	the superior temporal gyrus in \{SM\} and prefrontal cortex in \{SB\}
	as task-specific group-discriminative brain networks. In summary,
	we compared the new approach to some competitive methods with different
	assumptions, and found consistent results regarding each of their
	hypotheses on connecting the two tasks. Such an approach fills a
	gap in existing multivariate methods for identifying biomarkers from
	brain imaging data.},
  timestamp = {2016-07-08T10:07:25Z},
  number = {1},
  journal = {NeuroImage},
  author = {Sui, Jing and Adali, T{\"u}lay and Pearlson, Godfrey and Yang, Honghui and Sponheim, Scott R. and White, Tonya and Calhoun, Vince D.},
  year = {2010},
  keywords = {Brain imaging data fusion,Canonical correlation analysis (CCA),Canonical correlation analysis
	(CCA),fMRI,Independent component analysis (ICA),Independent
	component analysis (ICA),Joint blind source,Multi-task,Schizophrenia,separation},
  pages = {123 - 134},
  owner = {afdidehf}
}

@article{Sui2013,
  title = {Three-way (N-way) fusion of brain imaging data based on mCCA + jICA 	and its application to discriminating schizophrenia},
  volume = {66},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2012.10.051},
  abstract = {Multimodal fusion is an effective approach to better understand brain
	diseases. However, most such instances have been limited to pair-wise
	fusion; because there are often more than two imaging modalities
	available per subject, there is a need for approaches that can combine
	multiple datasets optimally. In this paper, we extended our previous
	two-way fusion model called “multimodal \{CCA\} + joint ICA�?,
	to three or N-way fusion, that enables robust identification of correspondence
	among N data types and allows one to investigate the important question
	of whether certain disease risk factors are shared or distinct across
	multiple modalities. We compared “mCCA + jICA�? with its alternatives
	in a 3-way fusion simulation and verified its advantages in both
	decomposition accuracy and modal linkage detection. We also applied
	it to real functional Magnetic Resonance Imaging (fMRI)–Diffusion
	Tensor Imaging (DTI) and structural \{MRI\} fusion to elucidate the
	abnormal architecture underlying schizophrenia (n = 97) relative
	to healthy controls (n = 116). Both modality-common and modality-unique
	abnormal regions were identified in schizophrenia. Specifically,
	the visual cortex in fMRI, the anterior thalamic radiation (ATR)
	and forceps minor in DTI, and the parietal lobule, cuneus and thalamus
	in sMRI were linked and discriminated between patients and controls.
	One fMRI component with regions of activity in motor cortex and superior
	temporal gyrus individually discriminated schizophrenia from controls.
	Finally, three components showed significant correlation with duration
	of illness (DOI), suggesting that lower gray matter volumes in parietal,
	frontal, and temporal lobes and cerebellum are associated with increased
	DOI, along with white matter disruption in \{ATR\} and cortico-spinal
	tracts. Findings suggest that the identified fractional anisotropy
	changes may relate to the corresponding functional/structural changes
	in the brain that are thought to play a role in the clinical expression
	of schizophrenia. The proposed “mCCA + jICA�? method showed promise
	for elucidating the joint or coupled neuronal abnormalities underlying
	mental illnesses and improves our understanding of the disease process.},
  timestamp = {2016-07-11T17:06:24Z},
  journal = {NeuroImage},
  author = {Sui, Jing and He, Hao and Pearlson, Godfrey D. and Adali, T�lay and Kiehl, Kent A. and Yu, Qingbao and Clark, Vince P. and Castro, Eduardo and White, Tonya and Mueller, Bryon A. and Ho, Beng C. and Andreasen, Nancy C. and Calhoun, Vince D.},
  year = {2013},
  keywords = {DTI,fMRI,mCCA+jICA,multimodal fusion,N-way fusion,N-way
	fusion,Schizophrenia,sMRI},
  pages = {119 - 132},
  owner = {afdidehf}
}

@article{Sui2014,
  title = {Function-structure associations of the brain: Evidence from multimodal 	connectivity and covariance studies},
  volume = {102, Part 1},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2013.09.044},
  abstract = {Abstract Despite significant advances in multimodal imaging techniques
	and analysis approaches, unimodal studies are still the predominant
	way to investigate brain changes or group differences, including
	structural magnetic resonance imaging (sMRI), functional \{MRI\}
	(fMRI), diffusion tensor imaging (DTI) and electroencephalography
	(EEG). Multimodal brain studies can be used to understand the complex
	interplay of anatomical, functional and physiological brain alterations
	or development, and to better comprehend the biological significance
	of multiple imaging measures. To examine the function–structure
	associations of the brain in a more comprehensive and integrated
	manner, we reviewed a number of multimodal studies that combined
	two or more functional (fMRI and/or EEG) and structural (sMRI and/or
	DTI) modalities. In this review paper, we specifically focused on
	multimodal neuroimaging studies on cognition, aging, disease and
	behavior. We also compared multiple analysis approaches, including
	univariate and multivariate methods. The possible strengths and limitations
	of each method are highlighted, which can guide readers when selecting
	a method based on a given research question. In particular, we believe
	that multimodal fusion approaches will shed further light on the
	neuronal mechanisms underlying the major structural and functional
	pathophysiological features of both the healthy brain (e.g. development)
	or the diseased brain (e.g. mental illness) and, in the latter case,
	may provide a more sensitive measure than unimodal imaging for disease
	classification, e.g. multimodal biomarkers, which potentially can
	be used to support clinical diagnosis based on neuroimaging techniques.},
  timestamp = {2016-07-08T12:35:04Z},
  number = {0},
  journal = {NeuroImage},
  author = {Sui, Jing and Huster, Rene and Yu, Qingbao and Segall, Judith M. and Calhoun, Vince D.},
  year = {2014},
  note = {Multimodal Data Fusion},
  keywords = {Brain connectivity,Diffusion MRI,EEG,fMRI,multimodal fusion,sMRI},
  pages = {11 - 23},
  owner = {Fardin}
}

@inproceedings{Takhar2006,
  title = {A new compressive imaging camera architecture using optical-domain 	compression},
  abstract = {Compressive Sensing is an emerging field based on the revelation that
	a small number of linear projections of a compressible signal contain
	enough information for reconstruction and processing. It has many
	promising implications and enables the design of new kinds of Compressive
	Imaging systems and cameras. In this paper, we develop a new camera
	architecture that employs a digital micromirror array to perform
	optical calculations of linear projections of an image onto pseudorandom
	binary patterns. Its hallmarks include the ability to obtain an image
	with a single detection element while sampling the image fewer times
	than the number of pixels. Other attractive properties include its
	universality, robustness, scalability, progressivity, and computational
	asymmetry. The most intriguing feature of the system is that, since
	it relies on a single photon detector, it can be adapted to image
	at wavelengths that are currently impossible with conventional CCD
	and CMOS imagers.},
  timestamp = {2016-09-29T16:15:46Z},
  booktitle = {in Proc. of Computational Imaging IV at SPIE Electronic Imaging},
  author = {Takhar, Dharmpal and Laska, Jason N. and Wakin, Michael B. and Duarte, Marco F. and Baron, Dror and Sarvotham, Shriram and Kelly, Kevin
	F. and Baraniuk, Richard G.},
  year = {2006},
  keywords = {camera,compressed sensing,imaging,incoherent projections,linear programming,Random matrices,random
	matrices,Sparsity},
  pages = {43--52},
  annote = {PDF \& PPT},
  annote = {PDF & PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  annote = {PDF \& PPT},
  owner = {afdidehf}
}

@article{Tan2015,
  title = {The cluster graphical lasso for improved estimation of Gaussian graphical 	models},
  volume = {85},
  issn = {0167-9473},
  doi = {http://dx.doi.org/10.1016/j.csda.2014.11.015},
  abstract = {Abstract The task of estimating a Gaussian graphical model in the
	high-dimensional setting is considered. The graphical lasso, which
	involves maximizing the Gaussian log likelihood subject to a lasso
	penalty, is a well-studied approach for this task. A surprising connection
	between the graphical lasso and hierarchical clustering is introduced:
	the graphical lasso in effect performs a two-step procedure, in which
	(1) single linkage hierarchical clustering is performed on the variables
	in order to identify connected components, and then (2) a penalized
	log likelihood is maximized on the subset of variables within each
	connected component. Thus, the graphical lasso determines the connected
	components of the estimated network via single linkage clustering.
	The single linkage clustering is known to perform poorly in certain
	finite-sample settings. Therefore, the cluster graphical lasso, which
	involves clustering the features using an alternative to single linkage
	clustering, and then performing the graphical lasso on the subset
	of variables within each cluster, is proposed. Model selection consistency
	for this technique is established, and its improved performance relative
	to the graphical lasso is demonstrated in a simulation study, as
	well as in applications to a university webpage and a gene expression
	data sets.},
  timestamp = {2016-07-11T17:00:10Z},
  journal = {Computational Statistics \& Data Analysis},
  author = {Tan, Kean Ming and Witten, Daniela and Shojaie, Ali},
  year = {2015},
  keywords = {clustering,hierarchical,High-dimensional,linkage,Network,setting,Single,Sparsity},
  pages = {23 - 36},
  owner = {afdidehf}
}

@article{Theodorakopoulos2012,
  title = {Sparse Representations: Applications on computer vision and pattern 	recognition},
  timestamp = {2016-07-11T16:52:48Z},
  author = {Theodorakopoulos, Ilias},
  month = nov,
  year = {2012},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  owner = {Fardin}
}

@article{Tian2011,
  title = {A Spatio-Temporal Solution for the EEG/MEG Inverse Problem Using 	Group Penalization Methods},
  abstract = {The inverse problem encountered in electroencephalography (EEG) and
	magnetoencephalography (MEG) studies refers to estimating neural
	activity given limited scalp-recorded data. We propose a spatio-temporal
	solution using group penalization approaches. This proposed method
	is based on the assumption that the underlying sources of EEG/MEG
	measurements are smooth in the temporal domain, and focal in the
	spatial domain. It transforms the spatio-temporal problem to a high-dimensional
	linear regression problem with grouped predictors using a basis expansion.
	Then an iterative group elastic net algorithm is utilized to localize
	and estimate the source time courses. The proposed approach is shown
	to be effective on simulations and human MEG studies. A Spatio-Temporal
	solution for the EEG/MEG inverse problem using group penalization
	methods. Available from: http://www.researchgate.net/publication/258090313_A_Spatio-Temporal_solution_for_the_EEGMEG_inverse_problem_using_group_penalization_methods
	[accessed Jul 15, 2015].},
  timestamp = {2016-07-08T11:30:32Z},
  journal = {Statistics and its interface},
  author = {Tian, Tian siva and Li, zhimin},
  year = {2011},
  keywords = {EEG/MEG.,Group elastic net,inverse problem,Spatio-temporal data},
  pages = {521-533},
  owner = {afdidehf}
}

@phdthesis{Tsaig2007,
  title = {Sparse Solution Of Underdetermined Linear Systems: Algorithms And 	Applications},
  timestamp = {2016-07-11T16:54:20Z},
  school = {STANFORD UNIVERSITY},
  author = {Tsaig, Yaakov},
  month = jun,
  year = {2007},
  owner = {afdidehf}
}

@article{Tygert2009,
  title = {A fast algorithm for computing minimal-norm solutions to underdetermined 	systems of linear equations},
  abstract = {We introduce a randomized algorithm for computing the minimal-norm
	solution to an underdetermined system of linear equations. Given
	an arbitrary full-rank matrix Am�n with m < n, any vector bm�1,
	and any positive real number " less than 1, the procedure computes
	a vector xn�1 approximating to relative precision " or better the
	vector pn�1 of minimal Euclidean norm satisfying Am�n pn�1
	= bm�1. The algorithm typically requires O(mn log( p n/")+m3) floating-point
	operations, generally less than the O(m2 n) required by the classical
	schemes based on QR-decompositions or bidiagonalization. We present
	several numerical examples illustrating the performance of the algorithm.},
  timestamp = {2016-07-08T10:15:03Z},
  journal = {Computing Research Repository},
  author = {Tygert, Mark},
  month = jul,
  year = {2009},
  owner = {Fardin}
}

@article{Vergara2014,
  title = {A three-way parallel ICA approach to analyze links among genetics, 	brain structure and brain function},
  volume = {98},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2014.04.060},
  abstract = {Abstract Multi-modal data analysis techniques, such as the Parallel
	Independent Component Analysis (pICA), are essential in neuroscience,
	medical imaging and genetic studies. The pICA algorithm allows the
	simultaneous decomposition of up to two data modalities achieving
	better performance than separate \{ICA\} decompositions and enabling
	the discovery of links between modalities. However, advances in data
	acquisition techniques facilitate the collection of more than two
	data modalities from each subject. Examples of commonly measured
	modalities include genetic information, structural magnetic resonance
	imaging (MRI) and functional MRI. In order to take full advantage
	of the available data, this work extends the pICA approach to incorporate
	three modalities in one comprehensive analysis. Simulations demonstrate
	the three-way pICA performance in identifying pairwise links between
	modalities and estimating independent components which more closely
	resemble the true sources than components found by pICA or separate
	\{ICA\} analyses. In addition, the three-way pICA algorithm is applied
	to real experimental data obtained from a study that investigate
	genetic effects on alcohol dependence. Considered data modalities
	include functional \{MRI\} (contrast images during alcohol exposure
	paradigm), gray matter concentration images from structural \{MRI\}
	and genetic single nucleotide polymorphism (SNP). The three-way pICA
	approach identified links between a \{SNP\} component (pointing to
	brain function and mental disorder associated genes, including BDNF,
	\{GRIN2B\} and NRG1), a functional component related to increased
	activation in the precuneus area, and a gray matter component comprising
	part of the default mode network and the caudate. Although such findings
	need further verification, the simulation and in-vivo results validate
	the three-way pICA algorithm presented here as a useful tool in biomedical
	data fusion applications.},
  timestamp = {2016-07-08T11:33:36Z},
  journal = {NeuroImage},
  author = {Vergara, Victor M. and Ulloa, Alvaro and Calhoun, Vince D. and Boutte, David and Chen, Jiayu and Liu, Jingyu},
  year = {2014},
  keywords = {data fusion,functional MRI,Gray matter,ICA,SNP,Structural MRI},
  pages = {386 - 394},
  owner = {afdidehf}
}

@article{Vulliemoz2010,
  title = {Continuous EEG source imaging enhances analysis of EEG-fMRI in focal 	epilepsy},
  volume = {49},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2009.11.055},
  abstract = {Introduction EEG-correlated fMRI (EEG-fMRI) studies can reveal haemodynamic
	changes associated with Interictal Epileptic Discharges (IED). Methodological
	improvements are needed to increase sensitivity and specificity for
	localising the epileptogenic zone. We investigated whether the estimated
	\{EEG\} source activity improved models of the \{BOLD\} changes in
	EEG-fMRI data, compared to conventional « event-related » designs
	based solely on the visual identification of IED. Methods Ten patients
	with pharmaco-resistant focal epilepsy underwent EEG-fMRI. \{EEG\}
	Source Imaging (ESI) was performed on intra-fMRI averaged \{IED\}
	to identify the irritative zone. The continuous activity of this
	estimated \{IED\} source (cESI) over the entire recording was used
	for fMRI analysis (cESI model). The maps of \{BOLD\} signal changes
	explained by cESI were compared to results of the conventional IED-related
	model. Results \{ESI\} was concordant with non-invasive data in 13/15
	different types of IED. The cESI model explained significant additional
	\{BOLD\} variance in regions concordant with video-EEG, structural
	\{MRI\} or, when available, intracranial \{EEG\} in 10/15 IED. The
	cESI model allowed better detection of the \{BOLD\} cluster, concordant
	with intracranial \{EEG\} in 4/7 IED, compared to the \{IED\} model.
	In 4 \{IED\} types, cESI-related \{BOLD\} signal changes were diffuse
	with a pattern suggestive of contamination of the source signal by
	artefacts, notably incompletely corrected motion and pulse artefact.
	In one \{IED\} type, there was no significant \{BOLD\} change with
	either model. Conclusion Continuous \{EEG\} source imaging can improve
	the modelling of \{BOLD\} changes related to interictal epileptic
	activity and this may enhance the localisation of the irritative
	zone.},
  timestamp = {2016-07-08T12:03:30Z},
  number = {4},
  journal = {NeuroImage},
  author = {Vulliemoz, S. and Rodionov, R. and Carmichael, D. W. and Thornton, R. and Guye, M. and Lhatoo, S. D. and Michel, C. M. and Duncan, J.
	S. and Lemieux, L.},
  year = {2010},
  keywords = {EEG},
  pages = {3219 - 3229},
  owner = {afdidehf}
}

@article{Wang2015,
  title = {A homotopy-based sparse representation for fast and accurate shape 	prior modeling in liver surgical planning},
  volume = {19},
  issn = {1361-8415},
  doi = {http://dx.doi.org/10.1016/j.media.2014.10.003},
  abstract = {Abstract Shape prior plays an important role in accurate and robust
	liver segmentation. However, liver shapes have complex variations
	and accurate modeling of liver shapes is challenging. Using large-scale
	training data can improve the accuracy but it limits the computational
	efficiency. In order to obtain accurate liver shape priors without
	sacrificing the efficiency when dealing with large-scale training
	data, we investigate effective and scalable shape prior modeling
	method that is more applicable in clinical liver surgical planning
	system. We employed the Sparse Shape Composition (SSC) to represent
	liver shapes by an optimized sparse combination of shapes in the
	repository, without any assumptions on parametric distributions of
	liver shapes. To leverage large-scale training data and improve the
	computational efficiency of SSC, we also introduced a homotopy-based
	method to quickly solve the L 1 -norm optimization problem in SSC.
	This method takes advantage of the sparsity of shape modeling, and
	solves the original optimization problem in \{SSC\} by continuously
	transforming it into a series of simplified problems whose solution
	is fast to compute. When new training shapes arrive gradually, the
	homotopy strategy updates the optimal solution on the fly and avoids
	re-computing it from scratch. Experiments showed that \{SSC\} had
	a high accuracy and efficiency in dealing with complex liver shape
	variations, excluding gross errors and preserving local details on
	the input liver shape. The homotopy-based \{SSC\} had a high computational
	efficiency, and its runtime increased very slowly when repository’s
	capacity and vertex number rose to a large degree. When repository’s
	capacity was 10,000, with 2000 vertices on each shape, homotopy method
	cost merely about 11.29s to solve the optimization problem in SSC,
	nearly 2000 times faster than interior point method. The dice similarity
	coefficient (DSC), average symmetric surface distance (ASD), and
	maximum symmetric surface distance measurement was 94.31 ± 3.04%,
	1.12 ± 0.69 mm and 3.65 ± 1.40 mm respectively.},
  timestamp = {2016-07-08T10:18:18Z},
  number = {1},
  journal = {Medical Image Analysis},
  author = {Wang, Guotai and Zhang, Shaoting and Xie, Hongzhi and Metaxas, Dimitris
	N. and Gu, Lixu},
  year = {2015},
  keywords = {composition,Fast,Optimization,prior,Scalability,segmentation,Shape,Sparse},
  pages = {176 - 186},
  owner = {afdidehf}
}

@article{Wang2014c,
  title = {Restricted p-isometry properties of nonconvex block-sparse compressed 	sensing},
  volume = {104},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2014.03.040},
  abstract = {Abstract In this paper, by generalizing the notion of restricted p-isometry
	constant ( 0 &lt; p ≤ 1 ) defined by Chartrand and Staneva [1]
	to the setting of block-sparse signal recovery, we establish a general
	restricted p-isometry property (p-RIP) condition for recovery of
	(nearly) block-sparse signals via mixed l2/lp-minimization. Moreover,
	we derive a lower bound on the necessary number of Gaussian measurements
	for the p-RIP condition to hold with high probability, which shows
	clearly that fewer measurements with smaller p are needed for exact
	recovery of block-sparse signals via mixed l2/lp-minimization than
	when p=1.},
  timestamp = {2016-07-10T08:02:59Z},
  journal = {Signal Processing},
  author = {Wang, Yao and Wang, Jianjun and Xu, Zongben},
  year = {2014},
  keywords = {Block-sparse,Compressed,Gaussian,l2/lp-minimization,measurements,mixed,p-isometry,properties,recovery,Restricted,sensing,Signal},
  pages = {188 - 196},
  owner = {afdidehf}
}

@article{Wang2015c,
  title = {Randomized structural sparsity via constrained block subsampling 	for improved sensitivity of discriminative voxel identification},
  volume = {117},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2015.05.057},
  abstract = {Abstract In this paper, we consider voxel selection for functional
	Magnetic Resonance Imaging (fMRI) brain data with the aim of finding
	a more complete set of probably correlated discriminative voxels,
	thus improving interpretation of the discovered potential biomarkers.
	The main difficulty in doing this is an extremely high dimensional
	voxel space and few training samples, resulting in unreliable feature
	selection. In order to deal with the difficulty, stability selection
	has received a great deal of attention lately, especially due to
	its finite sample control of false discoveries and transparent principle
	for choosing a proper amount of regularization. However, it fails
	to make explicit use of the correlation property or structural information
	of these discriminative features and leads to large false negative
	rates. In other words, many relevant but probably correlated discriminative
	voxels are missed. Thus, we propose a new variant on stability selection
	“randomized structural sparsity�?, which incorporates the idea
	of structural sparsity. Numerical experiments demonstrate that our
	method can be superior in controlling for false negatives while also
	keeping the control of false positives inherited from stability selection.},
  timestamp = {2016-07-10T07:40:22Z},
  journal = {NeuroImage},
  author = {Wang, Yilun and Zheng, Junjie and Zhang, Sheng and Duan, Xunjuan and Chen, Huafu},
  year = {2015},
  keywords = {Block,Constrained,Feature,fMRI,Pattern,Randomized,recognition,selection,Sparsity,Stability,Structural,subsampling,Voxel},
  pages = {170 - 183},
  owner = {afdidehf}
}

@article{Wang2015d,
  title = {The recovery of sparse initial state based on compressed sensing 	for discrete-time linear system},
  issn = {0925-2312},
  doi = {http://dx.doi.org/10.1016/j.neucom.2015.06.042},
  abstract = {Abstract This paper considers the recovery of sparse initial state
	for deterministic discrete-time linear time-invariant systems based
	on the compressed sensing theory. A class of deterministic linear
	systems with the global observation matrices satisfying the restricted
	isometry property (RIP) is characterized. Sufficient conditions on
	the measurement time instants that guarantee the global observation
	matrix to be a \{RIP\} matrix are obtained. With respect to the recovery
	of the sparse initial state of a high-dimensional linear system,
	it is worth mentioning that the number of measurements can be significantly
	decreased in terms of compressed sensing theory.},
  timestamp = {2016-07-11T17:03:06Z},
  journal = {Neurocomputing},
  author = {Wang, Zhongmei and Zhang, Huanshui},
  year = {2015},
  keywords = {Compressed,constant,Discrete-time,isometry,linear,property,Restricted,sensing,system},
  pages = {-},
  owner = {afdidehf}
}

@inproceedings{Weinlich2011,
  title = {Sparse representation of dense motion vector fields for lossless 	compression of 4-D medical CT data},
  abstract = {We present a new method for data-adaptive compression of dense vector
	fields in dynamic medical volume data. Conventional block-based motion
	compensation used for temporal prediction in video compression cannot
	conveniently cope with deformable motion typically found in medical
	image sequences encoded over time. Based on an approximation of physiologic
	tissue motion between two succeeding slices in time direction computed
	by optical flow methods, we find the most significant motion vectors
	with respect to their prediction capability for a second 2-D slice
	out of the first one. By coding the components of these vectors,
	we are able to reconstruct a high quality dense motion vector field
	at the decoder using only minimal side-information. We show that
	our approach can achieve a smoother prediction than block-based motion
	compensation for such data, reducing storage demands in spatially
	predictive lossless compression. We also show that such a predictive
	approach can yield better compression ratios than JPEG 2000 intra
	coding.},
  timestamp = {2016-07-11T16:52:14Z},
  booktitle = {Signal Processing Conference, 2011 19th European},
  author = {Weinlich, A. and Amon, P. and Hutter, A. and Kaup, A.},
  month = aug,
  year = {2011},
  keywords = {4-D medical CT data,4-D
	medical CT data,biomedical imaging,computerised tomography,data-adaptive compression,data-adaptive
	compression,data compression,decoder,dense motion vector fields,dense motion vector
	fields,dense vector fields,dense
	vector fields,dynamic medical volume data,encoding,high quality
	dense motion vector field reconstruction,high quality dense
	motion vector field reconstruction,image coding,Image reconstruction,image representation,image
	representation,image sequences,image
	sequences,medical image processing,medical image sequences,medical image
	sequences,motion compensation,Noise,optical flow methods,optical
	flow methods,physiologic tissue motion approximation,physiologic
	tissue motion approximation,prediction capability,sparse representation,sparse
	representation,spatially predictive lossless compression,spatially
	predictive lossless compression,storage demands,storage
	demands,Transform coding,transform
	coding,Vectors},
  pages = {1352-1356},
  owner = {afdidehf}
}

@article{Wipf2009,
  title = {A unified Bayesian framework for MEG/EEG source imaging},
  volume = {44},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2008.02.059},
  abstract = {The ill-posed nature of the \{MEG\} (or related EEG) source localization
	problem requires the incorporation of prior assumptions when choosing
	an appropriate solution out of an infinite set of candidates. Bayesian
	approaches are useful in this capacity because they allow these assumptions
	to be explicitly quantified using postulated prior distributions.
	However, the means by which these priors are chosen, as well as the
	estimation and inference procedures that are subsequently adopted
	to affect localization, have led to a daunting array of algorithms
	with seemingly very different properties and assumptions. From the
	vantage point of a simple Gaussian scale mixture model with flexible
	covariance components, this paper analyzes and extends several broad
	categories of Bayesian inference directly applicable to source localization
	including empirical Bayesian approaches, standard \{MAP\} estimation,
	and multiple variational Bayesian (VB) approximations. Theoretical
	properties related to convergence, global and local minima, and localization
	bias are analyzed and fast algorithms are derived that improve upon
	existing methods. This perspective leads to explicit connections
	between many established algorithms and suggests natural extensions
	for handling unknown dipole orientations, extended source configurations,
	correlated sources, temporal smoothness, and computational expediency.
	Specific imaging methods elucidated under this paradigm include the
	weighted minimum l2-norm, FOCUSS, minimum current estimation, VESTAL,
	sLORETA, restricted maximum likelihood, covariance component estimation,
	beamforming, variational Bayes, the Laplace approximation, and automatic
	relevance determination, as well as many others. Perhaps surprisingly,
	all of these methods can be formulated as particular cases of covariance
	component estimation using different concave regularization terms
	and optimization rules, making general theoretical analyses and algorithmic
	extensions/improvements particularly relevant.},
  timestamp = {2016-07-08T11:35:11Z},
  number = {3},
  journal = {NeuroImage},
  author = {Wipf, David and Nagarajan, Srikantan},
  year = {2009},
  pages = {947-966},
  owner = {Fardin}
}

@TechReport{Woltersxxxx,
  author    = {Wolters, Carsten H. and Lew, Seok and Macleod, Rob S. and H\"{a}m\"{a}l\"{a}inen, Matti},
  title     = {Combined EEG/MEG source analysis using calibrated finite element head models},
  year      = {xxxx},
  abstract  = {We propose a new method for a combined MEG/EEG source analysis. We
	optimize the tissue conductivities of a realistically shaped four-compartment
	finite-element head volume conductor based on measured somatosensory
	evoked potentials (SEP) and fields (SEF). Our proposed method uses
	the source parameters from the MEG di-pole fit as a constraint for
	the conductivity estimation based on the EEG. The method was implemented
	with an iteration scheme to take into account the insensitivity of
	MEG to radial source orientations, resulting in more accurate conductivity
	estimation using the EEG data. Our simulation studies showed that
	the method was able to simultaneously estimate both for the brain
	and skull conductivities as well as the parameters of the underlying
	source in somatosensory cortex. The application to measured SEP and
	SEF data indicated a skull-brain conduc-tivity ratio of 1:25, which,
	in agreement with recent studies, is significantly lower than the
	commonly used ratio of 1:80. The individually optimized volume conductor
	model can be subsequently used for the analysis of clinical or cognitive
	data acquired from the same subject.},
  keywords  = {Combined EEG/MEG,finite element method,in vivo conductivity estima-tion,realistic four-compartment head modeling,realistic four-compartment head modeling,somatosensory evoked potentials and fields,somatosensory evoked potentials and fields,source analysis},
  owner     = {afdidehf},
  timestamp = {2016-10-21T14:05:57Z},
}

@article{Xiang2015,
  title = {Efficient nonconvex sparse group feature selection via continuous 	and discrete optimization},
  volume = {224},
  issn = {0004-3702},
  doi = {http://dx.doi.org/10.1016/j.artint.2015.02.008},
  abstract = {Abstract Sparse feature selection has proven to be effective in analyzing
	high-dimensional data. While promising, most existing works apply
	convex methods, which may be suboptimal in terms of the accuracy
	of feature selection and parameter estimation. In this paper, we
	consider both continuous and discrete nonconvex paradigms to sparse
	group feature selection, which are motivated by applications that
	require identifying the underlying group structure and performing
	feature selection simultaneously. The main contribution of this article
	is twofold: (1) computationally, we develop efficient optimization
	algorithms for both continuous and discrete formulations, of which
	the key step is a projection with two coupled constraints; (2) statistically,
	we show that the proposed continuous model reconstructs the oracle
	estimator. Therefore, consistent feature selection and parameter
	estimation are achieved simultaneously. Numerical results on synthetic
	and real-world data suggest that the proposed nonconvex methods compare
	favorably against their competitors, thus achieving desired goal
	of delivering high performance.},
  timestamp = {2016-07-08T12:19:34Z},
  journal = {Artificial Intelligence},
  author = {Xiang, Shuo and Shen, Xiaotong and Ye, Jieping},
  year = {2015},
  keywords = {analysis,Application,bound,Data,Discrete,EEG,Error,Nonconvex,Optimization},
  pages = {28 - 50},
  owner = {afdidehf}
}

@article{Yang2010,
  title = {Towards Compressive Geospatial Sensing Via Fusion of LIDAR and Hyperspectral 	Imaging},
  timestamp = {2016-07-11T17:08:36Z},
  author = {Yang, Allen Y.},
  year = {2010},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  annote = {PPT},
  howpublished = {GRID Workshop},
  owner = {Fardin}
}

@article{Zdunek2016,
  title = {Numerical Methods-Underdetermined problems (FOCUSS, M-FOCUSS, 	Applications)},
  timestamp = {2016-09-30T11:03:49Z},
  urldate = {2016-04-05},
  author = {Zdunek, Rafa\l{}},
  year = {2016},
  keywords = {problems,underdetermined},
  annote = {PPT}
}

@article{Zeng2014,
  title = {Sparse solution of underdetermined linear equations via adaptively 	iterative thresholding},
  volume = {97},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2013.10.031},
  abstract = {Abstract Finding the sparset solution of an underdetermined system
	of linear equations y=Ax has attracted considerable attention in
	recent years. Among a large number of algorithms, iterative thresholding
	algorithms are recognized as one of the most efficient and important
	classes of algorithms. This is mainly due to their low computational
	complexities, especially for large scale applications. The aim of
	this paper is to provide guarantees on the global convergence of
	a wide class of iterative thresholding algorithms. Since the thresholds
	of the considered algorithms are set adaptively at each iteration,
	we call them adaptively iterative thresholding (AIT) algorithms.
	As the main result, we show that as long as A satisfies a certain
	coherence property, \{AIT\} algorithms can find the correct support
	set within finite iterations, and then converge to the original sparse
	solution exponentially fast once the correct support set has been
	identified. Meanwhile, we also demonstrate that \{AIT\} algorithms
	are robust to the algorithmic parameters. In addition, it should
	be pointed out that most of the existing iterative thresholding algorithms
	such as hard, soft, half and smoothly clipped absolute deviation
	(SCAD) algorithms are included in the class of \{AIT\} algorithms
	studied in this paper.},
  timestamp = {2016-07-11T16:54:10Z},
  journal = {Signal Processing},
  author = {Zeng, Jinshan and Lin, Shaobo and Xu, Zongben},
  year = {2014},
  keywords = {global convergence,iterative thresholding algorithm,sparse solution.,underdetermined
	linear equations},
  pages = {152 - 161},
  owner = {Fardin}
}

@article{Zeng2014a,
  title = {Solving OSCAR regularization problems by fast approximate proximal 	splitting algorithms},
  volume = {31},
  issn = {1051-2004},
  doi = {http://dx.doi.org/10.1016/j.dsp.2014.03.010},
  abstract = {Abstract The \{OSCAR\} (octagonal selection and clustering algorithm
	for regression) regularizer consists of an l 1 norm plus a pairwise
	l ∞ norm (responsible for its grouping behavior) and was proposed
	to encourage group sparsity in scenarios where the groups are a priori
	unknown. The \{OSCAR\} regularizer has a non-trivial proximity operator,
	which limits its applicability. We reformulate this regularizer as
	a weighted sorted l 1 norm, and propose its grouping proximity operator
	(GPO) and approximate proximity operator (APO), thus making state-of-the-art
	proximal splitting algorithms (PSAs) available to solve inverse problems
	with \{OSCAR\} regularization. The \{GPO\} is in fact the \{APO\}
	followed by additional grouping and averaging operations, which are
	costly in time and storage, explaining the reason why algorithms
	with \{APO\} are much faster than those with GPO. The convergences
	of \{PSAs\} with \{GPO\} are guaranteed since \{GPO\} is an exact
	proximity operator. Although convergence of \{PSAs\} with \{APO\}
	is may not be guaranteed, we have experimentally found that \{APO\}
	behaves similarly to \{GPO\} when the regularization parameter of
	the pairwise l ∞ norm is set to an appropriately small value. Experiments
	on synthetic data and real-world data show the robustness and efficiency
	of APO, respectively, and experiments on recovery of group-sparse
	signals (with unknown groups) show that \{PSAs\} with \{APO\} are
	very fast and accurate.},
  timestamp = {2016-07-10T08:20:19Z},
  journal = {Digital Signal Processing},
  author = {Zeng, Xiangrong and Figueiredo, M�rio A. T.},
  year = {2014},
  keywords = {Algorithms,Alternating,direction,group,method,multipliers,of,operator,Proximal,Proximity,recovery,Signal,Sparsity,splitting},
  pages = {124 - 135},
  owner = {afdidehf}
}

@techreport{Zhang2006,
  title = {Model-selection consistency of the lasso in high dimensional linear 	regression.},
  timestamp = {2016-07-09T20:14:36Z},
  author = {Zhang, C.-h. and Huang, J.},
  year = {2006},
  owner = {afdidehf}
}

@article{Zhang2016,
  title = {Multiple-measurement vector based implementation for single-measurement 	vector sparse Bayesian learning with reduced complexity},
  volume = {118},
  issn = {0165-1684},
  doi = {http://dx.doi.org/10.1016/j.sigpro.2015.06.020},
  abstract = {Abstract Sparse Bayesian learning (SBL) has high computational complexity
	associated with matrix inversion in each iteration. In this paper,
	we investigate complexity reduced multiple-measurement vector (MMV)
	based implementation for single-measurement vector \{SBL\} problems.
	For problems with special structured sensing matrices, we propose
	two sub-optimal \{SBL\} schemes with significantly reduced complexity
	and slight estimation performance degradation, by exploiting the
	deterministic correlation in the converted \{MMV\} model explicitly.
	Two application scenarios on channel estimation in multicarrier systems
	and direction of arrival estimation are presented. Simulation results
	validate the effectiveness of the schemes.},
  timestamp = {2016-07-09T20:17:04Z},
  journal = {Signal Processing},
  author = {Zhang, Jian A. and Chen, Zhuo and Cheng, Peng and Huang, Xiaojing},
  year = {2016},
  keywords = {Bayesianlearning,Compressive,direction,ofarrivalestimation,sensing,Sparse},
  pages = {153 - 158},
  owner = {afdidehf}
}

@article{Zhang2008a,
  title = {Three-dimensional brain current source reconstruction from intra-cranial 	ECoG recordings},
  volume = {42},
  issn = {1053-8119},
  doi = {http://dx.doi.org/10.1016/j.neuroimage.2008.04.263},
  abstract = {We have investigated 3-dimensional brain current density reconstruction
	(CDR) from intracranial electrocorticogram (ECoG) recordings by means
	of finite element method (FEM). The brain electrical sources are
	modeled by a current density distribution and estimated from the
	\{ECoG\} signals with the aid of a weighted minimum norm estimation
	algorithm. A series of computer simulations were conducted to evaluate
	the performance of ECoG-CDR by comparing with the scalp \{EEG\} based
	\{CDR\} results. The present computer simulation results indicate
	that the ECoG-CDR provides enhanced performance in localizing single
	dipole sources which are located in regions underneath the implanted
	subdural \{ECoG\} grids, and in distinguishing and imaging multiple
	separate dipole sources, in comparison to the \{CDR\} results as
	obtained from the scalp \{EEG\} under the same conditions. We have
	also demonstrated the applicability of the present ECoG-CDR method
	to estimate 3-dimensional current density distribution from the subdural
	\{ECoG\} recordings in a human epilepsy patient. Eleven interictal
	epileptiform spikes (seven from the frontal region and four from
	parietal region) in an epilepsy patient undergoing surgical evaluation
	were analyzed. The present promising results indicate the feasibility
	and applicability of the developed ECoG-CDR method of estimating
	brain sources from intracranial electrical recordings, with detailed
	forward modeling using FEM.},
  timestamp = {2016-07-11T17:06:05Z},
  number = {2},
  journal = {NeuroImage},
  author = {Zhang, Yingchun and Drongelen, Wim van and Kohrman, Michael and He, Bin},
  year = {2008},
  keywords = {element,Finite,method},
  pages = {683 - 695},
  owner = {afdidehf}
}

@inproceedings{Zhang2012b,
  title = {Recovery of block sparse signals using the framework of block sparse 	Bayesian learning},
  doi = {10.1109/ICASSP.2012.6288632},
  abstract = {In this paper we study the recovery of block sparse signals and extend
	conventional approaches in two important directions; one is learning
	and exploiting intra-block correlation, and the other is generalizing
	signals' block structure such that the block partition is not needed
	to be known for recovery. We propose two algorithms based on the
	framework of block sparse Bayesian learning (bSBL). One algorithm,
	directly derived from the framework, requires a priori knowledge
	of the block partition. Another algorithm, derived from an expanded
	bSBL framework using the generalization method, can be used when
	the block partition is unknown. Experiments show that they have superior
	performance to state-of-the-art algorithms.},
  timestamp = {2016-07-10T07:48:08Z},
  booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International 	Conference on},
  author = {Zhang, Zhilin and Rao, B.D.},
  month = mar,
  year = {2012},
  keywords = {Bayesian methods,Bismuth,block sparse Bayesian learning,Block Sparse
	Model,block sparse signals recovery,Clustering algorithms,cluster
	structure,compressed sensing,Correlation,Covariance matrix,Covariance
	matrix,expanded bSBL framework,expanded
	bSBL framework,generalization method,intra-block correlation,intra-block
	correlation,Partitioning algorithms,Partitioning
	algorithms,signal processing,sparse Bayesian learning,sparse Bayesian
	learning,sparse signal recovery,sparse signal
	recovery},
  pages = {3345-3348}
}

@article{Zhang2015c,
  title = {Bilinear low-rank coding framework and extension for robust image 	recovery and feature representation},
  volume = {86},
  issn = {0950-7051},
  doi = {http://dx.doi.org/10.1016/j.knosys.2015.06.001},
  abstract = {Abstract We mainly study the low-rank image recovery problem by proposing
	a bilinear low-rank coding framework called Tensor Low-Rank Representation.
	For enhanced low-rank recovery and error correction, our method constructs
	a low-rank tensor subspace to reconstruct given images along row
	and column directions simultaneously by computing two low-rank matrices
	alternately from a nuclear norm minimization problem, so both column
	and row information of data can be effectively preserved. Our bilinear
	approach seamlessly integrates the low-rank coding and dictionary
	learning into a unified framework. Thus, our formulation can be treated
	as enhanced Inductive Robust Principal Component Analysis with noise
	removed by low-rank representation, and can also be considered as
	the enhanced low-rank representation with a clean informative dictionary
	via low-rank embedding. To enable our method to include outside images,
	the out-of-sample extension is also presented by regularizing the
	model to correlate image features with the low-rank recovery of the
	images. Comparison with other criteria shows that our model exhibits
	stronger robustness and enhanced performance. We also use the outputted
	bilinear low-rank codes for feature learning. Two unsupervised local
	and global low-rank subspace learning methods are proposed for extracting
	image features for classification. Simulations verified the validity
	of our techniques for image recovery, representation and classification.},
  timestamp = {2016-07-08T11:38:28Z},
  journal = {Knowledge-Based Systems},
  author = {Zhang, Zhao and Yan, Shuicheng and Zhao, Mingbo and Li, Fan-Zhang},
  year = {2015},
  keywords = {Bilinear,coding,extension,Image,learning,Low-rank,Out-of-sample,recovery,representation,Subspace},
  pages = {143 - 157},
  owner = {afdidehf}
}

@article{Zhao2006,
  title = {On Model Selection Consistency of Lasso},
  volume = {7},
  issn = {1532-4435},
  abstract = {Sparsity or parsimony of statistical models is crucial for their proper
	interpretations, as in sciences and social sciences. Model selection
	is a commonly used method to find such models, but usually involves
	a computationally heavy combinatorial search. Lasso (Tibshirani,
	1996) is now being used as a computationally feasible alternative
	to model selection. Therefore it is important to study Lasso for
	model selection purposes. In this paper, we prove that a single condition,
	which we call the Irrepresentable Condition, is almost necessary
	and sufficient for Lasso to select the true model both in the classical
	fixed p setting and in the large p setting as the sample size n gets
	large. Based on these results, sufficient conditions that are verifiable
	in practice are given to relate to previous works and help applications
	of Lasso for feature selection and sparse representation. This Irrepresentable
	Condition, which depends mainly on the covariance of the predictor
	variables, states that Lasso selects the true model consistently
	if and (almost) only if the predictors that are not in the true model
	are �irrepresentable� (in a sense to be clarified) by predictors
	that are in the true model. Furthermore, simulations are carried
	out to provide insights and understanding of this result.},
  timestamp = {2016-07-10T07:13:09Z},
  journal = {Journal of Machine Learning Research},
  author = {Zhao, Peng and Yu, Bin},
  month = dec,
  year = {2006},
  keywords = {consistency,Lasso,Model selection,regularization,Sparsity},
  pages = {2541-2563},
  acmid = {1248637},
  issue_date = {12/1/2006},
  numpages = {23},
  owner = {afdidehf}
}

@inproceedings{Zhen2013,
  title = {A sparse representation method for DOA estimation of coherent signals 	with mutual coupling},
  abstract = {This paper presents a sparse representation method for direction of
	arrival (DOA) estimation of coherent signals in the presence of unknown
	mutual coupling. Making use of the special structure of the mutual
	coupling matrix for uniform linear arrays (ULAs), we formulate the
	problem of DOA estimation of coherent signals with mutual coupling
	as an over-complete basis representation problem, where its solution
	is subject to a block-sparse one. The new method is distinctly different
	from the improved spatial smoothing method, for the reason that spatial
	smoothing technique is no longer required. Since the whole array,
	instead of the middle subarray, is used for DOA estimation of coherent
	signals, the proposed method can achieve a better performance.},
  timestamp = {2016-07-08T11:29:53Z},
  booktitle = {Control Conference (CCC), 2013 32nd Chinese},
  author = {Zhen, Wu and Dean, Zhao and Xin, Xu and Jisheng, Dai},
  month = jul,
  year = {2013},
  keywords = {array signal processing,Coherent signal,Coherent
	signal,coherent signals,Direction-of-arrival (DOA),Direction-of-arrival
	(DOA),direction-of-arrival estimation,direction of arrival estimation,direction of arrival
	estimation,DOA estimation,DOA
	estimation,Estimation,matrix algebra,Mutual coupling,Mutual
	coupling,mutual coupling matrix,mutual
	coupling matrix,mutual coupling
	matrix,over-complete basis representation problem,over-complete basis representation
	problem,signal representation,signal
	representation,signal resolution,Signal
	resolution,Signal to noise ratio,smoothing methods,smoothing
	methods,sparse representation,sparse representation method,sparse
	representation method,Spatial smoothing,Spatial
	smoothing,spatial smoothing method,uniform linear arrays,uniform
	linear arrays,Uniform linear array (ULA),Uniform
	linear array (ULA),unknown mutual coupling,Vectors},
  pages = {3771-3775},
  owner = {afdidehf}
}

@article{Zhu2015,
  title = {Compressive sensing and sparse decomposition in precision machining 	process monitoring: From theory to applications},
  issn = {0957-4158},
  doi = {http://dx.doi.org/10.1016/j.mechatronics.2015.04.017},
  abstract = {Abstract Precision machining has been claimed to be the backbone to
	modern industry. It has been widely applied to the key parts’ production
	in the aerospace, medical and automotive industries. One of the main
	problems related to precision machining productivity and safety is
	the machining condition. By utilizing the acquired information from
	sensory measurements to direct the further actions, the signal processing
	bridges the gap between human instruction and full automation. Traditional
	signal acquisition and processing methods are mainly based on Shannon’s
	Sampling theory, Fourier methods or wavelet analysis. While these
	techniques meet challenges in precision machining environment, such
	as machining at high speed, low signal to noise ratio, and high sampling
	rate. These factors limit their applications especially the online
	monitoring implementation. The newly developed compressive sensing
	theory and sparse decomposition techniques provide a possible solution
	to these problems, while limited studies have been investigated.
	This paper serves as an introduction to the theory and shows the
	theory’s potentials in machining condition monitoring by reviewing
	related literatures and demonstrating case studies from real experiments.},
  timestamp = {2016-07-08T12:00:51Z},
  journal = {Mechatronics},
  author = {Zhu, Kunpeng and Lin, Xin and Li, Kexuan and Jiang, Lili},
  year = {2015},
  keywords = {Condition,decomposition,machining,monitoring,Precision,Sparse},
  pages = {-},
  owner = {afdidehf}
}

@inproceedings{Boufounos2010,
  title = {Average Case Analysis of Sparse Recovery from Combined Fusion Frame Measurements},
  doi = {10.1109/CISS.2010.5464980},
  abstract = {Sparse representations have emerged as a powerful tool in signal and information processing, culminated by the success of new acquisition and processing techniques such as Compressed Sensing (CS). Fusion frames are very rich new signal representation methods that use collections of subspaces instead of vectors to represent signals. These exciting fields have been recently combined to introduce a new sparsity model for fusion frames. Signals that are sparse under the new model can be compressively sampled and uniquely reconstructed in ways similar to sparse signals using standard CS. The combination provides a promising new set of mathematical tools and signal models useful in a variety of applications. With the new model, a sparse signal has energy in very few of the subspaces of the fusion frame, although it does not need to be sparse within each of the subspaces it occupies. In this paper we demonstrate that although a worst-case analysis of recovery under the new model can often be quite pessimistic, an average case analysis is not and provides significantly more insight. Using a probability model on the sparse signal we show that under very mild conditions the probability of recovery failure decays exponentially with increasing dimension of the subspaces.},
  timestamp = {2016-07-08T11:36:26Z},
  booktitle = {2010 44th {{Annual Conference}} on {{Information Sciences}} and {{Systems}} ({{CISS}})},
  author = {Boufounos, P. and Kutyniok, G. and Rauhut, H.},
  month = mar,
  year = {2010},
  keywords = {average case analysis,combined fusion frame measurements,compressed sensing,Electric variables measurement,fusion frames,Information analysis,information processing,ℓ1;2 Minimization,ℓ1 Minimization,Laboratories,mathematical analysis,Mathematical model,mathematical tools,Mathematics,probability,Random Matrices,recovery failure,sensor fusion,Signal analysis,signal processing,signal representation,Signal representations,sparse recovery,sparse representations,worst-case analysis},
  pages = {1--6},
  annote = {read},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7RXPM7M8\\articleDetails.html:text/html;IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\X5E7HB55\\articleDetails.html:text/html}
}

@article{Boufounos2011,
  title = {Sparse {{Recovery From Combined Fusion Frame Measurements}}},
  volume = {57},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2143890},
  abstract = {Sparse representations have emerged as a powerful tool in signal and information processing, culminated by the success of new acquisition and processing techniques such as compressed sensing (CS). Fusion frames are very rich new signal representation methods that use collections of subspaces instead of vectors to represent signals. This work combines these exciting fields to introduce a new sparsity model for fusion frames. Signals that are sparse under the new model can be compressively sampled and uniquely reconstructed in ways similar to sparse signals using standard CS. The combination provides a promising new set of mathematical tools and signal models useful in a variety of applications. With the new model, a sparse signal has energy in very few of the subspaces of the fusion frame, although it does not need to be sparse within each of the subspaces it occupies. This sparsity model is captured using a mixed l1/l2 norm for fusion frames. A signal sparse in a fusion frame can be sampled using very few random projections and exactly reconstructed using a convex optimization that minimizes this mixed l1/l2 norm. The provided sampling conditions generalize coherence and RIP conditions used in standard CS theory. It is demonstrated that they are sufficient to guarantee sparse recovery of any signal sparse in our model. More over, a probabilistic analysis is provided using a stochastic model on the sparse signal that shows that under very mild conditions the probability of recovery failure decays exponentially with in creasing dimension of the subspaces.},
  timestamp = {2016-09-30T13:50:04Z},
  number = {6},
  journal = {IEEE Transactions on Information Theory},
  author = {Boufounos, P. and Kutyniok, G. and Rauhut, H.},
  month = jun,
  year = {2011},
  keywords = {<sub>1;2</sub>-minimization,$ell_1$-minimization,acquisition technique,Analytical models,Coherence,combined fusion frame measurement,compressed sensing,Compressed sensing (CS),convex optimization,convex programming,fusion frames,information processing,mathematical tools,minimization,mutual coherence,Null space,probabilistic analysis,Probabilistic logic,probability,Random Matrices,sampling condition,sensor fusion,sensors,signal detection,signal model,signal processing,signal representation,signal representation method,Signal sampling,Sparse matrices,sparse recovery,sparse representation,sparse signal,stochastic model,stochastic processes},
  pages = {3864--3876},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\A59FCNMH\\articleDetails.html:text/html}
}

@article{Fornasier2008,
  title = {Recovery {{Algorithms}} for {{Vector}}-{{Valued Data}} with {{Joint Sparsity Constraints}}},
  volume = {46},
  issn = {0036-1429},
  doi = {10.1137/0606668909},
  abstract = {Vector-valued data appearing in concrete applications often possess sparse expansions with respect to a preassigned frame for each vector component individually. Additionally, different components may also exhibit common sparsity patterns. Recently, there were introduced sparsity measures that take into account such joint sparsity patterns, promoting coupling of nonvanishing components. These measures are typically constructed as weighted \$$\backslash$ell\_1\$ norms of componentwise \$$\backslash$ell\_q\$ norms of frame coefficients. We show how to compute solutions of linear inverse problems with such joint sparsity regularization constraints by fast thresholded Landweber algorithms. Next we discuss the adaptive choice of suitable weights appearing in the definition of sparsity measures. The weights are interpreted as indicators of the sparsity pattern and are iteratively updated after each new application of the thresholded Landweber algorithm. The resulting two-step algorithm is interpreted as a double-minimization scheme for a suitable target functional. We show its \$$\backslash$ell\_2\$-norm convergence. An implementable version of the algorithm is also formulated, and its norm convergence is proven. Numerical experiments in color image restoration are presented.},
  timestamp = {2016-09-30T11:21:47Z},
  number = {2},
  urldate = {2016-05-30},
  journal = {SIAM Journal on Numerical Analysis},
  author = {Fornasier, M. and Rauhut, H.},
  month = jan,
  year = {2008},
  pages = {577--613},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\K2MA98A9\\0606668909.html:text/html}
}

@article{Model2006,
  series = {Sparse Approximations in Signal and Image ProcessingSparse Approximations in Signal and Image Processing},
  title = {Signal Reconstruction in Sensor Arrays Using Sparse Representations},
  volume = {86},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2005.05.033},
  abstract = {We propose a technique of multisensor signal reconstruction based on the assumption, that source signals are spatially sparse, as well as have sparse representation in a chosen dictionary in time domain. This leads to a large scale convex optimization problem, which involves combined l 1 - l 2 norm minimization. The optimization is carried by the truncated Newton method, using preconditioned conjugate gradients in inner iterations. The byproduct of reconstruction is the estimation of source locations.},
  timestamp = {2016-09-29T16:14:05Z},
  number = {3},
  urldate = {2016-05-30},
  journal = {Signal Processing},
  author = {Model, Dmitri and Zibulevsky, Michael},
  month = mar,
  year = {2006},
  keywords = {Multipath,Source localization,Source reconstruction,Wideband},
  pages = {624--638},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5N2XNZPR\\S0165168405002252.html:text/html}
}

@inproceedings{Zelinski2008,
  title = {Sparsity in {{MRI RF}} Excitation Pulse Design},
  doi = {10.1109/CISS.2008.4558531},
  abstract = {Magnetic resonance imaging (MRI) may be viewed as a two-stage experiment that yields a non-invasive spatial mapping of hydrogen nuclei in living subjects. Nuclear spins within a subject are first excited using a radio-frequency (RF) excitation pulse and proportions of excited spins are then detected using a resonant coil; images are then reconstructed from this data. Excitation pulses need to be tailored to a user's specific needs and in most applications need to be as short as possible, due to spin relaxation, tissue heating, signal-to-noise ratio (SNR), and data readout limitations. The design of short-duration excitation pulses is an important topic and the focus of our work. One may show that RF excitation pulse design, under certain conditions, involves choosing to deposit energy in a continuous, 3-D, Fourier-like domain ("excitation k-space") in order to form some desired excitation in the spatial domain. Energy may only be deposited along a 1-D contour, and there are limitations on where and how it may be placed; the most important fact is that excitation pulse duration directly corresponds to the length of the chosen contour and the rate it is traversed. The problem then is to find a sparse "trajectory" (and corresponding energy deposition) within this k-space such that a high-fidelity version of the desired excitation is formed in the spatial domain. We show how sparsity and simultaneous sparsity are applicable to 2-D and 3-D excitation pulse design and present a novel instance where simultaneous sparsity is desirable. We then discuss how to apply sparse approximation concepts to produce RF pulses. These "sparsity-enforced" designs, generated via convex relaxation techniques, significantly outperform conventional pulses: for fixed pulse duration, sparsity-enforced pulses always produce higher-fidelity excitations.},
  timestamp = {2016-09-30T13:30:06Z},
  booktitle = {42nd {{Annual Conference}} on {{Information Sciences}} and {{Systems}}, 2008. {{CISS}} 2008},
  author = {Zelinski, A. C. and Goyal, V. K. and Adalsteinsson, E. and Wald, L. L.},
  month = mar,
  year = {2008},
  keywords = {1D contour,biological tissues,biomedical MRI,Coils,cone programming,continuous 3D Fourier-like domain,convex relaxation,data readout limitation,energy deposition,excitation k-space,excitation pulse duration,excited spin,Focusing,Fourier transforms,Heating,Hydrogen,hydrogen nuclei,image reconstruction,living subjects,magnetic resonance imaging,medical image processing,MRI RF excitation pulse,MRI RF pulse sequence design,noninvasive spatial mapping,Nuclear magnetic resonance,nuclear spin,Pulse generation,Radio frequency,radiofrequency excitation pulse,resonant coil,second-order cone programming,signal-to-noise ratio,Signal to noise ratio,simultaneous sparsity,sparse approximation,sparse trajectory,sparsity,spin relaxation,tissue heating},
  pages = {252--257},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\G6UCJEV5\\articleDetails.html:text/html}
}

@article{Zelinski2008a,
  title = {Sparsity-{{Enforced Slice}}-{{Selective MRI RF Excitation Pulse Design}}},
  volume = {27},
  issn = {0278-0062},
  doi = {10.1109/TMI.2008.920605},
  abstract = {We introduce a novel algorithm for the design of fast slice-selective spatially-tailored magnetic resonance imaging (MRI) excitation pulses. This method, based on sparse approximation theory, uses a second-order cone optimization to place and modulate a small number of slice-selective sine-like radio-frequency (RF) pulse segments ("spokes") in excitation fc-space, enforcing sparsity on the number of spokes allowed while si multaneously encouraging those that remain to be placed and modulated in a way that best forms a user-defined in-plane target magnetization. Pulses are designed to mitigate B1 inhomogeneity in a water phantom at 7 T and to produce highly-structured excitations in an oil phantom on an eight-channel parallel excitation system at 3 T. In each experiment, pulses generated by the spar- sity-enfoldquoced method outperform those created via conventional Fourier-based techniques, e.g., when attempting to produce a uniform magnetization in the presence of severe B1 inhomogeneity, a 5.7-ms 15-spoke pulse generated by the sparsity-enforced method produces an excitation with 1.28 times lower root mean square error than conventionally-designed 15-spoke pulses. To achieve this same level of uniformity, the conventional methods need to use 29-spoke pulses that are 7.8 ms long.},
  timestamp = {2016-09-30T13:30:33Z},
  number = {9},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Zelinski, A. C. and Wald, L. L. and Setsompop, K. and Goyal, V. K. and Adalsteinsson, E.},
  month = sep,
  year = {2008},
  keywords = {<sub>1</sub> inhomogeneity mitigation,3-D RF excitation,algorithm,Algorithm design and analysis,algorithms,Approximation methods,B1,biomedical MRI,Brain,Fourier-based techniques,high field strength,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Imaging phantoms,Imaging; Three-Dimensional,inhomogeneity mitigation,Magnetic modulators,magnetic resonance imaging,magnetic resonance imaging (MRI) radio-frequency (RF) pulse sequence design,magnetization,MRI RF pulse sequence design,Optimization methods,parallel transmission,phantom,phantoms,Pulse generation,Pulse modulation,Radio frequency,RF excitation pulse design,root mean square error,Signal Processing; Computer-Assisted,sparse approximation,sparsity-enforced slice-selective MRI,three-dimensional (3-D) RF excitation},
  pages = {1213--1229},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7M8I8DTA\\articleDetails.html:text/html}
}

@article{Daudet2006,
  title = {Sparse and Structured Decompositions of Signals with the Molecular Matching Pursuit},
  volume = {14},
  issn = {1558-7916},
  doi = {10.1109/TSA.2005.858540},
  abstract = {This paper describes the Molecular Matching Pursuit (MMP), an extension of the popular Matching Pursuit (MP) algorithm for the decomposition of signals. The MMP is a practical solution which introduces the notion of structures within the framework of sparse overcomplete representations; these structures are based on the local dependency of significant time-frequency or time-scale atoms. We show that this algorithm is well adapted to the representation of real signals such as percussive audio signals. This is at the cost of a slight sub-optimality in terms of the rate of convergence for the approximation error, but the benefits are numerous, most notably a significant reduction in the computational cost, which facilitates the processing of long signals. Results show that this algorithm is very promising for high-quality adaptive coding of audio signals},
  timestamp = {2016-09-30T13:31:35Z},
  number = {5},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author = {Daudet, L.},
  month = sep,
  year = {2006},
  keywords = {approximation error,audio coding,audio signal adaptive coding,convergence,cost reduction,Costs,Dictionaries,Discrete wavelet transforms,Iterative algorithms,Matching pursuit,Matching pursuit algorithms,molecular matching pursuit,overcomplete representations,parametric audio coding,percussive audio signals,Pursuit algorithms,signal decomposition,signal processing,signal representation,sparse overcomplete representations,time-frequency analysis,Time frequency analysis,time-frequency atoms,time-frequency transforms,time-scale atoms},
  pages = {1808--1816},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TFG8J5IG\\articleDetails.html:text/html}
}

@inproceedings{Keriven2013,
  title = {Structured Sparsity Using Backwards Elimination for {{Automatic Music Transcription}}},
  doi = {10.1109/MLSP.2013.6661917},
  abstract = {Musical signals can be thought of as being sparse and structured, with few elements active at a given instant and temporal continuity of active elements observed. Greedy algorithms such as Orthogonal Matching Pursuit (OMP), and structured variants, have previously been proposed for Automatic Music Transcription (AMT), however some problems have been noted. Hence, we propose the use of a backwards elimination strategy in order to perform sparse decompositions for AMT, in particular with a proposed alternative sparse cost function. However, the main advantage of this approach is the ease with which structure can be incorporated. The use of group sparsity is shown to give increased AMT performance, while a molecular method incorporating onset information is seen to provide further improvements with little computational effort.},
  timestamp = {2016-09-30T13:31:57Z},
  booktitle = {2013 {{IEEE International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Keriven, N. and O'Hanlon, K. and Plumbley, M. D.},
  month = sep,
  year = {2013},
  keywords = {alternative sparse cost function,AMT performance,automatic music transcription,backwards elimination,backwards elimination strategy,compressed sensing,Cost function,Dictionaries,Greedy algorithms,group sparsity,Matching pursuit algorithms,molecular method,music,musical signals,music transcription,onset information,sparse decompositions,Sparse matrices,spectrogram,structured sparsity,structured variants,Transforms,Vectors},
  pages = {1--6},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CUSIFG52\\articleDetails.html:text/html}
}

@inproceedings{Krstulovic2005,
  title = {A Comparison of Two Extensions of the Matching Pursuit Algorithm for the Harmonic Decomposition of Sounds},
  doi = {10.1109/ASPAA.2005.1540219},
  abstract = {In the framework of audio signal analysis, it is desired to obtain sparse representations that are able to reflect the harmonic structures, e.g., issued from musical instruments. In this paper, we compare two approaches which introduce some explicit models of harmonic features into the matching pursuit analysis framework. The first approach is the harmonic matching pursuit (HMP), where the harmonic structures are modeled by sets of harmonically related Gabor atoms which are directly optimized in the analysis loop. The second approach, called meta-molecular matching pursuit (M3P), is based on the a posteriori agglomeration of elementary features coming from a short time Fourier transform. We discuss the pros and cons of each method through experiments involving different audio signals, and conclude on possible approaches for combining the two methods.},
  timestamp = {2016-09-30T13:31:14Z},
  booktitle = {{{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}}, 2005.},
  author = {Krstulovic, S. and Gribonval, R. and Leveau, P. and Daudet, L.},
  month = oct,
  year = {2005},
  keywords = {Acoustics,acoustic signal processing,Algorithm design and analysis,a posteriori agglomeration,audio signal analysis,audio signal processing,Dictionaries,Fourier transforms,Gabor atoms,Harmonic analysis,harmonic decomposition,harmonics,harmonic structures,Instruments,iterative methods,matching pursuit algorithm,Matching pursuit algorithms,metamolecular matching pursuit,musical instruments,Pursuit algorithms,short time Fourier transform,Signal analysis,Signal processing algorithms,sparse representations,time-frequency analysis},
  pages = {259--262},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RCHMB27I\\articleDetails.html:text/html}
}

@article{Tropp2005d,
  title = {Recovery of Short, Complex Linear Combinations via \$$\backslash$ell\_1\$ Minimization},
  volume = {51},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.844057},
  abstract = {This note provides a condition under which lscr1 minimization (also known as basis pursuit) can recover short linear combinations of complex vectors chosen from fixed, overcomplete collection. This condition has already been established in the real setting by Fuchs, who used convex analysis. The proof given here is more direct},
  timestamp = {2016-05-30T16:09:08Z},
  number = {4},
  journal = {IEEE Transactions on Information Theory},
  author = {Tropp, J. A.},
  month = apr,
  year = {2005},
  keywords = {algorithms,approximation,Atomic measurements,basis pursuit,convex analysis,convex programming,Dictionaries,Hilbert space,Hilbert spaces,Ice,L1 minimization,Linear approximation,linear combination,linear program,linear programming,mathematical programming,minimisation,Minimization methods,redundant dictionaries,Signal synthesis,Software standards,Sparse matrices,sparse representations,Vectors},
  pages = {1568--1570},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VNZNJE62\\articleDetails.html:text/html}
}

@article{Zhao2009,
  title = {The Composite Absolute Penalties Family for Grouped and Hierarchical Variable Selection},
  volume = {37},
  abstract = {Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the L1-penalized squared error minimization method Lasso has been popular in regression models and beyond. In this paper, we combine different norms including L1 to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached},
  timestamp = {2017-04-19T14:40:19Z},
  number = {6},
  journal = {Ann. Stat.},
  author = {Zhao, Peng and Rocha, Guilherme and Yu, Bin},
  year = {2009},
  pages = {3468--3497},
  file = {Citeseer - Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8WKJ3VQ3\\summary.html:text/html}
}

@article{Foygel2010,
  title = {Exact Block-Wise Optimization in Group Lasso and Sparse Group Lasso for Linear Regression},
  abstract = {The group lasso is a penalized regression method, used in regression problems where the covariates are partitioned into groups to promote sparsity at the group level. Existing methods for finding the group lasso estimator either use gradient projection methods to update the entire coefficient vector simultaneously at each step, or update one group of coefficients at a time using an inexact line search to approximate the optimal value for the group of coefficients when all other groups' coefficients are fixed. We present a new method of computation for the group lasso in the linear regression case, the Single Line Search (SLS) algorithm, which operates by computing the exact optimal value for each group (when all other coefficients are fixed) with one univariate line search. We perform simulations demonstrating that the SLS algorithm is often more efficient than existing computational methods. We also extend the SLS algorithm to the sparse group lasso problem via the Signed Single Line Search (SSLS) algorithm, and give theoretical results to support both algorithms.},
  timestamp = {2016-05-30T17:04:56Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1010.3320},
  primaryClass = {stat},
  urldate = {2016-05-30},
  journal = {arXiv:1010.3320 [stat]},
  author = {Foygel, Rina and Drton, Mathias},
  month = oct,
  year = {2010},
  keywords = {Statistics - Machine Learning},
  annote = {Comment: We have been made aware of the earlier work by Puig et al. (2009) which derives the same result for the (non-sparse) group lasso setting. We leave this manuscript available as a technical report, to serve as a reference for the previously untreated sparse group lasso case, and for timing comparisons of various methods in the group lasso setting. The manuscript is updated to include this reference},
  file = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DB7EH7V8\\1010.html:text/html}
}

@article{Kim2006,
  title = {Blockwise Sparse Regression},
  volume = {16},
  abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): proposed the grouped LASSO, which achieves shrink-age and selection simultaneously, as LASSO does, but works on blocks of covariates. That is, the grouped LASSO provides a model where some blocks of regression co-ecients are exactly zero. The grouped LASSO is useful when there are meaningful blocks of covariates such as polynomial regression and dummy variables from cat-egorical variables. In this paper, we propose an extension of the grouped LASSO, called `Blockwise Sparse Regression ' (BSR). The BSR achieves shrinkage and se-lection simultaneously on blocks of covariates similarly to the grouped LASSO, but it works for general loss functions including generalized linear models. An ecient computational algorithm is developed and a blockwise standardization method is proposed. Simulation results show that the BSR compromises the ridge and LASSO for logistic regression. The proposed method is illustrated with two datasets.},
  timestamp = {2016-05-30T17:08:49Z},
  urldate = {2016-05-30},
  journal = {Statistica Sinica},
  author = {Kim, Yuwon and Kim, Jinseog and Kim, Yongdai},
  year = {2006},
  pages = {375--390},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\BXI8PC82\\citations\;jsessionid=1F2E76386D60B72F4F8024C339B7A997.html:text/html}
}

@inproceedings{Cand`es2005c,
  title = {Error correction via linear programming},
  doi = {10.1109/SFCS.2005.5464411},
  abstract = {Suppose we wish to transmit a vector f \ensuremath{\epsilon} Rn reliably.
	A frequently discussed approach consists in encoding f with an m
	by n coding matrix A. Assume now that a fraction of the entries of
	Af are corrupted in a completely arbitrary fashion by an error e.
	We do not know which entries are affected nor do we know how they
	are affected. Is it possible to recover f exactly from the corrupted
	m-dimensional vector y = Af + e?},
  timestamp = {2016-09-29T15:27:31Z},
  booktitle = {46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)},
  author = {Cand{\`e}s, E. and Rudelson, M. and Tao, T. and Vershynin, R.},
  month = oct,
  year = {2005},
  keywords = {Decoding,Decoding,encoding,Encoding,Error correction,Error correction,error correction codes,Error correction codes,Functional analysis,functional analysis,functional
	analysis,Linear code,Linear code,linear programming,linear programming,Mathematics,Mathematics,Particle measurements,Particle measurements,Vectors,Vectors},
  pages = {668--681}
}

@article{Cand`es2006d,
  title = {Compressive sampling},
  volume = {3},
  abstract = {Conventional wisdom and common practice in acquisition and reconstruction
	of images from frequency data follow the basic principle of the Nyquist
	density sampling theory. This principle states that to reconstruct
	an image, the number of Fourier samples we need to acquire must match
	the desired resolution of the image, i.e. the number of pixels in
	the image. This paper surveys an emerging theory which goes by the
	name of ``compressive sampling'' or ``compressed sensing,'' and which
	says that this conventional wisdom is inaccurate. Perhaps surprisingly,
	it is possible to reconstruct images or signals of scientific interest
	accurately and sometimes even exactly from a number of samples which
	is far smaller than the desired resolution of the image/signal, e.g.
	the number of pixels in the image. It is believed that compressive
	sampling has far reaching implications. For example, it suggests
	the possibility of new data acquisition protocols that translate
	analog information into digital form with fewer sensors than what
	was considered necessary. This new sampling theory may come to underlie
	procedures for sampling and compressing data simultaneously. In this
	short survey, we provide some of the key mathematical insights underlying
	this new theory, and explain some of the interactions between compressive
	sampling and other fields such as statistics, information theory,
	coding theory, and theoretical computer science.},
  timestamp = {2016-09-30T10:46:22Z},
  urldate = {2016-05-15},
  journal = {Proceedings of the international congress of mathematicians},
  author = {Cand{\`e}s, Emmanuel J.},
  year = {2006},
  pages = {1433--1452}
}

@article{Gilbert2006,
  title = {Algorithmic linear dimension reduction in the $\ell_1$ norm for sparse vectors},
  abstract = {Using a number of different algorithms, we can recover approximately
	a sparse signal with limited noise, i.e, a vector of length d with
	at least d-m zeros or near-zeros, using little more than mlog(d)
	nonadaptive linear measurements rather than the d measurements needed
	to recover an arbitrary signal of length d. We focus on two important
	properties of such algorithms. \textbullet{} Uniformity. A single
	measurement matrix should work simultaneously for all signals. \textbullet{}
	Computational Efficiency. The time to recover such an msparse signal
	should be close to the obvious lower bound, mlog(d/m). This paper
	develops a new method for recovering msparse signals that is simultaneously
	uniform and quick. We present a reconstruction algorithm whose run
	time, O(mlog2(m) log2(d)), is sublinear in the length d of the signal.
	The reconstruction error is within a logarithmic factor (in m) of
	the optimal m-term approximation error in `1. In particular, the
	algorithm recovers m-sparse signals perfectly and noisy signals are
	recovered with polylogarithmic distortion. Our algorithm makes O(mlog2(d))
	measurements, which is within a logarithmic factor of optimal. We
	also present a smallspace implementation of the algorithm. These
	sketching techniques and the corresponding reconstruction algorithms
	provide an algorithmic dimension reduction in the `1 norm. In particular,
	vectors of support m in dimension d can be linearly embedded into
	O(mlog2 d) dimensions with polylogarithmic distortion. We can reconstruct
	a vector from its low-dimensional sketch in time O(mlog2(m) log2(d)).
	Furthermore, this reconstruction is stable and robust under small
	perturbations.},
  timestamp = {2016-09-29T15:36:31Z},
  journal = {in Proc. 44th Annu. Allerton Conf. Communication, Control, Computing},
  author = {Gilbert, A. C. and Strauss, M. J. and Tropp, J. A. and Vershynin, R.},
  year = {2006}
}

@article{Gribonval2004a,
  title = {On the Strong Uniqueness of Highly Sparse Representations from Redundant Dictionaries},
  volume = {3195},
  timestamp = {2017-04-19T15:29:02Z},
  urldate = {2016-05-16},
  journal = {Int. Conf. Ind. Compon. Anal. Blind Signal Sep.},
  author = {Gribonval, R{\'e}mi and Nielsen, Morten},
  year = {2004},
  pages = {201--208},
  annote = {Independent Component Analysis and Blind Signal Separation}
}

@inproceedings{Indyk2008,
  address = {Philadelphia, PA, USA},
  series = {SODA '08},
  title = {Explicit Constructions for Compressed Sensing of Sparse Signals},
  abstract = {Over the recent years, a new approach for obtaining a succinct approximate
	representation of n-dimensional vectors (or signals) has been discovered.
	For any signal x, the succinct representation of x is equal to Ax,
	where A is a carefully chosen R x n real matrix, R \ensuremath{\ll}
	n. Often, A is chosen at random from some distribution over R x n
	matrices. The vector Ax is often refered to as the measurement vector
	or a sketch of x. Although the dimension of Ax is much shorter than
	of x, it contains plenty of useful information about x.},
  timestamp = {2016-10-27T14:30:20Z},
  urldate = {2016-05-15},
  booktitle = {Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Indyk, Piotr},
  year = {2008},
  pages = {30-33}
}

@article{Linial2005,
  title = {How neighborly can a centrally symmetric polytope be?},
  abstract = {We show that there exist k-neighborly centrally symmetric d-dimensional
	polytopes with 2(n+d) vertices, where k(d,n)=Theta(d/(1+log ((d+n)/d))).
	We also show that this bound is tight.},
  timestamp = {2016-07-08T12:44:30Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {arxiv},
  urldate = {2016-05-16},
  journal = {arXiv:math/0507280},
  author = {Linial, Nathan and Novik, Isabella},
  month = jul,
  year = {2005},
  keywords = {52B05,52B05,52B15,52B15,52B35,52B35,Mathematics - Combinatorics,Mathematics - Combinatorics},
  annote = {Comment: 7 pages},
  annote = {Comment: 7 pages},
  archiveprefix = {arXiv}
}

@article{Vershik1992,
  title = {Asymptotic behavior of the number of faces of random polyhedra and 	the neighborliness problem},
  volume = {11},
  issn = {0272-9903},
  abstract = {Asymptotic behavior of the number of faces of random polyhedra and
	the neighborliness problem on ResearchGate, the professional network
	for scientists.},
  timestamp = {2016-09-30T11:11:41Z},
  number = {2},
  urldate = {2016-05-16},
  journal = {Sel Math Soviet},
  author = {Vershik, A. M. and Sporyshev, P. V.},
  month = jan,
  year = {1992},
  pages = {181--201}
}

@article{Obozinski2011,
  title = {Support Union Recovery in High-Dimensional Multivariate Regression},
  volume = {39},
  issn = {0090-5364},
  doi = {10.1214/09-AOS776},
  abstract = {In multivariate regression, a \$K\$-dimensional response vector is regressed upon a common set of \$p\$ covariates, with a matrix \$B\^*$\backslash$in$\backslash$mathbb\{R\}\^\{p$\backslash$times K\}\$ of regression coefficients. We study the behavior of the multivariate group Lasso, in which block regularization based on the \$$\backslash$ell\_1/$\backslash$ell\_2\$ norm is used for support union recovery, or recovery of the set of \$s\$ rows for which \$B\^*\$ is nonzero. Under high-dimensional scaling, we show that the multivariate group Lasso exhibits a threshold for the recovery of the exact row pattern with high probability over the random design and noise that is specified by the sample complexity parameter \$$\backslash$theta(n,p,s):=n/[2$\backslash$psi(B\^*)$\backslash$log(p-s)]\$. Here \$n\$ is the sample size, and \$$\backslash$psi(B\^*)\$ is a sparsity-overlap function measuring a combination of the sparsities and overlaps of the \$K\$-regression coefficient vectors that constitute the model. We prove that the multivariate group Lasso succeeds for problem sequences \$(n,p,s)\$ such that \$$\backslash$theta(n,p,s)\$ exceeds a critical level \$$\backslash$theta\_u\$, and fails for sequences such that \$$\backslash$theta(n,p,s)\$ lies below a critical level \$$\backslash$theta\_\{$\backslash$ell\}\$. For the special case of the standard Gaussian ensemble, we show that \$$\backslash$theta\_\{$\backslash$ell\}=$\backslash$theta\_u\$ so that the characterization is sharp. The sparsity-overlap function \$$\backslash$psi(B\^*)\$ reveals that, if the design is uncorrelated on the active rows, \$$\backslash$ell\_1/$\backslash$ell\_2\$ regularization for multivariate regression never harms performance relative to an ordinary Lasso approach and can yield substantial improvements in sample complexity (up to a factor of \$K\$) when the coefficient vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the multivariate group Lasso. We complement our analysis with simulations that demonstrate the sharpness of our theoretical results, even for relatively small problems.},
  timestamp = {2016-05-31T13:00:31Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0808.0711},
  number = {1},
  urldate = {2016-05-31},
  journal = {The Annals of Statistics},
  author = {Obozinski, Guillaume and Wainwright, Martin J. and Jordan, Michael I.},
  month = feb,
  year = {2011},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  pages = {1--47},
  annote = {Comment: Published in at http://dx.doi.org/10.1214/09-AOS776 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
  file = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Q5E7X7PD\\0808.html:text/html}
}

@article{Jenatton2009,
  title = {Structured {{Variable Selection}} with {{Sparsity}}-{{Inducing Norms}}},
  abstract = {We consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms. These are defined as sums of Euclidean norms on certain subsets of variables, extending the usual \$$\backslash$ell\_1\$-norm and the group \$$\backslash$ell\_1\$-norm by allowing the subsets to overlap. This leads to a specific set of allowed nonzero patterns for the solutions of such problems. We first explore the relationship between the groups defining the norm and the resulting nonzero patterns, providing both forward and backward algorithms to go back and forth from groups to patterns. This allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns. We also present an efficient active set algorithm, and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings.},
  timestamp = {2016-05-31T13:03:35Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0904.3523},
  primaryClass = {stat},
  urldate = {2016-05-31},
  journal = {arXiv:0904.3523 [stat]},
  author = {Jenatton, Rodolphe and Audibert, Jean-Yves and Bach, Francis},
  month = apr,
  year = {2009},
  keywords = {Statistics - Machine Learning},
  file = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TND8V8GQ\\0904.html:text/html}
}

@article{Heath2006,
  title = {On Quasi-Orthogonal Signatures for {{CDMA}} Systems},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.864469},
  abstract = {Sum capacity optimal signatures in synchronous code-division multiple-access (CDMA) systems are functions of the codebook length as well as the number of active users. A new signature set must be assigned every time the number of active users changes. This correspondence considers signature sets that are less sensitive to changes in the number of active users. Equiangular signature sequences are proven to solve a certain max-min signal-to-interference-plus-noise problem, which results from their interference invariance. Unions of orthonormal bases have subsets that come close to satisfying the Welch bound. Bounds on the maximum number of bases with minimum maximum correlation are derived and a new construction algorithm is provided. Connections are made between these signature design problems, Grassmannian line packing, frame theory, and algebraic geometry},
  timestamp = {2016-09-30T13:50:26Z},
  number = {3},
  journal = {IEEE Transactions on Information Theory},
  author = {Heath, R. W. and Strohmer, T. and Paulraj, A. J.},
  month = mar,
  year = {2006},
  keywords = {algebraic geometry,Circuits,codebook length,code division multiple access,code-division multiple-access,Code-division multiple-access (CDMA) systems,Code division multiplexing,Colored noise,Encoding,equiangular signature sequence,Fading,frame theory,Gaussian channels,Geometry,Grassmannian line packing,Interference,interference (signal),Iterative algorithms,minimax techniques,minimum maximum correlation,Multiaccess communication,number theory,quasiorthogonal signature,sequences,signal-to-interference-plus-noise problem,sum capacity,synchronous CDMA system,Walsh sequences,Welch bound,White noise},
  pages = {1217--1226},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\WTJCSP3V\\articleDetails.html:text/html}
}

@article{Welch1974,
  title = {Lower Bounds on the Maximum Cross Correlation of Signals ({{Corresp}}.)},
  volume = {20},
  issn = {0018-9448},
  doi = {10.1109/TIT.1974.1055219},
  abstract = {Some communication systems require sets of signals with impulse-like autocorrelation functions and small cross correlation. There is considerable literature on signals with impulse-like autocorrelation functions hut little on sets of signals with small cross correlation. A possible reason is that designers put too severe a restriction on cross correlation magnitudes. This correspondence establishes lower bounds on how small the cross correlation and autocorrelation can simultaneously be.},
  timestamp = {2016-09-30T13:50:43Z},
  number = {3},
  journal = {IEEE Transactions on Information Theory},
  author = {Welch, L.},
  month = may,
  year = {1974},
  keywords = {Communication systems,Correlation functions,Detectors,Matched filters,Sampling methods,Signal design,Timing},
  pages = {397--399},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5K9JIBV6\\articleDetails.html:text/html}
}

@incollection{Rosenfeld1997,
  series = {Algorithms and Combinatorics},
  title = {In {{Praise}} of the {{Gram Matrix}}},
  copyright = {\textcopyright{}1997 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-64393-4 978-3-642-60406-5},
  abstract = {Summary We use the Gram matrix to prove that the largest number of points in R d such that the distance between all pairs is an odd integer (the square root of an odd integer) is $\leq$ d + 2 and we characterize all dimensions d for which the upper bound is attained. We also use the Gram matrix to obtain an upper bound for the smallest angle determined by sets of n lines through the origin in R d .},
  language = {en},
  timestamp = {2016-10-07T14:24:46Z},
  number = {14},
  urldate = {2016-06-01},
  booktitle = {The {{Mathematics}} of {{Paul Erd{\H o}s II}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Rosenfeld, Moshe},
  editor = {Graham, Ronald L. and Ne{\v s}et{\v r}il, Jaroslav},
  year = {1997},
  keywords = {Combinatorics,Economic Theory/Quantitative Economics/Mathematical Methods,Geometry,Mathematical Logic and Foundations,number theory,Statistics; general},
  pages = {318--323},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3PBRW4ZC\\978-3-642-60406-5_29.html:text/html},
  doi = {10.1007/978-3-642-60406-5_29}
}

@article{Audenaert2006,
  title = {Norm {{Compression Inequalities}} for {{Block Partitioned Matrices}}},
  timestamp = {2016-06-03T14:48:16Z},
  author = {Audenaert, Koenraad M.R.},
  year = {2006},
  annote = {read}
}

@article{Elad2010b,
  title = {On the {{Role}} of {{Sparse}} and {{Redundant Representations}} in {{Image Processing}}},
  volume = {98},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2009.2037655},
  abstract = {Much of the progress made in image processing in the past decades can be attributed to better modeling of image content and a wise deployment of these models in relevant applications. This path of models spans from the simple l2-norm smoothness through robust, thus edge preserving, measures of smoothness (e.g. total variation), and until the very recent models that employ sparse and redundant representations. In this paper, we review the role of this recent model in image processing, its rationale, and models related to it. As it turns out, the field of image processing is one of the main beneficiaries from the recent progress made in the theory and practice of sparse and redundant representations. We discuss ways to employ these tools for various image-processing tasks and present several applications in which state-of-the-art results are obtained.},
  timestamp = {2016-06-08T13:44:23Z},
  number = {6},
  journal = {Proceedings of the IEEE},
  author = {Elad, M. and Figueiredo, M. A. T. and Ma, Y.},
  month = jun,
  year = {2010},
  keywords = {Additive white noise,Deconvolution,denoising,Dictionaries,dictionary learning,edge preserving,Filling,Frames,Gaussian noise,image content,Image processing,Image resolution,Image restoration,Image sampling,inpainting,Inspection,l2-norm smoothness,Noise reduction,redundant dictionaries,redundant representations,Robustness,smoothing methods,sparse representations,Spatial resolution,superresolution,wavelets},
  pages = {972--982},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\77IQRPJA\\articleDetails.html:text/html}
}

@inproceedings{Obozinski2008,
  title = {Union Support Recovery in High-Dimensional Multivariate Regression},
  doi = {10.1109/ALLERTON.2008.4797530},
  abstract = {In the problem of multivariate regression, a K-dimensional response vector is regressed upon a common set of p covariates, with a matrix B* isin RopfptimesK of regression coefficients. We study the behavior of the group Lasso using lscr1/lscr2 regularization for the union support problem, meaning that the set of s rows for which B* is non-zero is recovered exactly. Studying this problem under high-dimensional scaling, we show that group Lasso recovers the exact row pattern with high probability over the random design and noise for scalings of (n, p, s) such that the sample complexity parameter given by thetas(n, p, s) := n/[2psi(B*) log(p - s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the number of non-zero rows, and psi(B*) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. This sparsity-overlap function reveals that, if the design is uncorrelated on the active rows, block lscr1/lscr2 regularization for multivariate regression never harms performance relative to an ordinary Lasso approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the group Lasso.},
  timestamp = {2016-06-27T09:29:15Z},
  booktitle = {2008 46th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}},
  author = {Obozinski, G. and Wainwright, M. J. and Jordan, M. I.},
  month = sep,
  year = {2008},
  keywords = {Additive noise,Collaborative work,computational complexity,group Lasso,high-dimensional multivariate regression,H infinity control,k-dimensional response vector,Large-scale systems,Multivariate regression,Predictive models,regression analysis,Size measurement,Statistical learning,Statistics,union support problem,Vectors},
  pages = {21--26},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HG7J2WET\\articleDetails.html:text/html}
}

@article{Peter2015,
  title = {Damping {{Noise}}-{{Folding}} and {{Enhanced Support Recovery}} in {{Compressed Sensing}}},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2461521},
  abstract = {The practice of compressed sensing suffers importantly in terms of the efficiency/accuracy trade-off when acquiring noisy signals prior to measurement. It is rather common to find results treating the noise affecting the measurements, avoiding in this way to face the so-called noise-folding phenomenon, related to the noise in the signal, eventually amplified by the measurement procedure. In this paper, we present two new decoding procedures, combining l1-minimization followed by either a regularized selective least p-powers or an iterative hard thresholding, which not only are able to reduce this component of the original noise, but also have enhanced properties in terms of support identification with respect to the sole l1-minimization or iteratively re-weighted l1-minimization. We prove such features, providing relatively simple and precise theoretical guarantees. We additionally confirm and support the theoretical results by extensive numerical simulations, which give a statistics of the robustness of the new decoding procedures with respect to more classical methods based on l1-minimization.},
  timestamp = {2016-06-27T09:32:48Z},
  number = {22},
  journal = {IEEE Transactions on Signal Processing},
  author = {Peter, S. and Artina, M. and Fornasier, M.},
  month = nov,
  year = {2015},
  keywords = {$ell_1$ -minimization,approximation error,compressed sensing,Decoding,decoding procedures,Iterative decoding,iterative hard thresholding,iteratively reweighted l1-minimization,iterative methods,measurement procedure,minimisation,Noise,noise-folding damping,Noise folding in compressed sensing,Noise measurement,noisy signal acquisition,numerical simulations,phase transitions,regularized selective least p-powers,Robustness,selective least $p$-powers,signal denoising,signal detection,sole l1-minimization,support identification,support recovery enhancement},
  pages = {5990--6002},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IEBS4I5Z\\articleDetails.html:text/html}
}

@inproceedings{Reeves2009,
  title = {A Note on Optimal Support Recovery in Compressed Sensing},
  doi = {10.1109/ACSSC.2009.5470153},
  abstract = {Recovery of the support set (or sparsity pattern) of a sparse vector from a small number of noisy linear projections (or samples) is a {\^A}\textquestiondown{}compressed sensing{\^A}\textquestiondown{} problem that arises in signal processing and statistics. Although many computationally efficient recovery algorithms have been studied, the optimality (or gap from optimality) of these algorithms is, in general, not well understood. In this note, approximate support recovery under a Gaussian prior is considered, and it is shown that optimal estimation depends on the recovery metric in general. By contrast, it is shown that in the SNR limits, there exist uniformly near-optimal estimators, namely, the ML estimate in the high SNR case, and a computationally trivial thresholding algorithm in the low SNR case.},
  timestamp = {2016-06-27T09:38:07Z},
  booktitle = {2009 {{Conference Record}} of the {{Forty}}-{{Third Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Reeves, G. and Gastpar, M.},
  month = nov,
  year = {2009},
  keywords = {Algorithm design and analysis,compressed sensing,compressed sensing problem,computational complexity,Distortion measurement,Indexing,low SNR,maximum likelihood estimation,optimal approximate support recovery algorithm,signal processing,Signal processing algorithms,signal reconstruction,Signal sampling,Statistics,trivial thresholding algorithm,uniformly near-optimal estimators,Vectors},
  pages = {1576--1580},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\WWVP5Q7B\\articleDetails.html:text/html}
}

@article{Jin2013,
  title = {Support {{Recovery}} of {{Sparse Signals}} in the {{Presence}} of {{Multiple Measurement Vectors}}},
  volume = {59},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2238605},
  abstract = {This paper studies the performance limits in the support recovery of sparse signals based on multiple measurement vectors (MMV). An information-theoretic analytical framework inspired by the connection to the single-input multiple-output multiple-access channel communication is established to reveal the performance limits in the support recovery of sparse signals with fixed number of nonzero entries. Sharp sufficient and necessary conditions for asymptotically successful support recovery are derived in terms of the number of measurements per vector, the number of nonzero rows, the measurement noise level, and the number of measurement vectors. Through the interpretations of the results, the benefit of having MMV for sparse signal recovery is illustrated, thus providing a theoretical foundation to the performance improvement enabled by MMV as observed in many existing simulation results. In particular, it is shown that the structure (rank) of the matrix formed by the nonzero entries plays an important role in the performance limits of support recovery.},
  timestamp = {2016-06-27T09:40:28Z},
  number = {5},
  journal = {IEEE Transactions on Information Theory},
  author = {Jin, Y. and Rao, B. D.},
  month = may,
  year = {2013},
  keywords = {Algorithm design and analysis,compressed sensing,Matching pursuit algorithms,matrix structure,measurement errors,measurement noise level,MMV,multimeasurement vector,multiple access channel,multiple access channel communication,Multiple measurement vectors,Noise,Noise measurement,nonzero entry,nonzero rows,performance limit,Receivers,single input multiple output,Sparse matrices,sparse signal recovery,sufficient and necessary condition,Vectors,wireless channels},
  pages = {3139--3157},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NQIUCHFE\\articleDetails.html:text/html}
}

@inproceedings{Shekaramiz2015,
  title = {On the Block-Sparsity of Multiple-Measurement Vectors},
  doi = {10.1109/DSP-SPE.2015.7369556},
  abstract = {Based on the compressive sensing (CS) theory, it is possible to recover signals, which are either compressible or sparse under some suitable basis, via a small number of non-adaptive linear measurements. In this paper, we investigate recovering of block-sparse signals via multiple measurement vectors (MMVs) in the presence of noise. In this case, we consider one of the existing algorithms which provides a satisfactory estimate in terms of minimum meansquared error but a non-sparse solution. Here, the algorithm is first modified to result in sparse solutions. Then, further modification is performed to account for the unknown block sparsity structure in the solution, as well. The performance of the proposed algorithm is demonstrated by experimental simulations and comparisons with some other algorithms for the sparse recovery problem.},
  timestamp = {2016-06-27T09:45:52Z},
  booktitle = {2015 {{IEEE Signal Processing}} and {{Signal Processing Education Workshop}} ({{SP}}/{{SPE}})},
  author = {Shekaramiz, M. and Moon, T. K. and Gunther, J. H.},
  month = aug,
  year = {2015},
  keywords = {block-sparse signal,block-sparsity,block sparsity structure,compressed sensing,compressive sensing theory,experimental simulation,Matching pursuit algorithms,mean square error methods,minimum meansquared error,MMV,multiple measurement vector,multiple-measurement vector,Multiple measurement vectors (MMVs),nonadaptive linear measurement,nonsparse solution,sensors,signal processing,Signal processing algorithms,Sparse matrices,sparse recovery,Support recovery,Support vector machines,Yttrium},
  pages = {220--225},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8J9VJ5PK\\articleDetails.html:text/html}
}

@inproceedings{Jin2010,
  title = {Performance Tradeoffs for Exact Support Recovery of Sparse Signals},
  doi = {10.1109/ISIT.2010.5513492},
  abstract = {We study the tradeoffs between the number of measurements, the signal sparsity level, and the measurement noise level for exact support recovery of sparse signals via random noisy measurements. By drawing analogy between exact support recovery and communication over the Gaussian multiple access channel, and exploiting mathematical tools developed for the latter problem, we derive sharp asymptotic sufficient and necessary conditions for exact support recovery. Specifically, when the number of nonzero entries is held fixed, the exact asymptotics on the number of measurements for support recovery is developed. When the number of nonzero entries increases in certain manners, we obtain sufficient conditions tighter than existing results. The proposed information theoretic framework for analyzing the performance of support recovery is further demonstrated to be capable of dealing with a variety of sparse signal recovery models.},
  timestamp = {2016-06-27T09:54:17Z},
  booktitle = {2010 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Jin, Y. and Kim, Y. H. and Rao, B. D.},
  month = jun,
  year = {2010},
  keywords = {Channel estimation,Drives,Electric variables measurement,Engineering drawings,Gaussian channels,Gaussian multiple access channel,Matching pursuit algorithms,Maximum likelihood decoding,Noise level,noise level measurement,Noise measurement,Performance analysis,performance tradeoff,Pursuit algorithms,Signal analysis,sparse signal recovery model},
  pages = {1558--1562},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\J9XV92AB\\articleDetails.html:text/html}
}

@article{Aeron2010,
  title = {Information {{Theoretic Bounds}} for {{Compressed Sensing}}},
  volume = {56},
  issn = {0018-9448},
  doi = {10.1109/TIT.2010.2059891},
  abstract = {In this paper, we derive information theoretic performance bounds to sensing and reconstruction of sparse phenomena from noisy projections. We consider two settings: output noise models where the noise enters after the projection and input noise models where the noise enters before the projection. We consider two types of distortion for reconstruction: support errors and mean-squared errors. Our goal is to relate the number of measurements, m , and SNR, to signal sparsity, k, distortion level, d, and signal dimension, n . We consider support errors in a worst-case setting. We employ different variations of Fano's inequality to derive necessary conditions on the number of measurements and SNR required for exact reconstruction. To derive sufficient conditions, we develop new insights on max-likelihood analysis based on a novel superposition property. In particular, this property implies that small support errors are the dominant error events. Consequently, our ML analysis does not suffer the conservatism of the union bound and leads to a tighter analysis of max-likelihood. These results provide order-wise tight bounds. For output noise models, we show that asymptotically an SNR of ((n)) together with (k (n/k)) measurements is necessary and sufficient for exact support recovery. Furthermore, if a small fraction of support errors can be tolerated, a constant SNR turns out to be sufficient in the linear sparsity regime. In contrast for input noise models, we show that support recovery fails if the number of measurements scales as o(n(n)/SNR), implying poor compression performance for such cases. Motivated by the fact that the worst-case setup requires significantly high SNR and substantial number of measurements for input and output noise models, we consider a Bayesian setup. To derive necessary conditions, we develop novel extensions to Fano's inequality to handle continuous domains and arbitrary distortions. We then develop a new max-likelihood analysis over the set - - of rate distortion quantization points to characterize tradeoffs between mean-squared distortion and the number of measurements using rate-distortion theory. We show that with constant SNR the number of measurements scales linearly with the rate-distortion function of the sparse phenomena.},
  timestamp = {2016-06-27T10:01:00Z},
  number = {10},
  journal = {IEEE Transactions on Information Theory},
  author = {Aeron, S. and Saligrama, V. and Zhao, M.},
  month = oct,
  year = {2010},
  keywords = {Bayesian setup,compressed sensing,Distortion,distortion level,Distortion measurement,Fano inequality,Fano's inequality,information theoretic bounds,input noise models,linear sparsity regime,maximum likelihood estimation,max-likelihood analysis,mean-squared error distortion,mean square error methods,Noise measurement,noisy projections,order-wise tight bounds,output noise models,Rate-distortion,rate distortion quantization points,rate distortion theory,rate-distortion theory,sensing capacity,sensors,signal dimension,signal reconstruction,signal sparsity,Signal to noise ratio,SNR,sparse phenomena reconstruction,sparsity,superposition property,Support recovery,worst-case setup},
  pages = {5111--5130},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MM75A5UK\\articleDetails.html:text/html}
}

@article{Chretien2014,
  title = {Sparse {{Recovery With Unknown Variance}}: {{A LASSO}}-{{Type Approach}}},
  volume = {60},
  issn = {0018-9448},
  shorttitle = {Sparse {{Recovery With Unknown Variance}}},
  doi = {10.1109/TIT.2014.2301162},
  abstract = {We address the issue of estimating the regression vector $\beta$ in the generic s-sparse linear model y = X$\beta$ + z, with $\beta$ $\in$ $\mathbb{R}$p, y $\in$ $\mathbb{R}$n, z )V (0, $\sigma$2I), and p $>$ n when the variance $\sigma$2 is unknown. We study two least absolute shrinkage and selection operator (LASSO)-type methods that jointly estimate $\beta$ and the variance. These estimators are minimizers of the l1 penalized least-squares functional, where the relaxation parameter is tuned according to two different strategies. In the first strategy, the relaxation parameter is of the order $\sigma$\^$\surd$log p, where $\sigma$\^2 is the empirical variance. In the second strategy, the relaxation parameter is chosen so as to enforce a tradeoff between the fidelity and the penalty terms at optimality. For both estimators, our assumptions are similar to the ones proposed by Cand{\`e}s and Plan in Ann. Stat. (2009), for the case where $\sigma$2 is known. We prove that our estimators ensure exact recovery of the support and sign pattern of $\beta$ with high probability. We present simulation results showing that the first estimator enjoys nearly the same performances in practice as the standard LASSO (known variance case) for a wide range of the signal-to-noise ratio. Our second estimator is shown to outperform both in terms of false detection, when the signal-to-noise ratio is low.},
  timestamp = {2016-11-16T10:03:45Z},
  number = {7},
  journal = {IEEE Transactions on Information Theory},
  author = {Chr{\'e}tien, S. and Darses, S.},
  month = jul,
  year = {2014},
  keywords = {Coherence,compressed sensing,Context,Estimation,estimation theory,exact support recovery,generic s-sparse linear model,high dimensional regression,(l_1) penalization,l1 penalized least-squares functional minimizers,LASSO,LASSO-type approach,least absolute shrinkage-and-selection operator-type methods,penalty terms,regression analysis,regression vector estimation,relaxation parameter,signal-to-noise ratio,Signal to noise ratio,sign pattern,sparse recovery,sparse regression,Standards,Symmetric matrices,unknown variance,Vectors},
  pages = {3970--3988},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\BWJPJDGW\\articleDetails.html:text/html}
}

@article{Wu2013,
  title = {The {{Exact Support Recovery}} of {{Sparse Signals With Noise}} via {{Orthogonal Matching Pursuit}}},
  volume = {20},
  issn = {1070-9908},
  doi = {10.1109/LSP.2012.2233734},
  abstract = {Orthogonal matching pursuit (OMP) algorithm is a classical greedy algorithm in Compressed Sensing. In this letter, we study the performance of OMP in recovering the support of a sparse signal from a few noisy linear measurements. We consider two types of bounded noise and our analysis is in the framework of restricted isometry property (RIP). It is shown that under some conditions on RIP and the minimum magnitude of the nonzero elements of the sparse signal, OMP with proper stopping rules can recover the support of the signal exactly from the noisy observation. We also discuss the case of Gaussian noise. Our conditions on RIP improve some existing results.},
  timestamp = {2016-06-27T10:10:14Z},
  number = {4},
  journal = {IEEE Signal Processing Letters},
  author = {Wu, R. and Huang, W. and Chen, D. R.},
  month = apr,
  year = {2013},
  keywords = {Algorithm design and analysis,bounded noise,compressed sensing,exact support recovery,Gaussian noise,greedy algorithm,Greedy algorithms,Indexes,iterative methods,Matching pursuit algorithms,Noise measurement,noisy linear measurement,noisy observation,nonzero elements,orthogonal matching pursuit,orthogonal matching pursuit algorithm,restricted isometry property,sparse signals,stopping rules,Support recovery,Vectors},
  pages = {403--406},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KKATBRTJ\\articleDetails.html:text/html}
}

@article{Wang2015f,
  title = {Support {{Recovery With Orthogonal Matching Pursuit}} in the {{Presence}} of {{Noise}}},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2015.2468676},
  abstract = {Support recovery of sparse signals from compressed linear measurements is a fundamental problem in compressed sensing (CS). In this article, we study the orthogonal matching pursuit (OMP) algorithm for the recovery of support under noise. We consider two signal-to-noise ratio (SNR) settings: 1) the SNR depends on the sparsity level K of input signals, and 2) the SNR is an absolute constant independent of K. For the first setting, we establish necessary and sufficient conditions for the exact support recovery with OMP, expressed as lower bounds on the SNR. Our results indicate that in order to ensure the exact support recovery of all K-sparse signals with the OMP algorithm, the SNR must at least scale linearly with the sparsity level K. In the second setting, since the necessary condition on the SNR is not fulfilled, the exact support recovery with OMP is impossible. However, our analysis shows that recovery with an arbitrarily small but constant fraction of errors is possible with the OMP algorithm. This result may be useful for some practical applications where obtaining some large fraction of support positions is adequate.},
  timestamp = {2016-06-27T11:14:23Z},
  number = {21},
  journal = {IEEE Transactions on Signal Processing},
  author = {Wang, J.},
  month = nov,
  year = {2015},
  keywords = {approximation theory,compressed linear measurements,compressed sensing,Compressed sensing (CS),K-sparse signals,Matching pursuit algorithms,minimum-to-average ratio (MAR),Noise measurement,OMP algorithm,orthogonal matching pursuit algorithm,orthogonal matching pursuit (OMP),restricted isometry property (RIP),Signal processing algorithms,Signal to noise ratio,signal-to-noise ratio (SNR),Sparse matrices,Support recovery},
  pages = {5868--5877},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\6Z7V27H7\\articleDetails.html:text/html}
}

@article{Determe2016,
  title = {On {{The Exact Recovery Condition}} of {{Simultaneous Orthogonal Matching Pursuit}}},
  volume = {23},
  issn = {1070-9908},
  doi = {10.1109/LSP.2015.2506989},
  abstract = {Several exact recovery criteria (ERC) ensuring that orthogonal matching pursuit (OMP) identifies the correct support of sparse signals have been developed in the last few years. These ERC rely on the restricted isometry property (RIP), the associated restricted isometry constant (RIC) and sometimes the restricted orthogonality constant (ROC). In this paper, three of the most recent ERC for OMP are examined. The contribution is to show that these ERC remain valid for a generalization of OMP, entitled simultaneous orthogonal matching pursuit (SOMP), that is capable to process several measurement vectors simultaneously and return a common support estimate for the underlying sparse vectors. The sharpness of the bounds is also briefly discussed in light of previous works focusing on OMP.},
  timestamp = {2016-07-10T07:16:40Z},
  number = {1},
  journal = {IEEE Signal Processing Letters},
  author = {Determe, J. F. and Louveaux, J. and Jacques, L. and Horlin, F.},
  month = jan,
  year = {2016},
  keywords = {Atomic measurements,compressed sensing,exact recovery condition,exact recovery criteria,iterative methods,Matching pursuit algorithms,Mathematical model,measurement vectors,restricted isometry constant,restricted isometry property,restricted orthogonality constant,signal processing,Signal processing algorithms,simultaneous orthogonal matching pursuit,SOMP,sparse signals,sparse vectors,time-frequency analysis,Vectors},
  pages = {164--168},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ASE7G4JC\\articleDetails.html:text/html;IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\N7DK5A4X\\articleDetails.html:text/html}
}

@article{Ding2013c,
  title = {Perturbation {{Analysis}} of {{Orthogonal Matching Pursuit}}},
  volume = {61},
  issn = {1053-587X},
  doi = {10.1109/TSP.2012.2222377},
  abstract = {Orthogonal Matching Pursuit (OMP) is a canonical greedy pursuit algorithm for sparse approximation. Previous studies of OMP have considered the recovery of a sparse signal through $\Phi$ and y = $\Phi$x + b, where is a matrix with more columns than rows and denotes the measurement noise. In this paper, based on Restricted Isometry Property (RIP), the performance of OMP is analyzed under general perturbations, which means both y and $\Phi$ are perturbed. Though the exact recovery of an almost sparse signal x is no longer feasible, the main contribution reveals that the support set of the best k-term approximation of x can be recovered under reasonable conditions. The error bound between x and the estimation of OMP is also derived. By constructing an example it is also demonstrated that the sufficient conditions for support recovery of the best k-term approximation of are rather tight. When x is strong-decaying, it is proved that the sufficient conditions for support recovery of the best k-term approximation of x can be relaxed, and the support can even be recovered in the order of the entries' magnitude. Our results are also compared in detail with some related previous ones.},
  timestamp = {2016-06-27T11:22:29Z},
  number = {2},
  journal = {IEEE Transactions on Signal Processing},
  author = {Ding, J. and Chen, L. and Gu, Y.},
  month = jan,
  year = {2013},
  keywords = {Approximation methods,best k term approximation,canonical greedy pursuit algorithm,compressed sensing,Compressed sensing (CS),general perturbation,general perturbations,Greedy algorithms,iterative methods,Matching pursuit algorithms,measurement noise,Noise,orthogonal matching pursuit,orthogonal matching pursuit (OMP),perturbation analysis,perturbation techniques,Pollution measurement,restricted isometry property,restricted isometry property (RIP),sensors,Signal processing algorithms,sparse approximation,sparse signal,strong-decaying signals,Support recovery,Vectors},
  pages = {398--410},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RDAH2CED\\articleDetails.html:text/html}
}

@article{Herzet2016,
  title = {Relaxed {{Recovery Conditions}} for {{OMP}}/{{OLS}} by {{Exploiting Both Coherence}} and {{Decay}}},
  volume = {62},
  issn = {0018-9448},
  doi = {10.1109/TIT.2015.2490660},
  abstract = {We propose extended coherence-based conditions for exact sparse support recovery using orthogonal matching pursuit and orthogonal least squares. Unlike standard uniform guarantees, we embed some information about the decay of the sparse vector coefficients in our conditions. As a result, the standard condition $\mu$ $<$; 1/(2k - 1) (where $\mu$ denotes the mutual coherence and k the sparsity level) can be weakened as soon as the nonzero coefficients obey some decay, both in the noiseless and the bounded-noise scenarios. Furthermore, the resulting condition is approaching $\mu$ $<$; 1/k for strongly decaying sparse signals. Finally, in the noiseless setting, we prove that the proposed conditions, in particular the bound $\mu$ $<$; 1/k, are the tightest achievable guarantees based on mutual coherence.},
  timestamp = {2016-06-27T11:24:15Z},
  number = {1},
  journal = {IEEE Transactions on Information Theory},
  author = {Herzet, C. and Dr{\'e}meau, A. and Soussen, C.},
  month = jan,
  year = {2016},
  keywords = {Algorithm design and analysis,Electronic mail,exact recovery,extended coherence-based condition,least squares approximations,Least squares methods,Matching pursuit algorithms,mutual coherence,OFDM,OLS,OMP,orthogonal least square,orthogonal least-squares,orthogonal leastsquares,orthogonal matching pursuit,Probabilistic logic,relaxed recovery condition,signal processing,sparse decaying representations,sparse signal decaying,sparse vector coefficient decay},
  pages = {459--470},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9SIS7R34\\articleDetails.html:text/html}
}

@inproceedings{Ding2012a,
  title = {Robustness of Orthogonal Matching Pursuit for Multiple Measurement Vectors in Noisy Scenario},
  doi = {10.1109/ICASSP.2012.6288748},
  abstract = {In this paper, we consider orthogonal matching pursuit (OMP) algorithm for multiple measurement vectors (MMV) problem. The robustness of OMPMMV is studied under general perturbations-when the measurement vectors as well as the sensing matrix are incorporated with additive noise. The main result shows that although exact recovery of the sparse solutions is unrealistic in noisy scenario, recovery of the support set of the solutions is guaranteed under suitable conditions. Specifically, a sufficient condition is derived that guarantees exact recovery of the sparse solutions in noiseless scenario.},
  timestamp = {2016-06-27T11:26:24Z},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ding, J. and Chen, L. and Gu, Y.},
  month = mar,
  year = {2012},
  keywords = {Additive noise,Approximation algorithms,compressed sensing,compressive sensing,compressive sensing (CS),CS,general perturbations,iterative methods,Matching pursuit algorithms,MMV problem,multiple measurement vector problem,Multiple measurement vectors (MMV),OMP algorithm,orthogonal matching pursuit al- gorithm,orthogonal matching pursuit (OMP),perturbation,perturbation theory,Robustness,sensing matrix,sensors,signal reconstruction,Sparse matrices,sparse solution recovery,time-frequency analysis,Upper bound,Vectors},
  pages = {3813--3816},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F2R9JARV\\articleDetails.html:text/html}
}

@inproceedings{Ding2012b,
  title = {Performance Analysis of {{Orthogonal Matching Pursuit}} under General Perturbations},
  doi = {10.1109/ICCNC.2012.6167553},
  abstract = {As a canonical greedy algorithm, Orthogonal Matching Pursuit (OMP) is used for sparse approximation. Previous studies have mainly considered non-perturbed observations y = $\Phi$x, and focused on the exact recovery of x through y and $\Phi$. Here, $\Phi$ is a matrix with more columns than rows, and x is a sparse signal to be recovered. This paper deals with performance of OMP under general perturbations - from both y and $\Phi$. The main contribution shows that exact recovery of the support set of x can be guaranteed under suitable conditions. Such conditions are RIP-based, and involve the concept of sparsity, relative perturbation, and the smallest nonzero entry. In addition, certain conditions are given under which the support set of x can be reconstructed in the order of its entries' magnitude. In the end, it is pointed out that the conditions can be relaxed at the expense of a decrease in the accuracy of the recovery.},
  timestamp = {2016-06-27T11:27:17Z},
  booktitle = {2012 {{International Conference}} on {{Computing}}, {{Networking}} and {{Communications}} ({{ICNC}})},
  author = {Ding, J. and Chen, L. and Gu, Y.},
  month = jan,
  year = {2012},
  keywords = {Accuracy,approximation theory,canonical greedy algorithm,compressed sensing,general perturbations,Greedy algorithms,information theory,iterative methods,Matching pursuit algorithms,matrix,Noise,nonzero entry,orthogonal matching pursuit,orthogonal matching pursuit (OMP),Performance analysis,relative perturbation,restricted isometry property (RIP),RIP-based condition,signal reconstruction,sparse approximation,Sparse matrices,sparse signal recovery,sparsity concept,Support recovery,Vectors},
  pages = {892--896},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XXQT88Q5\\articleDetails.html:text/html}
}

@inproceedings{Wang2013e,
  title = {Performance of Orthogonal Matching Pursuit for Multiple Measurement Vectors with Noise},
  doi = {10.1109/ChinaSIP.2013.6625299},
  abstract = {Orthogonal matching pursuit (OMP) algorithm for the multiple measurement vectors (MMV) is a greedy method to find the sparse matrix with few nonzero rows that represents the measurement vectors under the sensing matrix. This paper analyzes the recovery performance of OMP for MMV (OMPMMV) in the bounded noise scenarios, and provides the sufficient conditions that are related to the sensing matrix and sparse matrix for exact support recovery. We start with the intuitive sufficient conditions for exact support recovery, and then apply these conditions to scenarios of two types of bounded noise. The results show that under some conditions on the coherence of the sensing matrix and the minimum $\mathscr{l}$2 norm of any nonzero row vector from the sparse matrix, exact support recovery of sparse matrix can be guaranteed.},
  timestamp = {2016-06-27T11:29:25Z},
  booktitle = {2013 {{IEEE China Summit International Conference}} on {{Signal}} and {{Information Processing}} ({{ChinaSIP}})},
  author = {Wang, Y. and Fu, T. and Gao, M. and Ding, S.},
  month = jul,
  year = {2013},
  keywords = {bounded noise,bounded noise scenarios,Coherence,compressed sensing,exact support recovery,greedy method,iterative methods,Matching pursuit algorithms,matrix algebra,measurement errors,minimum ℓ2 norm,Multiple measurement vectors,Multiple measurement vectors (MMV),Noise,Noise measurement,nonzero rows,OMPMMV algorihm,orthogonal matching pursuit algorithm,orthogonal matching pursuit (OMP),sensing matrix,sensors,Sparse matrices,sparse matrix,Sufficient conditions,Vectors},
  pages = {67--71},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P9P5Q5DG\\articleDetails.html:text/html}
}

@article{Tawfic2015,
  title = {Strong Recovery Conditions for Least Support Orthogonal Matching Pursuit in Noisy Case},
  volume = {51},
  issn = {0013-5194},
  doi = {10.1049/el.2015.0222},
  abstract = {A least support denoising-orthogonal matching pursuit (LSD-OMP) algorithm to reconstruct the sparse signal using less number of iterations from noisy measurements is presented. The algorithm achieves correct support recovery without requiring sparsity knowledge. An improved restricted isometry property-based condition is derived over the best-known results. Experimental results demonstrate that the LSD-OMP achieves good performance on recovering sparse signals, outperforming the latest state-of-the art method in terms of reconstructed signal-to-noise ratio and running time.},
  timestamp = {2016-06-27T11:30:48Z},
  number = {17},
  journal = {Electronics Letters},
  author = {Tawfic, I. S. and Kayhan, S. Ko{\c c}},
  year = {2015},
  keywords = {isometry property,iterative methods,least support denoising orthogonal matching pursuit,LSD-OMP algorithm,noisy case,noisy measurements,signal reconstruction,sparse signal reconstruction,sparsity knowledge},
  pages = {1368--1370},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5IV2GVC5\\articleDetails.html:text/html}
}

@article{Lee2016,
  title = {{{MAP Support Detection}} for {{Greedy Sparse Signal Recovery Algorithms}} in {{Compressive Sensing}}},
  volume = {PP},
  issn = {1053-587X},
  doi = {10.1109/TSP.2016.2580527},
  abstract = {A reliable support detection is essential for a greedy algorithm to reconstruct a sparse signal accurately from compressed and noisy measurements. This paper proposes a novel support detection method for greedy algorithms, which is referred to as ``maximum a posteriori (MAP) support detection''. Unlike existing support detection methods that identify support indices with the largest correlation value in magnitude per iteration, the proposed method selects them with the largest likelihood ratios computed under the true and null support hypotheses by simultaneously exploiting the distributions of a sensing matrix, a sparse signal, and noise. Leveraging this technique, MAP-Matching Pursuit (MAP-MP) is first presented to show the advantages of exploiting the proposed support detection method, and a sufficient condition for perfect signal recovery is derived for the case when the sparse signal is binary. Subsequently, a set of iterative greedy algorithms, called MAPgeneralized Orthogonal Matching Pursuit (MAP-gOMP), MAPCompressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-Subspace Pursuit (MAP-SP) are presented to demonstrate the applicability of the proposed support detection method to existing greedy algorithms. From empirical results, it is shown that the proposed greedy algorithms with highly reliable support detection can be better, faster, and easier to implement than basis pursuit via linear programming.},
  timestamp = {2016-06-27T11:32:07Z},
  number = {99},
  journal = {IEEE Transactions on Signal Processing},
  author = {Lee, N.},
  year = {2016},
  keywords = {Correlation,Greedy algorithms,Matching pursuit algorithms,Noise measurement,sensors,Signal processing algorithms,Sparse matrices},
  pages = {1--1},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\I975AP6U\\articleDetails.html:text/html}
}

@inproceedings{Pal2013a,
  title = {Conditions for Identifiability in Sparse Spatial Spectrum Sensing},
  abstract = {Spatial Spectrum estimation is a key technique used in a wide variety of problems arising in signal processing and communication, particularly those employing multiple antennas. In many scenarios such as direction finding using antenna arrays, it is crucial to estimate which directions in space contribute to active sources (indicated by a non zero power). It has been recently shown that if the sources from different directions are statistically uncorrelated, it is possible to identify as many as O(M2) active sources using only M physical antennas. A sparse representation for the spatial spectrum was further exploited to reconstruct the spectrum using convex optimization techniques. In this paper, we consider the situation when there is non zero cross correlation between the sources impinging from different directions. We investigate if, fundamentally, it still possible to identify more sources than the number of physical sensors and what role the cross correlation terms play. Recovery guarantees are developed to ensure uniqueness of the sparse representation for spectrum sensing. They are further extended to establish conditions under which a greedy heuristic, namely the Orthogonal Matching Pursuit algorithm will successfully recover the sparse spectrum. It is shown that in both cases, it is possible to recover support of larger size provided the correlation terms are small compared to the power of the impinging signals.},
  timestamp = {2016-06-27T11:33:46Z},
  booktitle = {21st {{European Signal Processing Conference}} ({{EUSIPCO}} 2013)},
  author = {Pal, P. and Vaidyanathan, P. P.},
  month = sep,
  year = {2013},
  keywords = {active sources,antenna arrays,convex optimization techniques,convex programming,Correlated Sources,Correlation,Covariance matrices,identifiability,iterative methods,Khatri-Rao Product,Kruskal Rank Orthogonal Matching Pursuit,Matching pursuit algorithms,M physical antennas,multiple antennas,Noise measurement,nonzero cross correlation,orthogonal matching pursuit algorithm,radio spectrum management,sensors,signal detection,signal processing,Sparse matrices,sparse representation,sparse spatial spectrum sensing,Sparse Spectrum Estimation,spatial spectrum estimation,time-frequency analysis,Vectors},
  pages = {1--5},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TFHA7HCI\\articleDetails.html:text/html}
}

@article{Lee2012,
  title = {Subspace {{Methods}} for {{Joint Sparse Recovery}}},
  volume = {58},
  issn = {0018-9448},
  doi = {10.1109/TIT.2012.2189196},
  abstract = {We propose robust and efficient algorithms for the joint sparse recovery problem in compressed sensing, which simultaneously recover the supports of jointly sparse signals from their multiple measurement vectors obtained through a common sensing matrix. In a favorable situation, the unknown matrix, which consists of the jointly sparse signals, has linearly independent nonzero rows. In this case, the MUltiple SIgnal Classification (MUSIC) algorithm, originally proposed by Schmidt for the direction of arrival estimation problem in sensor array processing and later proposed and analyzed for joint sparse recovery by Feng and Bresler, provides a guarantee with the minimum number of measurements. We focus instead on the unfavorable but practically significant case of rank defect or ill-conditioning. This situation arises with a limited number of measurement vectors, or with highly correlated signal components. In this case, MUSIC fails and, in practice, none of the existing methods can consistently approach the fundamental limit. We propose subspace-augmented MUSIC (SA-MUSIC), which improves on MUSIC such that the support is reliably recovered under such unfavorable conditions. Combined with a subspace-based greedy algorithm, known as Orthogonal Subspace Matching Pursuit, which is also proposed and analyzed in this paper, SA-MUSIC provides a computationally efficient algorithm with a performance guarantee. The performance guarantees are given in terms of a version of the restricted isometry property. In particular, we also present a non-asymptotic perturbation analysis of the signal subspace estimation step, which has been missing in the previous studies of MUSIC.},
  timestamp = {2016-06-27T11:34:58Z},
  number = {6},
  journal = {IEEE Transactions on Information Theory},
  author = {Lee, K. and Bresler, Y. and Junge, M.},
  month = jun,
  year = {2012},
  keywords = {Arrays,compressed sensing,Direction of arrival estimation,direction-of-arrival estimation,Estimation,Greedy algorithms,jointly sparse signals,Joints,joint sparse recovery,joint sparsity,matrix algebra,Multiple measurement vectors (MMV),Multiple signal classification,multiple signal classification algorithm,non-asymptotic perturbation analysis,orthogonal subspace matching pursuit,perturbation theory,restricted isometry property (RIP),sensor array processing,signal classification,Sparse matrices,spectrum-blind sampling,subspace-based greedy algorithm,subspace estimation,subspace methods,Vectors},
  pages = {3613--3641},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IA8TSA3D\\articleDetails.html:text/html}
}

@article{Shen2014a,
  title = {Analysis of Generalised Orthogonal Matching Pursuit Using Restricted Isometry Constant},
  volume = {50},
  issn = {0013-5194},
  doi = {10.1049/el.2014.1012},
  abstract = {In compressive sensing, the generalised orthogonal matching pursuit (gOMP) algorithm is one kind of sparse signal recovery algorithm, which generalises the OMP algorithm by selecting a fixed number of atoms at each iteration. Restricted isometry constant-based sufficient conditions to guarantee the correct support identification and the successful recovery of a sparse signal using the gOMP algorithm in a noiseless case are proposed. The proposed sufficient bounds are more relaxed compared with the existing ones.},
  timestamp = {2016-06-27T11:36:25Z},
  number = {14},
  journal = {Electronics Letters},
  author = {Shen, Y. and Li, B. and Pan, W. and Li, J.},
  month = jul,
  year = {2014},
  keywords = {compressed sensing,compressive sensing,generalised orthogonal matching pursuit algorithm,gOMP algorithm,pattern matching,restricted isometry constant,sparse signal recovery algorithm},
  pages = {1020--1022},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\37HH38QW\\articleDetails.html:text/html}
}

@inproceedings{Hu2015,
  title = {An {{Orthogonal Matching Pursuit}} with {{Thresholding Algorithm}} for {{Block}}-{{Sparse Signal Recovery}}},
  doi = {10.1109/ISCMI.2015.36},
  abstract = {In this paper, a block version of the orthogonal matching pursuit with thresholding algorithm is proposed. Compared with the block version of the orthogonal matching pursuit algorithm, the block orthogonal matching pursuit algorithm works in a less greedy fashion in order to improve support estimating efficiency in each iteration. The lower and upper bounds of the threshold are theoretical derived for exact recovering the block-sparse signal in noiseless environment. When the noise is bounded with unknown distribution, the algorithm can also estimate the support exactly under certain ssumptions. Experimental results are provided to illustrate the validity of our main results.},
  timestamp = {2016-06-27T11:37:34Z},
  booktitle = {2015 {{Second International Conference}} on {{Soft Computing}} and {{Machine Intelligence}} ({{ISCMI}})},
  author = {Hu, R. and Xiang, Y. and Fu, Y. and Rong, R. and Chen, Z.},
  month = nov,
  year = {2015},
  keywords = {Artificial Intelligence,block orthogonal matching pursuit algorithm,block-sparse signal,block-sparse signal recovery,bounded noise,Coherence,compressed sensing,compressive sensing,Dictionaries,Electronic mail,Indexes,iterative methods,Matching pursuit algorithms,Noise,noiseless environment,OMP,Thresholding,thresholding algorithm,Upper bound},
  pages = {56--59},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JXP2T2XX\\articleDetails.html:text/html}
}

@article{Plumbley2007,
  title = {On {{Polar Polytopes}} and the {{Recovery}} of {{Sparse Representations}}},
  volume = {53},
  issn = {0018-9448},
  doi = {10.1109/TIT.2007.903129},
  abstract = {Suppose we have a signal y which we wish to represent using a linear combination of a number of basis atoms ai,y=Sigmaixiai=Ax. The problem of finding the minimum l0 norm representation for y is a hard problem. The basis pursuit (BP) approach proposes to find the minimum l1 norm representation instead, which corresponds to a linear program (LP) that can be solved using modern LP techniques, and several recent authors have given conditions for the BP (minimum l1 norm) and sparse (minimum l0 norm) representations to be identical. In this paper, we explore this sparse representation problem using the geometry of convex polytopes, as recently introduced into the field by Donoho. By considering the dual LP we find that the so-called polar polytope P* of the centrally symmetric polytope P whose vertices are the atom pairs plusmnai is particularly helpful in providing us with geometrical insight into optimality conditions given by Fuchs and Tropp for non-unit-norm atom sets. In exploring this geometry, we are able to tighten some of these earlier results, showing for example that a condition due to Fuchs is both necessary and sufficient for l1-unique-optimality, and there are cases where orthogonal matching pursuit (OMP) can eventually find all l1-unique-optimal solutions with m nonzeros even if the exact recover condition (ERC) fails for m.},
  timestamp = {2016-06-27T11:41:51Z},
  number = {9},
  journal = {IEEE Transactions on Information Theory},
  author = {Plumbley, M. D.},
  month = sep,
  year = {2007},
  keywords = {basis pursuit approach,Basis Pursuit (BP),convex polytopes,Dictionaries,exact recover condition,Geometry,information theory,iterative methods,linear program,linear programming,Matching pursuit algorithms,Optimization methods,orthogonal matching pursuit,orthogonal matching pursuit (OMP),Pattern Recognition,polar polytopes,polytopes,signal processing,Sparse matrices,sparse representations,Statistical learning,Vectors},
  pages = {3188--3195},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VJE2UNWD\\articleDetails.html:text/html}
}

@article{Blanchard2014,
  title = {Greedy {{Algorithms}} for {{Joint Sparse Recovery}}},
  volume = {62},
  issn = {1053-587X},
  doi = {10.1109/TSP.2014.2301980},
  abstract = {Five known greedy algorithms designed for the single measurement vector setting in compressed sensing and sparse approximation are extended to the multiple measurement vector scenario: Iterative Hard Thresholding (IHT), Normalized IHT (NIHT), Hard Thresholding Pursuit (HTP), Normalized HTP (NHTP), and Compressive Sampling Matching Pursuit (CoSaMP). Using the asymmetric restricted isometry property (ARIP), sufficient conditions for all five algorithms establish bounds on the discrepancy between the algorithms' output and the optimal row-sparse representation. When the initial multiple measurement vectors are jointly sparse, ARIP-based guarantees for exact recovery are also established. The algorithms are then compared via the recovery phase transition framework. The strong phase transitions describing the family of Gaussian matrices which satisfy the sufficient conditions are obtained via known bounds on the ARIP constants. The algorithms' empirical weak phase transitions are compared for various numbers of multiple measurement vectors. Finally, the performance of the algorithms is compared against a known rank aware greedy algorithm, Rank Aware Simultaneous Orthogonal Matching Pursuit + MUSIC. Simultaneous recovery variants of NIHT, NHTP, and CoSaMP all outperform the rank-aware algorithm.},
  timestamp = {2016-06-27T11:43:10Z},
  number = {7},
  journal = {IEEE Transactions on Signal Processing},
  author = {Blanchard, J. D. and Cermak, M. and Hanle, D. and Jing, Y.},
  month = apr,
  year = {2014},
  keywords = {Algorithm design and analysis,Approximation algorithms,Approximation methods,approximation theory,ARIP constants,asymmetric restricted isometry property,compressed sensing,compressive sampling matching pursuit,CoSaMP,Gaussian matrices,Greedy algorithms,hard thresholding pursuit,iterative hard thresholding,iterative methods,joint sparse recovery,joint sparsity,Matching pursuit algorithms,matrix algebra,Multiple measurement vectors,NHTP,normalized HTP,normalized IHT,optimal row-sparse representation,performance comparison,recovery phase transition framework,row sparse matrices,Sampling methods,Signal processing algorithms,single measurement vector setting,sparse approximation,Vectors},
  pages = {1694--1704},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\STIKKMXM\\articleDetails.html:text/html}
}

@article{Wen2015,
  title = {A {{Sharp Condition}} for {{Exact Support Recovery}} of {{Sparse Signals}} with {{Orthogonal Matching Pursuit}}},
  abstract = {Support recovery of sparse signals from noisy measurements with orthogonal matching pursuit (OMP) have been extensively studied in the literature. In this paper, we show that for any \$K\$-sparse signal \$$\backslash$x\$, if the sensing matrix \$$\backslash$A\$ satisfies the restricted isometry property (RIP) with restricted isometry constant \$$\backslash$delta\_\{K+1\} $<$ 1/$\backslash$sqrt \{K+1\}\$, then under some constraints on the minimum magnitude of the nonzero elements of \$$\backslash$x\$, OMP exactly recovers the support of \$$\backslash$x\$ from the measurements \$$\backslash$y=$\backslash$A$\backslash$x+$\backslash$v\$ in \$K\$ iterations, where \$$\backslash$v\$ is a noise vector that is \$l\_2\$ or \$l\_\{$\backslash$infty\}\$ bounded. The RIP condition is sharp in terms of \$$\backslash$delta\_\{K+1\}\$ since for any given positive integer \$K$\backslash$geq 2\$ and any \$1/$\backslash$sqrt\{K+1\}$\backslash$leq t$<$1\$, there always exist a \$K\$-sparse \$$\backslash$x\$ and a matrix \$$\backslash$A\$ satisfying \$$\backslash$delta\_\{K+1\}=t\$, for which OMP may fail to recover the signal \$$\backslash$x\$ in \$K\$ iterations. Also, our constraints on the minimum magnitude of nonzero elements of \$$\backslash$x\$ are much weaker than existing ones. Moreover, we propose necessary conditions for exact support recovery of \$$\backslash$x\$ on the minimum magnitude of the nonzero elements of \$$\backslash$x\$.},
  timestamp = {2016-06-27T11:45:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.07248},
  primaryClass = {cs, math},
  urldate = {2016-06-27},
  journal = {arXiv:1512.07248 [cs, math]},
  author = {Wen, Jinming and Zhou, Zhengchun and Wang, Jian and Tang, Xiaohu and Mo, Qun},
  month = dec,
  year = {2015},
  keywords = {Computer Science - Information Theory},
  annote = {Comment: Part of the results will appear in ISIT 2016},
  file = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\A8FEAECS\\1512.html:text/html}
}

@inproceedings{Scarlett2015,
  title = {Limits on Support Recovery with Probabilistic Models: {{An}} Information-Theoretic Framework},
  shorttitle = {Limits on Support Recovery with Probabilistic Models},
  doi = {10.1109/ISIT.2015.7282872},
  abstract = {The support recovery problem consists of determining a sparse subset of a set of variables that is relevant in generating a set of observations, and arises in a diverse range of settings such as group testing, compressive sensing, and subset selection in regression. In this paper, we provide a unified approach to support recovery problems, considering general probabilistic observation models relating a sparse data vector to an observation vector. We study the information-theoretic limits for both exact and partial support recovery, taking a novel approach motivated by thresholding techniques in channel coding. We provide general achievability and converse bounds characterizing the trade-off between the error probability and number of measurements, and we specialize these bounds the linear and 1-bit compressive sensing models. Our conditions not only provide scaling laws, but also explicit matching or near-matching constant factors. Moreover, our converse results not only provide conditions under which the error probability fails to vanish, but also conditions under which it tends to one.},
  timestamp = {2016-06-27T12:16:07Z},
  booktitle = {2015 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Scarlett, J. and Cevher, V.},
  month = jun,
  year = {2015},
  keywords = {channel coding,compressed sensing,Computational modeling,Decoding,Error probability,exact support recovery,general probabilistic observation models,information theory,Mutual information,observation vector,partial support recovery,probabilistic model,probability,Random variables,signal processing,sparse data vector,support recovery problem,Testing},
  pages = {2331--2335},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IC9N94KT\\articleDetails.html:text/html}
}

@inproceedings{Wang2008a,
  title = {Information-Theoretic Limits on Sparse Support Recovery: {{Dense}} versus Sparse Measurements},
  shorttitle = {Information-Theoretic Limits on Sparse Support Recovery},
  doi = {10.1109/ISIT.2008.4595380},
  abstract = {We study the information-theoretic limits of exactly recovering the support of a sparse signal using noisy projections defined by various classes of measurement matrices. Our analysis is high-dimensional in nature, in which the number of observations n, the ambient signal dimension p, and the signal sparsity k are all allowed to tend to infinity in a general manner. This paper makes two novel contributions. First, we provide sharper necessary conditions for exact support recovery using general (non-Gaussian) dense measurement matrices. Combined with previously known sufficient conditions, this result yields a sharp characterization of when the optimal decoder can recover a signal with linear sparsity (k = Theta(p)) using a linear scaling of observations (n = Theta(p)) in the presence of noise. Our second contribution is to prove necessary conditions on the number of observations n required for asymptotically reliable recovery using a class of gamma-sparsified measurement matrices, where the measurement sparsity gamma(n, p, k) isin (0,1] corresponds to the fraction of non-zero entries per row. Our analysis allows general scaling of the quadruplet (n, p, k, gamma), and reveals three different regimes, corresponding to whether measurement sparsity has no effect, a minor effect, or a dramatic effect on the information theoretic limits of the subset recovery problem.},
  timestamp = {2016-06-27T12:24:09Z},
  booktitle = {2008 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Wang, Wei and Wainwright, M. J. and Ramchandran, K.},
  month = jul,
  year = {2008},
  keywords = {Additive noise,ambient signal dimension,computational complexity,Decoding,Electric variables measurement,exact support recovery,Gaussian noise,general dense measurement matrices,H infinity control,Information analysis,information-theoretic limits,information theory,matrix algebra,measurement matrices,noisy projections,nonGaussian dense measurement matrices,optimal decoder,Signal analysis,signal processing,signal sparsity,Sparse matrices,sparse measurements,sparse support recovery,Sufficient conditions},
  pages = {2197--2201},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SKUZZVFF\\articleDetails.html:text/html}
}

@inproceedings{Karbasi2009,
  title = {Support Recovery in Compressed Sensing: {{An}} Estimation Theoretic Approach},
  shorttitle = {Support Recovery in Compressed Sensing},
  doi = {10.1109/ISIT.2009.5206485},
  abstract = {Compressed sensing (CS) deals with the reconstruction of sparse signals from a small number of linear measurements. One of the main challenges in CS is to find the support of a sparse signal from a set of noisy observations. In the CS literature, several information-theoretic bounds on the scaling law of the required number of measurements for exact support recovery have been derived, where the focus is mainly on random measurement matrices. In this paper, we investigate the support recovery problem from an estimation theory point of view, where no specific assumption is made on the underlying measurement matrix. By using the Hammersley-Chapman-Robbins (HCR) bound, we derive a fundamental lower bound on the performance of any unbiased estimator which provides necessary conditions for reliable {\^A}\textquestiondown{}2-norm support recovery. We then analyze the optimal decoder to provide conditions under which the HCR bound is achievable. This leads to a set of sufficient conditions for reliable {\^A}\textquestiondown{}2-norm support recovery.},
  timestamp = {2016-06-27T12:30:10Z},
  booktitle = {2009 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Karbasi, A. and Hormati, A. and Mohajer, S. and Vetterli, M.},
  month = jun,
  year = {2009},
  keywords = {compressed sensing,computational complexity,Decoding,estimation theoretic approach,estimation theory,Hammersley-Chapman-Robbins bound,information-theoretic bounds,information theory,linear measurements,matrix algebra,measurement matrix,Measurement standards,optimal decoder,Reconstruction algorithms,Robustness,Sampling methods,signal reconstruction,Sparse matrices,Sufficient conditions,support recovery problem},
  pages = {679--683},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\M9TZD6BC\\articleDetails.html:text/html}
}

@article{Malloy2014,
  title = {Sequential {{Testing}} for {{Sparse Recovery}}},
  volume = {60},
  issn = {0018-9448},
  doi = {10.1109/TIT.2014.2363846},
  abstract = {This paper studies sequential methods for recovery of sparse signals in high dimensions. When compared with fixed sample size procedures, in the sparse setting, sequential methods can result in a large reduction in the number of samples needed for reliable signal support recovery. Starting with a lower bound, we show any coordinate-wise sequential sampling procedure fails in the high dimensional limit provided the average number of measurements per dimension is less then log(s)/D(P0||P1), where s is the level of sparsity and D(P0||P1) is the Kullback-Leibler divergence between the underlying distributions. A series of sequential probability ratio tests, which require complete knowledge of the underlying distributions is shown to achieve this bound. Motivated by real-world experiments and recent work in adaptive sensing, we introduce a simple procedure termed sequential thresholding, which can be implemented when the underlying testing problem satisfies a monotone likelihood ratio assumption. Sequential thresholding guarantees exact support recovery provided the average number of measurements per dimension grows faster than log(s)/D(P0||P1), achieving the lower bound. For comparison, we show any nonsequential procedure fails provided the number of measurements grows at a rate less than log(n)/D(P1||P0), where n is the total dimension of the problem.},
  timestamp = {2016-06-27T13:26:39Z},
  number = {12},
  journal = {IEEE Transactions on Information Theory},
  author = {Malloy, M. L. and Nowak, R. D.},
  month = dec,
  year = {2014},
  keywords = {adaptive sensing,compressed sensing,coordinate-wise sequential sampling procedure,Extraterrestrial measurements,fixed sample size procedures,Frequency measurement,high dimensional limit,Indexes,Kullback-Leibler divergence,monotone likelihood ratio assumption,multi-armed bandits,probability,Random variables,Reliability,sensors,Sequential analysis,sequential probability ratio tests,sequential testing,sequential thresholding,Signal sampling,sparse recovery,sparse signal recovery,SPRT,Testing},
  pages = {7862--7873},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\X792RZPI\\articleDetails.html:text/html}
}

@inproceedings{Malloy2011,
  title = {On the Limits of Sequential Testing in High Dimensions},
  doi = {10.1109/ACSSC.2011.6190215},
  abstract = {This paper presents results pertaining to sequential methods for support recovery of sparse signals in noise. Specifically, we show that any sequential measurement procedure fails provided the average number of measurements per dimension grows slower then D(f0$\parallel$f1)-1 log s where s is the level of sparsity, and D(f0$\parallel$f1) the Kullback-Leibler divergence between the underlying distributions. For comparison, we show any non-sequential procedure fails provided the number of measurements grows at a rate less than D(f1$\parallel$f0)-1 log n, where n is the total dimension of the problem. Lastly, we show that a simple procedure termed sequential thresholding guarantees exact support recovery provided the average number of measurements per dimension grows faster than D(f0$\parallel$f1)-1(log s+log log n), a mere additive factor more than the lower bound.},
  timestamp = {2016-06-27T13:27:58Z},
  booktitle = {2011 {{Conference Record}} of the {{Forty Fifth Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}} ({{ASILOMAR}})},
  author = {Malloy, M. and Nowak, R.},
  month = nov,
  year = {2011},
  keywords = {additive factor,Additives,Error analysis,Error probability,high dimensions,Indexes,Kullback-Leibler divergence,Measurement uncertainty,nonsequential procedure,probability,sequential measurement procedure,sequential testing,sequential thresholding,signal denoising,signal representation,Size measurement,sparse signal recovery,Support recovery},
  pages = {1245--1249},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NT2G6W2M\\articleDetails.html:text/html}
}

@inproceedings{Omidiran2008,
  title = {High-Dimensional Subset Recovery in Noise: {{Sparse}} Measurements and Statistical Efficiency},
  shorttitle = {High-Dimensional Subset Recovery in Noise},
  doi = {10.1109/ISIT.2008.4595379},
  abstract = {We consider the problem of estimating the support of a vector beta* isin R" W based on observations contaminated by noise. A significant body of work has studied behavior of lscr1 -relaxations when applied to measurement matrices drawn from standard dense ensembles (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsified measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction 7 of non-zero entries, and the statistical efficiency, as measured by the minimal number of observations n required for exact support recovery with probability converging to one. Our main result is to prove that it is possible to let gamma rarr 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row while retaining the same statistical efficiency (sample size n) as dense ensembles. A variety of simulation results confirm the sharpness of our theoretical predictions.},
  timestamp = {2016-06-27T13:29:47Z},
  booktitle = {2008 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Omidiran, D. and Wainwright, M. J.},
  month = jul,
  year = {2008},
  keywords = {Additive noise,compressed sensing,high-dimensional subset recovery,linear programming,Measurement standards,Noise measurement,noisy observation,Particle measurements,Pollution measurement,probability,set theory,signal denoising,signal reconstruction,signal recovery,Size measurement,Sparse matrices,sparse measurement matrix,sparsified measurement ensemble,statistical efficiency},
  pages = {2192--2196},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F2UH25X5\\articleDetails.html:text/html}
}

@article{Pal2015,
  title = {Pushing the {{Limits}} of {{Sparse Support Recovery Using Correlation Information}}},
  volume = {63},
  issn = {1053-587X},
  doi = {10.1109/TSP.2014.2385033},
  abstract = {A new framework for the problem of sparse support recovery is proposed, which exploits statistical information about the unknown sparse signal in the form of its correlation. A key contribution of this paper is to show that if existing algorithms can recover sparse support of size s, then using such correlation information, the guaranteed size of recoverable support can be increased to O(s2), although the sparse signal itself may not be recoverable. This is proved to be possible by (a) formulating the sparse support recovery problem in terms of the covariance matrix of the measurements, and (b) designing a suitable measurement/sampling matrix which inherently exploits the correlation priors. The so-called Khatri-Rao product of the measurement matrix is shown to play an important role in deciding the level of recoverable sparsity. A systematic analysis of the proposed framework is also presented for the cases when the covariance matrix is only approximately known, by estimating it from finite number of measurements, obtained from the Multiple Measurement Vector (MMV) model. In this case, the use of LASSO on the estimated covariance matrix is proposed for recovering the support. However, the recovery may not be exact and hence a probabilistic guarantee is developed both for sources with arbitrary distribution as well as for Gaussian sources. In the latter case, it is shown that such recovery can happen with overwhelming probability as the number of available measurement vectors increases.},
  timestamp = {2016-06-27T13:32:15Z},
  number = {3},
  journal = {IEEE Transactions on Signal Processing},
  author = {Pal, P. and Vaidyanathan, P. P.},
  month = feb,
  year = {2015},
  keywords = {<sub>1</sub> minimization,arbitrary distribution,compressed sensing,Correlation,correlation information,correlation methods,covariance analysis,Covariance matrices,covariance matrix,Estimation,estimation theory,Gaussian source,joint support recovery,Khatri-Rao Product,Kruskal rank,LASSO,measurement-sampling matrix,MMV model,multiple measurement vector,multiple measurement vector model,probability,Signal processing algorithms,sparse,Sparse matrices,sparse signal recovery,sparse support recovery problem,statistical information,Vectors},
  pages = {711--726},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7AHF4KS9\\articleDetails.html:text/html}
}

@inproceedings{Sturm2012,
  title = {When exact recovery is exact recovery in compressed sensing simulation},
  abstract = {In a simulation of compressed sensing (CS), one must test whether the recovered solution  is the true solution x, i.e., exact recovery. Most CS simulations employ one of two criteria: 1) the recovered support is the true support; or 2) the normalized squared error is less than2. We analyze these exact recovery criteria independent of any recovery algorithm, but with respect to signal distributions that are often used in CS simulations. That is, given a pair (), when does exact recovery occur with respect to only one or both of these criteria for a given distribution of x? We show that, in a best case scenario, 2 sets a maximum allowed missed detection rate in a majority sense.},
  timestamp = {2017-06-23T10:22:05Z},
  booktitle = {Signal Processing Conference (EUSIPCO), 2012 Proceedings of the 20th European},
  author = {Sturm, B. L.},
  month = aug,
  year = {2012},
  keywords = {compressed sensing,compressed sensing,compressed sensing simulation,compressed sensing simulation,CS simulations,CS simulations,Educational
	institutions,Educational institutions,exact recovery,exact recovery,exact recovery criteria,exact recovery criteria,Indexes,Indexes,Laplace equations,Laplace
	equations,maximum allowed missed detection rate,maximum allowed missed detection rate,Noise,Noise,Noise measurement,Noise measurement,normalized squared error,normalized
	squared error,Signal processing algorithms,Signal processing algorithms,signal reconstruction,signal reconstruction},
  pages = {979--983},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8VSJ9Z8Z\\articleDetails.html:text/html}
}

@inproceedings{Tang2011b,
  title = {Verifiable and computable $\ell_\infty$ performance evaluation of $\ell_1$ sparse signal recovery},
  doi = {10.1109/CISS.2011.5766115},
  abstract = {In this paper, we develop verifiable and computable performance analysis of the ℓ∞ norms of the recovery errors for ℓ1 minimization algorithms. We define a family of goodness measures for arbitrary sensing matrices as a set of optimization problems, and design algorithms with a theoretical global convergence guarantee to compute these goodness measures. The proposed algorithms solve a series of second-order cone programs, or linear programs. As a by-product, we implement an efficient algorithm to verify a sufficient condition for exact ℓ1 recovery in the noise-free case. This implementation performs orders-of-magnitude faster than the state-of-the-art techniques. We derive performance bounds on the ℓ∞ norms of the recovery errors in terms of these goodness measures. We establish connections between other performance criteria (e.g., the ℓ2 norm, ℓ1 norm, and support recovery) and the ℓ∞ norm in a tight manner. We also analytically demonstrate that the developed goodness measures are non-degenerate for a large class of random sensing matrices, as long as the number of measurements is relatively large. Numerical experiments show that, compared with the restricted isometry based performance bounds, our error bounds apply to a wider range of problems and are tighter, when the sparsity levels of the signals are relatively low.},
  timestamp = {2016-07-11T17:11:20Z},
  booktitle = {2011 45th Annual Conference on Information Sciences and Systems (CISS)},
  author = {Tang, G. and Nehorai, A.},
  month = mar,
  year = {2011},
  keywords = {Algorithm design and analysis,Algorithm design and analysis,arbitrary sensing matrices,arbitrary sensing matrices,Compressive
	sensing,compressive sensing,computable performance analysis,computable performance analysis,Context,Context,design algorithms,design algorithms,goodness measures,goodness
	measures,linear programs,linear programs,minimisation,minimisation,Minimization,minimization,minimization algorithms,minimization algorithms,Noise
	measurement,Noise measurement,Optimization,Optimization,optimization problems,optimization problems,random sensing matrices,random sensing matrices,recovery errors,recovery
	errors,restricted isometry-based performance bounds,restricted isometry-based performance bounds,second-order cone programs,second-order
	cone programs,Sensors,sensors,signal restoration,signal restoration,signal sparsity level,signal sparsity level,sparse
	matrices,Sparse matrices,sparse signal recovery,sparse signal recovery,theoretical global convergence guarantee,theoretical global convergence guarantee,verifiable
	sufficient condition,verifiable sufficient condition},
  pages = {1-6},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S4MFX4EF\\articleDetails.html:text/html}
}

@article{Lu2012,
  title = {Exact {{Reconstruction Conditions}} for {{Regularized Modified Basis Pursuit}}},
  volume = {60},
  issn = {1053-587X},
  doi = {10.1109/TSP.2012.2186445},
  abstract = {In this work, we obtain sufficient conditions for exact recovery of regularized modified basis pursuit (reg-mod-BP) and discuss when the obtained conditions are weaker than those for modified compressive sensing or for basis pursuit (BP). The discussion is also supported by simulation comparisons. Reg-mod-BP provides a solution to the sparse recovery problem when both an erroneous estimate of the signal's support, denoted by , and an erroneous estimate of the signal values on are available.},
  timestamp = {2016-06-27T13:37:17Z},
  number = {5},
  journal = {IEEE Transactions on Signal Processing},
  author = {Lu, W. and Vaswani, N.},
  month = may,
  year = {2012},
  keywords = {Complexity theory,compressed sensing,compressive sensing,erroneous estimate,estimation theory,exact reconstruction conditions,exact recovery,magnetic resonance imaging,modified compressive sensing,modified-CS,partially known support,reg-mod-BP,regularized modified basis pursuit,signal reconstruction,signal support,signal values,simulation comparisons,Sparse matrices,sparse reconstruction,sparse recovery problem,Sufficient conditions,Support vector machines,Tin,Vectors},
  pages = {2634--2640},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F9Z5D5UU\\articleDetails.html:text/html}
}

@inproceedings{Atia2011,
  title = {A {{Mutual Information Characterization}} for {{Sparse Signal Processing}}},
  timestamp = {2016-06-28T09:26:11Z},
  author = {Atia, G. and Saligrama, V.},
  year = {2011}
}

@article{Reeves2013,
  title = {Approximate {{Sparsity Pattern Recovery}}: {{Information}}-{{Theoretic Lower Bounds}}},
  volume = {59},
  issn = {0018-9448},
  shorttitle = {Approximate {{Sparsity Pattern Recovery}}},
  doi = {10.1109/TIT.2013.2253852},
  abstract = {Recovery of the sparsity pattern (or support) of an unknown sparse vector from a small number of noisy linear measurements is an important problem in compressed sensing. In this paper, the high-dimensional setting is considered. It is shown that if the measurement rate and per-sample signal-to-noise ratio (SNR) are finite constants independent of the length of the vector, then the optimal sparsity pattern estimate will have a constant fraction of errors. Lower bounds on the measurement rate needed to attain a desired fraction of errors are given in terms of the SNR and various key parameters of the unknown vector. The tightness of the bounds in a scaling sense, as a function of the SNR and the fraction of errors, is established by comparison with existing achievable bounds. Near optimality is shown for a wide variety of practically motivated signal models.},
  timestamp = {2016-06-28T09:33:31Z},
  number = {6},
  journal = {IEEE Transactions on Information Theory},
  author = {Reeves, G. and Gastpar, M. C.},
  month = jun,
  year = {2013},
  keywords = {approximate sparsity pattern recovery,approximation theory,compressed sensing,constant fraction,Distortion measurement,entropy,information-theoretic bounds,information theoretic lower bounds,information theory,Noise measurement,noisy linear measurements,optimal sparsity pattern estimation,random matrix theory,Rate-distortion,signal-to-noise ratio,Signal to noise ratio,SNR,sparsity,Support recovery,unknown sparse vector,Vectors},
  pages = {3451--3465},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8GW6UVDF\\articleDetails.html:text/html}
}

@incollection{Ghalehjegh2010,
  series = {Lecture Notes in Computer Science},
  title = {Fast {{Block}}-{{Sparse Decomposition Based}} on {{SL0}}},
  copyright = {\textcopyright{}2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-15994-7 978-3-642-15995-4},
  abstract = {In this paper we present a new algorithm based on Smoothed $\mathscr{l}$0 (SL0), called Block SL0 (BSL0), for Under-determined Systems of Linear Equations (USLE) in which the nonzero elements of the unknown vector occur in clusters. Contrary to the previous algorithms such as Block Orthogonal Matching Pursuit (BOMP) and mixed $\mathscr{l}$2/$\mathscr{l}$1 norm, our approach provides a fast algorithm, while providing the same (or better) accuracy. Moreover, we will see experimentally that BSL0 has better performance than SL0, BOMP and mixed $\mathscr{l}$2/$\mathscr{l}$1 norm when the number of nonzero elements of the source vector approaches the upper bound of uniqueness theorem.},
  language = {en},
  timestamp = {2016-07-01T14:42:57Z},
  number = {6365},
  urldate = {2016-07-01},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ghalehjegh, Sina Hamidi and Babaie-Zadeh, Massoud and Jutten, Christian},
  editor = {Vigneron, Vincent and Zarzoso, Vicente and Moreau, Eric and Gribonval, R{\'e}mi and Vincent, Emmanuel},
  month = sep,
  year = {2010},
  keywords = {Algorithm Analysis and Problem Complexity,Discrete Mathematics in Computer Science,Image Processing and Computer Vision,Pattern Recognition,Simulation and Modeling,Special Purpose and Application-Based Systems},
  pages = {426--433},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\6GXCB892\\10.html:text/html},
  doi = {10.1007/978-3-642-15995-4_53}
}

@techreport{Gallier2015,
  title = {Vector Norms and Matrix Norms},
  timestamp = {2016-07-11T17:11:09Z},
  author = {Gallier, Jean H.},
  year = {2015}
}

@article{Petukhovxxxx,
  title = {$\ell^1$-greedy Algorithm for Finding Solutions of Underdetermined 	Linear Systems},
  timestamp = {2016-07-08T09:11:28Z},
  author = {Petukhov, Alexander},
  year = {xxxx},
  annote = {PPT},
  howpublished = {University of Georgia},
  owner = {Fardin}
}

@article{Gribonval2003,
  title = {Sparse decompositions in "incoherent" dictionaries},
  volume = {1},
  doi = {10.1109/ICIP.2003.1246891},
  abstract = {The purpose of this paper is to generalize a result by Donoho, Huo,
	Elad and Bruckstein on sparse representations of signals/images in
	a union of two orthonormal bases. We consider general (redundant)
	dictionaries in finite dimension, and derive sufficient conditions
	on a signal/image for having a unique sparse representation in such
	a dictionary. In particular, it is proved that the result of Donoho
	and Huo, concerning the replacement of a combinatorial optimization
	problem with a linear programming problem when searching for sparse
	representations, has an analog for dictionaries that may be highly
	redundant. The special case where the dictionary is given by a union
	of several orthonormal bases is studied in more detail and some examples
	are given.},
  timestamp = {2017-04-19T15:13:12Z},
  journal = {in Proc. IEEE Int. Conf. Image Process.},
  author = {Gribonval, R. and Nielsen, M.},
  month = sep,
  year = {2003},
  keywords = {combinatorial mathematics,combinatorial optimization problem,Dictionaries,Hilbert
	space,image coding,Image Processing,image representation,incoherent
	dictionary,linear programming,multidimensional signal processing,Noise
	reduction,NSP,null space property,signal processing,source separation,sparse
	decomposition,Sufficient conditions},
  pages = {I-33--I-36},
  annote = {read},
  owner = {Fardin}
}

@phdthesis{Tropp2004b,
  type = {Computational and Applied Mathematics},
  title = {Topics in sparse approximation},
  abstract = {Sparse approximation problems request a good approximation of an input
	signal as a linear combination of elementary signals, yet they stipulate
	that the approximation may involve only a few of the elementary signals.
	This class of problems arises throughout applied mathematics, statistics,
	and electrical engineering, but small theoretical progress has been
	made over the last fifty years. This dissertation offers four main
	contributions to the theory of sparse approximation. The first two
	contributions concern the analysis of two types of numerical algorithms
	for sparse approximation: greedy methods and convex relaxation methods.
	Greedy methods make a sequence of locally optimal choices in an effort
	to obtain a globally optimal solution. Convex relaxation methods
	replace the combinatorial sparse approximation problem with a related
	convex optimization in hope that their solutions will coincide. This
	work delineates conditions under which greedy methods and convex
	relaxation methods actually succeed in solving a well-defined sparse
	approximation problem in part or in full. The conditions for both
	classes of algorithms are remarkably similar, in spite of the fact
	that the two analyses differ significantly. The study of these algorithms
	yields geometric conditions on the collection of elementary signals
	which ensure that sparse approximation problems are computationally
	tractable. One may interpret these conditions as a requirement that
	the elementary signals should form a good packing of points in projective
	space. The third contribution of this work is an alternating projection
	algorithm that can produce good packings of points in projective
	space. The output of this algorithm frequently matches the best recorded
	solutions of projective packing problems. It can also address many
	related packing problems that have never been studied numerically.
	Finally, the dissertation develops a novel connection between sparse
	approximation problems and clustering problems. This perspective
	shows that many clustering problems from the literature can be viewed
	as sparse approximation problems where the collection of elementary
	signals must be learned along with the optimal sparse approximation.
	This treatment also yields many novel clustering problems, and it
	leads to a numerical method for solving them.},
  language = {eng},
  timestamp = {2017-04-19T14:54:01Z},
  urldate = {2016-04-13},
  school = {Univ. Texas at Austin},
  author = {Tropp, Joel Aaron},
  month = aug,
  year = {2004}
}

@article{Bhatia2007,
  title = {Particular Formulae for the {{Moore}}\textendash{}{{Penrose}} Inverse of a Columnwise Partitioned Matrix},
  volume = {421},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2006.03.031},
  abstract = {An essential part of Cegielski's [Obtuse cones and Gram matrices with non-negative inverse, Linear Algebra Appl. 335 (2001) 167\textendash{}181] considerations of some properties of Gram matrices with nonnegative inverses, which are pointed out to be crucial in constructing obtuse cones, consists in developing some particular formulae for the Moore\textendash{}Penrose inverse of a columnwise partitioned matrix A=(A1:A2) under the assumption that it is of full column rank. In the present paper, these results are generalized and extended. The generalization consists in weakening the assumption mentioned above to the requirement that the ranges of A1 and A2 are disjoint, while the extension consists in introducing the conditions referring to the class of all generalized inverses of A.},
  timestamp = {2016-07-20T14:18:19Z},
  number = {1},
  urldate = {2016-07-20},
  journal = {Linear Algebra and its Applications},
  author = {Bhatia, Rajendra and Guralnick, Robert and Kirkland, Steve and Wolkowicz, Henry and Baksalary, Jerzy K. and Baksalary, Oskar Maria},
  month = feb,
  year = {2007},
  keywords = {Generalized inverse,Gram matrix,Moore–Penrose inverse,Partitioned matrix},
  pages = {16--23},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\EKEIIWWP\\S0024379506001959.html:text/html}
}

@article{Xu2008,
  title = {Particular Formulae for the {{Moore}}-{{Penrose}} Inverses of the Partitioned Bounded Linear Operators},
  volume = {428},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2008.01.021},
  abstract = {For any two complex Hilbert spaces H and K, let BL(H,K) be the set of bounded linear operators from H to K, and H$\oplus$K be the direct sum of H and K. Given three Hilbert spaces H1,H2,H3 and two operators A1$\in$BL(H1,H3), A2$\in$BL(H2,H3), a partitioned bounded linear operator A=(A1,A2)$\in$BL(H1$\oplus$H2,H3) can be induced, where Ah1h2=A1h1+A2h2 for hi$\in$Hi,i=1,2. In this note we study the Moore-Penrose inverse A\textdagger{} of such a partitioned bounded linear operator A, and generalize a recent result of J. K. Baksalary and O. M. Baksalary [Particular formulae for the Moore-Penrose inverse of a columnwise partitioned matrix, Linear Algebra Appl. 421(2007) 16\textendash{}23] from finite matrices to Hilbert space operators.},
  timestamp = {2016-07-20T14:20:25Z},
  number = {11},
  urldate = {2016-07-20},
  journal = {Linear Algebra and its Applications},
  author = {Xu, Qingxiang and Hu, Xiaoxia},
  month = jun,
  year = {2008},
  keywords = {Moore-Penrose inverse,Partitioned bounded linear operator},
  pages = {2941--2946},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\J5QGI7PN\\S0024379508000517.html:text/html}
}

@inproceedings{Babaie-Zadeh2012,
  title = {Weighted Sparse Signal Decomposition},
  doi = {10.1109/ICASSP.2012.6288652},
  abstract = {Standard sparse decomposition (with applications in many different areas including compressive sampling) amounts to finding the minimum $\mathscr{l}$0-norm solution of an underdetermined system of linear equations. In this decomposition, all atoms are treated `uniformly' for being included or not in the decomposition. However, one may wish to weigh more or less certain atoms, or, assign higher costs to some other atoms to be included in the decomposition. This can happen for example when there is prior information available on each atom. This motivates generalizing the notion of minimal $\mathscr{l}$0-norm solution to that of minimal weighted $\mathscr{l}$0-norm solution. On the other hand, relaxing weighted $\mathscr{l}$0-norm via the weighted $\mathscr{l}$1-norm is challenging. This paper deals with minimal weighted $\mathscr{l}$0-norm solutions of underdetermined linear systems, provides conditions for their uniqueness, and develops an algorithm for their estimation.},
  timestamp = {2016-07-26T11:47:45Z},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Babaie-Zadeh, M. and Mehrdad, B. and Giannakis, G. B.},
  month = mar,
  year = {2012},
  keywords = {Approximation methods,Compressive sampling,Educational institutions,linear equations,Linear systems,Mathematical model,minimization,signal processing,sparse decomposition,Sparse matrices,underdetermined linear systems,Vectors,weighted compressive sampling,Weighted sparse decomposition,weighted sparse signal decomposition},
  pages = {3425--3428},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\6Q6Z675J\\articleDetails.html:text/html}
}

@article{Leus2016,
  title = {Sparse {{Sensing}} for {{Statistical Inference}}},
  timestamp = {2016-08-25T09:55:47Z},
  author = {Leus, Geert and Chepuri, Sundeep Prabhakar},
  year = {2016},
  annote = {PPT}
}

@article{Kowalski2008,
  title = {Sparsity and Persistence: Mixed Norms Provide Simple Signal Models with Dependent Coefficients},
  volume = {3},
  issn = {1863-1703, 1863-1711},
  shorttitle = {Sparsity and Persistence},
  doi = {10.1007/s11760-008-0076-1},
  abstract = {Sparse regression often uses $\mathscr{l}$p norm priors (with p $<$ 2). This paper demonstrates that the introduction of mixed-norms in such contexts allows one to go one step beyond in signal models, and promote some different, structured, forms of sparsity. It is shown that the particular case of the $\mathscr{l}$1,2 and $\mathscr{l}$2,1 norms leads to new group shrinkage operators. Mixed norm priors are shown to be particularly efficient in a generalized basis pursuit denoising approach, and are also used in a context of morphological component analysis. A suitable version of the Block Coordinate Relaxation algorithm is derived for the latter. The group-shrinkage operators are then modified to overcome some limitations of the mixed-norms. The proposed group shrinkage operators are tested on simulated signals in specific situations, to illustrate and compare their different behaviors. Results on real data are also used to illustrate the relevance of the approach.},
  language = {en},
  timestamp = {2016-09-13T11:48:57Z},
  number = {3},
  urldate = {2016-09-13},
  journal = {Signal, Image and Video Processing},
  author = {Kowalski, Matthieu and Torr{\'e}sani, Bruno},
  month = sep,
  year = {2008},
  pages = {251--264},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9F34TIJJ\\s11760-008-0076-1.html:text/html}
}

@TechReport{Zhang2005a,
  author      = {Zhang, Y.},
  title       = {A Simple Proof for Recoverability of $\ell_1$-Minimization: Go Over or Under?},
  institution = {Depart. Comput. Appl. Math., Rice Univ., Houston},
  year        = {2005},
  annote      = {Depart. Comput. Appl. Math., Rice Univ., Houston, TX, Tech. Rep.     TR05-09},
  timestamp   = {2017-06-23T11:46:55Z},
}

@article{Yao2003,
  title = {Equivalent Physical Models and Formulation of Equivalent Source Layer in High-Resolution {{EEG}} Imaging},
  volume = {48},
  issn = {0031-9155},
  abstract = {In high-resolution EEG imaging, both equivalent dipole layer (EDL) and equivalent charge layer (ECL) assumed to be located just above the cortical surface have been proposed as high-resolution imaging modalities or as intermediate steps to estimate the epicortical potential. Presented here are the equivalent physical models of these two equivalent source layers (ESL) which show that the strength of EDL is proportional to the surface potential of the layer when the outside of the layer is filled with an insulator, and that the strength of ECL is the normal current of the layer when the outside is filled with a perfect conductor. Based on these equivalent physical models, closed solutions of ECL and EDL corresponding to a dipole enclosed by a spherical layer are given. These results provide the theoretical basis of ESL applications in high-resolution EEG mapping.},
  language = {eng},
  timestamp = {2016-09-20T14:57:04Z},
  number = {21},
  journal = {Physics in Medicine and Biology},
  author = {Yao, Dezhong and He, Bin},
  month = nov,
  year = {2003},
  keywords = {Action Potentials,Animals,Brain,Brain Mapping,Computer Simulation,Electroencephalography,Humans,Models; Neurological,Quality Control,Scalp,Skull,Synaptic Transmission},
  pages = {3475--3483},
  annote = {read},
  pmid = {14653557}
}

@article{Yao2000,
  title = {Electric Potential Produced by a Dipole in a Homogeneous Conducting Sphere},
  volume = {47},
  issn = {0018-9294},
  doi = {10.1109/10.846691},
  abstract = {The potential produced by a dipole in a homogeneous conducting sphere is useful in a simulation study. The current available solutions still suffer from some shortcomings. In this communication, a closed solution is developed for the precise calculation of the potential anywhere in the spherical model.},
  timestamp = {2016-09-20T14:59:32Z},
  number = {7},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Yao, Dezhong},
  month = jul,
  year = {2000},
  keywords = {Automation,bioelectric potentials,Biomedical engineering,Brain modeling,brain models,Cities and towns,closed solution,Computational modeling,Conductors,EEG forward problem,Electric potential,electric potential produced by dipole,Electroencephalography,Electronic mail,Electrophysiology,homogeneous conducting sphere,Humans,Models; Neurological,Polynomials,precise potential calculation,simulation study},
  pages = {964--966},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\K47HAA3C\\846691.html:text/html}
}

@article{Wolters2007,
  title = {Volume Conduction},
  volume = {2},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.1738},
  language = {en},
  timestamp = {2016-09-20T15:04:30Z},
  number = {3},
  urldate = {2016-09-20},
  journal = {Scholarpedia},
  author = {Wolters, Carsten and Munck, Jan},
  year = {2007},
  pages = {1738}
}

@article{Geselowitz1967,
  title = {On {{Bioelectric Potentials}} in an {{Inhomogeneous Volume Conductor}}},
  volume = {7},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(67)86571-8},
  abstract = {Green's theorem is used to derive two sets of expressions for the quasi-static potential distribution in an inhomogeneous volume conductor. The current density in passive regions is assumed to be linearly related instantaneously to the electric field. Two equations are derived relating potentials to an arbitrary distribution of impressed currents. In one, surfaces of discontinuity in electrical conductivity are replaced by double layers and in the other, by surface charges. A multipole equivalent generator is defined and related both to the potential distribution on the outer surface of the volume conductor and to the current sources. An alternative result involves the electric field at the outer surface rather than the potential. Finally, the impressed currents are related to electrical activity at the membranes of active cells. The normal component of membrane current density is assumed to be equal at both membrane surfaces. One expression is obtained involving the potentials at the inner and outer surfaces of the membrane. A second expression involves the transmembrane potential and the normal component of membrane current.},
  timestamp = {2016-09-20T15:12:17Z},
  number = {1},
  urldate = {2016-09-20},
  journal = {Biophysical Journal},
  author = {Geselowitz, David B.},
  month = jan,
  year = {1967},
  pages = {1--11},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\85IJTH7Z\\S0006349567865718.html:text/html}
}

@article{Yang2015d,
  title = {Formal {{Expressions}} for the {{Electromagnetic Potentials}} in {{Any Gauge}}},
  timestamp = {2016-09-20T15:16:37Z},
  author = {Yang, Kuo-ho and Mcdonald, Kirk T.},
  year = {2015}
}

@article{Wolski2016,
  title = {Electromagnetic Potentials},
  timestamp = {2016-09-20T15:28:18Z},
  journal = {University of liverpool},
  author = {Wolski, Andi},
  year = {2016}
}

@article{zotero-null-15257,
  title = {Maxwell's {{Equations}}},
  timestamp = {2016-09-20T15:30:53Z}
}

@article{MIT2016,
  title = {Distributed {{Current Sources}} and {{Associated Fields}}},
  timestamp = {2016-09-20T15:34:32Z},
  author = {MIT},
  year = {2016}
}

@article{Dezhong2000,
  title = {High-Resolution {{EEG}} Mappings: A Spherical Harmonic Spectra Theory and Simulation Results},
  volume = {111},
  issn = {1388-2457},
  shorttitle = {High-Resolution {{EEG}} Mappings},
  doi = {10.1016/S1388-2457(99)00205-9},
  abstract = {Shown first is the equivalence between the multiple expansion (ME) of the brain electrical generator and the spherical harmonic spectra (SHS) of the potential generated by the electrical generator in an infinite volume conductor. Based on the equivalence, the SHS and the spatial filters which connect the SHS with the ME are deduced, in a concentric 3 sphere conductor and for the 5 EEG source mappings. They are cortical potential mapping (CPM), scalp Laplacian mapping (LM), pseudo-cortical potential mapping (PCPM), equivalent dipole layer mapping (EDM) and equivalent charge layer mapping (ECM). The theoretical simulation study of the spatial filters and mappings indicate that all 5 mappings provide higher resolution imaging maps of brain electrical activity than the scalp potential map. In the inverse problem, a spherical spline fit algorithm is provided to reconstruct the SHS of the scalp recording potential, and then the SHS and maps of the 5 mappings are reconstructed by utilizing the spatial filters and the SHS of the scalp potential. The results indicate that the correlativity order between a reconstructed map and the actual cortical potential map is CPM$\geq$EDM\&gt;PCPM\&gt;LM\&gt;ECM. An empirical VEP data study shows that any one of the 5 mappings also provides higher spatial resolution than the scalp potential map.},
  timestamp = {2016-09-21T12:26:53Z},
  number = {1},
  urldate = {2016-09-21},
  journal = {Clinical Neurophysiology},
  author = {Dezhong, Yao},
  month = jan,
  year = {2000},
  keywords = {Cortical imaging,High-resolution EEG mapping,Multiple expansion,Spherical harmonic spectra,Visual evoked potential},
  pages = {81--92},
  annote = {read},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S63PK88H\\S1388245799002059.html:text/html}
}

@article{Rodriguez-Falces2013,
  title = {A Novel Approach to Teach the Generation of Bioelectrical Potentials from a Descriptive and Quantitative Perspective},
  volume = {37},
  copyright = {Copyright \textcopyright{} 2013 the American Physiological Society},
  issn = {1043-4046, 1522-1229},
  doi = {10.1152/advan.00064.2013},
  abstract = {In electrophysiology studies, it is becoming increasingly common to explain experimental observations using both descriptive methods and quantitative approaches. However, some electrophysiological phenomena, such as the generation of extracellular potentials that results from the propagation of the excitation source along the muscle fiber, are difficult to describe and conceptualize. In addition, most traditional approaches aimed at describing extracellular potentials consist of complex mathematical machinery that gives no chance for physical interpretation. The aim of the present study is to present a new method to teach the formation of extracellular potentials around a muscle fiber from both a descriptive and quantitative perspective. The implementation of this method was tested through a written exam and a satisfaction survey. The new method enhanced the ability of students to visualize the generation of bioelectrical potentials. In addition, the new approach improved students' understanding of how changes in the fiber-to-electrode distance and in the shape of the excitation source are translated into changes in the extracellular potential. The survey results show that combining general principles of electrical fields with accurate graphic imagery gives students an intuitive, yet quantitative, feel for electrophysiological signals and enhances their motivation to continue their studies in the biomedical engineering field.},
  language = {en},
  timestamp = {2016-09-22T11:30:01Z},
  number = {4},
  urldate = {2016-09-22},
  journal = {Advances in Physiology Education},
  author = {Rodriguez-Falces, Javier},
  month = dec,
  year = {2013},
  pages = {327--336},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CUU275P2\\327.full.html:text/html},
  pmid = {24292909}
}

@article{Zhao2015a,
  title = {New Conditions for Uniformly Recovering Sparse Signals via Orthogonal Matching Pursuit},
  volume = {106},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2014.06.010},
  abstract = {Recently, lots of work has been done on conditions of guaranteeing sparse signal recovery using orthogonal matching pursuit (OMP). However, none of the existing conditions is both necessary and sufficient in terms of the so-called restricted isometric property, coherence, cumulative coherence (Babel function), or other verifiable quantities in the literature. Motivated by this observation, we propose a new measure of a matrix, named as union cumulative coherence, and present both sufficient and necessary conditions under which the OMP algorithm can uniformly recover sparse signals for all sensing matrices. The proposed condition guarantees a uniform recovery of sparse signals using OMP, and reveals the capability of OMP in sparse recovery. We demonstrate by examples that the proposed condition can be used to more effectively determine the recoverable sparse signals via OMP than the conditions existing in the literature. Furthermore, sparse recovery from noisy measurements is also considered in terms of the proposed union cumulative coherence.},
  timestamp = {2016-09-30T13:51:59Z},
  urldate = {2016-09-23},
  journal = {Signal Processing},
  author = {Zhao, Junxi and Song, Rongfang and Zhao, Jie and Zhu, Wei-Ping},
  month = jan,
  year = {2015},
  keywords = {compressive sensing,Cumulative coherence,orthogonal matching pursuit,Restricted isometric property,sparse signal},
  pages = {106--113},
  annote = {read},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\EWZKTPKK\\S016516841400276X.html:text/html}
}

@book{Toga2002,
  address = {Amsterdam ; Boston},
  edition = {2 edition},
  title = {Brain {{Mapping}}: {{The Methods}}, {{Second Edition}}},
  isbn = {978-0-12-693019-1},
  shorttitle = {Brain {{Mapping}}},
  abstract = {Investigation of the functional architecture of the human brain using modern noninvasive imaging techniques is a rapidly expanding area of research. A proper knowledge of methodology is needed to appreciate the burgeoning literature in the field. This timely publication provides an excellent catalogue of the main techniques.The authors offer an invaluable analysis of mapping strategies and techniques, providing everything from the foundations to the major pitfalls and practical applications of the modern techniques used in neuroimaging. Contains over 1000 full color pages with more than 200 color figures.                                       Spanning the methodological gamut from the molecular level to the whole brain while discussing anatomy, physiology, and pathology, as well as their integration, Brain Mapping: The Methods, 2e, brings the reader a comprehensive, well-illustrated and entirely readable description of the methods for brain mapping. Drs. Toga and Mazziotta provide everything from the foundations to the major pitfalls and practical applications of the technique by assembling an impressive group of experts, all widely known in their field, who contribute an outstanding set of chapters.},
  language = {English},
  timestamp = {2016-09-24T09:21:29Z},
  publisher = {{Academic Press}},
  author = {Toga, Arthur W. and PhD, John C. Mazziotta MD},
  month = oct,
  year = {2002}
}

@phdthesis{Berg2009,
  title = {Convex Optimization for Generalized Sparse Recovery},
  abstract = {The past decade has witnessed the emergence of compressed sensing as a way of acquiring sparsely representable signals in a compressed form. These developments have greatly motivated research in sparse signal recovery, which lies at the heart of compressed sensing, and which has recently found its use in altogether new applications.

In the first part of this thesis we study the theoretical aspects of joint-sparse recovery by means of sum-of-norms minimization, and the ReMBo-ell\_1 algorithm, which combines boosting techniques with ell\_1-minimization. For the sum-of-norms approach we derive necessary and sufficient conditions for recovery, by extending existing results to the joint-sparse setting. We focus in particular on minimization of the sum of ell\_1, and ell\_2 norms, and give concrete examples where recovery succeeds with one formulation but not with the other. We base our analysis of ReMBo-ell\_1 on its geometrical interpretation, which leads to a study of orthant intersections with randomly oriented subspaces. This work establishes a clear picture of the mechanics behind the method, and explains the different aspects of its performance.

The second part and main contribution of this thesis is the development of a framework for solving a wide class of convex optimization problems for sparse recovery. We provide a detailed account of the application of the framework on several problems, but also consider its limitations. The framework has been implemented in the SPGL1 algorithm, which is already well established as an effective solver. Numerical results show that our algorithm is state-of-the-art, and compares favorably even with solvers for the easier\,\textemdash\,but less natural\,\textemdash\,Lagrangian formulations.

The last part of this thesis discusses two supporting software packages: Sparco, which provides a suite of test problems for sparse recovery, and Spot, a Matlab toolbox for the creation and manipulation of linear operators. Spot greatly facilitates rapid prototyping in sparse recovery and compressed sensing, where linear operators form the elementary building blocks.},
  timestamp = {2016-09-25T20:14:39Z},
  school = {The University of British Columbia},
  author = {van den Berg, Ewout},
  year = {2009}
}

@article{Rauhut2008,
  title = {Compressed {{Sensing}} and {{Redundant Dictionaries}}},
  volume = {54},
  issn = {0018-9448},
  doi = {10.1109/TIT.2008.920190},
  abstract = {This paper extends the concept of compressed sensing to signals that are not sparse in an orthonormal basis but rather in a redundant dictionary. It is shown that a matrix, which is a composition of a random matrix of certain type and a deterministic dictionary, has small restricted isometry constants. Thus, signals that are sparse with respect to the dictionary can be recovered via basis pursuit (BP) from a small number of random measurements. Further, thresholding is investigated as recovery algorithm for compressed sensing, and conditions are provided that guarantee reconstruction with high probability. The different schemes are compared by numerical experiments.},
  timestamp = {2016-09-28T14:37:11Z},
  number = {5},
  journal = {IEEE Transactions on Information Theory},
  author = {Rauhut, H. and Schnass, K. and Vandergheynst, P.},
  month = may,
  year = {2008},
  keywords = {basis pursuit,Basis Pursuit (BP),compressed sensing,data compression,Decoding,Dictionaries,Greedy algorithms,linear programming,Matching pursuit algorithms,matrix algebra,Nuclear magnetic resonance,orthogonal matching pursuit,probability,random matrix,recovery algorithm,redundant dictionary,restricted isometry constants,Signal processing algorithms,signal reconstruction,signal thresholding,sparse approximation,Sparse matrices,sparse signal,Spectroscopy,Thresholding},
  pages = {2210--2219},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JSM6Z977\\4494699.html:text/html}
}

@article{Davenport2010,
  title = {Analysis of {{Orthogonal Matching Pursuit Using}} the {{Restricted Isometry Property}}},
  volume = {56},
  issn = {0018-9448},
  doi = {10.1109/TIT.2010.2054653},
  abstract = {Orthogonal matching pursuit (OMP) is the canonical greedy algorithm for sparse approximation. In this paper we demonstrate that the restricted isometry property (RIP) can be used for a very straightforward analysis of OMP. Our main conclusion is that the RIP of order K+1 (with isometry constant $\delta$ $<$; [ 1/( 3$\surd$K)]) is sufficient for OMP to exactly recover any K-sparse signal. The analysis relies on simple and intuitive observations about OMP and matrices which satisfy the RIP. For restricted classes of K-sparse signals (those that are highly compressible), a relaxed bound on the isometry constant is also established. A deeper understanding of OMP may benefit the analysis of greedy algorithms in general. To demonstrate this, we also briefly revisit the analysis of the regularized OMP (ROMP) algorithm.},
  timestamp = {2016-09-28T14:40:59Z},
  number = {9},
  journal = {IEEE Transactions on Information Theory},
  author = {Davenport, M. A. and Wakin, M. B.},
  month = sep,
  year = {2010},
  keywords = {Algorithm design and analysis,Approximation algorithms,Approximation methods,approximation theory,canonical greedy algorithm,compressive sensing,Dictionaries,Greedy algorithms,isometry constant,iterative methods,K-sparse signal,Matching pursuit algorithms,orthogonal matching pursuit,orthogonal matching pursuit (OMP),Pursuit algorithms,redundant dictionaries,regularized OMP algorithm,relaxed bound,restricted isometry property,restricted isometry property (RIP),Signal processing algorithms,Software algorithms,Software packages,sparse approximation,Sparse matrices},
  pages = {4395--4401},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JVCMQH4W\\5550495.html:text/html}
}

@article{Needell2010,
  title = {Signal {{Recovery From Incomplete}} and {{Inaccurate Measurements Via Regularized Orthogonal Matching Pursuit}}},
  volume = {4},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2010.2042412},
  abstract = {We demonstrate a simple greedy algorithm that can reliably recover a vector v ?? ??d from incomplete and inaccurate measurements x = ??v + e. Here, ?? is a N x d measurement matrix with N$<$$<$d, and e is an error vector. Our algorithm, Regularized Orthogonal Matching Pursuit (ROMP), seeks to provide the benefits of the two major approaches to sparse recovery. It combines the speed and ease of implementation of the greedy methods with the strong guarantees of the convex programming methods. For any measurement matrix ?? that satisfies a quantitative restricted isometry principle, ROMP recovers a signal v with O(n) nonzeros from its inaccurate measurements x in at most n iterations, where each iteration amounts to solving a least squares problem. The noise level of the recovery is proportional to ??logn ||e||2. In particular, if the error term e vanishes the reconstruction is exact.},
  timestamp = {2016-09-28T14:45:59Z},
  number = {2},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  author = {Needell, D. and Vershynin, R.},
  month = apr,
  year = {2010},
  keywords = {Compressed sensing (CS),convex programming,convex programming methods,error vector,Greedy algorithms,inaccurate measurements,incomplete measurements,iterative methods,least squares approximations,least squares problem,measurement matrix,orthogonal matching pursuit,regularized orthogonal matching pursuit,signal reconstruction,signal recovery,simple greedy algorithm,sparse approximation problem,sparse recovery,time-frequency analysis,uncertainty principle},
  pages = {310--316},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\C3BCNCW7\\5419092.html:text/html}
}

@article{Dai2009,
  title = {Subspace {{Pursuit}} for {{Compressive Sensing Signal Reconstruction}}},
  volume = {55},
  issn = {0018-9448},
  doi = {10.1109/TIT.2009.2016006},
  abstract = {We propose a new method for reconstruction of sparse signals with and without noisy perturbations, termed the subspace pursuit algorithm. The algorithm has two important characteristics: low computational complexity, comparable to that of orthogonal matching pursuit techniques when applied to very sparse signals, and reconstruction accuracy of the same order as that of linear programming (LP) optimization methods. The presented analysis shows that in the noiseless setting, the proposed algorithm can exactly reconstruct arbitrary sparse signals provided that the sensing matrix satisfies the restricted isometry property with a constant parameter. In the noisy setting and in the case that the signal is not exactly sparse, it can be shown that the mean-squared error of the reconstruction is upper-bounded by constant multiples of the measurement and signal perturbation energies.},
  timestamp = {2016-09-28T14:50:39Z},
  number = {5},
  journal = {IEEE Transactions on Information Theory},
  author = {Dai, W. and Milenkovic, O.},
  month = may,
  year = {2009},
  keywords = {Algorithm design and analysis,compressive sensing,computational complexity,Energy measurement,isometry property,iterative methods,linear programming,Matching pursuit algorithms,mean square error method,mean square error methods,Optimization methods,orthogonal matching pursuit,orthogonal matching pursuit techniques,programming optimization methods,Pursuit algorithms,Reconstruction algorithms,restricted isometry property,sensing matrix,Signal analysis,signal perturbation energies,signal reconstruction,Sparse matrices,sparse signal reconstruction,subspace pursuit algorithm,time-frequency analysis},
  pages = {2230--2249},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\G9N5VQ7G\\4839056.html:text/html}
}

@article{Lee2013c,
  title = {Oblique {{Pursuits}} for {{Compressed Sensing}}},
  volume = {59},
  issn = {0018-9448},
  doi = {10.1109/TIT.2013.2254172},
  abstract = {Compressed sensing is a new data acquisition paradigm enabling universal, simple, and reduced-cost acquisition, by exploiting a sparse signal model. Most notably, recovery of the signal by computationally efficient algorithms is guaranteed for certain randomized acquisition systems. However, there is a discrepancy between the theoretical guarantees and practical applications. In applications, including Fourier imaging in various modalities, the measurements are acquired by inner products with vectors selected randomly (sampled) from a frame. Currently available guarantees are derived using the so-called restricted isometry property (RIP), which has only been shown to hold under ideal assumptions. For example, the sampling from the frame needs to be independent and identically distributed with the uniform distribution, and the frame must be tight. In practice though, one or more of the ideal assumptions are typically violated and none of the RIP-based guarantees applies. Motivated by this discrepancy, we propose two related changes in the existing framework: 1) a generalized RIP called the restricted biorthogonality property (RBOP); and 2) correspondingly modified versions of existing greedy pursuit algorithms, which we call oblique pursuits. Oblique pursuits are guaranteed using the RBOP without requiring ideal assumptions; hence, the guarantees apply to practical acquisition schemes. Numerical results show that oblique pursuits also perform competitively with, or sometimes better than their conventional counterparts.},
  timestamp = {2016-09-28T14:51:58Z},
  number = {9},
  journal = {IEEE Transactions on Information Theory},
  author = {Lee, K. and Bresler, Y. and Junge, M.},
  month = sep,
  year = {2013},
  keywords = {Argon,Biorthogonality,compressed sensing,data acquisition paradigm,Dictionaries,Fourier analysis,Fourier imaging,greedy pursuit algorithms,oblique projection,oblique pursuits,Pursuit algorithms,randomized acquisition systems,RBOP,restricted biorthogonality property,restricted isometry property,restricted isometry property (RIP),RIP,sensors,Sparse matrices,sparse signal model,Vectors},
  pages = {6111--6141},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UKKCCRBE\\6484162.html:text/html}
}

@article{Wang2012a,
  title = {Near Optimal Bound of Orthogonal Matching Pursuit Using Restricted Isometric Constant},
  volume = {2012},
  issn = {1687-6180},
  doi = {10.1186/1687-6180-2012-8},
  abstract = {As a paradigm for reconstructing sparse signals using a set of under sampled measurements, compressed sensing has received much attention in recent years. In identifying the sufficient condition under which the perfect recovery of sparse signals is ensured, a property of the sensing matrix referred to as the restricted isometry property (RIP) is popularly employed. In this article, we propose the RIP based bound of the orthogonal matching pursuit (OMP) algorithm guaranteeing the exact reconstruction of sparse signals. Our proof is built on an observation that the general step of the OMP process is in essence the same as the initial step in the sense that the residual is considered as a new measurement preserving the sparsity level of an input vector. Our main conclusion is that if the restricted isometry constant $\delta$Kof the sensing matrix satisfies$\delta$K$<$$\surd$K-1$\surd$K-1+K$\delta$K$<$K-1$\surd$K-1+K$\surd$$<$math xmlns="http://www.w3.org/1998/Math/MathML"$>$$<$semantics$>$$<$mrow$>$ $<$mrow$>$ $<$msub$>$ $<$mrow$>$ $<$mi$>\delta$$<$/mi$>$ $<$/mrow$>$ $<$mrow$>$ $<$mi$>$K$<$/mi$>$ $<$/mrow$>$ $<$/msub$>$ $<$mo class="MathClass-rel"$>$\&lt;$<$/mo$>$ $<$mfrac$>$ $<$mrow$>$ $<$msqrt$>$ $<$mrow$>$ $<$mi$>$K$<$/mi$>$ $<$mo class="MathClass-bin"$>$-$<$/mo$>$ $<$mn$>$1$<$/mn$>$ $<$/mrow$>$ $<$/msqrt$>$ $<$/mrow$>$ $<$mrow$>$ $<$msqrt$>$ $<$mrow$>$ $<$mi$>$K$<$/mi$>$ $<$mo class="MathClass-bin"$>$-$<$/mo$>$ $<$mn$>$1$<$/mn$>$ $<$mo class="MathClass-bin"$>$+$<$/mo$>$ $<$mi$>$K$<$/mi$>$ $<$/mrow$>$ $<$/msqrt$>$ $<$/mrow$>$ $<$/mfrac$>$ $<$/mrow$>$ $<$/mrow$>$$<$annotation-xml encoding="application/xhtml+xml" name="alternate-representation"$>$$<$a href="https://static-content-springer-com.gaelnomade.ujf-grenoble.fr/image/art\%3A10.1186\%2F1687-6180-2012-8/MediaObjects/13634\_2011\_Article\_188\_Equa\_HTML.gif" target="\_blank" rel="noopener noreferrer"$>$$<$img src="https://static-content-springer-com.gaelnomade.ujf-grenoble.fr/image/art\%3A10.1186\%2F1687-6180-2012-8/MediaObjects/13634\_2011\_Article\_188\_Equa\_HTML.gif" alt="https://static-content-springer-com.gaelnomade.ujf-grenoble.fr/image/art\%3A10.1186\%2F1687-6180-2012-8/MediaObjects/13634\_2011\_Article\_188\_Equa\_HTML.gif"$>$$<$/img$>$$<$/a$>$$<$/annotation-xml$>$$<$/semantics$>$$<$/math$>$then the OMP algorithm can perfectly recover K($>$ 1)-sparse signals from measurements. We show that our bound is sharp and indeed close to the limit conjectured by Dai and Milenkovic.},
  language = {en},
  timestamp = {2016-09-28T14:58:04Z},
  number = {1},
  urldate = {2016-09-28},
  journal = {EURASIP Journal on Advances in Signal Processing},
  author = {Wang, Jian and Kwon, Seokbeop and Shim, Byonghyo},
  month = jan,
  year = {2012},
  pages = {8},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\N2MXIGVF\\1687-6180-2012-8.html:text/html}
}

@article{Liu2012a,
  title = {The {{Orthogonal Super Greedy Algorithm}} and {{Applications}} in {{Compressed Sensing}}},
  volume = {58},
  issn = {0018-9448},
  doi = {10.1109/TIT.2011.2177632},
  abstract = {The general theory of greedy approximation is well developed. Much less is known about how specific features of a dictionary can be used to our advantage. In this paper, we discuss incoherent dictionaries. We build a new greedy algorithm which is called the orthogonal super greedy algorithm (OSGA). We show that the rates of convergence of OSGA and the orthogonal matching pursuit (OMP) with respect to incoherent dictionaries are the same. Based on the analysis of the number of orthogonal projections and the number of iterations, we observed that OSGA is times simpler (more efficient) than OMP. Greedy approximation is also a fundamental tool for sparse signal recovery. The performance of orthogonal multimatching pursuit, a counterpart of OSGA in the compressed sensing setting, is also analyzed under restricted isometry property conditions.},
  timestamp = {2016-09-28T14:59:16Z},
  number = {4},
  journal = {IEEE Transactions on Information Theory},
  author = {Liu, E. and Temlyakov, V. N.},
  month = apr,
  year = {2012},
  keywords = {Approximation algorithms,Approximation methods,approximation theory,compressed sensing,convergence rate,Dictionaries,Greedy algorithms,greedy approximation general theory,isometry property condition,iteration number,iterative methods,Matching pursuit algorithms,OMP,orthogonal matching pursuit,orthogonal multimatching pursuit,orthogonal multimatching pursuit (OMMP),orthogonal super greedy algorithm,orthogonal super greedy algorithm (OSGA),OSGA,signal reconstruction,sparse signal recovery,time-frequency analysis,Vectors},
  pages = {2040--2047},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PUKBA99U\\6092487.html:text/html}
}

@article{Wang2012c,
  title = {On the {{Recovery Limit}} of {{Sparse Signals Using Orthogonal Matching Pursuit}}},
  volume = {60},
  issn = {1053-587X},
  doi = {10.1109/TSP.2012.2203124},
  abstract = {Orthogonal matching pursuit (OMP) is a greedy search algorithm popularly being used for the recovery of compressive sensed sparse signals. In this correspondence, we show that if the isometry constant $\delta$K+1 of the sensing matrix $\Phi$ satisfies $\delta$K+1 $<$; 1/(1/$\surd$K+1) then the OMP algorithm can perfectly recover K-sparse signals from the compressed measurements y=$\Phi$x. Our bound offers a substantial improvement over the recent result of Davenport and Wakin and also closes gap between the recovery bound and fundamental limit over which the perfect recovery of the OMP cannot be guaranteed.},
  timestamp = {2016-09-28T15:02:33Z},
  number = {9},
  journal = {IEEE Transactions on Signal Processing},
  author = {Wang, J. and Shim, B.},
  month = sep,
  year = {2012},
  keywords = {Algorithm design and analysis,compressed sensing,Compressed sensing (CS),compressive sensed sparse signal,greedy search algorithm,Indexes,isometry constant,iterative methods,Matching pursuit algorithms,matrix algebra,OMP algorithm,orthogonal matching pursuit,orthogonal matching pursuit (OMP),restricted isometry property (RIP),sensing matrix,sensors,Sparse matrices,sparse signal,sparse signal recovery limit,Vectors},
  pages = {4973--4976},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AN88KUKC\\6213142.html:text/html}
}

@article{Wang2016,
  title = {Exact {{Recovery}} of {{Sparse Signals Using Orthogonal Matching Pursuit}}: {{How Many Iterations Do We Need}}?},
  volume = {64},
  issn = {1053-587X},
  shorttitle = {Exact {{Recovery}} of {{Sparse Signals Using Orthogonal Matching Pursuit}}},
  doi = {10.1109/TSP.2016.2568162},
  abstract = {Orthogonal matching pursuit (OMP) is a greedy algorithm widely used for the recovery of sparse signals from compressed measurements. In this paper, we analyze the number of iterations required for the OMP algorithm to perform exact recovery of sparse signals. Our analysis shows that OMP can accurately recover all K-sparse signals within [2.8 K] iterations when the measurement matrix satisfies a restricted isometry property (RIP). Our result improves upon the recent result of Zhang and also bridges the gap between Zhang's result and the fundamental limit of OMP at which exact recovery of K-sparse signals cannot be uniformly guaranteed.},
  timestamp = {2016-09-28T15:02:44Z},
  number = {16},
  journal = {IEEE Transactions on Signal Processing},
  author = {Wang, J. and Shim, B.},
  month = aug,
  year = {2016},
  keywords = {Algorithm design and analysis,compressed measurements,Energy measurement,Indexes,iterative methods,K-sparse signal recovery,Matching pursuit algorithms,matrix algebra,measurement matrix,OMP,OMP algorithm,orthogonal matching pursuit,orthogonal matching pursuit (OMP),restricted isometry property,restricted isometry property (RIP),Signal processing algorithms,signal restoration,Size measurement,Sparse matrices,sparse signal recovery,time-frequency analysis},
  pages = {4194--4202},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5C4A8TJR\\7469405.html:text/html}
}

@article{Mo2012,
  title = {A {{Remark}} on the {{Restricted Isometry Property}} in {{Orthogonal Matching Pursuit}}},
  volume = {58},
  issn = {0018-9448},
  doi = {10.1109/TIT.2012.2185923},
  abstract = {This paper demonstrates that if the restricted isometry constant $\delta$K+1 of the measurement matrix A satisfies [$\delta$K+1 $<$; 1 $\surd$K+1] then a greedy algorithm called Orthogonal Matching Pursuit (OMP) can recover every K-sparse signal x in K iterations from Ax. By contrast, a matrix is also constructed with the restricted isometry constant [$\delta$K+1 = 1 $\surd$K] such that OMP can not recover some K-sparse signal x in K iterations. This result positively verifies the conjecture given by Dai and Milenkovic in 2009.},
  timestamp = {2016-09-28T15:04:20Z},
  number = {6},
  journal = {IEEE Transactions on Information Theory},
  author = {Mo, Q. and Shen, Y.},
  month = jun,
  year = {2012},
  keywords = {compressed sensing,Educational institutions,greedy algorithm,Greedy algorithms,Indexes,iterative methods,K-sparse signal,Matching pursuit algorithms,measurement matrix,minimization,orthogonal matching pursuit,restricted isometry property,signal reconstruction,sparse signal reconstruction,Vectors},
  pages = {3654--3656},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CBTSVJUP\\6166888.html:text/html}
}

@article{Rudelson2005,
  title = {Geometric Approach to Error-Correcting Codes and Reconstruction of Signals},
  volume = {2005},
  issn = {1073-7928, 1687-0247},
  doi = {10.1155/IMRN.2005.4019},
  abstract = {We develop an approach through geometric functional analysis to reconstruction of signals from few linear measurements and to error-correcting codes. An error-correcting code encodes an n-letter word x into an m-letter word y in such a way that x can be decoded correctly when any r letters of y are corrupted. We show that most linear orthogonal transformations Q : $\mathbb{R}$n $\rightarrow$ $\mathbb{R}$m form efficient and robust error-correcting codes over reals. The decoder (which corrects the corrupted components of y) is the metric projection onto the range of Q in the $\mathscr{l}$1-norm. This yields robust error-correcting codes over reals (and over alphabets of polynomial size), with a Gilbert-Varshamov type bound, and with quadratic time encoders and polynomial time decoders. An equivalent problem arises in signal processing: how to reconstruct a signal that belongs to a small class from few linear measurements? We prove that for most sets of Gaussian measurements, all signals of small support can be exactly reconstructed by the L1-norm minimization. This is an improvement of recent results of Donoho and of Candes and Tao. An equivalent problem in combinatorial geometry is the existence of ``neighborly'' symmetric polytopes, that is, polytopes with fixed number of facets and maximal number of lower-dimensional facets. We prove that most sections of a cube form such polytopes. Our work thus belongs to a common ground of coding theory, signal processing, combinatorial geometry, and geometric functional analysis. Our argument, which is based on concentration of measure and improving Lipschitzness by random projections, may be of independent interest in geometric functional analysis.},
  language = {en},
  timestamp = {2016-09-29T15:30:58Z},
  number = {64},
  urldate = {2016-09-29},
  journal = {International Mathematics Research Notices},
  author = {Rudelson, Mark and Vershynin, Roman},
  month = jan,
  year = {2005},
  pages = {4019--4041},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7VAD4KPQ\\4019.html:text/html}
}

@incollection{Vaiter2015,
  series = {Applied and Numerical Harmonic Analysis},
  title = {Low {{Complexity Regularization}} of {{Linear Inverse Problems}}},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  isbn = {978-3-319-19748-7 978-3-319-19749-4},
  abstract = {Inverse problems and regularization theory is a central theme in imaging sciences, statistics, and machine learning. The goal is to reconstruct an unknown vector from partial indirect, and possibly noisy, measurements of it. A now standard method for recovering the unknown vector is to solve a convex optimization problem that enforces some prior knowledge about its structure. This chapter delivers a review of recent advances in the field where the regularization prior promotes solutions conforming to some notion of simplicity/low complexity. These priors encompass as popular examples sparsity and group sparsity (to capture the compressibility of natural signals and images), total variation and analysis sparsity (to promote piecewise regularity), and low rank (as natural extension of sparsity to matrix-valued data). Our aim is to provide a unified treatment of all these regularizations under a single umbrella, namely the theory of partial smoothness. This framework is very general and accommodates all low complexity regularizers just mentioned, as well as many others. Partial smoothness turns out to be the canonical way to encode low-dimensional models that can be linear spaces or more general smooth manifolds. This review is intended to serve as a one stop shop toward the understanding of the theoretical properties of the so-regularized solutions. It covers a large spectrum including (i) recovery guarantees and stability to noise, both in terms of $\mathscr{l}$ 2-stability and model (manifold) identification; (ii) sensitivity analysis to perturbations of the parameters involved (in particular the observations), with applications to unbiased risk estimation; (iii) convergence properties of the forward-backward proximal splitting scheme that is particularly well suited to solve the corresponding large-scale regularized optimization problem.},
  language = {en},
  timestamp = {2016-10-07T14:20:53Z},
  urldate = {2016-09-30},
  booktitle = {Sampling {{Theory}}, a {{Renaissance}}},
  publisher = {{Springer International Publishing}},
  author = {Vaiter, Samuel and Peyr{\'e}, Gabriel and Fadili, Jalal},
  editor = {Pfander, G{\"o}tz E.},
  year = {2015},
  keywords = {Appl.Mathematics/Computational Methods of Engineering,Approximations and Expansions,Functions of a Complex Variable,Information and Communication; Circuits,Signal; Image and Speech Processing},
  pages = {103--153},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HFCPJVQT\\978-3-319-19749-4_3.html:text/html},
  doi = {10.1007/978-3-319-19749-4_3}
}

@article{Mishali2011a,
  title = {Xampling: {{Analog}} to Digital at Sub-{{Nyquist}} Rates},
  volume = {5},
  issn = {1751-858X},
  shorttitle = {Xampling},
  doi = {10.1049/iet-cds.2010.0147},
  abstract = {The authors present a sub-Nyquist analog-to-digital converter of wideband inputs. The circuit realises the recently proposed modulated wideband converter, which is a flexible platform for sampling signals according to their actual bandwidth occupation. The theoretical work enables, for example, a sub-Nyquist wideband communication receiver, which has no prior information on the transmitter carrier positions. The present design supports input signals with 2 GHz Nyquist rate and 120 MHz spectrum occupancy, with arbitrary transmission frequencies. The sampling rate is as low as 280 MHz. To the best of the authors knowledge, this is the first reported hardware that performs sub-Nyquist sampling and reconstruction of wideband signals. The authors describe the various circuit design considerations, with an emphasis on the non-ordinary challenges the converter introduces: mixing a signal with a multiple set of sinusoids, rather than a single local oscillator, and generation of highly transient periodic waveforms, with transient intervals on the order of the Nyquist rate. Hardware experiments validate the design and demonstrate sub-Nyquist sampling and signal reconstruction.},
  timestamp = {2016-09-30T11:17:32Z},
  number = {1},
  journal = {IET Circuits, Devices Systems},
  author = {Mishali, M. and Eldar, Y. C. and Dounaevsky, O. and Shoshan, E.},
  month = jan,
  year = {2011},
  keywords = {analog-to-digital converter,analogue-digital conversion,circuit design considerations,frequency 2 GHz,frequency 120 MHz,frequency 280 MHz,integrated circuit design,signal reconstruction,single local oscillator,subNyquist wideband communication receiver,transient intervals,transient periodic waveforms,wideband signals,Xampling},
  pages = {8--20},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MH4SWW6J\\5692791.html:text/html}
}

@Article{Eldar2010b,
  author        = {Eldar, Y.C. and Kuppinger, P. and B{\"o}lcskei, H.},
  title         = {Compressed {{Sensing}} of {{Block}}-{{Sparse Signals}}: {{Uncertainty Relations}} and {{Efficient Recovery}}},
  journal       = {IEEE Transactions on Signal Processing},
  year          = {2010},
  volume        = {58},
  number        = {6},
  pages         = {3042--3054},
  month         = jun,
  issn          = {1053-587X, 1941-0476},
  abstract      = {We consider compressed sensing of block-sparse signals, i.e., sparse signals that have nonzero coefficients occurring in clusters. An uncertainty relation for block-sparse signals is derived, based on a block-coherence measure, which we introduce. We then show that a block-version of the orthogonal matching pursuit algorithm recovers block \$k\$-sparse signals in no more than \$k\$ steps if the block-coherence is sufficiently small. The same condition on block-coherence is shown to guarantee successful recovery through a mixed \$$\backslash$ell\_2/$\backslash$ell\_1\$-optimization approach. This complements previous recovery results for the block-sparse case which relied on small block-restricted isometry constants. The significance of the results presented in this paper lies in the fact that making explicit use of block-sparsity can provably yield better reconstruction properties than treating the signal as being sparse in the conventional sense, thereby ignoring the additional structure in the problem.},
  annote        = {Comment: Submitted to the IEEE Trans. on Signal Processing, version 2 has updated figures},
  archiveprefix = {arXiv},
  doi           = {10.1109/TSP.2010.2044837},
  eprint        = {0906.3173},
  eprinttype    = {arxiv},
  file          = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3NEW3R3M\\0906.html:text/html},
  keywords      = {Computer Science - Information Theory},
  shorttitle    = {Compressed {{Sensing}} of {{Block}}-{{Sparse Signals}}},
  timestamp     = {2016-09-30T13:36:29Z},
  urldate       = {2016-09-30},
}

@inproceedings{Maleki2009,
  title = {Coherence Analysis of Iterative Thresholding Algorithms},
  doi = {10.1109/ALLERTON.2009.5394802},
  abstract = {There is a recent surge of interest in developing algorithms for finding sparse solutions of underdetermined systems of linear equations y = {\^A}\textquestiondown{}x. In many applications, extremely large problem sizes are envisioned, with at least tens of thousands of equations and hundreds of thousands of unknowns. For such problem sizes, low computational complexity is paramount. The best studied l1 minimization algorithm is not fast enough to fulfill this need. Iterative thresholding algorithms have been proposed to address this problem. In this paper we want to analyze three of these algorithms theoretically, and give sufficient conditions under which they recover the sparsest solution.},
  timestamp = {2016-10-04T15:31:15Z},
  booktitle = {47th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}, 2009. {{Allerton}} 2009},
  author = {Maleki, A.},
  month = sep,
  year = {2009},
  keywords = {Algorithm design and analysis,coherence analysis,computational complexity,Equations,Greedy algorithms,Iterative algorithms,iterative methods,iterative thresholding algorithms,Large-scale systems,linear equations underdetermined systems,Matching pursuit algorithms,Minimization methods,Polynomials,signal processing,Signal processing algorithms,Sparse matrices,sparse solutions},
  pages = {236--243},
  annote = {read},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8WU4MK95\\5394802.html:text/html}
}

@article{Schnass2008,
  title = {Dictionary {{Preconditioning}} for {{Greedy Algorithms}}},
  volume = {56},
  issn = {1053-587X},
  doi = {10.1109/TSP.2007.911494},
  abstract = {This paper introduces the concept of sensing dictionaries. It presents an alteration of greedy algorithms like thresholding or (orthogonal) matching pursuit which improves their performance in finding sparse signal representations in redundant dictionaries while maintaining the same complexity. These algorithms can be split into a sensing and a reconstruction step, and the former will fail to identify correct atoms if the cumulative coherence of the dictionary is too high. We thus modify the sensing step by introducing a special sensing dictionary. The correct selection of components is then determined by the cross cumulative coherence which can be considerably lower than the cumulative coherence. We characterize the optimal sensing matrix and develop a constructive method to approximate it. Finally, we compare the performance of thresholding and OMP using the original and modified algorithms.},
  timestamp = {2016-10-04T16:17:34Z},
  number = {5},
  journal = {IEEE Transactions on Signal Processing},
  author = {Schnass, K. and Vandergheynst, P.},
  month = may,
  year = {2008},
  keywords = {approximation theory,computational complexity,cross cumulative coherence,dictionary preconditioning,Greedy algorithms,matrix algebra,OMP,optimal sensing matrix,orthogonal matching pursuit,preconditioning,sensing dictionary,signal reconstruction,signal representation,signal thresholding,sparse approximation,sparse signal representation,Thresholding},
  pages = {1994--2002},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\R7CQTHEH\\4479507.html:text/html}
}

@book{Christensen2016,
  address = {Cham},
  series = {Applied and Numerical Harmonic Analysis},
  title = {An {{Introduction}} to {{Frames}} and {{Riesz Bases}}},
  isbn = {978-3-319-25611-5 978-3-319-25613-9},
  timestamp = {2016-10-05T09:51:55Z},
  urldate = {2016-10-05},
  publisher = {{Springer International Publishing}},
  author = {Christensen, Ole},
  year = {2016}
}

@book{AdamsStrattonJulius1941,
  title = {Electromagnetic {{Theory}}},
  lccn = {31016},
  language = {eng},
  timestamp = {2016-10-05T09:54:44Z},
  urldate = {2016-10-05},
  publisher = {{Mcgraw Hill Book Company}},
  author = {{Adams Stratton Julius}},
  collaborator = {{Osmania University} and {Digital Library Of India}},
  year = {1941},
  keywords = {NATURAL SCIENCES}
}

@book{Christensen2008,
  address = {Boston},
  title = {Frames and {{Bases}}},
  isbn = {978-0-8176-4677-6 978-0-8176-4678-3},
  language = {en},
  timestamp = {2016-10-05T09:55:41Z},
  urldate = {2016-10-05},
  publisher = {{Birkh{\"a}user Boston}},
  author = {Christensen, Ole},
  year = {2008}
}

@book{Gill1990,
  address = {Redwood City, Calif.},
  title = {Numerical {{Linear Algebra}} and {{Optimization}}},
  volume = {1},
  isbn = {978-0-201-12649-5},
  language = {English},
  timestamp = {2016-10-05T13:49:27Z},
  publisher = {{Perseus Books}},
  author = {Gill, Philip E. and Murray, Walter and Wright, Margaret H.},
  month = sep,
  year = {1990}
}

@book{Bertsekas1999,
  edition = {2},
  title = {{Nonlinear Programming}},
  isbn = {978-1-886529-00-7},
  language = {Anglais},
  timestamp = {2016-10-05T13:52:08Z},
  publisher = {{Athena Scientific}},
  author = {Bertsekas, Dimitri P.},
  month = sep,
  year = {1999}
}

@article{Maleki2010,
  title = {Optimally {{Tuned Iterative Reconstruction Algorithms}} for {{Compressed Sensing}}},
  volume = {4},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2009.2039176},
  abstract = {We conducted an extensive computational experiment, lasting multiple CPU-years, to optimally select parameters for two important classes of algorithms for finding sparse solutions of underdetermined systems of linear equations. We make the optimally tuned implementations available at sparselab.stanford.edu; they run ??out of the box?? with no user tuning: it is not necessary to select thresholds or know the likely degree of sparsity. Our class of algorithms includes iterative hard and soft thresholding with or without relaxation, as well as CoSaMP, subspace pursuit and some natural extensions. As a result, our optimally tuned algorithms dominate such proposals. Our notion of optimality is defined in terms of phase transitions, i.e., we maximize the number of nonzeros at which the algorithm can successfully operate. We show that the phase transition is a well-defined quantity with our suite of random underdetermined linear systems. Our tuning gives the highest transition possible within each class of algorithms. We verify by extensive computation the robustness of our recommendations to the amplitude distribution of the nonzero coefficients as well as the matrix ensemble defining the underdetermined system. Our findings include the following. 1) For all algorithms, the worst amplitude distribution for nonzeros is generally the constant-amplitude random-sign distribution, where all nonzeros are the same amplitude. 2) Various random matrix ensembles give the same phase transitions; random partial isometries may give different transitions and require different tuning. 3) Optimally tuned subspace pursuit dominates optimally tuned CoSaMP, particularly so when the system is almost square.},
  timestamp = {2016-10-07T10:17:32Z},
  number = {2},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  author = {Maleki, A. and Donoho, D. L.},
  month = apr,
  year = {2010},
  keywords = {compressed sensing,degree of sparsity,Iterative algorithms,iterative methods,iterative reconstruction algorithms,linear equations,matrix ensembles,optimal tuning,phase transition,Random Matrices,random partial isometries,signal reconstruction,Sparse matrices,sparse solutions,Thresholding,tuning,underdetermined linear systems},
  pages = {330--341},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\W2H54EUJ\\5419070.html:text/html}
}

@article{Blumensath2008,
  title = {Iterative {{Thresholding}} for {{Sparse Approximations}}},
  volume = {14},
  issn = {1069-5869, 1531-5851},
  doi = {10.1007/s00041-008-9035-z},
  abstract = {Sparse signal expansions represent or approximate a signal using a small number of elements from a large collection of elementary waveforms. Finding the optimal sparse expansion is known to be NP hard in general and non-optimal strategies such as Matching Pursuit, Orthogonal Matching Pursuit, Basis Pursuit and Basis Pursuit De-noising are often called upon. These methods show good performance in practical situations, however, they do not operate on the $\mathscr{l}$0 penalised cost functions that are often at the heart of the problem. In this paper we study two iterative algorithms that are minimising the cost functions of interest. Furthermore, each iteration of these strategies has computational complexity similar to a Matching Pursuit iteration, making the methods applicable to many real world problems. However, the optimisation problem is non-convex and the strategies are only guaranteed to find local solutions, so good initialisation becomes paramount. We here study two approaches. The first approach uses the proposed algorithms to refine the solutions found with other methods, replacing the typically used conjugate gradient solver. The second strategy adapts the algorithms and we show on one example that this adaptation can be used to achieve results that lie between those obtained with Matching Pursuit and those found with Orthogonal Matching Pursuit, while retaining the computational complexity of the Matching Pursuit algorithm.},
  language = {en},
  timestamp = {2016-10-07T11:27:57Z},
  number = {5-6},
  urldate = {2016-10-07},
  journal = {Journal of Fourier Analysis and Applications},
  author = {Blumensath, Thomas and Davies, Mike E.},
  month = sep,
  year = {2008},
  pages = {629--654},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NS8PACQI\\s00041-008-9035-z.html:text/html}
}

@book{Foucart2013,
  title = {A Mathematical Introduction to Compressive Sensing},
  timestamp = {2016-10-07T14:19:20Z},
  publisher = {{Birkh{\"a}user}},
  author = {Foucart, Simon and Rauhut, Holger},
  year = {2013},
  annote = {ISBN: 978-0-8176-4947-0 (Print) 978-0-8176-4948-7 (Online)},
  owner = {Fardin}
}

@article{Duarte2008,
  title = {Single-{{Pixel Imaging}} via {{Compressive Sampling}}},
  volume = {25},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914730},
  abstract = {In this article, the authors present a new approach to building simpler, smaller, and cheaper digital cameras that can operate efficiently across a broader spectral range than conventional silicon-based cameras. The approach fuses a new camera architecture based on a digital micromirror device with the new mathematical theory and algorithms of compressive sampling.},
  timestamp = {2016-10-10T09:54:12Z},
  number = {2},
  journal = {IEEE Signal Processing Magazine},
  author = {Duarte, M. F. and Davenport, M. A. and Takbar, D. and Laska, J. N. and Sun, T. and Kelly, K. F. and Baraniuk, R. G.},
  month = mar,
  year = {2008},
  keywords = {broad spectral range,cameras,Charge coupled devices,CMOS image sensors,Compressive sampling,Computer architecture,data compression,digital camera,Digital cameras,digital micromirror device,digital photography,Hyperspectral imaging,image reconstruction,Image sampling,Layout,Lenses,micromirrors,Optical computing,Sampling methods,single pixel imaging},
  pages = {83--91},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JQCFPVKX\\4472247.html:text/html}
}

@inproceedings{Chen2014c,
  title = {High Speed Single-Pixel Imaging via Time Domain Compressive Sampling},
  abstract = {We report a high speed single-pixel microscopic imaging method by compressive sampling accomplished with time-stretch technique. By this method, we demonstrate a single-pixel imaging system with 1000 times faster than the conventional single-pixel cameras.},
  timestamp = {2016-10-10T09:54:05Z},
  booktitle = {2014 {{Conference}} on {{Lasers}} and {{Electro}}-{{Optics}} ({{CLEO}}) - {{Laser Science}} to {{Photonic Applications}}},
  author = {Chen, H. and Weng, Z. and Liang, Y. and Lei, C. and Xing, F. and Chen, M. and Xie, S.},
  month = jun,
  year = {2014},
  keywords = {cameras,compressed sensing,conventional single-pixel cameras,fibre optic sensors,high speed single-pixel microscopic imaging method,Image sampling,image sensors,Imaging,Optical fiber dispersion,Optical fibers,Optical pulses,Optical variables measurement,Pulse measurements,single-pixel imaging system,time domain compressive sampling,time-stretch technique},
  pages = {1--2},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\EBTJ3BN5\\6988920.html:text/html}
}

@article{Haldar2011,
  title = {Compressed-{{Sensing MRI With Random Encoding}}},
  volume = {30},
  issn = {0278-0062},
  doi = {10.1109/TMI.2010.2085084},
  abstract = {Compressed sensing (CS) has the potential to reduce magnetic resonance (MR) data acquisition time. In order for CS-based imaging schemes to be effective, the signal of interest should be sparse or compressible in a known representation, and the measurement scheme should have good mathematical properties with respect to this representation. While MR images are often compressible, the second requirement is often only weakly satisfied with respect to commonly used Fourier encoding schemes. This paper investigates the use of random encoding for CS-MRI, in an effort to emulate the ``universal'' encoding schemes suggested by the theoretical CS literature. This random encoding is achieved experimentally with tailored spatially-selective radio-frequency (RF) pulses. Both simulation and experimental studies were conducted to investigate the imaging properties of this new scheme with respect to Fourier schemes. Results indicate that random encoding has the potential to outperform conventional encoding in certain scenarios. However, our study also indicates that random encoding fails to satisfy theoretical sufficient conditions for stable and accurate CS reconstruction in many scenarios of interest. Therefore, there is still no general theoretical performance guarantee for CS-MRI, with or without random encoding, and CS-based methods should be developed and validated carefully in the context of specific applications.},
  timestamp = {2016-10-10T09:53:57Z},
  number = {4},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Haldar, J. P. and Hernando, D. and Liang, Z. P.},
  month = apr,
  year = {2011},
  keywords = {algorithms,biomedical MRI,Brain,compressed sensing,compressed-sensing MRI,data acquisition,Encoding,Fourier analysis,Fourier encoding scheme,Humans,Image coding,image reconstruction,Imaging,imaging properties,magnetic resonance data acquisition time,magnetic resonance imaging,magnetic resonance imaging (MRI),mathematical properties,measurement scheme,Monte Carlo Method,Noise,Phantoms; Imaging,Radio frequency,radio-frequency encoding,random encoding,Reproducibility of Results,Signal Processing; Computer-Assisted,spatially-selective radio-frequency pulse,universal encoding scheme},
  pages = {893--903},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UTRRKW9E\\5599301.html:text/html}
}

@article{Rao2012a,
  title = {Universal {{Measurement Bounds}} for {{Structured Sparse Signal Recovery}}},
  timestamp = {2016-10-10T11:45:19Z},
  journal = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  author = {Rao, Nikhil and Recht, Benjamin and Nowak, Robert},
  year = {2012},
  pages = {942--950}
}

@article{vandenBerg2008,
  title = {Probing the {{Pareto Frontier}} for {{Basis Pursuit Solutions}}},
  volume = {31},
  issn = {1064-8275},
  doi = {10.1137/080714488},
  abstract = {The basis pursuit problem seeks a minimum one-norm solution of an underdetermined least-squares problem. Basis pursuit denoise (BPDN) fits the least-squares problem only approximately, and a single parameter determines a curve that traces the optimal trade-off between the least-squares fit and the one-norm of the solution. We prove that this curve is convex and continuously differentiable over all points of interest, and show that it gives an explicit relationship to two other optimization problems closely related to BPDN. We describe a root-finding algorithm for finding arbitrary points on this curve; the algorithm is suitable for problems that are large scale and for those that are in the complex domain. At each iteration, a spectral gradient-projection method approximately minimizes a least-squares problem with an explicit one-norm constraint. Only matrix-vector operations are required. The primal-dual solution of this problem gives function and derivative information needed for the root-finding method. Numerical experiments on a comprehensive set of test problems demonstrate that the method scales well to large problems.},
  timestamp = {2016-10-11T15:24:24Z},
  number = {2},
  urldate = {2016-10-11},
  journal = {SIAM Journal on Scientific Computing},
  author = {{van den Berg}, E. and Friedlander, M.},
  month = nov,
  year = {2008},
  pages = {890--912},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\I2AI5ZM6\\080714488.html:text/html}
}

@incollection{Zhao2012a,
  series = {Lecture Notes in Computer Science},
  title = {The {{Method}} for {{Constructing Block Sparse Measurement Matrix Based}} on {{Orthogonal Vectors}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-34777-1 978-3-642-34778-8},
  abstract = {Compressive sensing is a new way of information processing which recover the original signal through acquiring much fewer measurements with a measurement matrix. The measurement matrix has an important effect in signal sampling and reconstruction algorithm. However, there are two main problems in currently existing matrices: the difficulty of hardware implementation and high computation complexity. In this paper, we proposed a class of highly sparse and deterministic scrambled block measurement matrices based on orthogonal vectors (SBOV). It could improve sensing efficiency and reduce computation complexity. Those matrices constructed by the proposed method only need very little memory space and they could be easily implemented in hardware due to their simple entries. Some experiments show the better imaging performance comparable to scrambled block Hadamard matrix (SBH) and dense partial Hadamard matrix. SBOV matrices are simpler and sparser than SBH matrix.},
  language = {en},
  timestamp = {2016-10-11T15:30:15Z},
  number = {7674},
  urldate = {2016-10-11},
  booktitle = {Advances in {{Multimedia Information Processing}} \textendash{} {{PCM}} 2012},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Zhao, Ruizhen and Qin, Zhou and Tang, Jinhui},
  editor = {Lin, Weisi and Xu, Dong and Ho, Anthony and Wu, Jianxin and He, Ying and Cai, Jianfei and Kankanhalli, Mohan and Sun, Ming-Ting},
  month = dec,
  year = {2012},
  keywords = {block and sparse matrix,compressive sensing,Computer Communication Networks,Information Storage and Retrieval,Information Systems Applications (incl. Internet),measurement matrix,Multimedia Information Systems,orthogonal vectors,Software Engineering,User Interfaces and Human Computer Interaction},
  pages = {872--879},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\R3J4QFW6\\978-3-642-34778-8_82.html:text/html},
  doi = {10.1007/978-3-642-34778-8_82}
}

@incollection{Gretsistas2012,
  series = {Lecture Notes in Computer Science},
  title = {Group {{Polytope Faces Pursuit}} for {{Recovery}} of {{Block}}-{{Sparse Signals}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-28550-9 978-3-642-28551-6},
  abstract = {Polytope Faces Pursuit is an algorithm that solves the standard sparse recovery problem. In this paper, we consider the case of block structured sparsity, and propose a novel algorithm based on the Polytope Faces Pursuit which incorporates this prior knowledge. The so-called Group Polytope Faces Pursuit is a greedy algorithm that adds one group of dictionary atoms at a time and adopts a path following approach based on the geometry of the polar polytope associated with the dual linear program. The complexity of the algorithm is of similar order to Group Orthogonal Matching Pursuit. Numerical experiments demonstrate the validity of the algorithm and illustrate that in certain cases the proposed algorithm outperforms the Group Orthogonal Matching Pursuit algorithm.},
  language = {en},
  timestamp = {2016-10-11T15:31:53Z},
  number = {7191},
  urldate = {2016-10-11},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Gretsistas, Aris and Plumbley, Mark D.},
  editor = {Theis, Fabian and Cichocki, Andrzej and Yeredor, Arie and Zibulevsky, Michael},
  month = mar,
  year = {2012},
  keywords = {Algorithm Analysis and Problem Complexity,block-sparsity,Discrete Mathematics in Computer Science,Image Processing and Computer Vision,Pattern Recognition,polytopes,Simulation and Modeling,sparse representations,Special Purpose and Application-Based Systems},
  pages = {255--262},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QQBJDFVZ\\978-3-642-28551-6_32.html:text/html},
  doi = {10.1007/978-3-642-28551-6_32}
}

@incollection{Varol2014,
  series = {Lecture Notes in Computer Science},
  title = {Supervised {{Block Sparse Dictionary Learning}} for {{Simultaneous Clustering}} and {{Classification}} in {{Computational Anatomy}}},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-10469-0 978-3-319-10470-6},
  abstract = {An important prerequisite for computational neuroanatomy is the spatial normalization of the data. Despite its importance for the success of the subsequent statistical analysis, image alignment is dealt with from the perspective of image matching, while its influence on the group analysis is neglected. The choice of the template, the registration algorithm as well as the registration parameters, all confound group differences and impact the outcome of the analysis. In order to limit their influence, we perform multiple registrations by varying these parameters, resulting in multiple instances for each sample. In order to harness the high dimensionality of the data and emphasize the group differences, we propose a supervised dimensionality reduction technique that takes into account the organization of the data. This is achieved by solving a supervised dictionary learning problem for block-sparse signals. Structured sparsity allows the grouping of instances across different independent samples, while label supervision allows for discriminative dictionaries. The block structure of dictionaries allows constructing multiple classifiers that treat each dictionary block as a basis of a subspace that spans a separate band of information. We formulate this problem as a convex optimization problem with a geometric programming (GP) component. Promising results that demonstrate the potential of the proposed approach are shown for an MR image dataset of Autism subjects.},
  language = {en},
  timestamp = {2016-10-11T15:33:23Z},
  number = {8674},
  urldate = {2016-10-11},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}} \textendash{} {{MICCAI}} 2014},
  publisher = {{Springer International Publishing}},
  author = {Varol, Erdem and Davatzikos, Christos},
  editor = {Golland, Polina and Hata, Nobuhiko and Barillot, Christian and Hornegger, Joachim and Howe, Robert},
  month = sep,
  year = {2014},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Health Informatics,Image Processing and Computer Vision,Imaging / Radiology,Pattern Recognition},
  pages = {446--453},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\WMPEZUVX\\978-3-319-10470-6_56.html:text/html},
  doi = {10.1007/978-3-319-10470-6_56}
}

@incollection{Zhang2014f,
  series = {Advances in Intelligent Systems and Computing},
  title = {A {{New Image}}-{{Fusion Technique Based}} on {{Blocked Sparse Representation}}},
  copyright = {\textcopyright{}2014 Springer India},
  isbn = {978-81-322-1758-9 978-81-322-1759-6},
  abstract = {An image-fusion scheme based on blocked sparse representation is presented in the paper. Firstly, the source images are segmented into patches and then the patches are sparsely represented with learned redundant dictionary. Following that, a salient feature of each sparse coefficient vector is calculated by integrating the sparsity and the l1l1 l\^\{1\} -norm of the sparse coefficient vector. Next, the sparse coefficient vectors are fused by adopting the weighted average rule in which the weighted factors are proportional to the salient features of the sparse coefficient vectors. Finally, the fusion image is constructed by the fused coefficient vector with the learned redundant dictionary. Experiments show that the fusion algorithm is effective and superior to the method-based wavelet decomposition.},
  language = {en},
  timestamp = {2016-10-11T15:35:14Z},
  number = {255},
  urldate = {2016-10-11},
  booktitle = {Proceedings of {{International Conference}} on {{Computer Science}} and {{Information Technology}}},
  publisher = {{Springer India}},
  author = {Zhang, Yongping and Chen, Yaojia},
  editor = {Patnaik, Srikanta and Li, Xiaolong},
  year = {2014},
  keywords = {Computational Intelligence,Computer Communication Networks,Computer Imaging; Vision; Pattern Recognition and Graphics,Fusion rules,Multi-focus images,sparse representations,sparsity},
  pages = {53--60},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\C25G7SM7\\978-81-322-1759-6_7.html:text/html},
  doi = {10.1007/978-81-322-1759-6_7}
}

@article{Wang2014e,
  title = {{{SBL}}-Based Multi-Task Algorithms for Recovering Block-Sparse Signals with Unknown Partitions},
  volume = {2014},
  issn = {1687--6180},
  doi = {10.1186/1687-6180-2014-14},
  abstract = {We consider in this paper the problem of reconstructing block-sparse signals with unknown block partitions. In the first part of this work, we extend the block-sparse Bayesian learning (BSBL) originally developed for recovering a single block-sparse signal in a single compressive sensing (CS) task scenario to the case of multiple CS tasks. A new multi-task signal recovery algorithm, called the extended multi-task block-sparse Bayesian learning (EMBSBL), is proposed. EMBSBL exploits the statistical correlation among multiple signals as well as the intra-block correlation within individual signals to improve performance. Besides, it does not need a priori information on block partition. As the second part of this paper, we develop the EMBSBL-based synthesized multi-task signal recovery algorithm, namely SEMBSBL, to make it applicable to the single CS task case. The idea is to synthesize new CS tasks from the single CS task via circular-shifting operations and utilizes the minimum description length principle to determine the proper set of the synthesized CS tasks for signal reconstruction. SEMBSBL can achieve better signal reconstruction performance over other algorithms that recover block-sparse signals individually. Simulations corroborate the theoretical developments.},
  language = {en},
  timestamp = {2016-10-11T15:37:09Z},
  number = {1},
  urldate = {2016-10-11},
  journal = {EURASIP Journal on Advances in Signal Processing},
  author = {Wang, Ying-Gui and Yang, Le and Liu, Zheng and Jiang, Wen-Li},
  month = feb,
  year = {2014},
  pages = {14},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XV79K3W5\\1687-6180-2014-14.html:text/html}
}

@incollection{Xiao2014,
  series = {Lecture Notes in Computer Science},
  title = {Weighted {{Block}}-{{Sparse Low Rank Representation}} for {{Face Clustering}} in {{Videos}}},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-10598-7 978-3-319-10599-4},
  abstract = {In this paper, we study the problem of face clustering in videos. Specifically, given automatically extracted faces from videos and two kinds of prior knowledge (the face track that each face belongs to, and the pairs of faces that appear in the same frame), the task is to partition the faces into a given number of disjoint groups, such that each group is associated with one subject. To deal with this problem, we propose a new method called weighted block-sparse low rank representation (WBSLRR) which considers the available prior knowledge while learning a low rank data representation, and also develop a simple but effective approach to obtain the clustering result of faces. Moreover, after using several acceleration techniques, our proposed method is suitable for solving large-scale problems. The experimental results on two benchmark datasets demonstrate the effectiveness of our approach.},
  language = {en},
  timestamp = {2016-10-11T15:38:06Z},
  number = {8694},
  urldate = {2016-10-11},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  publisher = {{Springer International Publishing}},
  author = {Xiao, Shijie and Tan, Mingkui and Xu, Dong},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  month = sep,
  year = {2014},
  keywords = {Artificial Intelligence (incl. Robotics),block-sparsity,Computer Graphics,face clustering,Image Processing and Computer Vision,low rank representation,Pattern Recognition,Subspace clustering},
  pages = {123--138},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9H9KIXEG\\978-3-319-10599-4_9.html:text/html},
  doi = {10.1007/978-3-319-10599-4_9}
}

@article{Lin2013a,
  title = {Block Sparse Recovery via Mixed $l_2/l_1$ Minimization},
  volume = {29},
  issn = {1439-8516, 1439-7617},
  doi = {10.1007/s10114-013-1564-y},
  abstract = {We consider efficient methods for the recovery of block sparse signals
	from underdetermined system of linear equations. We show that if
	the measurement matrix satisfies the block RIP with $\delta$2s $<$
	0.4931, then every block s-sparse signal can be recovered through
	the proposed mixed l2/l1-minimization approach in the noiseless case
	and is stably recovered in the presence of noise and mismodeling
	error. This improves the result of Eldar and Mishali (in IEEE Trans.
	Inform. Theory 55: 5302\textendash{}5316, 2009). We also give another
	sufficient condition on block RIP for such recovery method: $\delta$s
	$<$ 0.307.},
  language = {en},
  timestamp = {2016-10-11T15:43:14Z},
  number = {7},
  urldate = {2016-10-11},
  journal = {Acta Mathematica Sinica, English Series},
  author = {Lin, Jun Hong and Li, Song},
  month = jan,
  year = {2013},
  pages = {1401-1412}
}

@incollection{Chen2013b,
  series = {Lecture Notes in Computer Science},
  title = {Non-Negative {{Sparse Representation Based}} on {{Block NMF}} for {{Face Recognition}}},
  copyright = {\textcopyright{}2013 Springer International Publishing Switzerland},
  isbn = {978-3-319-02960-3 978-3-319-02961-0},
  abstract = {This paper attempts to utilize the basis images of block non-negative matrix factorization (BNMF) to serve as the sparse learning dictionary, which is more suitable for non-negative sparse representation (NSR) because they have non-negative compatibility. Based on BNMF-basis-image dictionary, the NSR features of query facial images can be learnt directly by solving l 1-regularized least square problems. The NSR-feature based algorithm is then developed and successfully applied to face recognition. Subsequently, to further enhance the discriminant power of NSR method, this paper also proposes a feature fusion approach via combining NSR-feature with BNMF-feature. The proposed algorithms are tested on ORL and FERET face databases. Experimental results show that the proposed NSR+BNMF method greatly outperforms two single-feature based methods, namely NSR method and BNMF method.},
  language = {en},
  timestamp = {2016-10-11T15:47:30Z},
  number = {8232},
  urldate = {2016-10-11},
  booktitle = {Biometric {{Recognition}}},
  publisher = {{Springer International Publishing}},
  author = {Chen, Meng and Chen, Wen-Sheng and Chen, Bo and Pan, Binbin},
  editor = {Sun, Zhenan and Shan, Shiguan and Yang, Gongping and Zhou, Jie and Wang, Yunhong and Yin, YiLong},
  year = {2013},
  keywords = {Algorithm Analysis and Problem Complexity,Biometrics,Block Non-negative Matrix Factorization,Computer Graphics,face recognition,Image Processing and Computer Vision,Information Systems Applications (incl. Internet),Non-negative Sparse Representation,Pattern Recognition},
  pages = {26--33},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\U86PDU77\\978-3-319-02961-0_4.html:text/html},
  doi = {10.1007/978-3-319-02961-0_4}
}

@incollection{Azghani2014,
  series = {Applied and Numerical Harmonic Analysis},
  title = {Sparse {{Signal Processing}}},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-08800-6 978-3-319-08801-3},
  abstract = {Conventional sampling techniques are based on Shannon-Nyquist theory which states that the required sampling rate for perfect recovery of a band-limited signal is at least twice its bandwidth. The band-limitedness property of the signal plays a significant role in the design of conventional sampling and reconstruction systems. As the natural signals are not necessarily band-limited, a low-pass filter is applied to the signal prior to its sampling for the purpose of antialiasing. Most of the signals we are faced with are sparse rather than band-limited (or low pass). It means that they have a small number of non-zero coefficients in some domain such as time, discrete cosine transform (DCT), discrete wavelet transform (DWT), or discrete fourier transform (DFT). This characteristic of the signal is the foundation for the emerging of a new signal sampling theory called Compressed Sampling, an extension of random sampling. In this chapter, an overview of compressed sensing, together with a summary of its popular recovery techniques, is presented. Moreover, as a well-known example of structured sparsity, the block sparse recovery problem is investigated and the related recovery approaches are illustrated.},
  language = {en},
  timestamp = {2016-10-11T15:51:10Z},
  urldate = {2016-10-11},
  booktitle = {New {{Perspectives}} on {{Approximation}} and {{Sampling Theory}}},
  publisher = {{Springer International Publishing}},
  author = {Azghani, Masoumeh and Marvasti, Farokh},
  editor = {Zayed, Ahmed I. and Schmeisser, Gerhard},
  year = {2014},
  keywords = {Approximations and Expansions,Signal; Image and Speech Processing,Special Functions},
  pages = {189--213},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\BXW27CDX\\978-3-319-08801-3_8.html:text/html},
  doi = {10.1007/978-3-319-08801-3_8}
}

@incollection{Ostromsky1996,
  series = {Lecture Notes in Computer Science},
  title = {Parallel Solution of Sparse Problems by Using a Sequence of Large Dense Blocks},
  copyright = {\textcopyright{}1996 Springer-Verlag},
  isbn = {978-3-540-62095-2 978-3-540-49643-4},
  abstract = {Linear least squares problems arise in many important fields of science and engineering (such as econometry, geodesy, statistics, structural analysis, fluid dynamics, etc.). Applications from all these fields are to be handled numerically in the solution of various models used in the development of new industrial products. At the National Environmental Research Institute (Denmark), linear least squares problems have been applied in the efforts to determine optimal values of some of the parameters involved in air pollution models. Large and sparse least squares problems can be handled by treating a sequence of dense blocks. Standard parallel subroutines, which perform orthogonal decomposition of dense matrices (as, for example, subroutines from LAPACK, SCALAPACK or NAG), can be used to handle successively the dense blocks. A preliminary reordering procedure, LORA, is to be applied at the beginning of the computational process. The size of the blocks can be adjusted to the particular computer used. Results obtained on a Silicon Graphics POWER CHALLENGE are given. The same method can also be used in the solution of large and sparse systems of linear algebraic equations.},
  language = {en},
  timestamp = {2016-10-11T15:53:10Z},
  number = {1184},
  urldate = {2016-10-11},
  booktitle = {Applied {{Parallel Computing Industrial Computation}} and {{Optimization}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ostromsky, Tz and Salvini, S. and Wasniewski, J. and Zlatev, Z.},
  editor = {Wa{\'s}niewski, Jerzy and Dongarra, Jack and Madsen, Kaj and Olesen, Dorte},
  month = aug,
  year = {1996},
  keywords = {Algorithm Analysis and Problem Complexity,Computer System Implementation,Discrete Mathematics in Computer Science,Engineering; general,Programming Techniques,Theory of Computation},
  pages = {555--564},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IW2RW4MU\\3-540-62095-8_60.html:text/html},
  doi = {10.1007/3-540-62095-8_60}
}

@inproceedings{Li2015m,
  title = {Direction Finding Based on Single Channel Array Using Block-Sparse Recovery},
  doi = {10.1109/ICCWAMTIP.2015.7493899},
  abstract = {A new method of direction finding based on the single channel array is proposed. By calculating the output average powers for different weight vectors, a sparse representation model on covariance vectors is constructed. The direction finding problem is converted into an error-based block-sparse recovery problem, which can be efficiently solved by the method of sparse recovery. Compared with the existing methods of covariance matrix recovery and sparse recovery, the proposed method can resolve both uncorrelated and coherent signals, as well as obtaining better accuracy. Simulation results demonstrate the effectiveness and efficiency of the proposed method.},
  timestamp = {2016-10-12T11:30:09Z},
  booktitle = {2015 12th {{International Computer Conference}} on {{Wavelet Active Media Technology}} and {{Information Processing}} ({{ICCWAMTIP}})},
  author = {Li, H. and Xu, X. and Ye, Z.},
  month = dec,
  year = {2015},
  keywords = {Array signal processing,Covariance matrices,covariance matrix recovery,covariance vectors,direction finding,error-based block-sparse recovery problem,Estimation,Power generation,radio direction-finding,Receivers,Sensor arrays,signal processing,Signal to noise ratio,single channel array,Single Channel Receiver,sparse recovery,sparse representation model},
  pages = {20--25},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XRR6V65G\\7493899.html:text/html}
}

@inproceedings{Abtahi2016,
  title = {Iterative Block-Sparse Recovery Method for Distributed {{MIMO}} Radar},
  doi = {10.1109/IWCIT.2016.7491622},
  abstract = {In this paper, an iterative method for block-sparse recovery is suggested for target parameters estimation in a distributed MIMO radar system. The random sampling has been used as the sensing scheme in the receivers. The simulation results prove that the proposed method is superior to the other state-of-the-art techniques in the accuracy of the target estimation task.},
  timestamp = {2016-10-12T11:29:45Z},
  booktitle = {2016 {{Iran Workshop}} on {{Communication}} and {{Information Theory}} ({{IWCIT}})},
  author = {Abtahi, A. and Azghani, M. and Tayefi, J. and Marvasti, F.},
  month = may,
  year = {2016},
  keywords = {compressed sensing,Compressive sampling,distributed MIMO Radar,distributed MIMO radar system,Estimation,iterative block-sparse recovery method,iterative methods,MIMO radar,radar signal processing,random sampling,Receivers,Sampling methods,Signal to noise ratio,sparse signal processing,target parameter estimation,Transmitters},
  pages = {1--4},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AEFNHDT5\\7491622.html:text/html}
}

@inproceedings{Weiland2015,
  title = {Coherent {{MIMO}} Radar Imaging with Model-Aware Block Sparse Recovery},
  doi = {10.1109/CAMSAP.2015.7383827},
  abstract = {We propose a framework for coherent MIMO radar imaging that allows to combine range imaging using iterative block sparse recovery with conventional, grid-less azimuth imaging. In each iteration of a block-sparse range recovery algorithm, we incorporate a projection step using an arbitrary azimuth imaging algorithm. The proposed imaging framework is hence not limited to azimuth imaging on a grid. This is in contrast to imaging approaches using sparse recovery that are available in the literature and that usually require rather coarse angular grids. Our simulation results show that this model-aware imaging framework achieves improved range and azimuth imaging performance in comparison to separate azimuth estimation after range imaging with block sparse recovery or the conventionally used matched filtering approach.},
  timestamp = {2016-10-12T11:29:52Z},
  booktitle = {2015 {{IEEE}} 6th {{International Workshop}} on {{Computational Advances}} in {{Multi}}-{{Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Weiland, L. and Wiese, T. and Utschick, W.},
  month = dec,
  year = {2015},
  keywords = {Azimuth,azimuth estimation,block sparse range recovery algorithm,block sparsity,Channel estimation,coherent MIMO radar imaging,combine range imaging,compressed sensing,Estimation,gridless azimuth imaging,Imaging,iterative block sparse recovery,matched filter,MIMO radar,model aware block sparse recovery,model aware imaging,radar imaging,sparse recovery},
  pages = {425--428},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VRAC8W9J\\7383827.html:text/html}
}

@inproceedings{Weiland2015a,
  title = {Coherent {{MIMO}} Radar Range Imaging with Block Sparse Recovery},
  doi = {10.1109/MILCOM.2015.7357495},
  abstract = {We consider range imaging for coherent MIMO radar, that is, the joint estimation of the channel matrices for all range bins of the radar scene. The fundamental problem is the lack of ideal ambiguity properties of the available waveforms. This is why matched filters are generally suboptimal estimators. Approaches like the instrumental variable filter overcome this problem to a certain extent, but need prior knowledge of the interfering range bins. We take a different approach and show that range imaging can be formulated as a block sparse recovery problem. The block structure arises as the coefficients of the channel matrix of a range bin are either all zero or nonzero. In a second step, high-resolution methods for azimuth estimation can be used. This is in contrast to other sparse recovery approaches in coherent MIMO radar imaging where range, Doppler, and azimuth estimation is performed simultaneously and resolution is limited by coarse grids. We make a first step towards the analysis of sparse recovery based range imaging for coherent MIMO radar by presenting numerical recovery results using iterative algorithms. Our simulation results demonstrate that the channel matrices for all range bins can be estimated reliably, even for a large number of targets.},
  timestamp = {2016-10-12T11:30:01Z},
  booktitle = {{{MILCOM}} 2015 - 2015 {{IEEE Military Communications Conference}}},
  author = {Weiland, L. and Wiese, T. and Utschick, W.},
  month = oct,
  year = {2015},
  keywords = {azimuth estimation,block sparse recovery,block sparsity,Channel estimation,channel matrices,coherent MIMO radar imaging,compressed sensing,Doppler radar,estimation theory,Imaging,instrumental variable filter,Iterative algorithms,iterative methods,joint estimation,Matched filters,MIMO radar,Radar antennas,radar imaging,radar scene,range imaging,Sparse matrices,sparse recovery,suboptimal estimators},
  pages = {517--522},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PQ3GTKQE\\7357495.html:text/html}
}

@inproceedings{Li2014g,
  title = {Iteratively Reweighted Least Squares for Block-Sparse Recovery},
  doi = {10.1109/ICIEA.2014.6931321},
  abstract = {The compressive sensing (CS) theory has shown that sparse signals can be reconstructed exactly from much fewer measurements than traditionally believed. What's more, using $\mathscr{l}$p-norm minimization with p $<$; 1 can do so with much fewer measurements than with p=1. In this paper, a novel algorithm is proposed for computing local minima of the nonconvex problem in the block-sparse system. A series of experiments are presented to show the remarkable performance of our proposed algorithm in block sparse signal recovery, and compare the recovery ability of this algorithm with the IRLS and BOMP algorithm.},
  timestamp = {2016-10-12T11:54:53Z},
  booktitle = {2014 9th {{IEEE Conference}} on {{Industrial Electronics}} and {{Applications}}},
  author = {Li, S. and Li, Q. and Li, G. and He, X. and Chang, L.},
  month = jun,
  year = {2014},
  keywords = {BIRLS,block-sparse recovery,block sparse signal reconstruction,compressed sensing,compressive sensing,concave programming,CS theory,Dictionaries,Equations,iteratively reweighted least squares,iterative methods,least squares approximations,ℓp-norm minimization,minimization,nonconvex optimization,nonconvex problem,Signal processing algorithms,signal reconstruction,Sparse matrices,sparse signal reconstruction,underdetermined systems of linear equations,Vectors},
  pages = {1061--1066},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IIA9QJIF\\6931321.html:text/html}
}

@article{Yang2016,
  title = {Localized {{Low}}-{{Rank Promoting}} for {{Recovery}} of {{Block}}-{{Sparse Signals With Intrablock Correlation}}},
  volume = {23},
  issn = {1070-9908},
  doi = {10.1109/LSP.2016.2599525},
  abstract = {We consider the problem of recovering block-sparse signals with intrablock correlated entries. The block partition of the sparse signal is assumed unknown a priori. To exploit the block-sparse structure as well as the local smoothness of the sparse signal, consecutive coefficients of the sparse signal are organized into a number of 2\texttimes{}2 matrices, and the log-determinant function is used to promote the low rankness of these 2\texttimes{}2 matrices. We show that such a log-determinant function has the ability to promote the block-sparsity and local smoothness simultaneously. An iterative reweighted method is developed by iteratively minimizing a surrogate function of the original objective function. Simulation results show that our proposed method offers competitive performance for recovering block-sparse signals with intrablock correlated entries.},
  timestamp = {2016-10-12T11:54:18Z},
  number = {10},
  journal = {IEEE Signal Processing Letters},
  author = {Yang, L. and Fang, J. and Li, H. and Zeng, B.},
  month = oct,
  year = {2016},
  keywords = {Bayes methods,block partition,block-sparse signal,block-sparse signal recovery,block-sparse structure,compressed sensing,Correlation,intrablock correlated entries,intrablock correlation,iterative methods,iterative reweighted method,Learning systems,linear programming,localized low-rank promoting,local smoothness,log-determinant function,matrix algebra,objective function,Signal processing algorithms,smoothing methods,Sparse matrices,surrogate function,time-frequency analysis},
  pages = {1399--1403},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CQBZTQ33\\7542130.html:text/html}
}

@inproceedings{Huang2016,
  title = {Block Sparse Signal Recovery with a {{Block}}-{{Toeplitz}} Structured Measurement Matrix},
  doi = {10.1109/ChiCC.2016.7554108},
  abstract = {Recently, many applications about the recovery of block sparse signals have arisen, which can be casted as the recovery of a block sparse signal x from a measurement equation y = $\Phi$x. In this paper, we investigate block sparse signal recovery problems when $\Phi$ is assumed to be a block-concatenation of Toeplitz matrices. The algorithm of StOMP is extended to the block sparse case, and the algorithm of tBlock-StOMP is proposed. Specifically, tBlock-StOMP combines advantages of StOMP with the structural characteristics of $\Phi$ to pursue high efficiency in block sparse signal recovery. Furthermore, a modified algorithm of tBlock-StOMP, termed mtBlock-StOMP, is proposed. Compared with many other recovery algorithms, numerical simulations demonstrate that tBlock-StOMP as well as mtBlock-StOMP results in evident effectiveness in block sparse reconstruction problems.},
  timestamp = {2016-10-12T11:55:06Z},
  booktitle = {2016 35th {{Chinese Control Conference}} ({{CCC}})},
  author = {Huang, B. and Zhou, T.},
  month = jul,
  year = {2016},
  keywords = {Algorithm design and analysis,block-concatenation,block sparse,block sparse reconstruction problems,block sparse signal recovery problems,block-Toeplitz structured measurement matrix,compressive sensing,Gaussian distribution,Greedy algorithms,Matching pursuit algorithms,measurement equation,minimization,modified tBlock-StOMP algorithm,mtBlock-StOMP algorithm,parameter estimation,Signal processing algorithms,signal reconstruction,Sparse matrices,structural characteristics,Toeplitz matrices},
  pages = {4865--4870},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\4MISKPBH\\7554108.html:text/html}
}

@inproceedings{Fedorov2016,
  title = {Robust {{Bayesian}} Method for Simultaneous Block Sparse Signal Recovery with Applications to Face Recognition},
  doi = {10.1109/ICIP.2016.7533085},
  abstract = {In this paper, we present a novel Bayesian approach to recover simultaneously block sparse signals in the presence of outliers. The key advantage of our proposed method is the ability to handle non-stationary outliers, i.e. outliers which have time varying support. We validate our approach with empirical results showing the superiority of the proposed method over competing approaches in synthetic data experiments as well as the multiple measurement face recognition problem.},
  timestamp = {2016-10-12T11:55:26Z},
  booktitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Fedorov, I. and Giri, R. and Rao, B. D. and Nguyen, T. Q.},
  month = sep,
  year = {2016},
  keywords = {Bayes methods,Dictionaries,Encoding,Face,face recognition,Indexes,Learning,Robustness,signal representation},
  pages = {3872--3876},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FTQTGP9B\\7533085.html:text/html}
}

@inproceedings{Yang2016a,
  title = {A Greedy Pursuit Algorithm for Arbitrary Block Sparse Signal Recovery},
  doi = {10.1109/ISCAS.2016.7527470},
  abstract = {In this paper, we propose a novel greedy iteration algorithm, called block matching pursuit (BMP), for arbitrary block sparse signal recovery. BMP can recover the target signal without prior information of block structure or block length and can estimate the true sparsity level. In each iteration, BMP processes a correlation test to estimate nonzero entries of signal with a fixed step size, and then gathers all possible nonzero entries to form a candidate list, finally checks the list to choose correct entries according to current estimated sparsity level. The simulation results show that BMP can recover the signal of interest in noiseless or noisy case and achieves an outstanding recovery performance.},
  timestamp = {2016-10-12T11:54:26Z},
  booktitle = {2016 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Yang, E. and Yan, X. and Qin, K.},
  month = may,
  year = {2016},
  keywords = {arbitrary block sparse signal,arbitrary block sparse signal recovery,block matching pursuit,Correlation,Estimation,Greedy algorithms,greedy iteration,greedy iteration algorithm,greedy pursuit algorithm,Indexes,iterative methods,Matching pursuit algorithms,Noise measurement,signal reconstruction,Simulation,sparsity level estimation},
  pages = {1234--1237},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3A2AAQ4C\\7527470.html:text/html}
}

@inproceedings{Wiese2016,
  title = {Exact Recovery of Structured Block-Sparse Signals with Model-Aware Orthogonal Matching Pursuit},
  doi = {10.1109/SPAWC.2016.7536779},
  abstract = {We provide recovery guarantees for the model-aware OMP algorithm, which is an extension of the known block-OMP algorithm. The new algorithm exploits additional structural information about the unknown signal. This leads to successful recovery under weaker conditions on the sensing matrix, namely, a restricted isometry property with respect to structured signals, only. A possible application of this algorithm is for channel estimation in wireless communication systems where nonlinear delay and angle estimation problems need to be solved. For this application, our method only discretizes the delay parameter and not the angular parameter. As such, it is situated between very complex nonlinear maximum-likelihood estimation and very efficient compressive sensing based methods that discretize all parameters.},
  timestamp = {2016-10-12T11:54:33Z},
  booktitle = {2016 {{IEEE}} 17th {{International Workshop}} on {{Signal Processing Advances}} in {{Wireless Communications}} ({{SPAWC}})},
  author = {Wiese, T. and Weiland, L. and Utschick, W.},
  month = jul,
  year = {2016},
  keywords = {angle estimation problem,angular parameter,block-OMP algorithm,Channel estimation,Complexity theory,complex nonlinear maximum-likelihood estimation,compressed sensing,compressive sensing-based method,delay parameter,Delays,iterative methods,Matching pursuit algorithms,matrix algebra,maximum likelihood estimation,model-aware compressive sensing,model-aware OMP algorithm,model-aware orthogonal matching pursuit,model-based compressive sensing,nonlinear delay,orthogonal matching pursuit,radiocommunication,recovery guarantee,restricted isometry,restricted isometry property,sensing matrix,signal restoration,Sparse matrices,structured block-sparse signal recovery,structured signals,time-frequency analysis,Transmission line matrix methods,Union of subspaces,wireless communication systems},
  pages = {1--5},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KZNXKMT3\\7536779.html:text/html}
}

@inproceedings{Scarlett2016,
  title = {Partial Recovery Bounds for the Sparse Stochastic Block Model},
  doi = {10.1109/ISIT.2016.7541630},
  abstract = {In this paper, we study the information-theoretic limits of community detection in the symmetric two-community stochastic block model, with intra-community and inter-community edge probabilities a/n and b/n respectively. We consider the sparse setting, in which a and b do not scale with n, and provide upper and lower bounds on the proportion of community labels recovered on average. We provide a numerical example for which the bounds are near-matching for moderate values of a - b, and matching in the limit as a - b grows large.},
  timestamp = {2016-10-12T11:54:41Z},
  booktitle = {2016 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Scarlett, J. and Cevher, V.},
  month = jul,
  year = {2016},
  keywords = {Belief propagation,Biological system modeling,community detection,community labels,Computational modeling,Decoding,edge detection,Error analysis,information-theoretic limits,information theory,inter-community edge probability,intra-community edge probability,partial recovery bounds,sparse stochastic block model,stochastic processes,symmetric two-community stochastic block model},
  pages = {1904--1908},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Q9DN9UA5\\7541630.html:text/html}
}

@inproceedings{He2016,
  title = {An Iteratively Reweighted Method for Recovery of Block-Sparse Signal with Unknown Block Partition},
  doi = {10.1109/ICASSP.2016.7472526},
  abstract = {In this paper, a new iteratively reweighted least squares method is proposed for recovery of block-sparse signals with unknown cluster patterns. In many practical applications, sparse signals have block-sparse structures with nonzero coefficients occurring in clusters, while the prior information of the cluster pattern is usually unavailable. To address this issue, we propose an element-overlapping log-sum functional to encourage the sparseness and the cluster pattern simultaneously. The algorithm is developed by iteratively minimizing a convex surrogate function that majorizes the original objective function, which results in an iteratively reweighted process that alternates between estimating the sparse signal and refining the weights of the surrogate function. Convergence of the iterations to a local minimum of the penalty function is also guaranteed. Numerical results are provided to illustrate the effectiveness of the proposed method.},
  timestamp = {2016-10-12T11:55:21Z},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {He, Q. and Fang, J. and Chen, Z. and Li, S.},
  month = mar,
  year = {2016},
  keywords = {Bayes methods,block-sparse signal recovery,block sparse signal recovery,Clustering algorithms,cluster patterns,convergence,convex programming,convex surrogate function,element overlapping log sum functional,element-overlapping log-sum functional,Estimation,iteratively reweighted,iteratively reweighted method,iterative methods,least squares approximations,least squares method,minimization,Noise measurement,nonzero coefficients,Partitioning algorithms,signal processing,unknown block partition},
  pages = {4488--4492},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\GNF6GI8B\\7472526.html:text/html}
}

@inproceedings{He2016a,
  title = {Sparse Recovery of Multiple Measurement Vectors in Impulsive Noise: {{A}} Smooth Block Successive Minimization Algorithm},
  shorttitle = {Sparse Recovery of Multiple Measurement Vectors in Impulsive Noise},
  doi = {10.1109/ICASSP.2016.7472537},
  abstract = {This paper considers the sparse recovery problem of multiple measurement vector (MMV) model corrupted in impulsive noise. To ensure outlier-robust sparse recovery, we formulate an MMV problem that includes the generalized $\mathscr{l}$p-norm (1 $<$; p $<$; 2) divergence data-fidelity term added to the $\mathscr{l}$2,0 joint sparsity-promoting regularizer. The joint sparse penalty, however, is non-continuous and hence non-differentiable, which inevitably raises difficulty in optimization when using a gradient-based method. To address this, we build a smooth approximation for the $\mathscr{l}$2,0-based sparse metric via the log-sum based sparse-encouraging surrogate function. Then, we propose a block successive upper-bound minimization algorithm for the smooth MMV problem by solving a series of subproblems based on the block coordinate descent (BCD) method. Furthermore, local convergence of the proposed algorithm to a stationary point of the smooth problem is proved. Experiments demonstrate its efficiency and robust recovery performance for suppressing impulsive noise.},
  timestamp = {2016-10-12T11:55:15Z},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {He, Z. Q. and Shi, Z. P. and Huang, L. and Li, H. and So, H. C.},
  month = mar,
  year = {2016},
  keywords = {Approximation algorithms,approximation theory,Atmospheric measurements,BCD method,block coordinate descent method,block successive upper-bound minimization algorithm,compressed sensing,convergence,gradient-based method,gradient methods,impulse noise,impulsive noise,impulsive noise suppression,ℓ2;0-based sparse metric,ℓ2;0 joint sparsity-promoting regularizer,log-sum based sparse-encouraging surrogate function,ℓp-norm generalization,minimisation,minimization,MMV model,multiple measurement vector model,Multiple measurement vectors,Noise measurement,Optimization,outlier-robust sparse signal recovery problem,Particle measurements,smooth block successive minimization algorithm,sparse signal recovery,Vectors},
  pages = {4543--4547},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZB69JUUW\\7472537.html:text/html}
}

@article{Rajamohan2016,
  title = {Joint {{Block Sparse Signal Recovery Problem}} and {{Applications}} in {{LTE Cell Search}}},
  volume = {PP},
  issn = {0018-9545},
  doi = {10.1109/TVT.2016.2552247},
  abstract = {We consider the problem of jointly recovering block sparse signals that share the same support set, using multiple measurement vectors (MMV). We consider the generalized MMV (GMMV) model, where the different measurement vectors could have been obtained using different sensing matrices. We study greedy and convex programming based recovery algorithms and theoretically establish their support recovery guarantees. Our results present insights on how the correlation between the block sparse signals plays a role in the recovery performance. Next, we consider the problem of cell search in heterogeneous cellular networks (HetNets). With the cell search process, the mobile terminal (MT) acquires the synchronization parameters such as frame timing, residual frequency offset and the physical layer identity of a base station (BS) suitable for its connection. In HetNets, due to the increased density of BS, the MT may receive strong interference from several BS in its neighborhood. We establish that the problem of cell search in HetNets can be solved using the GMMV joint block sparse signal recovery framework. We numerically study the performance of the cell search algorithms proposed using our framework and show that they perform significantly better than the successive interference cancellation algorithm existing in the literature.},
  timestamp = {2016-10-12T11:54:47Z},
  number = {99},
  journal = {IEEE Transactions on Vehicular Technology},
  author = {Rajamohan, N. and Joshi, A. and Kannu, A. Pachai},
  year = {2016},
  keywords = {block sparse signal recovery,Correlation,downlink synchronization,generalized multiple measurement vectors,Greedy algorithms,heterogeneous cellular networks,Matching pursuit algorithms,Multiple measurement vectors,Noise measurement,Search problems,sensors,Sparse matrices,Synchronization},
  pages = {1--1},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TWKV5NPA\\7450167.html:text/html}
}

@article{Korki2016,
  title = {Bayesian {{Hypothesis Testing}} for {{Block Sparse Signal Recovery}}},
  volume = {20},
  issn = {1089-7798},
  doi = {10.1109/LCOMM.2016.2518169},
  abstract = {A novel block Bayesian hypothesis testing algorithm (BBHTA) is presented for reconstructing block-sparse signals with unknown block structures. The BBHTA detects and recovers the supports and then estimates the amplitudes of block sparse signal. The support detection and recovery are performed by a Bayesian hypothesis testing. Using the detected and reconstructed supports, the nonzero amplitudes are then estimated by linear minimum mean-square error estimation. Numerical experiments demonstrate the effectiveness of BBHTA.},
  timestamp = {2016-10-12T11:55:00Z},
  number = {3},
  journal = {IEEE Communications Letters},
  author = {Korki, M. and Zayyani, H. and Zhang, J.},
  month = mar,
  year = {2016},
  keywords = {Bayesian hypothesis testing,Bayes methods,BBHTA,belief networks,Bernoulli-Gaussian hidden Markov model,Bernoulli???Gaussian hidden Markov model,Block-sparse,block-sparse signal reconstruction,block sparse signal recovery,Correlation,Estimation,Hidden Markov models,least mean squares methods,linear minimum mean-square error estimation,Markov processes,Mathematical model,nonzero amplitudes,novel block Bayesian hypothesis testing algorithm,signal reconstruction,support detection,Testing,unknown block structures},
  pages = {494--497},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CNKKAIK5\\7383259.html:text/html}
}

@inproceedings{Karanam2015,
  title = {Sparse Re-Id: {{Block}} Sparsity for Person Re-Identification},
  shorttitle = {Sparse Re-Id},
  doi = {10.1109/CVPRW.2015.7301392},
  abstract = {This paper presents a novel approach to solve the problem of person re-identification in non-overlapping camera views. We hypothesize that the feature vector of a probe image approximately lies in the linear span of the corresponding gallery feature vectors in a learned embedding space. We then formulate the re-identification problem as a block sparse recovery problem and solve the associated optimization problem using the alternating directions framework. We evaluate our approach on the publicly available PRID 2011 and iLIDS-VID multi-shot re-identification datasets and demonstrate superior performance in comparison with the current state of the art.},
  timestamp = {2016-10-12T12:10:51Z},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Karanam, S. and Li, Y. and Radke, R. J.},
  month = jun,
  year = {2015},
  keywords = {alternating direction framework,block sparse recovery problem,cameras,Cost function,Dictionaries,feature extraction,feature vector,gallery feature vectors,iLIDS-VID multishot re-identification dataset,learned embedding space,learning (artificial intelligence),Mathematical model,minimisation,minimization,nonoverlapping camera views,optimization problem,person re-identification problem,probe image,Probes,publicly available PRID 2011 dataset,Sparse matrices,sparse re-id,Strips},
  pages = {33--40},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NZ5RSAZB\\7301392.html:text/html}
}

@inproceedings{Elkhalil2016,
  title = {Block Compressed Sensing for Feedback Reduction in Relay-Aided Multiuser Full Duplex Networks},
  doi = {10.1109/SPAWC.2016.7536773},
  abstract = {Opportunistic user selection is a simple technique that exploits the spatial diversity in multiuser relay-aided networks. Nonetheless, channel state information (CSI) from all users (and cooperating relays) is generally required at a central node in order to make selection decisions. Practically, CSI acquisition generates a great deal of feedback overhead that could result in significant transmission delays. In addition to this, the presence of a full-duplex cooperating relay corrupts the fed back CSI by additive noise and the relay's loop (or self) interference. This could lead to transmission outages if user selection is based on inaccurate feedback information. In this paper, we propose an opportunistic full-duplex feedback algorithm that tackles the above challenges. We cast the problem of joint user signal-to-noise ratio (SNR) and the relay loop interference estimation at the base-station as a block sparse signal recovery problem in compressive sensing (CS). Using existing CS block recovery algorithms, the identity of the strong users is obtained and their corresponding SNRs are estimated. Numerical results show that the proposed technique drastically reduces the feedback overhead and achieves a rate close to that obtained by techniques that require dedicated error-free feedback from all users. Numerical results also show that there is a trade-off between the feedback interference and load, and for short coherence intervals, full-duplex feedback achieves higher throughput when compared to interference-free (half-duplex) feedback.},
  timestamp = {2016-10-12T12:10:42Z},
  booktitle = {2016 {{IEEE}} 17th {{International Workshop}} on {{Signal Processing Advances}} in {{Wireless Communications}} ({{SPAWC}})},
  author = {Elkhalil, K. and Eltayeb, M. and Kammoun, A. and Al-Naffouri, T. Y. and Bahrami, H. R.},
  month = jul,
  year = {2016},
  keywords = {Additive noise,base-station,block compressed sensing,block sparse signal recovery,channel state information,coherence interval,compressed sensing,compressive sensing,CS block recovery algorithm,CSI,decode-and-forward,Downlink,error-free feedback,Estimation,Fading channels,Feedback,feedback overhead,feedback reduction,full-duplex cooperating relay,full-duplex feedback algorithm,full-duplex relaying,Interference,interference-free feedback,opportunistic user selection,relay-aided multiuser full duplex network,relay loop interference estimation,relay networks (telecommunication),Relays,scheduling,signal-to-noise ratio,Signal to noise ratio,SNR,spatial diversity,transmission outages},
  pages = {1--6},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\E9JPENPX\\7536773.html:text/html}
}

@article{Meng2016,
  title = {{{3D}} Inversion of Full Gravity Gradient Tensor Data Using {{SL0}} Sparse Recovery},
  volume = {127},
  issn = {0926-9851},
  doi = {10.1016/j.jappgeo.2016.02.010},
  abstract = {We present a new method dedicated to the interpretation of full gravity gradient tensor data, based on SL0 sparse recovery inversion. The SL0 sparse recovery method aims to find out the minimum value of the objective function to fit the data function and to solve the non-zero solution to the objective function. Based on continuous iteration, we can easily obtain the final global minimum (namely the property and space attribute of the inversion target). We consider which type of tensor data combination produces the best inversion results based on the inversion results of different full gravity gradient tensor data combinations (separate tensor data and combined tensor data). We compare the recovered models obtained by inverting the different combinations of different gravity gradient tensor components to understand how different component combinations contribute to the resolution of the recovered model. Based on the comparison between the SL0 sparse recovery inversion results and the smoothest and focusing inversion results of the full gravity gradient tensor data, we show that SL0 sparse recovery inversion can obtain more stable and efficient inversion results with relatively sharp edge information, and that this method can also produce a stable solution of the inverse problem for complex geological structures. This new method to resolve very large full gravity gradient tensor datasets has the considerable advantage of being highly efficient; the full gravity gradient tensor inversion requires very little time. This new method is very effective in explaining the full gravity tensor which is very sensitive to small changes in local anomaly. The numerical simulation and inversion results of the compositional model indicates that including multiple components for inversion increases the resolution of the recovered density model and improves the structure delineation. We apply our inversion method to invert the gravity gradient tensor survey data from the Vinton salt dome, Louisiana. The results of inversion are in good agreement with the known formation in the region, which supports the validity of our method.},
  timestamp = {2016-10-12T13:01:42Z},
  urldate = {2016-10-12},
  journal = {Journal of Applied Geophysics},
  author = {Meng, Zhaohai},
  month = apr,
  year = {2016},
  keywords = {3D inversion,Full gravity gradient tensor,Model objective function,Modified Newton-gradient method,SL0 norm,sparse recovery},
  pages = {112--128},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NJFFJPXK\\S0926985116300398.html:text/html}
}

@article{Lei2015,
  title = {An Improved {{IRLS}} Algorithm for Sparse Recovery with Intra-Block Correlation},
  volume = {126},
  issn = {0030-4026},
  doi = {10.1016/j.ijleo.2015.02.006},
  abstract = {Non-convex l2/lq(0 \&lt; q \&lt; 1) minimization method can efficiently recover the block-sparse signals whose non-zero coefficients occur in a few blocks. However, in many applications such as face recognition and fetal ECG monitoring, real-world signals also exhibit intra-block correlations aside from standard block-sparsity. In order to recover such signals exactly and robustly, the block sparse Bayesian learning framework is studied in this paper. In contrast to l2/lq norm minimization the proposed method involves a quadratic Mahalanobis distance measure on the block and a covariance matrix on the intra-block correlation. The improved iteratively reweighted least-squares algorithm for the induced framework is proposed than the recent known for mixed l2/lq optimization. The proposed algorithm is tested and compared with the mixed l2/lq algorithm on a series of signals modeled by autoregressive processes. Numerical results demonstrate the outperformance of the proposed algorithm and meanfulness of the novel strategy, especially in low sample ratio and large unknown noise level.},
  timestamp = {2016-10-12T13:02:08Z},
  number = {7\textendash{}8},
  urldate = {2016-10-12},
  journal = {Optik - International Journal for Light and Electron Optics},
  author = {Lei, Yang and Song, Zhanjie},
  month = apr,
  year = {2015},
  keywords = {Intra-block correlation,Iteratively reweighted least-squares,Mixed l2/lq,Sparse Bayesian learning},
  pages = {850--854},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Z9IPRNC3\\S0030402615000431.html:text/html}
}

@Article{Ayaz2016,
  author    = {Ayaz, U. and Dirksen, S. and Rauhut, H.},
  title     = {Uniform Recovery of Fusion Frame Structured Sparse Signals},
  journal   = {Applied and Computational Harmonic Analysis},
  year      = {2016},
  volume    = {41},
  number    = {2},
  pages     = {341--361},
  month     = sep,
  issn      = {1063-5203},
  abstract  = {We consider the problem of recovering fusion frame sparse signals from incomplete measurements. These signals are composed of a small number of nonzero blocks taken from a family of subspaces. First, we show that, by using a-priori knowledge of a coherence parameter associated with the angles between the subspaces, one can uniformly recover fusion frame sparse signals with a significantly reduced number of vector-valued (sub-)Gaussian measurements via mixed $\mathscr{l}$ 1 / $\mathscr{l}$ 2 -minimization. We prove this by establishing an appropriate version of the restricted isometry property. Our result complements previous nonuniform recovery results in this context, and provides stronger stability guarantees for noisy measurements and approximately sparse signals. Second, we determine the minimal number of scalar-valued measurements needed to uniformly recover all fusion frame sparse signals via mixed $\mathscr{l}$ 1 / $\mathscr{l}$ 2 -minimization. This bound is achieved by scalar-valued subgaussian measurements. In particular, our result shows that the number of scalar-valued subgaussian measurements cannot be further reduced using knowledge of the coherence parameter. As a special case it implies that the best known uniform recovery result for block sparse signals using subgaussian measurements is optimal.},
  doi       = {10.1016/j.acha.2016.03.006},
  file      = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\555CV56Z\\S1063520316000294.html:text/html},
  keywords  = {block sparsity,compressed sensing,fusion frames,Mixed ℓ 1 / ℓ 2 -minimization,restricted isometry property},
  series    = {Sparse Representations with Applications in Imaging Science, Data Analysis, and Beyond, Part IISI: ICCHAS Outgrowth, part 2},
  timestamp = {2016-10-12T13:01:24Z},
  urldate   = {2016-10-12},
}

@article{Huo2016,
  series = {Real-Time Signal Processing in Embedded Systems},
  title = {{{3D}} Sparse Signal Recovery via {{3D}} Orthogonal Matching Pursuit},
  volume = {64},
  issn = {1383-7621},
  doi = {10.1016/j.sysarc.2015.10.005},
  abstract = {Though many three-dimensional (3D) compressive sensing schemes have been proposed, recovery algorithms in most of these schemes are designed for 1D or 2D signals, which cause some serious drawbacks, e.g., huge memory usage, and high decoder complexity. This paper proposes a 3D separable operator (3DSO) which is able to completely exploit the spatial and spectral correlation to sparsify and samples the 3D signal in three dimensions. A 3D orthogonal matching pursuit (3D-OMP) algorithm is then employed to recover the 3D sparse signal, which is able to reduce the computational complexity of the decoder significantly with guaranteed accuracy. In the proposed algorithm, we represent each 3D signal as a weighted sum of 3D atoms, which allow us to sample the 3D signal with 3D separable sensing operator. Then the best matched atoms are selected to construct the 3D support set, and the 3D signal is optimally recovered from the 3D support set in the sense of the least squares. Experimental results show that the 3D-OMP approach achieves higher recovery quality but requires less computational time than the Kronecker Compressive Sensing (KCS) scheme.},
  timestamp = {2016-10-12T13:02:17Z},
  urldate = {2016-10-12},
  journal = {Journal of Systems Architecture},
  author = {Huo, Yingqiu and Fang, Yong and Huang, Lei},
  month = mar,
  year = {2016},
  keywords = {3D orthogonal matching pursuit,3D separable operator,3D separable sampling,3D sparse signal,compressive sensing},
  pages = {3--10},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NXNEFHVV\\S1383762115001198.html:text/html}
}

@article{Ren2017,
  title = {Dynamic Recovery for Block Sparse Signals},
  volume = {130},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2016.06.027},
  abstract = {As an important extension to sparsity, Block Structured Sparsity (BSS) has been widely investigated and a large set of algorithms have been designed to recover signals with BSS. In this paper, a dynamic system is proposed to recover signals with BSS, namely D-BSS. Particularly, D-BSS turns to exploiting the dynamic systems governed by an $\mathscr{l}$ 2 , 1 norm constraint. It can be proved that the equilibrium of D-BSS is equivalent to optimum of traditional BSS algorithms, e.g., Group-lasso. Simulation results are given to illustrate the desirable performance of the D-BSS system, especially the improvement of convergence rate.},
  timestamp = {2016-10-12T13:01:35Z},
  urldate = {2016-10-12},
  journal = {Signal Processing},
  author = {Ren, Junying and Wei, Chen and Yu, Lei and Zhang, Haijian and Sun, Hong},
  month = jan,
  year = {2017},
  keywords = {Block structured sparsity,Dynamic system,sparsity},
  pages = {197--203},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\D8ZD8WAN\\S0165168416301384.html:text/html}
}

@article{He2016b,
  title = {Learning Group-Based Sparse and Low-Rank Representation for Hyperspectral Image Classification},
  volume = {60},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2016.04.009},
  abstract = {Previous studies have demonstrated that the structured sparse representation can yield significant improvements in spectral\textendash{}spatial hyperspectral classification. However, a dictionary that contains all of the training samples in the sparsity-aware methods is ineffective in capturing the class-discriminative information. This paper makes the first attempt to learn group-based sparse and low-rank representation for improving the dictionary. First, super-pixel segmentation is applied to obtain homogeneous regions that act as spatial groups. Dictionary is then learned with group-based sparse and low-rank regularizations to achieve common representation matrix for the same spatial group. Those group-based sparse and low-rank regularizations facilitate identifying both local and global structure of the hyperspectral image (HSI). Finally, representation matrices of test samples are employed to determine the class labels by a linear support vector machine (SVM). Experimental results on two benchmark HSIs show that the proposed method achieves better performance than the state-of-the-art methods, even with small sample sizes.},
  timestamp = {2016-10-12T13:02:25Z},
  urldate = {2016-10-12},
  journal = {Pattern Recognition},
  author = {He, Zhi and Liu, Lin and Zhou, Suhong and Shen, Yi},
  month = dec,
  year = {2016},
  keywords = {Classification,dictionary learning,Hyperspectral image (HSI),Low-rank representation,sparse representation},
  pages = {1041--1056},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MUUISAHI\\S0031320316300425.html:text/html}
}

@article{Lv2010,
  title = {Block Orthogonal Greedy Algorithm for Stable Recovery of Block-Sparse Signal Representations},
  volume = {90},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2010.05.034},
  abstract = {Recently, block-sparse signals, whose nonzero coefficients appearing in blocks, have received much attention. A corresponding block-based orthogonal greedy algorithm (OGA) was proved by Eldar to successfully recover ideal noiseless block-sparse signals under a certain condition on block-coherence. In this paper, the stability problem of block OGA used to recover the noisy block-sparse signals is studied and the corresponding approximation bounds are derived. The theoretical bounds presented in this paper are more general and are proven to include those reported by Donoho and Tseng. Numerical experimental results are presented to support the validity and correctness of theoretical derivation. The simulation results also show that in the noisy case, the block OGA can be proved to achieve better reconstruction performance than the OGA when the conventional sparse signals are represented in block-sparse forms.},
  timestamp = {2016-10-12T13:01:52Z},
  number = {12},
  urldate = {2016-10-12},
  journal = {Signal Processing},
  author = {Lv, Xiaolei and Wan, Chunru and Bi, Guoan},
  month = dec,
  year = {2010},
  keywords = {Block-coherence,Block-sparse,Orthogonal greedy algorithm,Reconstruction bound,sparse signal representation},
  pages = {3265--3277},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\GKX8D6Z6\\S0165168410002367.html:text/html}
}

@article{Liu2016,
  series = {Compositional Models and Structured Learning for Visual Recognition},
  title = {Structure-{{Constrained Low}}-{{Rank}} and {{Partial Sparse Representation}} with {{Sample Selection}} for Image Classification},
  volume = {59},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2016.01.026},
  abstract = {In this paper, we propose a novel Structure-Constrained Low-Rank and Partial Sparse Representation algorithm for image classification. First, a Structure-Constrained Low-Rank Dictionary Learning (SCLRDL) algorithm is proposed, which imposes both structure and low-rank restriction on the coefficient matrix. Second, under the assumption that the coefficient of test sample is sparse and correlated with the learned representation of training samples, we propose a Low-Rank and Partial Sparse Representation (LRPSR) algorithm which concatenates training samples and test sample to form a data matrix and finds a low-rank and sparse representation of the data matrix over learned dictionary by low-rank matrix recovery technique. Finally, we design a Sample Selection (SS) procedure to accelerate LRPSR. Experimental results on Caltech 101 and Caltech 256 show that our method outperforms most sparse or low-rank based image classification algorithm proposed recently.},
  timestamp = {2016-10-12T13:01:59Z},
  urldate = {2016-10-12},
  journal = {Pattern Recognition},
  author = {Liu, Yang and Li, Xueming and Liu, Chenyu and Liu, Haixu},
  month = nov,
  year = {2016},
  keywords = {dictionary learning,image classification,Low-rank,Sparse coding,structured sparsity},
  pages = {5--13},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\BZMNV89J\\S0031320316000479.html:text/html}
}

@article{Aggarwal,
  title = {Accelerated {{fMRI}} Reconstruction Using {{Matrix Completion}} with {{Sparse Recovery}} via {{Split Bregman}}},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2016.08.016},
  abstract = {In this work, we propose a new method of accelerated functional MRI reconstruction, namely, Matrix Completion with Sparse Recovery (MCwSR). The proposed method combines low rank condition with transform domain sparsity for fMRI reconstruction and is solved using state-of-the-art Split Bregman algorithm. We compare results with state-of-the-art fMRI reconstruction algorithms. Experimental results demonstrate better performance of MCwSR method compared to the existing methods with reference to normalized mean squared error (NMSE) and other reconstruction quality metrics. In addition, the proposed method is able to preserve voxel activation maps on brain volume. None of the other existing methods is able to demonstrate this property. This shows that the proposed method is accurate and faster, and preserves the voxel activation maps that is the key to study fMRI data.},
  timestamp = {2016-10-12T13:00:25Z},
  urldate = {2016-10-12},
  journal = {Neurocomputing},
  author = {Aggarwal, Priya and Gupta, Anubha},
  keywords = {Accelerated functional MRI,compressed sensing,L1 minimization,Nuclear norm,sparse recovery},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\4TUH79UI\\S0925231216308372.html:text/html}
}

@article{Nelson2014,
  series = {Special Issue on Sparse Approximate Solution of Linear Systems},
  title = {On Deterministic Sketching and Streaming for Sparse Recovery and Norm Estimation},
  volume = {441},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2012.12.025},
  abstract = {We study classic streaming and sparse recovery problems using deterministic linear sketches, including $\mathscr{l}$1/$\mathscr{l}$1 and $\mathscr{l}\infty$/$\mathscr{l}$1 sparse recovery problems (the latter also being known as $\mathscr{l}$1-heavy hitters), norm estimation, and approximate inner product. We focus on devising a fixed matrix A$\in$Rm\texttimes{}n and a deterministic recovery/estimation procedure which work for all possible input vectors simultaneously. Our results improve upon existing work, the following being our main contributions: \textbullet{} A proof that $\mathscr{l}\infty$/$\mathscr{l}$1 sparse recovery and inner product estimation are equivalent, and that incoherent matrices can be used to solve both problems. Our upper bound for the number of measurements is m=O($\epsilon$-2min\{logn,(logn/log(1/$\epsilon$))2\}), which holds for any 0$<$$\epsilon$$<$1/2. We can also obtain fast sketching and recovery algorithms by making use of the Fast Johnson\textendash{}Lindenstrauss transform. Both our running times and number of measurements improve upon previous work. We can also obtain better error guarantees than previous work in terms of a smaller tail of the input vector. \textbullet{} A new lower bound for the number of linear measurements required to solve $\mathscr{l}$1/$\mathscr{l}$1 sparse recovery. We show $\Omega$(k/$\epsilon$2+klog(n/k)/$\epsilon$) measurements are required to recover an x${'}$ with $\Vert$x-x${'}\Vert$1$\leqslant$(1+$\epsilon$)$\Vert$xtail(k)$\Vert$1, where xtail(k) is x projected onto all but its largest k coordinates in magnitude. \textbullet{} A tight bound of m=$\Theta$($\epsilon$-2log($\epsilon$2n)) on the number of measurements required to solve deterministic norm estimation, i.e., to recover $\Vert$x$\Vert$2$\pm\epsilon\Vert$x$\Vert$1. For all the problems we study, tight bounds are already known for the randomized complexity from previous work, except in the case of $\mathscr{l}$1/$\mathscr{l}$1 sparse recovery, where a nearly tight bound is known. Our work thus aims to study the deterministic complexities of these problems. We remark that some of the matrices used in our algorithms, although known to exist, currently are not yet explicit in the sense that deterministic polynomial time constructions are not yet known, although in all cases polynomial time Monte Carlo algorithms are known.},
  timestamp = {2016-10-12T13:24:45Z},
  urldate = {2016-10-12},
  journal = {Linear Algebra and its Applications},
  author = {Nelson, Jelani and Nguyẽn, Huy L. and Woodruff, David P.},
  month = jan,
  year = {2014},
  keywords = {Data streams,Heavy hitters,Norm estimation,sparse recovery,Streaming algorithms},
  pages = {152--167},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZCHAJPIJ\\S0024379513000128.html:text/html}
}

@article{Cerejeiras2011,
  title = {Inversion of the Noisy {{Radon}} Transform on {{SO}}(3) by {{Gabor}} Frames and Sparse Recovery Principles},
  volume = {31},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2011.01.005},
  abstract = {The inversion of the one-dimensional Radon transform on the rotation group SO(3) is an ill-posed inverse problem that can be applied to X-ray tomography with polycrystalline materials. This paper is concerned with the development of a method to stably approximate the inverse of the noisy Radon transform on SO(3). The proposed approach is composed by basic building blocks of the coorbit theory on homogeneous spaces, Gabor frame constructions and variational principles for sparse recovery. The performance of the finally obtained iterative approximation is studied through several experiments.},
  timestamp = {2016-10-12T13:25:45Z},
  number = {3},
  urldate = {2016-10-12},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Cerejeiras, Paula and Ferreira, Milton and K{\"a}hler, Uwe and Teschke, Gerd},
  month = nov,
  year = {2011},
  keywords = {Coorbit theory,Crystallography,Gabor frames,Radon transform on,sparse recovery,X-ray tomography},
  pages = {325--345},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UF9IPDNH\\S1063520311000194.html:text/html}
}

@article{Wang2011b,
  title = {Improved Stability Conditions of {{BOGA}} for Noisy Block-Sparse Signals},
  volume = {91},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2011.05.009},
  abstract = {The block orthogonal greedy algorithm (BOGA) has been proven to successfully recover block-sparse signals in noiseless environments and the associated stability problem dealing with noisy signals has also been studied in the literature. This paper demonstrates that the recovery conditions of the BOGA previously reported can be relaxed by using a different definition of the block-coherence. The presented results in this paper provide a generalization of those reported by Tseng for block-sparse signal and serve as a complement of the BOGA reported by Eldar for noisy signals.},
  timestamp = {2016-10-12T13:24:04Z},
  number = {11},
  urldate = {2016-10-12},
  journal = {Signal Processing},
  author = {Wang, Lu and Bi, Guoan and Wan, Chunru and Lv, Xiaolei},
  month = nov,
  year = {2011},
  keywords = {Block-coherence,Block orthogonal greedy algorithm,Block-sparse,Representation bound},
  pages = {2567--2574},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MDGP2WVR\\S0165168411001629.html:text/html}
}

@article{Nejati2016,
  title = {Denoising by Low-Rank and Sparse Representations},
  volume = {36},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2016.01.004},
  abstract = {Due to the ill-posed nature of image denoising problem, good image priors are of great importance for an effective restoration. Nonlocal self-similarity and sparsity are two popular and widely used image priors which have led to several state-of-the-art methods in natural image denoising. In this paper, we take advantage of these priors and propose a new denoising algorithm based on sparse and low-rank representation of image patches under a nonlocal framework. This framework consists of two complementary steps. In the first step, noise removal from groups of matched image patches is formulated as recovery of low-rank matrices from noisy data. This problem is then efficiently solved under asymptotic matrix reconstruction model based on recent results from random matrix theory which leads to a parameter-free optimal estimator. Nonlocal learned sparse representation is adopted in the second step to suppress artifacts introduced in the previous estimate. Experimental results, demonstrate the superior denoising performance of the proposed algorithm as compared with the state-of-the-art methods.},
  timestamp = {2016-10-12T13:24:56Z},
  urldate = {2016-10-12},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Nejati, Mansour and Samavi, Shadrokh and Derksen, Harm and Najarian, Kayvan},
  month = apr,
  year = {2016},
  keywords = {dictionary learning,Image denoising,Low-rank matrix recovery,Nonlocal self-similarity,Optimal singular value shrinkage,random matrix theory,Rank minimization,sparse representation},
  pages = {28--39},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SHMIW8NM\\S1047320316000055.html:text/html}
}

@article{Kaltofen2016,
  series = {Special issue on the conference ISSAC 2014: Symbolic computation and computer algebra},
  title = {Sparse Multivariate Function Recovery with a Small Number of Evaluations},
  volume = {75},
  issn = {0747-7171},
  doi = {10.1016/j.jsc.2015.11.015},
  abstract = {In Kaltofen and Yang (2014) we give an algorithm based algebraic error-correcting decoding for multivariate sparse rational function interpolation from evaluations that can be numerically inaccurate and where several evaluations can have severe errors (``outliers''). Our 2014 algorithm can interpolate a sparse multivariate rational function from evaluations where the error rate 1 / q is quite high, say q = 5 .

For the algorithm with exact arithmetic and exact values at non-erroneous points, one avoids quadratic oversampling by using random evaluation points. Here we give the full probabilistic analysis for this fact, thus providing the missing proof to Theorem 2.1 in Section 2 of our ISSAC 2014 paper. Our argumentation already applies to our original 2007 sparse rational function interpolation algorithm (Kaltofen et al., 2007), where we have experimentally observed that for T unknown non-zero coefficients in a sparse candidate ansatz one only needs T + O ( 1 ) evaluations rather than O ( T 2 ) (cf. Cand{\`e}s and Tao sparse sensing), the latter of which we have proved in 2007. Here we prove that T + O ( 1 ) evaluations at random points indeed suffice.},
  timestamp = {2016-10-12T13:25:25Z},
  urldate = {2016-10-12},
  journal = {Journal of Symbolic Computation},
  author = {Kaltofen, Erich L. and Yang, Zhengfeng},
  month = jul,
  year = {2016},
  keywords = {Cauchy interpolation,Error correcting coding,Fault tolerance,Multivariate rational function model},
  pages = {209--218},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ESNS66AX\\S0747717115001145.html:text/html}
}

@article{Zhang2016a,
  title = {Efficient Block-Sparse Model-Based Algorithm for Photoacoustic Image Reconstruction},
  volume = {26},
  issn = {1746-8094},
  doi = {10.1016/j.bspc.2015.12.003},
  abstract = {The model-based algorithm for photoacoustic imaging (PAI) has been proved to be stable and accurate. However, its reconstruction is computationally burdensome which limits its application in the practical PAI. In this paper, we proposed a block-sparse discrete cosine transform (BS-DCT) model-based PAI reconstruction algorithm in order to improve the computational efficiency of the model-based PAI reconstruction. We adopted the discrete cosine transform (DCT) to eliminate the minor coefficients and reduce the data scale. A block-sparse based iterative method was proposed to accomplish the image reconstruction. Due to its block independent nature, we used the CPU-based parallel calculation implementation to accelerate the reconstruction. During the iterative reconstruction, the number of required iterations was reduced by adopting the fast-converging optimization Barzilai\textendash{}Borwein method. The numerical simulations and in-vitro experiments were carried out. The results has shown that the reconstruction quality is equivalent to the state-of-the-art iterative algorithms. Our algorithm requires less number of iterations with a reduced data scale and significant acceleration through the parallel calculation implementation. In conclusion, the BS-DCT algorithm may be an effectively accelerated practical algorithm for the PAI reconstruction.},
  timestamp = {2016-10-12T13:23:56Z},
  urldate = {2016-10-12},
  journal = {Biomedical Signal Processing and Control},
  author = {Zhang, Chen and Wang, Yuanyuan and Wang, Jin},
  month = apr,
  year = {2016},
  keywords = {Image reconstruction techniques,Medical and biological imaging,Photoacoustic imaging},
  pages = {11--22},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TUS7AJ6W\\S1746809415002013.html:text/html}
}

@article{Gifani2016,
  title = {Echocardiography Noise Reduction Using Sparse Representation},
  volume = {53},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2015.12.008},
  abstract = {The clarity and accuracy of echocardiography images are greatly reduced by speckle noise. Noise suppression, however, is difficult to achieve without also obscuring both rapidly moving structures and object edges. This research seeks to address these challenges by introducing a novel filtering framework based on temporal information and sparse representation. The proposed method involves smoothing intensity variation time curves (IVTCs) assessed in each pixel. Using an over-complete dictionary that contains prototype signal-atoms, IVTCs can be reconstructed as linear combinations of a few of these atoms. After a comprehensive comparison of sparse recovery algorithms, three were selected for our method: Bayesian Compressive Sensing (BCS), the Bregman iterative algorithm, and Orthogonal Matching Pursuit (OMP). The performance of the proposed method was then evaluated and compared with other speckle reduction filters. The experimental results demonstrate that the proposed algorithm can be used to achieve better-preserved edges and reduce blurring.},
  timestamp = {2016-10-12T13:25:34Z},
  urldate = {2016-10-12},
  journal = {Computers \& Electrical Engineering},
  author = {Gifani, Parisa and Behnam, Hamid and Haddadi, Farzan and Sani, Zahra Alizadeh and Gifani, Peyman},
  month = jul,
  year = {2016},
  keywords = {Adaptive thresholding,Echocardiographic images,Noise reduction,sparse representation,Temporal information},
  pages = {301--318},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\EW49ATC2\\S0045790615004371.html:text/html}
}

@article{Wang,
  title = {{{LRSR}}: {{Low}}-{{Rank}}-{{Sparse}} Representation for Subspace Clustering},
  issn = {0925-2312},
  shorttitle = {{{LRSR}}},
  doi = {10.1016/j.neucom.2016.07.015},
  abstract = {High-dimensional data in the real world often resides in low-dimensional subspaces. The state-of-the-art methods for subspace segmentation include Low Rank Representation (LRR) and Sparse Representation (SR). The former seeks the global lowest rank representation but restrictively assumes the independence among subspaces, whereas the latter seeks the clustering of disjoint or overlapped subspaces through locality measure, which may cause failure in the case of large noises. To this end, a Low Rank subspace Sparse Representation framework, hereafter referred to as LRSR, is proposed in this paper to recover and segment embedding subspaces simultaneously. Three major contributions can be claimed in this paper: First, a clean dictionary is constructed by optimizing its nuclear norm, low-rank-sparse coefficient matrix obtained using linearized alternating direction method (LADM). Second, both the convergence proof and the complexity analysis are given to prove the effectiveness and efficiency of our proposed LRSR algorithm. Third, the experiments on synthetic data and two benchmark datasets further verify that the LRSR enjoys the capability of clustering disjoint subspaces as well as the robustness against large noises, thanks to its considerations of both global and local subspace information. Therefore, it has been demonstrated in this research that our proposed LRSR algorithm outperforms the state-of-the-art subspace clustering methods, verified by both theoretical analysis and the empirical studies.},
  timestamp = {2016-10-12T13:22:12Z},
  urldate = {2016-10-12},
  journal = {Neurocomputing},
  author = {Wang, Jun and Shi, Daming and Cheng, Dansong and Zhang, Yongqiang and Gao, Junbin},
  keywords = {low rank representation,sparse representation,spectral clustering,Subspace clustering},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5BM3KQG3\\S0925231216307573.html:text/html}
}

@article{Zhu2013a,
  title = {Large Sparse Signal Recovery by Conjugate Gradient Algorithm Based on Smoothing Technique},
  volume = {66},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2013.04.022},
  abstract = {Finding sparse solutions to under-determined linear systems of equations have intensively involved in fields of machine learning, signal processing, compressive sensing, linear inverse problems and statistical inference. Generally, the task can be realized by solving $\mathscr{l}$ 1 -norm regularized minimization problems. However, the resulting problem is challenging due to the non-smoothness of the regularization term. Inspired by Nesterov's smoothing technique, this paper proposes, analyzes and tests a modified Polak\textendash{}Ribi{\`e}re\textendash{}Polyak conjugate gradient method to solve large-scale $\mathscr{l}$ 1 -norm least squares problem for sparse signal recovery. The per-iteration cost of the proposed algorithm is dominated by three matrix\textendash{}vector multiplications and the global convergence is guaranteed by results in optimization literature. Moreover, the algorithm is also accelerated by continuation loops as usual. The limited experiments show that this continuation technique benefits to its performance. Numerical experiments which decode a sparse signal from its limited measurements illustrate that the proposed algorithm performs better than NESTA\textemdash{}a recently developed code with Nesterov's smoothing technique and gradient algorithm.},
  timestamp = {2016-10-12T13:23:38Z},
  number = {1},
  urldate = {2016-10-12},
  journal = {Computers \& Mathematics with Applications},
  author = {Zhu, Hong and Xiao, Yunhai and Wu, Soon-Yi},
  month = aug,
  year = {2013},
  keywords = {compressive sensing,Conjugate gradient method,ℓ    1    -norm regularization,Non-smooth optimization,sparse solution},
  pages = {24--32},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FJTFDW56\\S0898122113002319.html:text/html}
}

@article{Zheng2016,
  series = {Advances in Neural Networks, Intelligent Control and Information ProcessingContaining a selection of papers from the 5th International Conference on Intelligent Control and Information Processing (ICICIP2014)},
  title = {Improved Sparse Representation with Low-Rank Representation for Robust Face Recognition},
  volume = {198},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.07.146},
  abstract = {In this paper, an approach to learn a robust sparse representation dictionary for face recognition is proposed. As well known, sparse representation algorithm can effectively tackle slight occlusion problems for face recognition. However, if images are corrupted by heavy noise, performance will be not guaranteed. In this paper, to enhance the robustness of sparse representation to serious noise in face images, we integrate low rank representation into dictionary learning to alleviate the influence of unfavorable factors such as large scale noise and occlusion. Among which we extract eigenfaces by singular value decomposition (SVD) from the low rank pictures to reduce dictionary atoms and, thereby, optimize the efficiency of improved algorithm. Otherwise, we characterize each image using the histogram of orientated gradient (HOG) feature which has been proven to be an effective descriptor for face recognition in particular. The performance of the proposed Low-rank and HOG feature based ESRC (LH\_ESRC) algorithm on several popular face databases such as the Extended Yale B database and CMU\_PIE face database shows the effectiveness of our method. In addition, we evaluate the robustness of our method by adding different proportions of randomly noise and block occlusion and real disgusts. Experimental results illustrate the benefits of our approach.},
  timestamp = {2016-10-12T13:23:47Z},
  urldate = {2016-10-12},
  journal = {Neurocomputing},
  author = {Zheng, Chun-Hou and Hou, Yi-Fu and Zhang, Jun},
  month = jul,
  year = {2016},
  keywords = {Eigenface,face recognition,HOG feature,low rank representation,sparse representation},
  pages = {114--124},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\I85XQ22I\\S0925231216003076.html:text/html}
}

@article{Li2015n,
  title = {Signal Recovery for Jointly Sparse Vectors with Different Sensing Matrices},
  volume = {108},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2014.10.010},
  abstract = {In this paper, we study a sparse multiple measurement vector problem in which we need to recover a set of jointly sparse vectors from incomplete measurements. Most related studies assumed that all these measurements correspond to the same compressed sensing matrix. Differently, we allow that the measurements come from different sensing matrices. To deal with different matrices, we establish an algorithm via applying block coordinate descent and Majorization\textendash{}Minimization techniques. The numerical examples demonstrate the effectiveness of this new algorithm, which allows us to design different matrices for better recovery performance.},
  timestamp = {2016-10-12T13:25:18Z},
  urldate = {2016-10-12},
  journal = {Signal Processing},
  author = {Li, Li and Huang, Xiaolin and Suykens, Johan A. K.},
  month = mar,
  year = {2015},
  keywords = {compressed sensing,multiple measurement vector,Re-weighted algorithm,sparse representation},
  pages = {451--458},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7U3QDIBP\\S0165168414004745.html:text/html}
}

@article{Osher2016,
  series = {Sparse Representations with Applications in Imaging Science, Data Analysis, and Beyond, Part IISI: ICCHAS Outgrowth, part 2},
  title = {Sparse Recovery via Differential Inclusions},
  volume = {41},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2016.01.002},
  abstract = {In this paper, we recover sparse signals from their noisy linear measurements by solving nonlinear differential inclusions, which is based on the notion of inverse scale space (ISS) developed in applied mathematics. Our goal here is to bring this idea to address a challenging problem in statistics, i.e. finding the oracle estimator which is unbiased and sign consistent using dynamics. We call our dynamics Bregman ISS and Linearized Bregman ISS. A well-known shortcoming of LASSO and any convex regularization approaches lies in the bias of estimators. However, we show that under proper conditions, there exists a bias-free and sign-consistent point on the solution paths of such dynamics, which corresponds to a signal that is the unbiased estimate of the true signal and whose entries have the same signs as those of the true signs, i.e. the oracle estimator. Therefore, their solution paths are regularization paths better than the LASSO regularization path, since the points on the latter path are biased when sign-consistency is reached. We also show how to efficiently compute their solution paths in both continuous and discretized settings: the full solution paths can be exactly computed piece by piece, and a discretization leads to Linearized Bregman iteration, which is a simple iterative thresholding rule and easy to parallelize. Theoretical guarantees such as sign-consistency and minimax optimal l 2 -error bounds are established in both continuous and discrete settings for specific points on the paths. Early-stopping rules for identifying these points are given. The key treatment relies on the development of differential inequalities for differential inclusions and their discretizations, which extends the previous results and leads to exponentially fast recovering of sparse signals before selecting wrong ones.},
  timestamp = {2016-10-12T13:24:18Z},
  number = {2},
  urldate = {2016-10-12},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Osher, Stanley and Ruan, Feng and Xiong, Jiechao and Yao, Yuan and Yin, Wotao},
  month = sep,
  year = {2016},
  keywords = {Differential inclusion,Early stopping regularization,Linearized Bregman,Statistical consistency},
  pages = {436--469},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\43V7UQV3\\S1063520316000038.html:text/html}
}

@article{Cao2016,
  title = {Pose and Illumination Variable Face Recognition via Sparse Representation and Illumination Dictionary},
  volume = {107},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2016.06.001},
  abstract = {This paper addresses the problem of face recognition under pose and illumination variations, and proposes a novel algorithm inspired by the idea of sparse representation (SR). In order to make the SR early designed for the pose-invariant face recognition suitable for the case of pose variation, a multi-pose weighted sparse representation (MW-SR) algorithm is proposed to emphasize the contributions of the similar poses in the representation of the test image. Furthermore, when some illumination variations are added to the images, it is more reasonable to take advantage of the results of pose variable recognition and avoid the traditional SR method that adds all kinds of images with pose and illumination variations in the training dictionary. Here, a novel idea of the proposed algorithms is adding a general illumination dictionary to the training dictionary, and that once the illumination dictionary is designed, it is common for the other face databases. Extensive experiments illustrate that the proposed algorithms perform better than some existing methods for the face recognition under pose and illumination variations.},
  timestamp = {2016-10-12T13:23:27Z},
  urldate = {2016-10-12},
  journal = {Knowledge-Based Systems},
  author = {Cao, Feilong and Hu, Heping and Lu, Jing and Zhao, Jianwei and Zhou, Zhenghua and Wu, Jiao},
  month = sep,
  year = {2016},
  keywords = {dictionary learning,face recognition,Illumination dictionary,sparse representation},
  pages = {117--128},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AW9QS7KP\\S0950705116301642.html:text/html}
}

@article{Majumdar2013,
  title = {Non-Convex Algorithm for Sparse and Low-Rank Recovery: {{Application}} to Dynamic {{MRI}} Reconstruction},
  volume = {31},
  issn = {0730-725X},
  shorttitle = {Non-Convex Algorithm for Sparse and Low-Rank Recovery},
  doi = {10.1016/j.mri.2012.08.011},
  abstract = {In this work we exploit two assumed properties of dynamic MRI in order to reconstruct the images from under-sampled K-space samples. The first property assumes the signal is sparse in the x-f space and the second property assumes the signal is rank-deficient in the x-t space. These assumptions lead to an optimization problem that requires minimizing a combined lp-norm and Schatten-p norm. We propose a novel FOCUSS based approach to solve the optimization problem. Our proposed method is compared with state-of-the-art techniques in dynamic MRI reconstruction. Experimental evaluation carried out on three real datasets shows that for all these datasets, our method yields better reconstruction both in quantitative and qualitative evaluation.},
  timestamp = {2016-10-12T13:25:11Z},
  number = {3},
  urldate = {2016-10-12},
  journal = {Magnetic Resonance Imaging},
  author = {Majumdar, Angshul and Ward, Rabab K. and Aboulnasr, Tyseer},
  month = apr,
  year = {2013},
  keywords = {compressed sensing,Low-rank matrix completion,Offline dynamic MRI reconstruction,sparse recovery},
  pages = {448--455},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UMMSZNXW\\S0730725X12003220.html:text/html}
}

@article{Yeh2014,
  title = {Self-Learning-Based Post-Processing for Image/Video Deblocking via Sparse Representation},
  volume = {25},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2014.02.012},
  abstract = {Blocking artifact, characterized by visually noticeable changes in pixel values along block boundaries, is a common problem in block-based image/video compression, especially at low bitrate coding. Various post-processing techniques have been proposed to reduce blocking artifacts, but they usually introduce excessive blurring or ringing effects. This paper proposes a self-learning-based post-processing framework for image/video deblocking by properly formulating deblocking as an MCA (morphological component analysis)-based image decomposition problem via sparse representation. Without the need of any prior knowledge (e.g., the positions where blocking artifacts occur, the algorithm used for compression, or the characteristics of image to be processed) about the blocking artifacts to be removed, the proposed framework can automatically learn two dictionaries for decomposing an input decoded image into its ``blocking component'' and ``non-blocking component.'' More specifically, the proposed method first decomposes a frame into the low-frequency and high-frequency parts by applying BM3D (block-matching and 3D filtering) algorithm. The high-frequency part is then decomposed into a blocking component and a non-blocking component by performing dictionary learning and sparse coding based on MCA. As a result, the blocking component can be removed from the image/video frame successfully while preserving most original visual details. Experimental results demonstrate the efficacy of the proposed algorithm.},
  timestamp = {2016-10-12T13:39:26Z},
  number = {5},
  urldate = {2016-10-12},
  journal = {Journal of Visual Communication and Image Representation},
  author = {Yeh, Chia-Hung and Kang, Li-Wei and Chiou, Yi-Wen and Lin, Chia-Wen and Fan Jiang, Shu-Jhen},
  month = jul,
  year = {2014},
  keywords = {Blocking artifact,Deblocking,dictionary learning,Image/video enhancement,Image/video restoration,Morphological component analysis,Post-processing,sparse representation},
  pages = {891--903},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IDCBPKAK\\S1047320314000509.html:text/html}
}

@article{Luo2014b,
  title = {A Novel {{T}}\textendash{}{{S}} Fuzzy Systems Identification with Block Structured Sparse Representation},
  volume = {351},
  issn = {0016-0032},
  doi = {10.1016/j.jfranklin.2013.05.008},
  abstract = {In this paper, we propose a fuzzy partition based T\textendash{}S fuzzy systems identification method with block structured sparse representation. Firstly, a novel fuzzy partition method is developed to learn fuzzy rule dictionaries by taking advantage of the geometrical structure of input variables and the functional relationship between input and output variables. Then, we explicitly focus on the block structured information existing in T\textendash{}S fuzzy models and cast the systems identification problem into an optimization problem with structured sparse representation. In such a way, accurate description of T\textendash{}S fuzzy model is established with far fewer numbers of fuzzy rules by selecting the important fuzzy rules and eliminating the redundant ones in the process of block structured sparse regression. Several numerical experiments on well-known benchmark data sets are carried out to illustrate the effectiveness of the proposed method.},
  timestamp = {2016-10-12T13:39:41Z},
  number = {7},
  urldate = {2016-10-12},
  journal = {Journal of the Franklin Institute},
  author = {Luo, Minnan and Sun, Fuchun and Liu, Huaping and Li, Zhijun},
  month = jul,
  year = {2014},
  pages = {3508--3523},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DMN38NGI\\S0016003213001804.html:text/html}
}

@article{Sun2016,
  title = {A Sparse Representation-Based Algorithm for the Voltage Fluctuation Detection of a Power System},
  volume = {48},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2015.09.022},
  abstract = {This paper proposes a novel algorithm for the voltage fluctuation detection of a power system based on sparse representation modeling. The contents of this research mainly include: (1) By first constructing a proper objective function of the frequency and phase, we convert the fundamental signal estimation problem to a simple mathematical convex optimization problem, which can be easily solved using an exhaustive search strategy. (2) From the viewpoint of signal restoration, we regard the voltage fluctuation detection as a signal inpainting problem and then develop an l 0 norm-based optimization equation that exploits the sparsity prior of fluctuation component to recover the desired representation vector. (3) With the assumption that the voltage envelope changes smoothly, we establish an l 2 norm-based regularization equation to further improve the regularity of the result. Experimental results show that the proposed algorithm performs well on demodulating the fundamental signal and voltage fluctuation component, with good ability of noise robustness, when compared to the classical Hilbert transform-based detection method and square demodulation method.},
  timestamp = {2016-10-12T13:39:33Z},
  urldate = {2016-10-12},
  journal = {Digital Signal Processing},
  author = {Sun, Dong and Gao, Qingwei and Lu, Yixiang and Zhu, Mingxing},
  month = jan,
  year = {2016},
  keywords = {orthogonal matching pursuit,Power system,Signal inpainting,sparse representation,Voltage fluctuation},
  pages = {259--268},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZT6FRDGI\\S1051200415003000.html:text/html}
}

@article{Zhang2016b,
  series = {Smart Computing for Large Scale Visual Data Sensing and ProcessingSelected papers from the 2014 International Conference on Smart Computing (SMARTCOMP 2014)},
  title = {Weighted Multifeature Hyperspectral Image Classification via Kernel Joint Sparse Representation},
  volume = {178},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.07.114},
  abstract = {The advantage of using multifeature information for classification has been widely recognized. Representation-based methods with multifeature combination learning have only recently attracted increasing attention for hyperspectral classification. However, nonlinearity in data and the computational load of processing multifeature information and contextual information have been two thorny issues. In this paper, we present a fast joint sparse representation model with multifeature combination learning and its kernel extensions for hyperspectral imagery classification. For several complementary features (spectral, shape, and texture), the proposed model simultaneously acquires a representation vector for each type of feature and encourages the representation vectors to share a common sparsity pattern by imposing the joint sparsity $\mathscr{l}$ r o w , 0 -norm regularization. Thus, the cross-feature information can be taken into account. For different features, different weights are assigned since they may not contribute equally to the final decision. Furthermore, kernel joint sparse representation model is presented to handle nonlinearity in the data. Kernel model projects the data into a high-dimensional space to improve the separability, achieving a better performance than the linear version. At the same time, we incorporate contextual neighborhood knowledge into the learned models. Experiments on several real hyperspectral images indicate that the proposed algorithms with much less memory requirements perform significantly faster than state-of-the-art algorithms, while exhibit highly competitive classification accuracy.},
  timestamp = {2016-10-12T13:39:19Z},
  urldate = {2016-10-12},
  journal = {Neurocomputing},
  author = {Zhang, Erlei and Zhang, Xiangrong and Jiao, Licheng and Liu, Hongying and Wang, Shuang and Hou, Biao},
  month = feb,
  year = {2016},
  keywords = {feature extraction,Hyperspectral imagery classification,joint sparse representation,kernel trick},
  pages = {71--86},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5T322U29\\S0925231215016136.html:text/html}
}

@article{Figueiredo2003,
  title = {An {{EM}} Algorithm for Wavelet-Based Image Restoration},
  volume = {12},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.814255},
  abstract = {This paper introduces an expectation-maximization (EM) algorithm for image restoration (deconvolution) based on a penalized likelihood formulated in the wavelet domain. Regularization is achieved by promoting a reconstruction with low-complexity, expressed in the wavelet coefficients, taking advantage of the well known sparsity of wavelet representations. Previous works have investigated wavelet-based restoration but, except for certain special cases, the resulting criteria are solved approximately or require demanding optimization methods. The EM algorithm herein proposed combines the efficient image representation offered by the discrete wavelet transform (DWT) with the diagonalization of the convolution operator obtained in the Fourier domain. Thus, it is a general-purpose approach to wavelet-based image restoration with computational complexity comparable to that of standard wavelet denoising schemes or of frequency domain deconvolution methods. The algorithm alternates between an E-step based on the fast Fourier transform (FFT) and a DWT-based M-step, resulting in an efficient iterative process requiring O(NlogN) operations per iteration. The convergence behavior of the algorithm is investigated, and it is shown that under mild conditions the algorithm converges to a globally optimal restoration. Moreover, our new approach performs competitively with, in some cases better than, the best existing methods in benchmark tests.},
  timestamp = {2016-10-14T11:37:22Z},
  number = {8},
  journal = {IEEE Transactions on Image Processing},
  author = {Figueiredo, M. A. T. and Nowak, R. D.},
  month = aug,
  year = {2003},
  keywords = {benchmark tests,computational complexity,convolution operator,Deconvolution,Discrete Fourier transforms,discrete wavelet transform,Discrete wavelet transforms,EM algorithm,expectation-maximization algorithm,fast Fourier transform,fast Fourier transforms,frequency domain deconvolution methods,globally optimal restoration,image reconstruction,image representation,Image restoration,Iterative algorithms,iterative process,Optimization methods,penalized likelihood,sparsity,wavelet-based image restoration,wavelet coefficients,wavelet denoising schemes,Wavelet domain},
  pages = {906--916},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\52XNQU8P\\1217267.html:text/html}
}

@incollection{Swirszcz2009,
  title = {Grouped {{Orthogonal Matching Pursuit}} for {{Variable Selection}} and {{Prediction}}},
  timestamp = {2016-10-13T14:21:21Z},
  urldate = {2016-10-13},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  publisher = {{Curran Associates, Inc.}},
  author = {{\'S}wirszcz, Grzegorz and Abe, Naoki and Lozano, Aur{\'e}lie C},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  year = {2009},
  pages = {1150--1158},
  file = {NIPS Snapshort:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JVK24EFN\\3878-grouped-orthogonal-matching-pursuit-for-variable-selection-and-prediction.html:text/html}
}

@article{Weinstein1999,
  title = {Lead {{Field Basis}} for {{FEM Source Localization}}},
  timestamp = {2016-10-14T11:27:30Z},
  author = {Weinstein, David and Zhukov, Leonid and Johnson, Chris},
  month = oct,
  year = {1999},
  pages = {16}
}

@article{MilanHoracek1997,
  title = {The Inverse Problem of Electrocardiography: {{A}} Solution in Terms of Single- and Double-Layer Sources on the Epicardial Surface},
  volume = {144},
  issn = {0025-5564},
  shorttitle = {The Inverse Problem of Electrocardiography},
  doi = {10.1016/S0025-5564(97)00024-2},
  abstract = {An approach to the inverse problem of electrocardiography that involves an estimation of the electric potentials (double-layer equivalent sources) on the heart's epicardial surface from the electrocardiographic potentials that are measurable on the body surface has received considerable attention. This report deals with a heretofore unexplored extension of this approach, one that yields, in addition to the electric potentials on the epicardial surface, the normal components of their gradients (single-layer equivalent sources). We show that this formulation has at least three advantages over the formulation in terms of epicardial potentials alone: (1) single-layer equivalent sources, which reflect the flow of current across the epicardial surface, are well suited for the imaging of regional ischemia and infarction; (2) the transfer matrix linking the epicardial and body-surface potentials for this formulation is less ill conditioned than that for the formulation in terms of potentials alone; (3) the input vector for inverse calculations consists of spatially filtered (rather than directly measured and therefore noisy) body-surface potentials. To establish the feasibility of this new formulation of the inverse problem and to compare it with the formulation in terms of potentials alone, we used a realistically shaped boundary-element model of the human torso. By calculating singular values of the transfer matrices for this model, we found that one for the new formulation is less ill conditioned. We then directly calculated epicardial and body-surface potentials for a single dipole located centrally and for three simultaneously active dipoles located eccentrically in the torso's heart region and used these results to test three methods that are prerequisites of a successful inverse solution: Tikhonov regularization, linearly constrained least squares, and an L-curve method. The feasibility of the new formulation was demonstrated by the fact that the method based on the linearly constrained least squares improved on overregularized Tikhonov solutions over a wide range of regularization parameters, and it yielded solutions that were more accurate than the best-possible Tikhonov solutions. Moreover, the L-curve solution procedure, which requires no a priori information about the solution, yielded slightly underregularized, but accurate, estimates for the optimal regularization parameter and the corresponding best-possible Tikhonov solution. Our results also showed that replacing\textemdash{}in the interest of computational economy\textemdash{}quadrature formulas for the planar triangles with various approximate formulas for the nodes of the model reduces the accuracy of the inverse solution.},
  timestamp = {2016-10-14T11:40:14Z},
  number = {2},
  urldate = {2016-10-14},
  journal = {Mathematical Biosciences},
  author = {Milan Hor{\'a}{\v c}ek, B. and Clements, John C.},
  month = sep,
  year = {1997},
  pages = {119--154},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TCJPBGH3\\S0025556497000242.html:text/html}
}

@article{Johnson1998,
  title = {Adaptive Local Regularization Methods for the Inverse {{ECG}} Problem},
  volume = {69},
  issn = {0079-6107},
  doi = {10.1016/S0079-6107(98)00017-0},
  abstract = {One of the fundamental problems in theoretical electrocardiography can be characterized by an inverse problem. We present new methods for achieving better estimates of heart surface potential distributions in terms of torso potentials through an inverse procedure. First, we outline an automatic adaptive refinement algorithm that minimizes the spatial discretization error in the transfer matrix, increasing the accuracy of the inverse solution. Second, we introduce a new local regularization procedure, which works by partitioning the global transfer matrix into sub-matrices, allowing for varying amounts of smoothing. Each submatrix represents a region within the underlying geometric model in which regularization can be specifically `tuned' using an a priori scheme based on the L-curve method. This local regularization method can provide a substantial increase in accuracy compared to global regularization schemes. Within this context of local regularization, we show that a generalized version of the singular value decomposition (GSVD) can further improve the accuracy of ECG inverse solutions compared to standard SVD and Tikhonov approaches. We conclude with specific examples of these techniques using geometric models of the human thorax derived from MRI data.},
  timestamp = {2016-10-14T11:40:45Z},
  number = {2-3},
  urldate = {2016-10-14},
  journal = {Progress in Biophysics and Molecular Biology},
  author = {Johnson, Christopher R. and MacLeod, Robert S.},
  month = mar,
  year = {1998},
  pages = {405--423},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2XP5ZR2D\\S0079610798000170.html:text/html}
}

@article{Johnston1994,
  title = {Inverse Electrocardiographic Transformations: Dependence on the Number of Epicardial Regions and Body Surface Data Points},
  volume = {120},
  issn = {0025-5564},
  shorttitle = {Inverse Electrocardiographic Transformations},
  doi = {10.1016/0025-5564(94)90051-5},
  abstract = {The inverse problem of electrocardiography, the computation of epicardial potentials from body surface potentials, is influenced by the desired resolution on the epicardium, the number of recording points on the body surface, and the method of limiting the inversion process. To examine the role of these variables in the computation of the inverse transform, Tikhonov's zero-order regularization and singular value decomposition (SVD) have been used to invert the forward transfer matrix. The inverses have been compared in a data-independent manner using the resolution and the noise amplification as endpoints. Sets of 32, 50, 192, and 384 leads were chosen as sets of body surface data, and 26, 50, 74, and 98 regions were chosen to represent the epicardium. The resolution and noise were both improved by using a greater number of electrodes on the body surface. When 60\% of the singular values are retained, the results show a trade-off between noise and resolution, with typical maximal epicardial noise levels of less than 0.5\% of maximum epicardial potentials for 26 epicardial regions, 2.5\% for 50 epicardial regions, 7.5\% for 74 epicardial regions, and 50\% for 98 epicardial regions. As the number of epicardial regions is increased, the regularization technique effectively fixes the noise amplification but markedly decreases the resolution, whereas SVD results in an increase in noise and a moderate decrease in resolution. Overall the regularization technique performs slightly better than SVD in the noise-resolution relationship. There is a region at the posterior of the heart that was poorly resolved regardless of the number of regions chosen. The variance of the resolution was such as to suggest the use of variable-size epicardial regions based on the resolution.},
  timestamp = {2016-10-14T11:40:22Z},
  number = {2},
  urldate = {2016-10-14},
  journal = {Mathematical Biosciences},
  author = {Johnston, Peter R. and Walker, Stephen J. and Hyttinen, Jari A. K. and Kilpatrick, David},
  month = apr,
  year = {1994},
  pages = {165--187},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VZPZWK5S\\0025556494900515.html:text/html}
}

@article{Wang2013f,
  title = {Inverse Electrocardiographic Source Localization of Ischemia: {{An}} Optimization Framework and Finite Element Solution},
  volume = {250},
  issn = {0021-9991},
  shorttitle = {Inverse Electrocardiographic Source Localization of Ischemia},
  doi = {10.1016/j.jcp.2013.05.027},
  abstract = {With the goal of non-invasively localizing cardiac ischemic disease using body-surface potential recordings, we attempted to reconstruct the transmembrane potential (TMP) throughout the myocardium with the bidomain heart model. The task is an inverse source problem governed by partial differential equations (PDE). Our main contribution is solving the inverse problem within a PDE-constrained optimization framework that enables various physically-based constraints in both equality and inequality forms. We formulated the optimality conditions rigorously in the continuum before deriving finite element discretization, thereby making the optimization independent of discretization choice. Such a formulation was derived for the L 2 -norm Tikhonov regularization and the total variation minimization. The subsequent numerical optimization was fulfilled by a primal\textendash{}dual interior-point method tailored to our problem's specific structure. Our simulations used realistic, fiber-included heart models consisting of up to 18,000 nodes, much finer than any inverse models previously reported. With synthetic ischemia data we localized ischemic regions with roughly a 10\% false-negative rate or a 20\% false-positive rate under conditions up to 5\% input noise. With ischemia data measured from animal experiments, we reconstructed TMPs with roughly 0.9 correlation with the ground truth. While precisely estimating the TMP in general cases remains an open problem, our study shows the feasibility of reconstructing TMP during the ST interval as a means of ischemia localization.},
  timestamp = {2016-10-14T11:39:59Z},
  urldate = {2016-10-14},
  journal = {Journal of Computational Physics},
  author = {Wang, Dafang and Kirby, Robert M. and MacLeod, Rob S. and Johnson, Chris R.},
  month = oct,
  year = {2013},
  keywords = {Bidomain model,Electrocardiography,Finite element method,Inverse problem,Myocardial ischemia,PDE optimization,Total variation},
  pages = {403--424},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FD2PKNTR\\S0021999113003677.html:text/html}
}

@article{Seger2005,
  title = {Lead Field Computation for the Electrocardiographic Inverse Problem\textemdash{}finite Elements versus Boundary Elements},
  volume = {77},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2004.10.005},
  abstract = {Summary
In order to be able to solve the inverse problem of electrocardiography, the lead field matrix (transfer matrix) has to be calculated. The two methods applied for computing this matrix, which are compared in this study, are the boundary element method (BEM) and the finite element method (FEM). The performance of both methods using a spherical model was investigated. For a comparable discretization level, the BEM yields smaller relative errors compared to analytical solutions. The BEM needs less computation time, but a larger amount of memory. Inversely calculated myocardial activation times using either the FEM or BEM computed lead field matrices give similar activation time patterns. The FEM, however, is also capable of considering anisotropic conductivities. This property might have an impact for future development, when also individual myocardial fiber architecture can be considered in the inverse formulation.},
  timestamp = {2016-10-14T11:40:07Z},
  number = {3},
  urldate = {2016-10-14},
  journal = {Computer Methods and Programs in Biomedicine},
  author = {Seger, M. and Fischer, G. and Modre, R. and Messnarz, B. and Hanser, F. and Tilg, B.},
  month = mar,
  year = {2005},
  keywords = {Bidomain theory,Boundary element method,Finite element method,Inverse problem of electrocardiography,Lead field matrix},
  pages = {241--252},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\USHAV7XE\\S0169260704002068.html:text/html}
}

@article{Johnston1995,
  title = {The Inverse Problem of Electrocardiology: {{The}} Performance of Inversion Techniques as a Function of Patient Anatomy},
  volume = {126},
  issn = {0025-5564},
  shorttitle = {The Inverse Problem of Electrocardiology},
  doi = {10.1016/0025-5564(94)00029-Y},
  abstract = {Sixteen anatomically correct bodies have been studied to determine the performance of the inversion techniques of zero-order Tikhonov regularization and singular value decomposition. The bodies have varying heart height and diameter, thickness of subcutaneuous fat layer, and distance of the heart from the left wall of the chest. Comparisons are made in terms of trade-off curves for noise amplification factor and spread of epicardial potentials. It was found that regularization performs better than singular value decomposition on all bodies; the larger the heart size, the more reliable the results; and for a given heart size, the thinner the subcutaneous fat layer, the more reliable the results. The distance of the heart from left wall of the chest was found to be a less significant factor for a given heart size.},
  timestamp = {2016-10-14T11:40:30Z},
  number = {2},
  urldate = {2016-10-14},
  journal = {Mathematical Biosciences},
  author = {Johnston, Peter R. and Kilpatrick, David},
  month = apr,
  year = {1995},
  pages = {125--145},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RH5BSK8Z\\002555649400029Y.html:text/html}
}

@article{Corrado2015,
  title = {Identification of Weakly Coupled Multiphysics Problems. {{Application}} to the Inverse Problem of Electrocardiography},
  volume = {283},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2014.11.041},
  abstract = {This work addresses the inverse problem of electrocardiography from a new perspective, by combining electrical and mechanical measurements. Our strategy relies on the definition of a model of the electromechanical contraction which is registered on ECG data but also on measured mechanical displacements of the heart tissue typically extracted from medical images. In this respect, we establish in this work the convergence of a sequential estimator which combines for such coupled problems various state of the art sequential data assimilation methods in a unified consistent and efficient framework. Indeed, we aggregate a Luenberger observer for the mechanical state and a Reduced-Order Unscented Kalman Filter applied on the parameters to be identified and a POD projection of the electrical state. Then using synthetic data we show the benefits of our approach for the estimation of the electrical state of the ventricles along the heart beat compared with more classical strategies which only consider an electrophysiological model with ECG measurements. Our numerical results actually show that the mechanical measurements improve the identifiability of the electrical problem allowing to reconstruct the electrical state of the coupled system more precisely. Therefore, this work is intended to be a first proof of concept, with theoretical justifications and numerical investigations, of the advantage of using available multi-modal observations for the estimation and identification of an electromechanical model of the heart.},
  timestamp = {2016-10-14T11:39:52Z},
  urldate = {2016-10-14},
  journal = {Journal of Computational Physics},
  author = {Corrado, Cesare and Gerbeau, Jean-Fr{\'e}d{\'e}ric and Moireau, Philippe},
  month = feb,
  year = {2015},
  keywords = {Data assimilation,Electrocardiogram,Electro-mechanics,Identification,Proper orthogonal decomposition,Reduced-Order Unscented Kalman Filter},
  pages = {271--298},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KKVNQC6B\\S0021999114008031.html:text/html}
}

@article{Cho2015,
  title = {Influence of the Head Model on {{EEG}} and {{MEG}} Source Connectivity Analyses},
  volume = {110},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2015.01.043},
  abstract = {The results of brain connectivity analysis using reconstructed source time courses derived from EEG and MEG data depend on a number of algorithmic choices. While previous studies have investigated the influence of the choice of source estimation method or connectivity measure, the effects of the head modeling errors or simplifications have not been studied sufficiently.

In the present simulation study, we investigated the influence of particular properties of the head model on the reconstructed source time courses as well as on source connectivity analysis in EEG and MEG. Therefore, we constructed a realistic head model and applied the finite element method to solve the EEG and MEG forward problems. We considered the distinction between white and gray matter, the distinction between compact and spongy bone, the inclusion of a cerebrospinal fluid (CSF) compartment, and the reduction to a simple 3-layer model comprising only the skin, skull, and brain. Source time courses were reconstructed using a beamforming approach and the source connectivity was estimated by the imaginary coherence (ICoh) and the generalized partial directed coherence (GPDC).

Our results show that in both EEG and MEG, neglecting the white and gray matter distinction or the CSF causes considerable errors in reconstructed source time courses and connectivity analysis, while the distinction between spongy and compact bone is just of minor relevance, provided that an adequate skull conductivity value is used. Large inverse and connectivity errors are found in the same regions that show large topography errors in the forward solution. Moreover, we demonstrate that the very conservative ICoh is relatively safe from the crosstalk effects caused by imperfect head models, as opposed to the GPDC.},
  timestamp = {2016-10-14T12:06:38Z},
  urldate = {2016-10-14},
  journal = {NeuroImage},
  author = {Cho, Jae-Hyun and Vorwerk, Johannes and Wolters, Carsten H. and Kn{\"o}sche, Thomas R.},
  month = apr,
  year = {2015},
  keywords = {Beamforming,Connectivity,EEG,Finite element model,Forward problem,Generalized partial directed coherence,Head modeling,Imaginary coherence,MEG,Source reconstruction},
  pages = {60--77},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZBHZ5HEW\\S1053811915000683.html:text/html}
}

@article{Berg1994,
  title = {A Fast Method for Forward Computation of Multiple-Shell Spherical Head Models},
  volume = {90},
  issn = {0013-4694},
  doi = {10.1016/0013-4694(94)90113-9},
  abstract = {Using a combination of 3 suitably located dipoles in a homogenous sphere, the scalp potential due to a dipole source in a 4-shell spherical head model can be approximated with a high degree of precision and a more than 30-fold increase in computing speed. Magnitudes and locations of the 3 equivalent dipoles can be fitted in a homogenous sphere to data generated from a source at one location in a 4-shell head model. The resulting parameters are used to compute scalp potentials for sources at other locations and orientations. Residual variance measures showed close agreement between the new approximation and a standard 4-shell computation method. Further tests of the method used scalp data from 500 randomly selected pairs of sources generated by the standard 4-shell computation and fitted using, for forward computations, the new approximation and the single-shell Ary-corrected head model. Errors with the new approximation were marginally larger than with the standard computation, but sources were located within 0.5 mm and 0.6$^\circ$ of the original position in 99\% of the fits. 99\% error limits for the Ary model were up to 18 mm and 25$^\circ$ and depended on the head model parameters.},
  timestamp = {2016-10-14T12:06:46Z},
  number = {1},
  urldate = {2016-10-14},
  journal = {Electroencephalography and Clinical Neurophysiology},
  author = {Berg, Patrick and Scherg, Michael},
  month = jan,
  year = {1994},
  keywords = {Ary correction,Dipole source localization,Forward problem,Four-shell model,Model misspecification},
  pages = {58--64},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5XMICS9K\\0013469494901139.html:text/html}
}

@article{Fiederer2016,
  title = {The Role of Blood Vessels in High-Resolution Volume Conductor Head Modeling of {{EEG}}},
  volume = {128},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2015.12.041},
  abstract = {Reconstruction of the electrical sources of human EEG activity at high spatio-temporal accuracy is an important aim in neuroscience and neurological diagnostics. Over the last decades, numerous studies have demonstrated that realistic modeling of head anatomy improves the accuracy of source reconstruction of EEG signals. For example, including a cerebro-spinal fluid compartment and the anisotropy of white matter electrical conductivity were both shown to significantly reduce modeling errors. Here, we for the first time quantify the role of detailed reconstructions of the cerebral blood vessels in volume conductor head modeling for EEG. To study the role of the highly arborized cerebral blood vessels, we created a submillimeter head model based on ultra-high-field-strength (7 T) structural MRI datasets. Blood vessels (arteries and emissary/intraosseous veins) were segmented using Frangi multi-scale vesselness filtering. The final head model consisted of a geometry-adapted cubic mesh with over 17 \texttimes{} 106 nodes. We solved the forward model using a finite-element-method (FEM) transfer matrix approach, which allowed reducing computation times substantially and quantified the importance of the blood vessel compartment by computing forward and inverse errors resulting from ignoring the blood vessels. Our results show that ignoring emissary veins piercing the skull leads to focal localization errors of approx. 5 to 15 mm. Large errors (\&gt; 2 cm) were observed due to the carotid arteries and the dense arterial vasculature in areas such as in the insula or in the medial temporal lobe. Thus, in such predisposed areas, errors caused by neglecting blood vessels can reach similar magnitudes as those previously reported for neglecting white matter anisotropy, the CSF or the dura \textemdash{} structures which are generally considered important components of realistic EEG head models. Our findings thus imply that including a realistic blood vessel compartment in EEG head models will be helpful to improve the accuracy of EEG source analyses particularly when high accuracies in brain areas with dense vasculature are required.},
  timestamp = {2016-10-14T12:06:30Z},
  urldate = {2016-10-14},
  journal = {NeuroImage},
  author = {Fiederer, L. D. J. and Vorwerk, J. and Lucka, F. and Dannhauer, M. and Yang, S. and D{\"u}mpelmann, M. and Schulze-Bonhage, A. and Aertsen, A. and Speck, O. and Wolters, C. H. and Ball, T.},
  month = mar,
  year = {2016},
  keywords = {7 T MRI,Blood vessel modeling,EEG source localization,Extended source model,FEM,Forward problem,Inverse problem,Submillimeter volume conductor head model},
  pages = {193--208},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KNU5AKT6\\S1053811915011544.html:text/html}
}

@article{Neilson2005,
  title = {A Computationally Efficient Method for Accurately Solving the {{EEG}} Forward Problem in a Finely Discretized Head Model},
  volume = {116},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2005.07.010},
  abstract = {Objective
Solution of the forward problem using realistic head models is necessary for accurate EEG source analysis. Realistic models are usually derived from volumetric magnetic resonance images that provide a voxel resolution of about 1 mm3. Electrical models could, therefore contain, for a normal adult head, over 4 million elements. Solution of the forward problem using models of this magnitude has so far been impractical due to issues of computation time and memory.
Methods
A preconditioner is proposed for the conjugate-gradient method that enables the forward problem to be solved using head models of this magnitude. It is applied to the system matrix constructed from the head anatomy using finite differences. The preconditioner is not computed explicitly and so is very efficient in terms of memory utilization.
Results
Using a spherical head model discretized into over 4 million volumes, we have been able to obtain accurate forward solutions in about 60 min on a 1 GHz Pentium III. L2 accuracy of the solutions was better than 2\%.
Conclusions
Accurate solution of the forward problem in EEG in a finely discretized head model is practical in terms of computation time and memory.
Significance
The results represent an important step in head modeling for EEG source analysis.},
  timestamp = {2016-10-14T12:06:21Z},
  number = {10},
  urldate = {2016-10-14},
  journal = {Clinical Neurophysiology},
  author = {Neilson, Lora A. and Kovalyov, Mikhail and Koles, Zoltan J.},
  month = oct,
  year = {2005},
  keywords = {EEG forward problem,EEG source analysis,Finite-difference head model,Preconditioned conjugate-gradient method},
  pages = {2302--2314},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UD32HP53\\S1388245705002798.html:text/html}
}

@article{Wagner2016,
  series = {Transcranial electric stimulation (tES) and Neuroimaging},
  title = {Using Reciprocity for Relating the Simulation of Transcranial Current Stimulation to the {{EEG}} Forward Problem},
  volume = {140},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2016.04.005},
  abstract = {To explore the relationship between transcranial current stimulation (tCS) and the electroencephalography (EEG) forward problem, we investigate and compare accuracy and efficiency of a reciprocal and a direct EEG forward approach for dipolar primary current sources both based on the finite element method (FEM), namely the adjoint approach (AA) and the partial integration approach in conjunction with a transfer matrix concept (PI). By analyzing numerical results, comparing to analytically derived EEG forward potentials and estimating computational complexity in spherical shell models, AA turns out to be essentially identical to PI. It is then proven that AA and PI are also algebraically identical even for general head models. This relation offers a direct link between the EEG forward problem and tCS. We then demonstrate how the quasi-analytical EEG forward solutions in sphere models can be used to validate the numerical accuracies of FEM-based tCS simulation approaches. These approaches differ with respect to the ease with which they can be employed for realistic head modeling based on MRI-derived segmentations. We show that while the accuracy of the most easy to realize approach based on regular hexahedral elements is already quite high, it can be significantly improved if a geometry-adaptation of the elements is employed in conjunction with an isoparametric FEM approach. While the latter approach does not involve any additional difficulties for the user, it reaches the high accuracies of surface-segmentation based tetrahedral FEM, which is considerably more difficult to implement and topologically less flexible in practice. Finally, in a highly realistic head volume conductor model and when compared to the regular alternative, the geometry-adapted hexahedral FEM is shown to result in significant changes in tCS current flow orientation and magnitude up to 45$^\circ$ and a factor of 1.66, respectively.},
  timestamp = {2016-10-14T12:06:12Z},
  urldate = {2016-10-14},
  journal = {NeuroImage},
  author = {Wagner, S. and Lucka, F. and Vorwerk, J. and Herrmann, C. S. and Nolte, G. and Burger, M. and Wolters, C. H.},
  month = oct,
  year = {2016},
  keywords = {EEG forward problem,Evaluation in realistic head model,Finite element method,Reciprocity,Transcranial current stimulation,Validation in multilayer sphere model},
  pages = {163--173},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KBC4MJ7C\\S1053811916300386.html:text/html}
}

@article{Mosher1999,
  title = {{{EEG}} and {{MEG}}: Forward Solutions for Inverse Methods},
  volume = {46},
  issn = {0018-9294},
  shorttitle = {{{EEG}} and {{MEG}}},
  doi = {10.1109/10.748978},
  abstract = {A solution of the forward problem is an important component of any method for computing the spatio-temporal activity of the neural sources of magnetoencephalography (MEG) and electroencephalography (EEG) data. The forward problem involves computing the scalp potentials or external magnetic field at a finite set of sensor locations for a putative source configuration. We present a unified treatment of analytical and numerical solutions of the forward problem in a form suitable for use in inverse methods. This formulation is achieved through factorization of the lead field into the product of the moment of the elemental current dipole source with a "kernel matrix" that depends on the head geometry and source and sensor locations, and a "sensor matrix" that models sensor orientation and gradiometer effects in MEG and differential measurements in EEG. Using this formulation and a recently developed approximation formula for EEG, based on the "Berg parameters", we present novel reformulations of the basic EEG and MEG kernels that dispel the myth that EEG is inherently more complicated to calculate than MEG. We also present novel investigations of different boundary element methods (BEMs) and present evidence that improvements over currently published BEM methods can be realized using alternative error-weighting methods. Explicit expressions for the matrix kernels for MEG and EEG for spherical and realistic head geometries are included.},
  timestamp = {2016-10-14T12:06:04Z},
  number = {3},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Mosher, J. C. and Leahy, R. M. and Lewis, P. S.},
  month = mar,
  year = {1999},
  keywords = {approximation formula,Berg parameters,boundary element methods,boundary-elements methods,differential measurements,EEG,Electric Conductivity,Electroencephalography,elemental current dipole source,external magnetic field,factorization,finite set of sensor locations,forward solutions,Galerkin method,Geometry,gradiometer effects,Head,head geometry,Humans,inverse methods,inverse problems,Kernel,kernel matrix,Lead,lead field,Magnetic analysis,Magnetic heads,Magnetic sensors,Magnetoencephalography,Matrix decomposition,matrix inversion,medical signal processing,MEG,Models; Anatomic,Models; Neurological,neural sources,parameter estimation,physiological models,putative source configuration,realistic head model,Scalp,scalp potentials,sensor matrix,sensor orientation,Signal Processing; Computer-Assisted,spatio-temporal activity,spherical head model,unified treatment,weighted residuals},
  pages = {245--259},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZD7MTX8R\\748978.html:text/html}
}

@article{Popescu2010,
  title = {Magnetoencephalography {{Source Localization Using}} the {{Source Affine Image Reconstruction}} ({{SAFFIRE}}) {{Algorithm}}},
  volume = {57},
  issn = {0018-9294},
  doi = {10.1109/TBME.2010.2047858},
  abstract = {Nonparametric iterative algorithms have been previously proposed to achieve high-resolution, sparse solutions to the bioelectromagnetic inverse problem applicable to multichannel magnetoencephalography and EEG recordings. Using a mmse estimation framework, we propose a new algorithm of this type denoted as source affine image reconstruction (SAFFIRE) aiming to reduce the vulnerability to initialization bias, augment robustness to noise, and decrease sensitivity to the choice of regularization. The proposed approach operates in a normalized lead-field space and employs an initial estimate based on matched filtering to combat the potential biasing effect of previously proposed initialization methods. SAFFIRE minimizes difficulties associated with the selection of the most appropriate regularization parameter by using two separate loading terms: a fixed noise-dependent term that can be directly estimated from the data and arises naturally from the mmse formulation, and an adaptive term (adjusted according to the update of the source estimate) that accounts for uncertainties of the forward model in real-experimental applications. We also show that a noncoherent integration scheme can be used within the SAFFIRE algorithm structure to further enhance the reconstruction accuracy and improve robustness to noise.},
  timestamp = {2016-10-14T12:05:57Z},
  number = {7},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Popescu, M. and Blunt, S. D. and Chan, T.},
  month = jul,
  year = {2010},
  keywords = {algorithms,bioelectromagnetic inverse problem,Brain,Computer Simulation,EEG recording,Electroencephalography,Head,Humans,Image Processing; Computer-Assisted,image reconstruction,integration,Inverse problem,inverse problems,iterative methods,Magnetoencephalography,magnetoencephalography (MEG),magnetoencephalography source localization,medical image processing,MMSE,MMSE formulation,multichannel magnetoencephalography,noncoherent integration scheme,nonparametric iterative algorithms,reconstruction accuracy,Robustness,SAFFIRE algorithm,source affine image reconstruction,Statistics; Nonparametric},
  pages = {1652--1662},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XE5X2BGP\\5451133.html:text/html}
}

@inproceedings{Haueisen2007,
  title = {The Influence of Forward Model Conductivities on {{EEG}}/{{MEG}} Source Reconstruction},
  doi = {10.1109/NFSI-ICFBI.2007.4387676},
  abstract = {In order to reconstruct the neuronal activity underlying measured EEG and MEG data both the forward problem (computing the electromagnetic field due to given sources) and the inverse problem (finding the best fitting sources to explain given data) have to be solved. The forward problem involves a model with the conductivities of the head, which can be as simple as a homogeneously conducting sphere or as complex as a finite element model consisting of millions of elements, each with a different anisotropic conductivity tensor. The question is addressed how complex the employed forward model should be, and, more specifically, the influence of anisotropic volume conduction is evaluated. For this purpose high resolution finite element models of the rabbit and the human head are employed in combination with individual conductivity tensors to quantify the influence of white matter anisotropy on the solution of the forward and inverse problem in EEG and MEG. Although the current state of the art in the analysis of this influence of brain tissue anisotropy on source reconstruction does not yet allow a final conclusion, the results available indicate that the expected average source localization error due to anisotropic white matter conductivity might be within the principal accuracy limits of current inverse procedures. However, in some percent of the cases a considerably larger localization error might occur. In contrast, dipole orientation and dipole strength estimation are influenced significantly by anisotropy. In conclusion, models taking into account tissue anisotropy information are expected to improve source estimation procedures.},
  timestamp = {2016-10-14T12:05:42Z},
  booktitle = {Joint {{Meeting}} of the 6th {{International Symposium}} on {{Noninvasive Functional Source Imaging}} of the {{Brain}} and {{Heart}} and the {{International Conference}} on {{Functional Biomedical Imaging}}, 2007. {{NFSI}}-{{ICFBI}} 2007},
  author = {Haueisen, J.},
  month = oct,
  year = {2007},
  keywords = {anisotropic conductivity tensor,Anisotropic magnetoresistance,bioelectric phenomena,biological tissues,Brain modeling,brain models,brain tissue anisotropy,Conductivity,dipole orientation,dipole strength estimation,EEG source reconstruction,Electroencephalography,Electromagnetic fields,Electromagnetic measurements,Finite element methods,Finite element model,Forward problem,head conductivity,human head,Inverse problem,inverse problems,Magnetoencephalography,medical signal processing,MEG source reconstruction,neuronal activity,neurophysiology,rabbit head,Rabbits,signal reconstruction,Tensile stress,white matter anisotropy},
  pages = {18--19},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QZU5NXDD\\4387676.html:text/html}
}

@inproceedings{Goncalves2000,
  title = {The Use of Electrical Impedance Tomography with the Inverse Problem of {{EEG}} and {{MEG}}},
  volume = {3},
  doi = {10.1109/IEMBS.2000.900615},
  abstract = {This study demonstrates the theoretical feasibility of the EIT method to estimate the equivalent electrical conductivities of brain, skull and scalp. On the other hand, it clearly shows that the presented method has the ability to compute equivalent electrical conductivities which compensate for errors committed on the geometry of the head. The use of the EIT estimated conductivities in the solution of the EEG inverse problem (IP) with a wrong head geometry proved to be effective in the decrease of the systematic errors of the dipole position. Also the use of EIT estimated conductivities in the solution of the EEG IP instead of conductivities affected with errors also improves, in general, the dipole position error. It is concluded that the method is not efficient in improving the dipole strength error. It is therefore demonstrated that the combination of EIT and EEG has the potential to reduce systematic errors in estimating the underlying generators of the EEG},
  timestamp = {2016-10-14T12:05:34Z},
  booktitle = {Proceedings of the 22nd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}, 2000},
  author = {Goncalves, S. and de Munck, J. C.},
  year = {2000},
  keywords = {Brain,Brain modeling,Conductivity,dipole position error,EEG,electrical impedance tomography use,electric impedance imaging,Electric variables measurement,Electrodes,Electroencephalography,equivalent electrical conductivities,error compensation,Forward problem,Impedance,Inverse problem,inverse problems,Magnetic field measurement,Magnetic heads,Magnetoencephalography,medical signal processing,MEG,Scalp,Skull,Tomography},
  pages = {2346--2349},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SPPK6N64\\900615.html:text/html}
}

@inproceedings{Babiloni1998,
  title = {Combined High Resolution {{EEG}} and {{MEG}} Data for Linear Inverse Estimate of Human Event-Related Cortical Activity},
  volume = {4},
  doi = {10.1109/IEMBS.1998.747035},
  abstract = {A new spatial deblurring method for the modeling of human event-related cortical activity from electroencephalography (EEG) and magnetoencephalography (MEG) data is proposed. This method includes high surface sampling of EEG-MEG data (128-50 sensors), realistic magnetic resonance-constructed subject's multi-compartment (scalp, skull, dura mater, cortex) head model, multi-dipole source model, and regularized linear inverse estimate based on boundary element mathematics. As a novelty, linear inverse estimates are regularized not assuming that covariance of background electromagnetic noise between sensors was zero. EEG and MEG data were recorded (separate sessions) while two normal subjects executed voluntary right one-digit movements. Linear inverse estimates of movement-related cortical activity from the combined EEG and MEG data showed higher spatial information content than those obtained from the MEG and EEG data considered separately. In conclusion, the new spatial deblurring method represents a powerful multi-modal neuroimaging approach to the noninvasive study of human brain functions},
  timestamp = {2016-10-21T13:30:36Z},
  booktitle = {Proceedings of the 20th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Babiloni, F. and Gratta, C. Del and Carducci, F. and Babiloni, C. and Roberti, G. M. and Pizzella, V. and Rossini, P. M. and Romani, G. L. and Urbano, A.},
  month = oct,
  year = {1998},
  keywords = {bioelectric potentials,boundary element mathematics,boundary-elements methods,Brain modeling,brain models,combined EEG-MEG,cortical activity modelling,Electroencephalography,forward solution,higher spatial information content,high resolution EEG,high surface sampling,human brain functions,human event-related cortical activity,Humans,inverse problems,linear inverse estimate,Magnetic resonance,Magnetic sensors,Magnetic separation,Magnetoencephalography,Mathematical model,medical signal processing,movement-related cortical activity,multi-compartment head model,multi-dipole source model,multi-modal neuroimaging,noninvasive study,regularized estimates,Sampling methods,signal restoration,spatial deblurring method,Spatial resolution,voluntary right one-digit movements},
  pages = {2151--2154},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AFB5ZUSJ\\747035.html:text/html}
}

@article{Sarvas1987,
  title = {Basic Mathematical and Electromagnetic Concepts of the Biomagnetic Inverse Problem},
  volume = {32},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/32/1/004},
  abstract = {Basic mathematical and physical concepts of the biomagnetic inverse problem are reviewed with some new approaches. The forward problem is discussed for both homogeneous and inhomogeneous media. Geselowitz' formulae and a surface integral equation are presented to handle a piecewise homogeneous conductor. The special cases of a spherically symmetric conductor and a horizontally layered medium are discussed in detail. The non-uniqueness of the solution of the magnetic inverse problem is discussed and the difficulty caused by the contribution of the electric potential to the magnetic field outside the conductor is studied. As practical methods of solving the inverse problem, a weighted least-squares search with confidence limits and the method of minimum norm estimate are discussed.},
  language = {en},
  timestamp = {2016-10-14T12:01:39Z},
  number = {1},
  urldate = {2016-10-14},
  journal = {Physics in Medicine and Biology},
  author = {Sarvas, J.},
  year = {1987},
  pages = {11}
}

@article{Zhang1995,
  title = {A Fast Method to Compute Surface Potentials Generated by Dipoles within Multilayer Anisotropic Spheres},
  volume = {40},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/40/3/001},
  abstract = {P. Berg and M. Scherg's fast computation method (see Electroenceph. Clin. Neurophysiol., vol. 90, p.58-64, 1994) is extended to multilayer anisotropic spheres. The Berg parameters can be dependent upon a dipole radial parameter or not, depending on the actual sphere conductivities and the layer the dipole is within. To find the Berg parameters, no specific electrode locations are required. Berg and Scherg's method is generally applicable whenever de Munck and Peters's (1993) addition-subtraction method can be used.},
  language = {en},
  timestamp = {2016-10-14T12:02:49Z},
  number = {3},
  urldate = {2016-10-14},
  journal = {Physics in Medicine and Biology},
  author = {Zhang, Zhi},
  year = {1995},
  pages = {335}
}

@article{Wu2016,
  title = {Bayesian {{Machine Learning}}: {{EEG}}/{{MEG}} Signal Processing Measurements},
  volume = {33},
  issn = {1053-5888},
  shorttitle = {Bayesian {{Machine Learning}}},
  doi = {10.1109/MSP.2015.2481559},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) are the most common noninvasive brain-imaging techniques for monitoring electrical brain activity and inferring brain function. The central goal of EEG/MEG analysis is to extract informative brain spatiotemporal?spectral patterns or to infer functional connectivity between different brain areas, which is directly useful for neuroscience or clinical investigations. Due to its potentially complex nature [such as nonstationarity, high dimensionality, subject variability, and low signal-to-noise ratio (SNR)], EEG/MEG signal processing poses some great challenges for researchers. These challenges can be addressed in a principled manner via Bayesian machine learning (BML). BML is an emerging field that integrates Bayesian statistics, variational methods, and machine-learning techniques to solve various problems from regression, prediction, outlier detection, feature extraction, and classification. BML has recently gained increasing attention and widespread successes in signal processing and big-data analytics, such as in source reconstruction, compressed sensing, and information fusion. To review recent advances and to foster new research ideas, we provide a tutorial on several important emerging BML research topics in EEG/MEG signal processing and present representative examples in EEG/MEG applications.},
  timestamp = {2016-10-20T11:49:52Z},
  number = {1},
  journal = {IEEE Signal Processing Magazine},
  author = {Wu, W. and Nagarajan, S. and Chen, Z.},
  month = jan,
  year = {2016},
  keywords = {Bayesian machine learning,Bayesian statistics,Bayes methods,big-data analytics,brain function,brain-imaging technique,Brain modeling,brain spatiotemporal spectral pattern,compressed sensing,EEG-MEG signal processing measurement,electrical brain activity monitoring,Electroencephalography,feature extraction,information fusion,learning (artificial intelligence),machine-learning technique,Magnetoencephalography,medical signal processing,neuroscience,regression analysis,signal classification,signal processing,Signal processing algorithms,Source reconstruction},
  pages = {14--36},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\48VGKAER\\7366707.html:text/html}
}

@article{Dahne2015,
  title = {Multivariate {{Machine Learning Methods}} for {{Fusing Multimodal Functional Neuroimaging Data}}},
  volume = {103},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2425807},
  abstract = {Multimodal data are ubiquitous in engineering, communications, robotics, computer vision, or more generally speaking in industry and the sciences. All disciplines have developed their respective sets of analytic tools to fuse the information that is available in all measured modalities. In this paper, we provide a review of classical as well as recent machine learning methods (specifically factor models) for fusing information from functional neuroimaging techniques such as: LFP, EEG, MEG, fNIRS, and fMRI. Early and late fusion scenarios are distinguished, and appropriate factor models for the respective scenarios are presented along with example applications from selected multimodal neuroimaging studies. Further emphasis is given to the interpretability of the resulting model parameters, in particular by highlighting how factor models relate to physical models needed for source localization. The methods we discuss allow for the extraction of information from neural data, which ultimately contributes to 1) better neuroscientific understanding; 2) enhance diagnostic performance; and 3) discover neural signals of interest that correlate maximally with a given cognitive paradigm. While we clearly study the multimodal functional neuroimaging challenge, the discussed machine learning techniques have a wide applicability, i.e., in general data fusion, and may thus be informative to the general interested reader.},
  timestamp = {2016-10-20T11:51:30Z},
  number = {9},
  journal = {Proceedings of the IEEE},
  author = {D{\"a}hne, S. and Bie{\ss}mann, F. and Samek, W. and Haufe, S. and Goltz, D. and Gundlach, C. and Villringer, A. and Fazli, S. and M{\"u}ller, K. R.},
  month = sep,
  year = {2015},
  keywords = {analytic tools,biomedical MRI,brain models,cognitive paradigm,data fusion,Data mining,Data models,diagnostic performance,EEG,EEG technique,Electroencephalography,factor models,feature extraction,fMRI,fMRI technique,fNIRS,fNIRS technique,functional near-infrared spectroscopy,image fusion,information extraction,infrared spectroscopy,learning (artificial intelligence),LFP technique,Machine learning,magnetic resonance imaging,Magnetoencephalography,medical image processing,MEG,MEG technique,multimodal functional neuroimaging data,multimodal neuroimaging,Multimodal sensors,multivariate machine learning methods,neural signals discovery,Neuroimaging,neuroscientific understanding,review},
  pages = {1507--1530},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KNJ4TQC8\\7182735.html:text/html}
}

@inproceedings{Chen2013c,
  title = {Probabilistic Initiation and Termination for {{MEG}} Multiple Dipole Localization Using Sequential {{Monte Carlo}} Methods},
  abstract = {The paper considers an electromagnetic inverse problem of localizing dipolar neural current sources on brain cortex using magnetoencephalography (MEG) or electroencephalography (EEG) data. We aim to localize the unknown and time-varying number of dipolar current sources using data from multiple MEG coil sensors. In this work, we model the problem in a Bayesian framework, we propose a linear prior detection method as well as a probabilistic approach for target number estimation, and target state initiation/termination. We then use a sequential Monte Carlo (SMC) algorithm to numerically estimate location and moment of the dipolar current sources. We apply the algorithm in both simulated and measured data. Results show that the proposed approach is able to estimate and localize the unknown and time-varying number of dipoles in simulated data with reasonable tracking accuracy and efficiency.},
  timestamp = {2016-10-20T11:54:45Z},
  booktitle = {2013 16th {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Chen, X. and S{\"a}rkk{\"a}, S. and Godsill, S.},
  month = jul,
  year = {2013},
  keywords = {Bayesian,Bayesian framework,Bayes methods,brain cortex,dipolar neural current source localization,Dipole,EEG,Electroencephalography,electroencephalography data,electromagnetic inverse problem,inverse problems,linear prior detection method,Localization,Magnetoencephalography,magnetoencephalography data,medical signal detection,medical signal processing,MEG/EEG,MEG multiple dipole localization,multiple MEG coil sensors,neurophysiology,probabilistic initiation,probabilistic termination,sequential Monte Carlo methods,SMC,target number estimation,target state initiation,target state termination},
  pages = {580--587},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XGFNFGNM\\6641332.html:text/html}
}

@article{Daunizeau2005,
  title = {Assessing the {{Relevance}} of {{fMRI}}-{{Based Prior}} in the {{EEG Inverse Problem}}: {{A Bayesian Model Comparison Approach}}},
  volume = {53},
  issn = {1053-587X},
  shorttitle = {Assessing the {{Relevance}} of {{fMRI}}-{{Based Prior}} in the {{EEG Inverse Problem}}},
  doi = {10.1109/TSP.2005.853220},
  abstract = {Characterizing the cortical activity from electro- and magneto-encephalography (EEG/MEG) data requires solving an ill-posed inverse problem that does not admit a unique solution. As a consequence, the use of functional neuroimaging, for instance, functional Magnetic Resonance Imaging (fMRI), constitutes an appealing way of constraining the solution. However, the match between bioelectric and metabolic activities is desirable but not assured. Therefore, the introduction of spatial priors derived from other functional modalities in the EEG/MEG inverse problem should be considered with caution. In this paper, we propose a Bayesian characterization of the relevance of fMRI-derived prior information regarding the EEG/MEG data. This is done by quantifying the adequacy of this prior to the data, compared with that obtained using an noninformative prior instead. This quantitative comparison, using the so-called Bayes factor, allows us to decide whether the informative prior should (or not) be included in the inverse solution. We validate our approach using extensive simulations, where fMRI-derived priors are built as perturbed versions of the simulated EEG sources. Moreover, we show how this inference framework can be generalized to optimize the way we should incorporate the informative prior.},
  timestamp = {2016-10-20T11:55:30Z},
  number = {9},
  journal = {IEEE Transactions on Signal Processing},
  author = {Daunizeau, J. and Grova, C. and Mattout, J. and Marrelec, G. and Clonda, D. and Goulard, B. and Pelegrini-Issac, M. and Lina, J. M. and Benali, H.},
  month = sep,
  year = {2005},
  keywords = {Bayes factor,Bayesian methods,Bayesian model comparison approach,Bayes methods,bioelectric phenomena,biomedical MRI,Brain modeling,Cancer,Current measurement,EEG,EEG inverse problem,Electroencephalography,fMRI,functional neuroimaging,fusion,inference framework,inverse problems,magnetic resonance imaging,Magnetoencephalography,medical image processing,MEG,Neuroimaging,neurophysiology,Positron emission tomography,prior,relevance},
  pages = {3461--3472},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\59KF6SZV\\1495883.html:text/html}
}

@inproceedings{Bidaut1996,
  title = {Multisensor and Multidimensional Biomedical Imaging (Including Volumetric Electro-Magnetic Tomography) for the Visualization and Assessment of Neurological (Dys)-Function},
  volume = {2},
  doi = {10.1109/IEMBS.1996.651932},
  abstract = {We have implemented a global system consisting of techniques and protocols for the combination (registration, visualization, navigation and processing) of various multidimensional biomedical imaging sensors, including all current modalities and also electro-magnetic tomography (EMT), to study, assess, and localize neurological (dys-)function for both clinical and research applications. The already well described interest for this combination stems from the broad variety of complementary information brought out by modern biomedical imaging modalities. In this context, the input of volumetric EMT permits direct sighting, in near real-time, of any EM (dys-)functional behavior. Besides allowing morphology, metabolism and function to be studied simultaneously and from different points of view, the global combination permitted by our approach is expected to show its best value when studying pathologies reflected by metabolic or electromagnetic dysfunctions such as drug-resistant epilepsy},
  timestamp = {2016-10-20T11:57:11Z},
  booktitle = {Proceedings of the 18th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}, 1996. {{Bridging Disciplines}} for {{Biomedicine}}},
  author = {Bidaut, L. and Pascual-Marqui, R. and Rufenacht, D. and Scherrer, J. R. and Terrier, F.},
  month = oct,
  year = {1996},
  keywords = {Biomedical imaging,biomedical NMR,Biosensors,brain imaging,computerised tomography,drug-resistant epilepsy,EEG based tomography,Electroencephalography,functional MRI,global system,Image processing,image reconstruction,image registration,image sensors,Magnetoencephalography,medical image processing,MEG based tomography,metabolism,Morphology,multidimensional biomedical imaging,Multidimensional systems,multisensor biomedical imaging,navigation,neurological dysfunction,neurological function,neurophysiology,PACS,Positron emission tomography,Protocols,registration,segmentation,sensor fusion,Sensor systems and applications,single photon emission computed tomography,SPECT,Tomography,visualization,volumetric electromagnetic tomography},
  pages = {694--695 vol.2},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SZIZZD8V\\651932.html:text/html}
}

@inproceedings{Li2015o,
  title = {Brain Activation Profiles in {{mTBI}}: {{Evidence}} from Combined Resting-State {{EEG}} and {{MEG}} Activity},
  shorttitle = {Brain Activation Profiles in {{mTBI}}},
  doi = {10.1109/EMBC.2015.7319994},
  abstract = {In this study, we compared the brain activation profiles obtained from resting state Electroencephalographic (EEG) and Magnetoencephalographic (MEG) activity in six mild traumatic brain injury (mTBI) patients and five orthopedic controls, using power spectral density (PSD) analysis. We first estimated intracranial dipolar EEG/MEG sources on a dense grid on the cortical surface and then projected these sources on a standardized atlas with 68 regions of interest (ROIs). Averaging the PSD values of all sources in each ROI across all control subjects resulted in a normative database that was used to convert the PSD values of mTBI patients into z-scores in eight distinct frequency bands. We found that mTBI patients exhibited statistically significant overactivation in the delta, theta, and low alpha bands. Additionally, the MEG modality seemed to better characterize the group of individual subjects. These findings suggest that resting-state EEG/MEG activation maps may be used as specific biomarkers that can help with the diagnosis of and assess the efficacy of intervention in mTBI patients.},
  timestamp = {2016-10-20T12:08:18Z},
  booktitle = {2015 37th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Li, L. and Pagnotta, M. F. and Arakaki, X. and Tran, T. and Strickland, D. and Harrington, M. and Zouridakis, G.},
  month = aug,
  year = {2015},
  keywords = {Biomedical imaging,brain activation profiles,Brain injuries,cortical surface,Covariance matrices,Databases,delta band,Electroencephalography,injuries,intracranial dipolar EEG sources,intracranial dipolar MEG sources,low alpha band,magnetic resonance imaging,Magnetoencephalography,medical signal processing,mild traumatic brain injury,mTBI,orthopedic controls,power spectral density analysis,resting state EEG activity,resting state MEG activity,theta band},
  pages = {6963--6966},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SPNV3H6G\\7319994.html:text/html}
}

@inproceedings{Huiskamp2002,
  title = {Inverse and Forward Modeling of Interictal Spikes in the {{EEG}}, {{MEG}} and {{ECoG}}},
  volume = {2},
  doi = {10.1109/IEMBS.2002.1106446},
  abstract = {We tested the validity of EEG and MEG source characterization of interictal spikes in epilepsy patients for which subdural grid registrations of the ECoG and accurate geometrical information from MRI and CT were available. A boundary element volume conduction model was used while for the inverse problem dipolar and distributed source models and epicortical potentials were tested. Results show that for a good qualitative correspondence between sources computed from EEG and ECoG signals distributed source models are to be preferred. For combined EEG-MEG the dipole model performs slightly better. For a good quantitative correspondence, proper modeling of the conductivity values and geometry is essential.},
  timestamp = {2016-10-20T12:13:27Z},
  booktitle = {Engineering in {{Medicine}} and {{Biology}}, 2002. 24th {{Annual Conference}} and the {{Annual Fall Meeting}} of the {{Biomedical Engineering Society EMBS}}/{{BMES Conference}}, 2002. {{Proceedings}} of the {{Second Joint}}},
  author = {Huiskamp, G. J. M.},
  year = {2002},
  keywords = {accurate geometrical information,bioelectric potentials,boundary-elements methods,boundary element volume conduction model,Brain modeling,brain models,combined EEG-MEG,Conductivity,conductivity values,CT,dipolar source models,diseases,Distributed computing,distributed source models,ECoG,EEG source characterization,Electroencephalography,epicortical potentials,Epilepsy,epilepsy patients,forward modeling,Geometry,interictal spikes,inverse modeling,inverse problems,magnetic resonance imaging,Magnetoencephalography,medical signal processing,MEG source characterization,MRI,Solid modeling,source separation,subdural grid registrations,Testing},
  pages = {1393--1394 vol.2},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KI7TQPIT\\1106446.html:text/html}
}

@article{Sohrabpour2016,
  title = {Noninvasive {{Electromagnetic Source Imaging}} and {{Granger Causality Analysis}}: {{An Electrophysiological Connectome}} ({{eConnectome}}) {{Approach}}},
  volume = {PP},
  issn = {0018-9294},
  shorttitle = {Noninvasive {{Electromagnetic Source Imaging}} and {{Granger Causality Analysis}}},
  doi = {10.1109/TBME.2016.2616474},
  abstract = {Combined source imaging techniques and directional connectivity analysis can provide useful information about the underlying brain networks in a noninvasive fashion. Source imaging techniques have been used successfully to either determine the source of activity or to extract source time-courses for Granger causality analysis, previously. In this work, we utilize source imaging algorithms to both find the network nodes (regions of interest) and then extract the activation time series for further Granger causality analysis. The aim of this work is to find network nodes objectively from noninvasive electromagnetic signals, extract activation timecourses and apply Granger analysis on the extracted series to study brain networks under realistic conditions. Methods: Source imaging methods are used to identify network nodes and extract time-courses and then Granger causality analysis is applied to delineate the directional functional connectivity of underlying brain networks. Computer simulations studies where the underlying network (nodes and connectivity pattern) is known were performed; additionally, this approach has been evaluated in partial epilepsy patients to study epilepsy networks from interictal and ictal signals recorded by EEG and/or MEG. Results: : Localization errors of network nodes are less than 5 mm and normalized connectivity errors of 20\% in estimating underlying brain networks in simulation studies. Additionally, two focal epilepsy patients were studied and the identified nodes driving the epileptic network were concordant with clinical findings from intracranial recordings or surgical resection. Conclusion: Our study indicates that combined source imaging algorithms with Granger causality analysis can identify underlying networks precisely (both in terms of network nodes location and internodal connectivity). Significance: The combined source imaging and Granger analysis technique is an effective tool for studying normal or pathological- brain conditions.},
  timestamp = {2016-10-21T14:04:20Z},
  number = {99},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Sohrabpour, A. and Ye, S. and Worrell, G. A. and Zhang, W. and He, B.},
  year = {2016},
  keywords = {brain models,Directed transfer function (DTF),Dynamic seizure imaging (DSI),Electroencephalography,Electromagnetics,Electromagnetic source imaging (ESI),Granger causality analysis,High-density EEG,Imaging,Inter-ictal spikes (IIS),Manganese,MEG,Network,Time series analysis},
  pages = {1--1},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XPB9IS2G\\7588130.html:text/html}
}

@article{Hirayama2015,
  title = {Unifying {{Blind Separation}} and {{Clustering}} for {{Resting}}-{{State EEG}}/{{MEG Functional Connectivity Analysis}}},
  volume = {27},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00747},
  abstract = {Unsupervised analysis of the dynamics (nonstationarity) of functional brain connectivity during rest has recently received a lot of attention in the neuroimaging and neuroengineering communities. Most studies have used functional magnetic resonance imaging, but electroencephalography (EEG) and magnetoencephalography (MEG) also hold great promise for analyzing nonstationary functional connectivity with high temporal resolution. Previous EEG/MEG analyses divided the problem into two consecutive stages: the separation of neural sources and then the connectivity analysis of the separated sources. Such nonoptimal division into two stages may bias the result because of the different prior assumptions made about the data in the two stages. We propose a unified method for separating EEG/MEG sources and learning their functional connectivity (coactivation) patterns. We combine blind source separation (BSS) with unsupervised clustering of the activity levels of the sources in a single probabilistic model. A BSS is performed on the Hilbert transforms of band-limited EEG/MEG signals, and coactivation patterns are learned by a mixture model of source envelopes. Simulation studies show that the unified approach often outperforms conventional two-stage methods, indicating further the benefit of using Hilbert transforms to deal with oscillatory sources. Experiments on resting-state EEG data, acquired in conjunction with a cued motor imagery or nonimagery task, also show that the states (clusters) obtained by the proposed method often correlate better with physiologically meaningful quantities than those obtained by a two-stage method.},
  timestamp = {2016-10-20T12:19:50Z},
  number = {7},
  journal = {Neural Computation},
  author = {i Hirayama, J. and Ogawa, T. and Hyv{\"a}rinen, A.},
  month = jul,
  year = {2015},
  pages = {1373--1404},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CQBGKXAP\\7226492.html:text/html}
}

@inproceedings{Muthuraman2014,
  title = {Coherent Source and Connectivity Analysis on Simultaneously Measured {{EEG}} and {{MEG}} Data during Isometric Contraction},
  doi = {10.1109/EMBC.2014.6945084},
  abstract = {The most well-known non-invasive electric and magnetic field measurement modalities are the electroencephalography (EEG) and magnetoencephalography (MEG). The first aim of the study was to implement the recently developed realistic head model which uses an integrative approach for both the modalities. The second aim of this study was to find the network of coherent sources and the modes of interactions within this network during isometric contraction (ISC) at (15-30 Hz) in healthy subjects. The third aim was to test the effective connectivity revealed by both the modalities analyzing them separately and combined. The Welch periodogram method was used to estimate the coherence spectrum between the EEG and the electromyography (EMG) signals followed by the realistic head modelling and source analysis method dynamic imaging of coherent sources (DICS) to find the network of coherent sources at the individual peak frequency within the beta band in healthy subjects. The last step was to identify the effective connectivity between the identified sources using the renormalized partial directed coherence method. The cortical and sub-cortical network comprised of the primary sensory motor cortex (PSMC), secondary motor area (SMA), and the cerebellum (C). The cortical and sub-cortical network responsible for the isometric contraction was similar in both the modalities when analysing them separately and combined. The SNR was not significantly different between the two modalities separately and combined. However, the coherence values were significantly higher in the combined modality in comparison to each of the modality separately. The effective connectivity analysis revealed plausible additional connections in the combined modality analysis.},
  timestamp = {2016-10-21T13:57:35Z},
  booktitle = {36th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Muthuraman, M. and Hellriegel, H. and Hoogenboom, N. and Anwar, A. R. and Mideksa, K. G. and Krause, H. and Schnitzler, A. and Raethjen, J. and Deuschl, G.},
  month = aug,
  year = {2014},
  keywords = {beta band,biomechanics,Brain modeling,cerebellum,Coherence,coherence spectrum,coherence values,coherent source network,combined modality analysis,DICS,dynamic imaging of coherent sources,EEG data,effective connectivity analysis,Electrodes,Electroencephalography,electromyography,electromyography signals,EMG,frequency 15 Hz to 30 Hz,Head,individual peak frequency,interaction modes,ISC,isometric contraction,magnetic field measurement modalities,Magnetic heads,Magnetoencephalography,medical signal processing,MEG data,neurophysiology,noninvasive electric field measurement modalities,physiological models,primary sensory motor cortex,PSMC,realistic head modelling,renormalized partial directed coherence method,secondary motor area,SMA,SNR,source analysis method,spectral analysis,subcortical network,Welch periodogram method},
  pages = {6365--6368},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\K8NJ6XVT\\6945084.html:text/html}
}

@book{Muller2007,
  title = {Multimodal {{Imaging}} in {{Neurology}}: {{Special Focus}} on {{MRI Applications}} and {{MEG}}},
  isbn = {978-1-59829-551-1},
  shorttitle = {Multimodal {{Imaging}} in {{Neurology}}},
  abstract = {The field of brain imaging is developing at a rapid pace and has greatly advanced the areas of cognitive and clinical neuroscience. The availability of neuroimaging techniques, especially magnetic resonance imaging (MRI), functional MRI (fMRI), diffusion tensor imaging (DTI) and magnetoencephalography (MEG) and magnetic source imaging (MSI) has brought about breakthroughs in neuroscience. To obtain comprehensive information about the activity of the human brain, different analytical approaches should be complemented. Thus, in "intermodal multimodality" imaging, great efforts have been made to combine the highest spatial resolution (MRI, fMRI) with the best temporal resolution (MEG or EEG). "Intramodal multimodality" imaging combines various functional MRI techniques (e.g., fMRI, DTI, and/or morphometric/volumetric analysis). The multimodal approach is conceptually based on the combination of different noninvasive functional neuroimaging tools, their registration and cointegration. In articular, the combination of imaging applications that map different functional systems is useful, such as fMRI as a technique for the localization of cortical function and DTI as a technique for mapping of white matter fiber bundles or tracts. This booklet gives an insight into the wide field of multimodal imaging with respect to concepts, data acquisition, and postprocessing. Examples for intermodal and intramodal multimodality imaging are also demonstrated. Table of Contents: Introduction / Neurological Measurement Techniques and First Steps of Postprocessing / Coordinate Transformation / Examples for Multimodal Imaging / Clinical Aspects of Multimodal Imaging / References / Biography},
  timestamp = {2016-10-20T12:23:18Z},
  urldate = {2016-10-20},
  publisher = {{Morgan \& Claypool}},
  author = {Muller, Hans-Peter and Kassubek, Jan},
  year = {2007},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2NF3BBR2\\articleDetails.html:text/html}
}

@inproceedings{Mideksa2012,
  title = {Source Analysis of Median Nerve Stimulated Somatosensory Evoked Potentials and Fields Using Simultaneously Measured {{EEG}} and {{MEG}} Signals},
  doi = {10.1109/EMBC.2012.6347093},
  abstract = {The sources of somatosensory evoked potentials (SEPs) and fields (SEFs), which is a standard paradigm, is investigated using multichannel EEG and MEG simultaneous recordings. The hypothesis that SEP \& SEF sources are generated in the posterior bank of the central sulcus is tested, and analyses are compared based on EEG only, MEG only, bandpass filtered MEG, and both combined. To locate the sources, the forward problem is first solved by using the boundary-element method for realistic head models and by using a locally-fitted-sphere approach for averaged head models consisting of a set of connected volumes, typically representing the skull, scalp, and brain. The location of each dipole is then estimated using fixed MUSIC and current-density-reconstruction (CDR) algorithms. For both analyses, the results demonstrate that the band-pass filtered MEG can localize the sources accurately at the desired region as compared to only EEG and unfiltered MEG. For CDR analysis, it looks like MEG affects EEG during the combined analyses. The MUSIC algorithm gives better results than CDR, and when comparing the two head models, the averaged and the realistic head models showed the same result.},
  timestamp = {2016-10-21T13:52:19Z},
  booktitle = {Annual {{International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Mideksa, K. G. and Hellriegel, H. and Hoogenboom, N. and Krause, H. and Schnitzler, A. and Deuschl, G. and Raethjen, J. and Heute, U. and Muthuraman, M.},
  month = aug,
  year = {2012},
  keywords = {Algorithm design and analysis,algorithms,band-pass filter,band-pass filters,bone,boundary-element method,boundary-elements methods,Brain,Brain Mapping,brain models,central sulcus,chemioception,Computer Simulation,current density,current-density-reconstruction algorithms,Electric Stimulation,Electroencephalography,Evoked Potentials; Somatosensory,filtering theory,Head,Humans,locally fitted-sphere approach,Magnetic heads,Magnetoencephalography,mechanoception,Median Nerve,median nerve stimulated somatosensory evoked fields,median nerve stimulated somatosensory evoked potentials,medical signal processing,Models; Neurological,multichannel EEG simultaneous recordings,multichannel MEG simultaneous recordings,Multiple signal classification,neurophysiology,realistic head models,Scalp,simultaneously measured EEG signals,simultaneously measured MEG signals,Skull,Somatosensory Cortex,source analysis},
  pages = {4903--4906},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KCVVEJEP\\6347093.html:text/html}
}

@inproceedings{Jiang2011b,
  title = {Research and {{Application}} of {{Non}}-{{Invasive Examine Technique}} in {{Clinical Diagnosis}}},
  doi = {10.1109/icbbe.2011.5780197},
  abstract = {Non-invasion examine technique is a fast evolving diagnosis technique in the clinical application, especial diffusion tensor imaging (DTI), functional magnetic resonance imaging (fMRI), electroencephalogram (EEG), and mangnetoencephalography (MEG). This article devotes to review the clinical application of non-invasive examine technique in clinical diagnosis. Firstly the basic theoretical background to DTI is discussed, following the clinical applications of DTI parameters, diffusion tensor imaging fibers tracking (DTI-FT), and DTI combined with fMRI, EEG, MEG are reviewed. Finally, the important role of DTI in clinical diagnosis, and the limitations of DTI are also briefly discussed.},
  timestamp = {2016-10-20T12:31:36Z},
  booktitle = {({{iCBBE}}) 2011 5th {{International Conference}} on {{Bioinformatics}} and {{Biomedical Engineering}}},
  author = {Jiang, S. and Liu, W. and Liu, M.},
  month = may,
  year = {2011},
  keywords = {Anisotropic magnetoresistance,biodiffusion,biomedical MRI,clinical diagnosis,diffusion tensor imaging,diffusion tensor imaging fibers tracking,DTI,EEG,electroencephalogram,Electroencephalography,fMRI,functional magnetic resonance imaging,Magnetoencephalography,mangnetoencephalography,MEG,noninvasive examine technique,review,reviews,Surgery,Tumors},
  pages = {1--4},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\46NUDEET\\5780197.html:text/html}
}

@article{Congedo2006,
  title = {Subspace {{Projection Filters}} for {{Real}}-{{Time Brain Electromagnetic Imaging}}},
  volume = {53},
  issn = {0018-9294},
  doi = {10.1109/TBME.2006.878055},
  abstract = {An increasing number of neuroimaging laboratories are becoming interested in real-time investigations of the human brain. The opportunities offered by real-time applications are inversely proportional to the latency of the brain activity response and to the computational delay of brain activity estimation. Electromagnetic tomographies, based on electroencephalography (EEG) or magnetoencephalography (MEG), feature immediacy of brain activity response and excellent time resolution, hence they are natural candidates. However their spatial resolution and signal-to-noise ratio are poor. In this paper, we develop data-independent and data-dependent subspace projection filters for the standardized low-resolution electromagnetic tomography (sLORETA), a weighted minimum norm inverse solution for EEG/MEG. The filters are designed for extracting time-series of source activity in any given region of interest. The data-independent filter is shown to reduce interference of sources originating in neighboring regions, whereas the data-dependent filter is shown to suppress sensor measurement noise. An effective and straightforward way to combine them is demonstrated. The result is a dual subspace projection allowing both noise suppression and interference reduction},
  timestamp = {2016-10-20T12:35:04Z},
  number = {8},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Congedo, M.},
  month = aug,
  year = {2006},
  keywords = {Beamforming,bioelectric phenomena,Brain,brain activity estimation,brain activity response,Brain Mapping,Computer Simulation,Computer Systems,Delay estimation,Diagnosis; Computer-Assisted,EEG,Electroencephalography,Electromagnetic fields,filter,filters,interference reduction,Interference suppression,inverse solution,Laboratories,Magnetoencephalography,medical image processing,MEG,minimum norm,Models; Neurological,Neuroimaging,Noise reduction,noise suppression,principal component analysis,quadratic form,Rayleigh quotient,real-time brain electromagnetic imaging,real-time neuroimaging,Signal Processing; Computer-Assisted,signal-to-noise ratio,sLORETA,spatial filters,Spatial resolution,standardized low-resolution electromagnetic tomography,subspace projection filters,time series,Tomography,weighted minimum norm inverse solution},
  pages = {1624--1634},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2UU8M6XQ\\1658157.html:text/html}
}

@inproceedings{Ueno1997,
  title = {Model Studies of Spreading Electrical Activities in the Brain on {{MEG}}/{{EEG}} Inverse Problem},
  volume = {3},
  doi = {10.1109/IEMBS.1997.756570},
  abstract = {Using magnetoencephalogram (MEG) and electroencephalogram (EEG) the source estimation is performed in recent research. We simulated the MEG source estimation (inverse problem) for the purpose of finding the spread and the amplitude of neural activities. We proposed the hexagonal spreading dipole (HSD) model to briefly describe the source's spread. Inverse simulation indicates that the source localization using the HSD model can be as accurate as the localization with a single dipole model. As the spread and the amplitude are associated with each other, it is effective to combine their information. However, it is significantly difficult to find the orientation of the HSD model, because the radial component of the neural source has few magnetic fields. The information of a cerebral cortex shape or EEG signal with a radial component should be applied to the source estimation algorithm},
  timestamp = {2016-10-20T12:37:30Z},
  booktitle = {Proceedings of the 19th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}, 1997},
  author = {Ueno, K. and Ueno, S.},
  month = oct,
  year = {1997},
  keywords = {amplitude of neural activities,Biomedical engineering,Brain,Brain modeling,brain models,Cerebral cortex,cerebral cortex shape,EEG source estimation,Electroencephalography,hexagonal spreading dipole model,Inverse problem,inverse problems,inverse simulation,Levenberg-Marquardt algorithm,Magnetic field measurement,Magnetic heads,Magnetoencephalography,medical signal detection,medical signal processing,Medical simulation,MEG source estimation,model studies,Neurons,parameter estimation,Shape,single dipole model,source estimation algorithm,Source localization,source orientation,spreading electrical activities},
  pages = {1180--1181 vol.3},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TV6BAWIR\\756570.html:text/html}
}

@inproceedings{Ilmoniemi1999,
  title = {Recent Advances and Applications of {{MEG}}},
  volume = {2},
  doi = {10.1109/IEMBS.1999.804286},
  abstract = {Simultaneous large-array MEG and multichannel EEG combined with structural MRI and fMRI information provides an accurate spatiotemporal image of brain activity. We have applied these techniques to study sensory processing, memory, language functions, and the interaction of different sensory modalities. New information from healthy subjects as well as several patient groups have been obtained and clinical applications are emerging},
  timestamp = {2016-10-20T12:38:13Z},
  booktitle = {[{{Engineering}} in {{Medicine}} and {{Biology}}, 1999. 21st {{Annual Conference}} and the 1999 {{Annual Fall Meetring}} of the {{Biomedical Engineering Society}}] {{BMES}}/{{EMBS Conference}}, 1999. {{Proceedings}} of the {{First Joint}}},
  author = {Ilmoniemi, R. J.},
  month = oct,
  year = {1999},
  keywords = {accurate spatiotemporal image,alcoholism,Alzheimer's disease,biomedical MRI,Brain,brain activity,brain surgery,central sulcus,chronic pain,clinical applications,developmental disorders,diseases,Electroencephalography,Epilepsy,epileptic activity,fMRI information,healthy subjects,Hospitals,Humans,Instruments,Laboratories,language functions;,large-array MEG,Magnetic field measurement,magnetic resonance imaging,Magnetoencephalography,memory,multichannel EEG,Parkinson's disease,patient groups,schizophrenia,sensory modality interaction,sensory processing,somatosensory activation,somatosensory phenomena,SQUIDs,stroke,structural MRI,sudden unilateral deafness},
  pages = {1124 vol.2--},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ECTEAVQT\\804286.html:text/html}
}

@article{Ent2001,
  title = {A Fast Method to Derive Realistic {{BEM}} Models for {{E}}/{{MEG}} Source Reconstruction},
  volume = {48},
  issn = {0018-9294},
  doi = {10.1109/10.966602},
  abstract = {A fast method for segmentation of a subject's skin, skull or brain compartment for electroencephalogram (EEG)/magnetoencephalogram (MEG) (E/MEG) source localization is proposed. The method is based on a description of volumes with spherical harmonics and a database of exact surfaces. Using the spherical harmonic coefficients, sets of basis surfaces are obtained for each compartment. New segmentations can be acquired by combining the appropriate basis surfaces to describe a delineation of the volume in a limited number of magnetic resonance (MR) slices. Alternatively, a representation of the skin can be derived from digitized head shape. Skull and brain then can be predicted from the skin representation with a prediction model also obtained from the segmentation database. Database segmentations were recomputed with the proposed method. Mean deviations from the originals were about 2 and 3 mm for compartments derived from MR and head shape. Dipole simulations with original surfaces for forward and computed segmentations for inverse calculations showed average dipole mislocalizations of 1.6 and 3.3 mm, respectively. With the proposed method highly accurate segmentation can be performed with much less effort and in much less time compared with other techniques. The method also is applicable when MR data is unavailable but a digitization of the head is},
  timestamp = {2016-10-20T12:40:29Z},
  number = {12},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {van 't Ent, D. and de Munck, J. C. and Kaas, A. L.},
  month = dec,
  year = {2001},
  keywords = {average dipole mislocalizations,boundary-elements methods,Brain modeling,Computational modeling,computed segmentations,Databases,EEG source reconstruction,Electroencephalography,for inverse calculations,harmonics,Head,Magnetic heads,Magnetic resonance,Magnetoencephalography,medical signal processing,MEG source reconstruction,MR data,Predictive models,realistic BEM models derivation method,Shape,signal reconstruction,Skin,skin representation,Skull,spherical harmonics,volume conductor},
  pages = {1434--1443},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZF9HXJKK\\966602.html:text/html}
}

@article{Wu2013a,
  title = {Matching {{Pursuit}} and {{Source Deflation}} for {{Sparse EEG}}/{{MEG Dipole Moment Estimation}}},
  volume = {60},
  issn = {0018-9294},
  doi = {10.1109/TBME.2013.2253101},
  abstract = {In this paper, we propose novel matching pursuit (MP)-based algorithms for EEG/MEG dipole source localization and parameter estimation for multiple measurement vectors with constant sparsity. The algorithms combine the ideas of MP for sparse signal recovery and source deflation, as employed in estimation via alternating projections. The source-deflated matching pursuit (SDMP) approach mitigates the problem of residual interference inherent in sequential MP-based methods or recursively applied (RAP)-MUSIC. Furthermore, unlike prior methods based on alternating projection, SDMP allows one to efficiently estimate the dipole orientation in addition to its location. Simulations show that the proposed algorithms outperform existing techniques under various conditions, including those with highly correlated sources. Results using real EEG data from auditory experiments are also presented to illustrate the performance of these algorithms.},
  timestamp = {2016-10-20T12:42:14Z},
  number = {8},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Wu, S. C. and Swindlehurst, A. L.},
  month = aug,
  year = {2013},
  keywords = {algorithms,Alternating projection,alternating projection estimation,Auditory Cortex,auditory experiment,Brain Mapping,brain models,deflation,dipole location,dipole orientation estimation,Dipole source localization,EEG-MEG dipole source localization,electric moments,Electroencephalography,electroencephalography (EEG),Evoked Potentials; Auditory,hearing,high correlated source,Humans,Indexes,magnetic moments,Magnetoencephalography,magnetoencephalography (MEG),Matching pursuit algorithms,matching pursuit (MP),medical signal processing,multiple measurement vector,Nerve Net,neurophysiology,parameter estimation,Pattern Recognition; Automated,RAP-MUSIC,recursively applied-MUSIC,Reproducibility of Results,residual interference,SDMP algorithm,Sensitivity and Specificity,sequential estimation,sequential MP-based method,source-deflated matching pursuit,source deflation,sparse EEG-MEG dipole moment estimation,sparse representations,sparse signal recovery,Vectors},
  pages = {2280--2288},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8EARV99J\\6480805.html:text/html}
}

@inproceedings{Lopez2012,
  title = {Random Location of Multiple Sparse Priors for Solving the {{MEG}}/{{EEG}} Inverse Problem},
  doi = {10.1109/EMBC.2012.6346234},
  abstract = {MEG/EEG brain imaging has become an important tool in neuroimaging. Current techniques based in Bayesian approaches require an a-priori definition of patch locations on the cortical manifold. Too many patches results in a complex optimisation problem, too few an under sampling of the solution space. In this work random locations of the possible active regions of the brain are proposed to iteratively arrive at a solution. We use Bayesian model averaging to combine different possible solutions. The proposed methodology was tested with synthetic MEG datasets reducing the localisation error of the approaches based on fixed locations. Real data from a visual attention study was used for validation.},
  timestamp = {2016-10-20T12:43:46Z},
  booktitle = {2012 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {L{\'o}pez, J. D. and Espinosa, J. J. and Barnes, G. R.},
  month = aug,
  year = {2012},
  keywords = {a-priori definition,Bayesian approaches,Bayesian methods,Bayesian model,Bayes methods,Bayes Theorem,Brain Mapping,Brain modeling,complex optimisation problem,Computational modeling,Computer Simulation,cortical manifold,Electroencephalography,Humans,Image Processing; Computer-Assisted,image reconstruction,Image sampling,inverse problems,iterative methods,localisation error,Magnetoencephalography,medical image processing,MEG-EEG brain imaging,MEG-EEG inverse problem,multiple sparse priors,Neuroimaging,neurophysiology,optimisation,patch locations,random location,Reproducibility of Results,solution space sampling,Thermodynamics,Visual Cortex},
  pages = {1534--1537},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\N26PIUUR\\6346234.html:text/html}
}

@inproceedings{Becker2012,
  title = {Tensor-Based Preprocessing of Combined {{EEG}}/{{MEG}} Data},
  abstract = {Due to their good temporal resolution, electroencephalography (EEG) and magnetoencephalography (MEG) are two often used techniques for brain source analysis. In order to improve the results of source localisation algorithms applied to EEG or MEG data, tensor-based preprocessing techniques can be used to separate the sources and reduce the noise. These methods are based on the Canonical Polyadic (CP) decomposition (also called Parafac) of space-time-frequency (STF) or space-time-wave-vector (STWV) data. In this paper, we analyse the combination of EEG and MEG data to enhance the performance of the tensor-based preprocessing. To this end, we consider the joint CP decomposition of two (or more) third order tensors with one or two identical loading matrices. We present the necessary modifications for several classical CP decomposition algorithms and examine the gain on performance in the EEG/MEG context by means of simulations.},
  timestamp = {2016-10-21T13:53:01Z},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}}), {{Proceedings}} of the 20th {{European}}},
  author = {Becker, H. and Comon, P. and Albera, L.},
  month = aug,
  year = {2012},
  keywords = {Brain modeling,brain source analysis,canonical polyadic decomposition,combined EEG/MEG data,CP decomposition algorithms,EEG,Electroencephalography,Joints,Loading,loading matrices,Load modeling,Magnetoencephalography,matrix algebra,Matrix decomposition,medical signal processing,MEG,Parafac,source localisation algorithms,space-time-frequency data,space-time-wave-vector data,STF data,STWV data,STWV/STF analysis,Tensile stress,tensor-based preprocessing techniques,tensors},
  pages = {275--279},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TNIE3C9G\\6333946.html:text/html}
}

@article{He2008,
  title = {Multimodal {{Functional Neuroimaging}}: {{Integrating Functional MRI}} and {{EEG}}/{{MEG}}},
  volume = {1},
  issn = {1937-3333},
  shorttitle = {Multimodal {{Functional Neuroimaging}}},
  doi = {10.1109/RBME.2008.2008233},
  abstract = {Noninvasive functional neuroimaging, as an important tool for basic neuroscience research and clinical diagnosis, continues to face the need of improving the spatial and temporal resolution. While existing neuroimaging modalities might approach their limits in imaging capability mostly due to fundamental as well as technical reasons, it becomes increasingly attractive to integrate multiple complementary modalities in an attempt to significantly enhance the spatiotemporal resolution that cannot be achieved by any modality individually. Electrophysiological and hemodynamic/metabolic signals reflect distinct but closely coupled aspects of the underlying neural activity. Combining fMRI and EEG/MEG data allows us to study brain function from different perspectives. In this review, we start with an overview of the physiological origins of EEG/MEG and fMRI, as well as their fundamental biophysics and imaging principles, we proceed with a review of the major advances in the understanding and modeling of neurovascular coupling and in the methodologies for the fMRI-EEG/MEG simultaneous recording. Finally, we summarize important remaining issues and perspectives concerning multimodal functional neuroimaging, including brain connectivity imaging.},
  timestamp = {2016-10-20T12:48:11Z},
  journal = {IEEE Reviews in Biomedical Engineering},
  author = {He, B. and Liu, Z.},
  year = {2008},
  keywords = {Animals,biomedical MRI,Brain,brain connectivity imaging,brain function,Brain Mapping,clinical diagnosis,EEG,Electroencephalography,electrophysiological signals,Electrophysiology,fMRI,functional MRI,haemodynamics,hemodynamic signals,human brain mapping,Humans,Image Processing; Computer-Assisted,Image resolution,magnetic resonance imaging,medical image processing,MEG,metabolic signals,multimodal functional neuroimaging,multimodal neuroimaging,Neuroimaging,neurophysiology,neuroscience,neuroscience research,neurovascular coupling,noninvasive functional neuroimaging,patient diagnosis,Signal resolution,Spatial resolution,Spatiotemporal phenomena,spatiotemporal resolution},
  pages = {23--40},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7MUQ4F8Q\\4664427.html:text/html}
}

@inproceedings{Gratta2007,
  title = {{{MEG}}-{{EEG}}-{{fMRI}}: {{What}} Can Be Gained in the {{Study}} of the {{Brain}} with a {{Multimodal Approach}}},
  shorttitle = {{{MEG}}-{{EEG}}-{{fMRI}}},
  doi = {10.1109/NFSI-ICFBI.2007.4387674},
  abstract = {In this short review we describe the potentialities, the difficulties, and the most common methods of combining EEG/MEG and fMRI data into a single neuroimaging technique with high spatial and temporal resolution. Two examples of application to brain studies, from our own experience, are also described.},
  timestamp = {2016-10-20T12:49:35Z},
  booktitle = {Joint {{Meeting}} of the 6th {{International Symposium}} on {{Noninvasive Functional Source Imaging}} of the {{Brain}} and {{Heart}} and the {{International Conference}} on {{Functional Biomedical Imaging}}, 2007. {{NFSI}}-{{ICFBI}} 2007},
  author = {del Gratta, C. and Brunetti, M. and Mantini, D. and Romani, G. L.},
  month = oct,
  year = {2007},
  keywords = {biomedical MRI,Brain,EEG,Electroencephalography,fMRI,Hemodynamics,High-resolution imaging,Image resolution,Magnetoencephalography,MEG,Neuroimaging,neurophysiology,Optical imaging,Positron emission tomography,Sampling methods,Signal resolution,single neuroimaging technique,Spatial resolution},
  pages = {8--13},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3R3DDMQX\\4387674.html:text/html}
}

@inproceedings{Haueisen2014,
  title = {Influence of Volume Conductor Modeling on Source Reconstruction in Magnetoencephalography and Electroencephalography},
  doi = {10.1109/URSIGASS.2014.6930127},
  abstract = {Summary form only given. The function and structure of the human brain is immensely complex and, at the same time, the key to understanding human behavior and many of today's prevailing diseases. In most cases, this system cannot be investigated directly, but only non-invasively from outside the head. Although several non-invasive measurement modalities are available, only magnetoencephalography (MEG) and electroencephalography (EEG) provide information with a high temporal resolution. In order to reconstruct the neuronal activity underlying measured EEG and MEG data both the forward problem (computing the electromagnetic field due to given sources) and the inverse problem (finding the best fitting sources to explain given data) have to be solved. The forward problem involves a source model and a model with the conductivities of the head. The conductivity model can be as simple as a homogeneously conducting sphere or as complex as a finite element model consisting of millions of elements, each with a different anisotropic conductivity tensor. The question is addressed how complex the employed forward model should be, and, more specifically, the influence of anisotropic volume conduction and the influence of conductivity inhomogeneities are evaluated. For this purpose high resolution finite element models of the rabbit and the human head are employed in combination with individual conductivity tensors to quantify the influence of white matter anisotropy on the solution of the forward and inverse problem in EEG and MEG. Although the current state of the art in the analysis of this influence of brain tissue anisotropy on source reconstruction does not yet allow a final conclusion, the results available indicate that the expected average source localization error due to anisotropic white matter conductivity might be within the principal accuracy limits of current inverse procedures. However, in some percent of the cases a considerably larger localization error might o- cur. In contrast, dipole orientation and dipole strength estimation are influenced significantly by anisotropy. Skull conductivity inhomogeneities such as the spongy bone structure embedded in the compact bone or surgical holes or fontanels in infants have a non-negligible effect on the EEG and MEG forward and inverse problem solution. Especially when source positions are expected to be in the vicinity of the conductivity inhomogeneity and when a large difference with respect to the skull conductivity is indicated, the modeling approach should take the inhomogeneities into account. In conclusion, models taking into account tissue anisotropy and conductivity inhomogeneities information are expected to improve source estimation procedures. Depending on the question addressed, the complexity of the forward and inverse solution approach has to be chosen.},
  timestamp = {2016-10-20T12:52:22Z},
  booktitle = {General {{Assembly}} and {{Scientific Symposium}} ({{URSI GASS}}), 2014 {{XXXIth URSI}}},
  author = {Haueisen, J. and Lau, S. and Flemming, L. and Sonntag, H. and Maess, B. and G{\"u}llmar, D.},
  month = aug,
  year = {2014},
  keywords = {bioelectric phenomena,Brain modeling,compact bone,Conductivity,conductivity inhomogeneity,conductivity model,conductivity tensors,Educational institutions,EEG,Electroencephalography,finite element analysis,high resolution finite element model,human brain,human head,infant fontanels,Magnetic heads,Magnetoencephalography,medical signal processing,MEG,Nonhomogeneous media,noninvasive measurement,rabbit head,signal reconstruction,skull conductivity,Source reconstruction,spongy bone structure,surgical holes,volume conductor modeling,white matter anisotropy},
  pages = {1--2},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RDRV8MU6\\6930127.html:text/html}
}

@article{Dammers2008,
  title = {Integration of {{Amplitude}} and {{Phase Statistics}} for {{Complete Artifact Removal}} in {{Independent Components}} of {{Neuromagnetic Recordings}}},
  volume = {55},
  issn = {0018-9294},
  doi = {10.1109/TBME.2008.926677},
  abstract = {In magnetoencephalography (MEG) and electroencephalography (EEG), independent component analysis is widely applied to separate brain signals from artifact components. A number of different methods have been proposed for the automatic or semiautomatic identification of artifact components. Most of the proposed methods are based on amplitude statistics of the decomposed MEG/EEG signal. We present a fully automated approach based on amplitude and phase statistics of decomposed MEG signals for the isolation of biological artifacts such as ocular, muscle, and cardiac artifacts (CAs). The performance of different artifact identification measures was investigated. In particular, we show that phase statistics is a robust and highly sensitive measure to identify strong and weak components that can be attributed to cardiac activity, whereas a combination of different measures is needed for the identification of artifacts caused by ocular and muscle activity. With the introduction of a rejection performance parameter, we are able to quantify the rejection quality for eye blinks and CAs. We demonstrate in a set of MEG data the good performance of the fully automated procedure for the removal of cardiac, ocular, and muscle artifacts. The new approach allows routine application to clinical measurements with small effect on the brain signal.},
  timestamp = {2016-10-20T16:26:42Z},
  number = {10},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Dammers*, J. and Schiek, M. and Boers, F. and Silex, C. and Zvyagintsev, M. and Pietrzyk, U. and Mathiak, K.},
  month = oct,
  year = {2008},
  keywords = {amplitude statistics,artifact identification,Artifact reduction,Artifacts,Artificial Intelligence,biological artifact removal,Biometry,Biophysics,blind source separation,blind source separation (BSS),Blinking,brain signal,cardiac activity,Content addressable storage,EEG,Electrocardiography,Electroencephalography,Electrooculography,Factor Analysis; Statistical,Humans,independent component analysis,independent component analysis (ICA),Linear Models,Magnetoencephalography,magnetoencephalography (MEG),magneto-encephalography (MEG),medical signal processing,MEG,muscle activity,Muscles,Myocardial Contraction,neuromagnetic recording,neurophysiology,neuroscience,ocular activity,Particle measurements,Pattern Recognition; Automated,Phase measurement,phase statistics,principal component analysis,Psychiatry,signal decomposition,Signal Processing; Computer-Assisted,Statistics,Weights and Measures},
  pages = {2353--2362},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QIR7W97M\\4536072.html:text/html}
}

@article{Ueno1988,
  title = {{{MEG}} and {{EEG Topography}} and a {{Source Model}} of {{Slow Wave Types}} of {{Abnormality}}},
  volume = {3},
  issn = {0882-4959},
  doi = {10.1109/TJMJ.1988.4563689},
  abstract = {A source model is proposed to simulate spatial distributions of abnormal MEG and EEG activities, such as the delta activity associated with brain tumors. The brain tumor itself is electrically silent, but the spherical shell around the tumor may cause abnormal neural activities. The sources of these neural activities are represented by combinations of multiple dipoles. The magnetic fields and electrical potentials arising from these dipoles are calculated over the surface of the head. The results show that, in a special case where the dipoles are oriented in the same direction or are oriented radially, the spatial MEG and EEG patterns are analogous to those generated by a single dipole or a pair of dipoles. The electrical conductivity of brain lesions varies with such pathological circumstances as edemas and calcification. The effect of inhomogeneities of brain lesions on spatial MEG and EEG patterns is also studied. It is found that only the magnitude of the magnetic fields and potentials and influenced by the inhomogeneities, while the spatial patterns are not influenced.},
  timestamp = {2016-10-20T12:56:31Z},
  number = {3},
  journal = {IEEE Translation Journal on Magnetics in Japan},
  author = {Ueno, S. and Iramina, K. and Harada, K. and Matsuoka, S.},
  month = mar,
  year = {1988},
  keywords = {Brain modeling,Conductivity,Electric potential,Electroencephalography,Lesions,Magnetic fields,Magnetic heads,Neoplasms,Nonuniform electric fields,Surface topography},
  pages = {281--287},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\M43E2RJ6\\4563689.html:text/html}
}

@article{Eichardt2008,
  title = {Reconstruction of {{Multiple Neuromagnetic Sources Using Augmented Evolution Strategies}} \#x2014; {{A Comparative Study}}},
  volume = {55},
  issn = {0018-9294},
  doi = {10.1109/TBME.2007.912656},
  abstract = {The localization of dipolar sources in the brain based on electroencephalography (EEG) or magnetoencephalography (MEG) data is a frequent problem in the neurosciences. Deterministic standard approaches such as the Levenberg-Marquardt (LM) method often have problems in finding the global optimum of the associated nonlinear optimization function, when two or more dipoles are to be reconstructed. In such cases, probabilistic approaches turned out to be superior, but their applicability in neuromagnetic source localizations is not yet satisfactory. The objective of this study was to find probabilistic optimization strategies that perform better in such applications. Thus, hybrid and nested evolution strategies (NES) which both realize a combination of global and local search by means of multilevel optimizations were newly designed. The new methods were bench-marked and compared to the established evolution strategies (ES), to fast evolution strategies (FES), and to the deterministic LM method by conducting a two-dipole fit with MEG data sets from neuropsychological experiments. The best results were achieved with NES.},
  timestamp = {2016-10-20T12:58:20Z},
  number = {2},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Eichardt, R. and Haueisen, J. and Knosche, T. R. and Schukat-Talamazzini, E. G.},
  month = feb,
  year = {2008},
  keywords = {Action Potentials,algorithms,Animals,augmented evolution strategy,Biomedical engineering,Biomedical informatics,Biomedical measurements,Brain,brain dipolar sources in the brain based on electroencephalography,Brain Mapping,Brain modeling,Computer science,Computer Simulation,Electroencephalography,Evolution strategies (ES),fast evolution strategy,Genetic algorithms,Humans,hybrid optimization strategies,inverse problems,Levenberg-Marquardt method,Magnetoencephalography,medical signal processing,Models; Neurological,multilevel optimization,multiple neuromagnetic sources,Nerve Net,nested evolution strategies (NES),nested evolution strategy,neuromagnetic source localization,Neurons,neurophysiology,nonlinear optimization function,optimisation,Optimization methods,signal reconstruction,Signal to noise ratio,Source localization},
  pages = {703--712},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\T9X5XM3Q\\4432732.html:text/html}
}

@inproceedings{Eichardt2007,
  title = {The {{Application}} of {{Single}}- and {{Multi}}-{{Level Fast Evolution Strategies}} for the {{Reconstruction}} of {{Multiple Neuromagnetic Sources}}},
  doi = {10.1109/NFSI-ICFBI.2007.4387699},
  abstract = {The localization of dipolar sources in the brain based on EEG or MEG data is a frequent problem in the neurosciences. Especially deterministic approaches often have problems in finding the global optimum of the associated non-linear optimization function, when two or more di poles are to be reconstructed. In such cases, probabilistic approaches turned out to be superior, but their applicability in neuromagnetic source localizations is not yet satisfactory. The objective of this study was the design of multi-level evolution strategies that perform better in such applications. We newly created nested fast evolution strategies which realize a combination of locally searching inner evolution strategies and globally searching outer fast evolution strategies. They were benchmarked and compared to single-level fast evolution strategy by conducting a two dipole fit with a MEG data set from a neuropsychological experiment. In the comparison, fast nested evolution strategies showed superior performance.},
  timestamp = {2016-10-20T12:59:19Z},
  booktitle = {Joint {{Meeting}} of the 6th {{International Symposium}} on {{Noninvasive Functional Source Imaging}} of the {{Brain}} and {{Heart}} and the {{International Conference}} on {{Functional Biomedical Imaging}}, 2007. {{NFSI}}-{{ICFBI}} 2007},
  author = {Eichardt, R. and Haueisen, J. and Knosche, T. R. and Schukat-Talamazzini, E. G.},
  month = oct,
  year = {2007},
  keywords = {bioelectric phenomena,Biomedical engineering,Biomedical informatics,Brain modeling,Computational modeling,Computer science,Computer Simulation,EEG,Electroencephalography,Humans,Magnetoencephalography,medical signal processing,MEG,multilevel fast evolution strategies,multiple neuromagnetic source reconstruction,neurophysiology,Optimization methods,signal reconstruction,Simulated annealing,single-level fast evolution strategies},
  pages = {105--108},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PXTTDZ72\\4387699.html:text/html}
}

@article{Mitra2006,
  title = {Concentration Maximization and Local Basis Expansions ({{LBEX}}) for Linear Inverse Problems},
  volume = {53},
  issn = {0018-9294},
  doi = {10.1109/TBME.2006.876629},
  abstract = {Linear inverse problems arise in biomedicine electro-encephalography and magnetoencephalography (EEG and MEG) and geophysics. The kernels relating sensors to the unknown sources are Green's functions of some partial differential equation. This knowledge is obscured when treating the discretized kernels simply as matrices. Consequently, physical understanding of the fundamental resolution limits has been lacking. We relate the inverse problem to spatial Fourier analysis, and the resolution limits to uncertainty principles, providing conceptual links to underlying physics. Motivated by the spectral concentration problem and multitaper spectral analysis, our approach constructs local basis sets using maximally concentrated linear combinations of the measurement kernels},
  timestamp = {2016-10-20T13:00:35Z},
  number = {9},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Mitra, P. P. and Maniar, H.},
  month = sep,
  year = {2006},
  keywords = {algorithms,Biosensors,Brain,Brain Mapping,Computer Simulation,concentration maximization,Diagnosis; Computer-Assisted,EEG,Electroencephalography,Evoked Potentials,Fourier analysis,Geophysics,Green functions,Green's function methods,Humans,Inverse problem,inverse problems,Kernel,kernels,Likelihood Functions,linear inverse problems,Linear Models,local basis expansions,Magnetic sensors,Magnetoencephalography,medical signal processing,MEG,Models; Neurological,multitaper,multitaper spectral analysis,partial differential equation,partial differential equations,resolution,resolution limits,spatial Fourier analysis,Spatial resolution,spectral analysis,spectral concentration problem,uncertainty principle,uncertainty principles},
  pages = {1775--1782},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\37RB9REM\\1673619.html:text/html}
}

@article{Waldorp2005,
  title = {The {{Wald}} Test and {{Crame}} Acute;r-{{Rao}} Bound for Misspecified Models in Electromagnetic Source Analysis},
  volume = {53},
  issn = {1053-587X},
  doi = {10.1109/TSP.2005.853213},
  abstract = {By using signal processing techniques, an estimate of activity in the brain from the electro- or magneto-encephalogram (EEG or MEG) can be obtained. For a proper analysis, a test is required to indicate whether the model for brain activity fits. A problem in using such tests is that often, not all assumptions are satisfied, like the assumption of the number of shells in an EEG. In such a case, a test on the number of sources (model order) might still be of interest. A detailed analysis is presented of the Wald test for these cases. One of the advantages of the Wald test is that it can be used when not all assumptions are satisfied. Two different, previously suggested, Wald tests in electromagnetic source analysis (EMSA) are examined: a test on source amplitudes and a test on the closeness of source pairs. The Wald test is analytically studied in terms of alternative hypotheses that are close to the hypothesis (local alternatives). It is shown that the Wald test is asymptotically unbiased, that it has the correct level and power, which makes it appropriate to use in EMSA. An accurate estimate of the Crame\textasciiacute{}r-Rao bound (CRB) is required for the use of the Wald test when not all assumptions are satisfied. The sandwich CRB is used for this purpose. It is defined for nonseparable least squares with constraints required for the Wald test on amplitudes. Simulations with EEG show that when the sensor positions are incorrect, or the number of shells is incorrect, or the conductivity parameter is incorrect, then the CRB and Wald test are still good, with a moderate number of trials. Additionally, the CRB and Wald test appear robust against an incorrect assumption on the noise covariance. A combination of incorrect sensor positions and noise covariance affects the possibility of detecting a source with small amplitude.},
  timestamp = {2016-10-20T13:01:48Z},
  number = {9},
  journal = {IEEE Transactions on Signal Processing},
  author = {Waldorp, L. J. and Huizenga, H. M. and Grasman, R. P. P. P.},
  month = sep,
  year = {2005},
  keywords = {Approximate model,Brain modeling,Conductivity,constrained optimization,covariance analysis,Cramer-Rao bound,electroencephalogram,Electroencephalography,Electromagnetic analysis,Electromagnetic modeling,electromagnetic source analysis,Fisher information with constraints,hypothesis,least squares approximations,Least squares methods,Magnetic analysis,magnetoencephalogram,Magnetoencephalography,medical signal processing,misspecified model,model checking,neurophysiology,noise covariance,Noise robustness,nonseparable least squares method,parameter constrained optimization,parameter covariance,separable least squares,signal processing,signal processing technique,source amplitude,Source localization,Testing,Wald test},
  pages = {3427--3435},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P3M9SVMR\\1495880.html:text/html}
}

@inproceedings{Wolters2004,
  title = {The Influence of Volume Conduction Effects on the {{EEG}}/{{MEG}} Reconstruction of the Sources of the {{Early Left Anterior Negativity}}},
  volume = {2},
  doi = {10.1109/IEMBS.2004.1404003},
  abstract = {To achieve a deeper understanding of language processing in the human brain, scientists and clinicians use Electroencephalography (EEG) and Magnetoencephalography (MEG) inverse methods to reconstruct sources of Event Related Potentials. There exists a persistent uncertainty regarding the influence of volume conduction effects such as the anisotropy of tissue conductivity of the skull and the white matter layers on the inverse results. In this paper, we will study the sensitivity to anisotropy of the source reconstruction of the Early Left Anterior Negativity (ELAN) component in language processing. For EEG, the presence of tissue anisotropy substantially compromises the restoration ability of an L1-norm current density approach. The centers of activity are strongly shifted along the Sylvian fissure in the anterior direction. In contrast, MEG in combination with the L1 norm approach is able to reconstruct the main features of the ELAN source distribution even in the presence of anisotropic conductivity.},
  timestamp = {2016-10-20T13:03:19Z},
  booktitle = {26th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}, 2004. {{IEMBS}} '04},
  author = {Wolters, C. H. and Anwander, A. and Maess, B. and MacLeod, R. S. and Friederici, A. D.},
  month = sep,
  year = {2004},
  keywords = {Anisotropic magnetoresistance,Brain modeling,Conductivity,EEG/MEG source reconstruction,ELAN,Electroencephalography,Finite element method,image reconstruction,Image segmentation,influence of skull and white matter anisotropy,inverse problems,L1 norm current density reconstruction,Magnetic heads,magnetic resonance imaging,Skull},
  pages = {3569--3572},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3WBF2AGV\\1404003.html:text/html}
}

@article{Ueno1986,
  title = {The {{MEG}} Topography and the Source Model of Abnormal Neural Activities Associated with Brain Lesions},
  volume = {22},
  issn = {0018-9464},
  doi = {10.1109/TMAG.1986.1064577},
  abstract = {A source model is proposed to simulate spatial distributions of abnormal MEG and EEG activities generated by abnormal neural activities such as the delta activity associated with brain tumors. Brain tumor itself is electrically silent and the spherical shell around the tumor might generate abnormal neural activities. The sources of these neural activities are represented by combinations of multiple current dipoles. The head is assumed to be a spherical volume conductor. Electrical potentials and magnetic fields over the surface of the sphere are calculated. The computer simulation shows that the MEG topography and EEG topography vary variously with combinations of location and orientation of the dipoles. In a special case, however, that the dipoles orient in the same direction or orient radially, the spatial patterns of the MEGs and EEGs generated by numerous dipoles are analogous to those generated by single dipoles.},
  timestamp = {2016-10-20T13:06:45Z},
  number = {5},
  journal = {IEEE Transactions on Magnetics},
  author = {Ueno, S. and Iramina, H. and Ozaki, H. and Harada, K.},
  month = sep,
  year = {1986},
  keywords = {Biomagnetics,Biomedical imaging,Brain,Brain modeling,Computer Simulation,Conductors,Electric potential,Electroencephalography,Lesions,Magnetic fields,Magnetic heads,Neoplasms,Surface topography},
  pages = {874--876},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7W3F3ZF5\\1064577.html:text/html}
}

@inproceedings{Radich1994,
  title = {{{EEG}} Dipole Localization Bounds for Head Models with Parameter Uncertainties},
  volume = {iv},
  doi = {10.1109/ICASSP.1994.389872},
  abstract = {The Cramer-Rao bound for unbiased location estimation of multiple current dipoles is derived under the assumption of a general head model parameterized by a combination of deterministic and stochastic parameters. The expression (also applicable to MEG) thus characterizes fundamental limits on EEG localization performance due to the effects of both statistical measurement noise and model uncertainty},
  timestamp = {2016-10-20T13:08:11Z},
  booktitle = {, 1994 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 1994. {{ICASSP}}-94},
  author = {Radich, B. and Buckley, K. M.},
  month = apr,
  year = {1994},
  keywords = {Array signal processing,Brain modeling,covariance matrix,Cramer-Rao bound,deterministic parameters,EEG dipole localization bounds,Electroencephalography,Forward contracts,head models,Magnetoencephalography,medical signal processing,MEG,model uncertainty,multiple current dipoles,Noise measurement,parameter estimation,parameter uncertainties,physiological models,Position measurement,random noise,sensor array,Sensor arrays,Sensor phenomena and characterization,statistical measurement noise,stochastic parameters,stochastic processes,unbiased location estimation,Uncertainty},
  pages = {IV/77--IV/80 vol.4},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3RBTVQC9\\389872.html:text/html}
}

@inproceedings{Fernandez1998,
  title = {Comparison of Different Source Localization Methods},
  doi = {10.1109/SIBGRA.1998.722767},
  abstract = {The localization of the neural generators of the brain's electromagnetic activity based an electrical or magnetic recordings requires the solution of an ill-posed inverse problem. In its general form, this inverse problem has no unique solution. The only way to deal with this problem, is to restrict the set of possible solutions by using either anatomical or mathematical constraints or a combination of both. On the basis of these restrictions, different reconstructions of the sources of EEG and MEG are possible, each providing a different representation of the electrical processes in the brain. Opposed to the more classical dipole localization methods, inverse procedures that produce distributed estimates of neuronal currents have received increasing attention in the last few years. In this paper, we compare different source localization methods using data from a pathological subject. Among these methods are: (1) spatio-temporal source models (BESA); (2) generalized minimum norm solutions (MNS) and (3) electromagnetic temporal tomography (EMTT). In order to assess their specific performance, each method is compared with the information contained in the CT},
  timestamp = {2016-10-20T13:09:11Z},
  booktitle = {International {{Symposium}} on {{Computer Graphics}}, {{Image Processing}}, and {{Vision}}, 1998. {{Proceedings}}. {{SIBGRAPI}} '98},
  author = {Fernandez, D. C. and Menendez, R. De Peralta and Andino, S. L. G. and Farras, E. O.},
  month = oct,
  year = {1998},
  keywords = {Argon,Artificial Intelligence,Brain,brain electromagnetic activity,Brain Mapping,computerised tomography,Cybernetics,dipole localization methods,EEG,electrical processes,electrical recordings,Electroencephalography,electromagnetic temporal tomography,generalized minimum norm solutions,Hospitals,ill-posed inverse problem,inverse problems,Laboratories,Magnetic recording,magnetic recordings,mathematical constraints,MEG,neural generators,neural nets,neuronal currents,Physics,source localization methods,spatio temporal source models},
  pages = {326--331},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P7QF2B3G\\722767.html:text/html}
}

@inproceedings{Dogandzic1998,
  title = {Localization of Evoked Electric Sources and Design of {{EEG}}/{{MEG}} Sensor Arrays},
  doi = {10.1109/SSAP.1998.739376},
  abstract = {We present a maximum likelihood (ML) method for estimating evoked dipole responses using electroencephalography (EEG) and magnetoencephalography (MEG) arrays and discuss EEG/MEG sensor array design. The electric source is modeled as a collection of current dipoles at fixed locations and the head as a spherical conductor. We permit the dipoles' moments to vary with time by modeling them as a linear combination of parametric or non-parametric basis functions and further discuss the case when the dipoles have fixed orientations in time. We estimate the dipoles' locations and moments, and derive the Fisher information matrix for the unknown parameters. Finally, we propose an array optimization criterion based on minimizing the volume of the linearized confidence region},
  timestamp = {2016-10-20T13:10:37Z},
  booktitle = {, {{Ninth IEEE SP Workshop}} on {{Statistical Signal}} and {{Array Processing}}, 1998. {{Proceedings}}},
  author = {Dogandzic, A. and Nehorai, A.},
  month = sep,
  year = {1998},
  keywords = {array optimization criterion,Array signal processing,bioelectric potentials,Brain modeling,Conductors,current dipoles,EEG/MEG,electric source,Electroencephalography,evoked dipole response,Fisher information matrix,Humans,linearized confidence region,location estimation,Magnetic field measurement,Magnetic heads,Magnetic sensors,Magnetoencephalography,matrix algebra,maximum likelihood estimation,maximum likelihood method,medical signal processing,minimisation,ML method,moment estimation,non-parametric basis functions,parameter estimation,parametric basis functions,Sensor arrays,spherical conductor,volume minimization},
  pages = {228--231},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SMUCJTWQ\\739376.html:text/html}
}

@inproceedings{Kettenman1998,
  title = {Magnetoencephalography [{{MEG}}/{{MRI}}/{{EEG}} in Epilepsy]},
  doi = {10.1049/ic:19980707},
  abstract = {When comparing the findings of magnetic source imaging (MSI) with other presurgical evaluations (EEG, MRI and intraoperative ECoG) in temporal lobe epilepsy lobar or even intralobar congruence can be found. The dipolar activity that can be recognized during a spike-wave event is localized in temporal neocortical or mesial regions. Further origin of epileptic activity can be localized by the method of spike averaging. The combination of MEG and MRI helps to build a bridge between morphological and functional localization. For clinical use MSI can serve as a guide for invasive recordings. Additionally it helps to detect functionally important brain regions and can serve as a pointer to discrete lesions in the MRI},
  timestamp = {2016-10-20T13:11:53Z},
  booktitle = {1998/444), {{IEE Colloquium}} on {{Electrical Engineering}} and {{Epilepsy}}: {{A Successful Partnership}} ({{Ref}}. {{No}}},
  author = {Kettenman, B.},
  month = jun,
  year = {1998},
  keywords = {dipolar activity,discrete lesions pointer,EEG,epileptic activity origin,functional localization,functionally important brain regions,intralobar congruence,intraoperative ECoG,lobar congruence,magnetic source imaging,Magnetoencephalography,MEG,mesial regions,morphological localization,MRI,presurgical evaluations,spike-wave event,temporal lobe epilepsy,temporal neocortical regions},
  pages = {8/1--8/5},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\N7EZUP4U\\710525.html:text/html}
}

@article{Mosher1998,
  title = {Recursive {{MUSIC}}: {{A}} Framework for {{EEG}} and {{MEG}} Source Localization},
  volume = {45},
  issn = {0018-9294},
  shorttitle = {Recursive {{MUSIC}}},
  doi = {10.1109/10.725331},
  abstract = {The multiple signal classification (MUSIC) algorithm can be used to locate multiple asynchronous dipolar sources from electroencephalography (EEG) and magnetocncephalography (MEG) data. The algorithm scans a single-dipole model through a three-dimensional (3-D) head volume and computes projections onto an estimated signal subspace. To locate the sources, the user must search the head volume for multiple local peaks in the projection metric. This task is time consuming and subjective. Here, the authors describe an extension of this approach which they refer to as recursive MUSIC (R-MUSIC). This new procedure automatically extracts the locations of the sources through a recursive use of subspace projections. The new method is also able to locate synchronous sources through the use of a spatio-temporal independent topographies (IT) model. This model defines a source as one or more nonrotating dipoles with a single time course. Within this framework, the authors are able to locate fixed, rotating, and synchronous dipoles. The recursive subspace projection procedure that they introduce here uses the metric of canonical or subspace correlations as a multidimensional form of correlation analysis between the model subspace and the data subspace, by recursively computing subspace correlations, the authors build up a model for the sources which account for a given set of data. They demonstrate here how R-MUSIC can easily extract multiple asynchronous dipolar sources that are difficult to find using the original MUSIC scan. The authors then demonstrate R-MUSIC applied to the more general IT model and show results for combinations of fixed, rotating, and synchronous dipoles.},
  timestamp = {2016-10-20T13:13:07Z},
  number = {11},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Mosher, J. C. and Leahy, R. M.},
  month = nov,
  year = {1998},
  keywords = {algorithms,Brain modeling,Classification algorithms,Computer Simulation,EEG source localization,Electroencephalography,estimated signal subspace,fixed dipoles,Humans,Least-Squares Analysis,Magnetic field measurement,Magnetic heads,Magnetoencephalography,medical signal processing,MEG source localization,Models; Neurological,multiple asynchronous dipolar sources,Multiple signal classification,multiple signal classification algorithm,physiological models,projection metric,recursive MUSIC,rotating dipoles,signal processing,Signal processing algorithms,Signal Processing; Computer-Assisted,single-dipole model,spatio-temporal independent topographies model,Surfaces,synchronous dipoles,synchronous sources,three-dimensional head volume},
  pages = {1342--1354},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5PDMGXPU\\725331.html:text/html}
}

@article{Dogandzic2000,
  title = {Estimating Evoked Dipole Responses in Unknown Spatially Correlated Noise with {{EEG}}/{{MEG}} Arrays},
  volume = {48},
  issn = {1053-587X},
  doi = {10.1109/78.815475},
  abstract = {We present maximum likelihood (ML) methods for estimating evoked dipole responses using electroencephalography (EEG) and magnetoencephalography (MEG) arrays, which allow for spatially correlated noise between sensors with unknown covariance. The electric source is modeled as a collection of current dipoles at fixed locations and the head as a spherical conductor. We permit the dipoles' moments to vary with time by modeling them as linear combinations of parametric or nonparametric basis functions. We estimate the dipoles' locations and moments and derive the Cramer-Rao bound for the unknown parameters. We also propose an ML based method for scanning the brain response data, which can be used to initialize the multidimensional search required to obtain the true dipole location estimates. Numerical simulations demonstrate the performance of the proposed methods},
  timestamp = {2016-10-20T13:16:11Z},
  number = {1},
  journal = {IEEE Transactions on Signal Processing},
  author = {Dogandzic, A. and Nehorai, A.},
  month = jan,
  year = {2000},
  keywords = {antenna arrays,Array signal processing,Brain modeling,brain response data,Conductors,Cramer-Rao bound,dipoles locations,EEG arrays,electric source,Electroencephalography,evoked dipole responses,Magnetic heads,Magnetic sensors,Magnetoencephalography,maximum likelihood estimation,maximum likelihood methods,medical signal processing,MEG arrays,ML based method,multidimensional search,Multidimensional systems,nonparametric basis functions,Numerical simulation,parametric basis functions,scanning,Sensor arrays,spatially correlated noise,spherical conductor,unknown parameters,unknown spatially correlated noise},
  pages = {13--25},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NJE63U33\\815475.html:text/html}
}

@incollection{Scholkopf2007,
  title = {A {{Probabilistic Algorithm Integrating Source Localization}} and {{Noise Suppression}} of {{MEG}} and {{EEG}} Data},
  isbn = {978-0-262-25691-9},
  abstract = {We have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked MEG/EEG data. Our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efficient than traditional multidipole fitting procedures. In simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or fixed orientation, at noise levels typical for averaged MEG data. Furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. Success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated.},
  timestamp = {2016-10-20T13:19:03Z},
  urldate = {2016-10-20},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19:{{Proceedings}} of the 2006 {{Conference}}},
  publisher = {{MIT Press}},
  author = {Sch{\"o}lkopf, Bernhard and Platt, John and Hofmann, Thomas},
  year = {2007},
  pages = {1625--1632},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3R525XA3\\articleDetails.html:text/html}
}

@inproceedings{Luan2015,
  title = {A Multimodal Framework for Integrating Biological Spectral Characteristics and {{MEG}}/{{EEG}} in Brain-Source Imaging},
  doi = {10.1109/ICNC.2015.7377983},
  abstract = {This paper presents a multimodal framework that considers the spatio-temporal-spectral characteristics of biophysics provided by electrocorticographic (ECoG) and magnetoencephalography (MEG)/electroencephalography (EEG). In this framework, three strategies are proposed to integrate biological characteristics into MEG/EEG source imaging, the coefficient constraint (CoCo) gives a conservative estimate; the exponent constraint (ExCo) yields a polarized resolution; the hybrid constraint (HyCo) dynamically balances the reliance on CoCo and ExCo, based on the quality of the MEG/EEG data, and takes full advantages of both, providing the framework with heuristic ability. Our contribution is a framework with the potential to use biological characteristics information when doing source imaging.},
  timestamp = {2016-10-21T14:03:34Z},
  booktitle = {11th {{International Conference}} on {{Natural Computation}} ({{ICNC}})},
  author = {Luan, F. and Zhang, Zhiming and Zhang, Tianqi},
  month = aug,
  year = {2015},
  keywords = {bioelectric potentials,biological spectral characteristic integration,Biomedical imaging,Brain modeling,brain-source imaging,brain source imaging,ECoG,EEG,EEG source imaging,electrocorticography,Electroencephalography,electromyography,Estimation,Imaging,Magnetoencephalography,MEG,MEG source imaging,multimodal framework,neurophysiology,Spatial resolution,Spatiotemporal phenomena,spatiotemporal-spectral characteristics,Standards},
  pages = {159--164},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Q336KAJJ\\7377983.html:text/html}
}

@inproceedings{Grosse-Wentrup2013,
  title = {How to {{Test}} the {{Quality}} of {{Reconstructed Sources}} in {{Independent Component Analysis}} ({{ICA}}) of {{EEG}}/{{MEG Data}}},
  doi = {10.1109/PRNI.2013.35},
  abstract = {We provide a simple method, based on volume conduction models, to quantify the neurophysiological plausibility of independent components (ICs) reconstructed from EEG/MEG data. We evaluate the method on EEG data recorded from 19 subjects and compare the results with two established procedures for judging the quality of ICs. We argue that our procedure provides a sound empirical basis for the inclusion or exclusion of ICs in the analysis of experimental data.},
  timestamp = {2016-10-20T13:25:30Z},
  booktitle = {2013 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}} ({{PRNI}})},
  author = {Grosse-Wentrup, M. and Harmeling, S. and Zander, T. and Hill, J. and Sch{\"o}lkopf, B.},
  month = jun,
  year = {2013},
  keywords = {brain models,Data models,EEG,EEG data,Electroencephalography,ICA,independent component analysis,Integrated circuit modeling,Magnetoencephalography,medical signal processing,MEG,MEG data,neurophysiological plausibility,neurophysiology,reconstructed sources,signal reconstruction,Surfaces,volume conduction models},
  pages = {102--105},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AKSSNQUJ\\6603567.html:text/html}
}

@article{He2011,
  title = {Electrophysiological {{Imaging}} of {{Brain Activity}} and {{Connectivity}} \#x2014;{{Challenges}} and {{Opportunities}}},
  volume = {58},
  issn = {0018-9294},
  doi = {10.1109/TBME.2011.2139210},
  abstract = {Unlocking the dynamic inner workings of the brain continues to remain a grand challenge of the 21st century. To this end, functional neuroimaging modalities represent an outstanding approach to better understand the mechanisms of both normal and abnormal brain functions. The ability to image brain function with ever increasing spatial and temporal resolution has made a significant leap over the past several decades. Further delineation of functional networks could lead to improved understanding of brain function in both normal and diseased states. This paper reviews recent advancements and current challenges in dynamic functional neuroimaging techniques, including electrophysiological source imaging, multimodal neuroimaging integrating fMRI with EEG/MEG, and functional connectivity imaging.},
  timestamp = {2016-10-20T13:27:56Z},
  number = {7},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {He, B. and Yang, L. and Wilke, C. and Yuan, H.},
  month = jul,
  year = {2011},
  keywords = {abnormal brain functions,bioelectric phenomena,biomedical MRI,Brain,brain activity,Brain Mapping,brain models,diseased states,diseases,dynamic functional neuroimaging techniques,EEG,Electroencephalography,electroencephalography (EEG),electrophysiological imaging,electrophysiological source imaging,functional connectivity,functional connectivity imaging,functional magnetic resonance imaging (fMRI),Head,Humans,Imaging,magnetic resonance imaging,Magnetoencephalography,magnetoencephalography (MEG),MEG,multimodal neuroimaging integrating fMRI,Nerve Net,neurophysiology,normal brain functions,source imaging,Spatial resolution},
  pages = {1918--1931},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DC744T52\\5742982.html:text/html}
}

@inproceedings{Antelis2010,
  title = {{{DYNAMO}}: {{Dynamic}} Multi-Model Source Localization Method for {{EEG}} and/or {{MEG}}},
  shorttitle = {{{DYNAMO}}},
  doi = {10.1109/IEMBS.2010.5626181},
  abstract = {This paper proposes a multiple model method that addresses the estimation of the EEG/MEG neural sources as a multihypothesis, multidimensional and dynamic estimation problem. The key aspect is the probabilistic integration of several neural models to simultaneously estimate and integrate the brain activity of different dynamic neural processes that are characterized by the number of sources, the dynamic of those sources and the initial conditions. The method was validated with EEG data gathered in a protocol to elicit error-related potentials, since there is evidence of the brain region that generate those signals. The results reveal that the proposed multiple model method is able to identify the brain structure associated with error processing, which is a preliminary indicator of the validity of the proposed method.},
  timestamp = {2016-10-21T13:45:20Z},
  booktitle = {Annual {{International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology}}},
  author = {Antelis, J. M. and Minguez, J.},
  month = aug,
  year = {2010},
  keywords = {algorithms,bioelectric potentials,biomedical measurement,brain activity,Brain modeling,Brain Waves,dynamic estimation problem,dynamic multimodel source localization method,DYNAMO,EEG neural source estimation,Electroencephalography,error processing associated brain structure,error related potentials,Estimation,Humans,Magnetoencephalography,Male,medical signal processing,MEG neural source estimation,Models; Neurological,multidimensional estimation problem,multihypothesis estimation problem,multiple model method,probabilistic integration,probability,Protocols,Scalp,visualization},
  pages = {5141--5144},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\65NGXI8S\\5626181.html:text/html}
}

@incollection{Cerutti2011,
  title = {Multimodal {{Integration}} of {{EEG}}, {{MEG}}, and {{Functional MRI}} in the {{Study Of Human Brain Activity}}},
  isbn = {978-1-118-00774-7},
  abstract = {This chapter contains sections titled: Introduction Cortical Activity Estimation from Noninvasive EEG and MEG Measurements Integration of EEG/MEG and fMRI data Appendix I. Electrical Forward Solution for a Realistic Head Model Appendix II. Magnetic Forward Solution},
  timestamp = {2016-10-20T13:33:58Z},
  urldate = {2016-10-20},
  booktitle = {Advanced {{Methods}} of {{Biomedical Signal Processing}}},
  publisher = {{Wiley-IEEE Press}},
  author = {Cerutti, Sergio and Marchesi, Carlo},
  year = {2011},
  pages = {153--167},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3USGI2FP\\articleDetails.html:text/html}
}

@article{Liu2006b,
  title = {Integration of {{EEG}}/{{MEG}} with {{MRI}} and {{fMRI}}},
  volume = {25},
  issn = {0739-5175},
  doi = {10.1109/MEMB.2006.1657787},
  abstract = {This article discusses different approaches that have been proposed for multimodal neuroimaging, with special emphasis on the integration of electroencephalography (EEG), magnetoencephalography (MEG), and magnetic resonance imaging (MRI), and functional MRI (fMRI). Some applications will be shown to illustrate the efficacy and importance of these techniques in clinical and neuroscience studies. Finally, some remaining challenges and problems in the multimodal integration will be discussed},
  timestamp = {2016-10-20T13:35:38Z},
  number = {4},
  journal = {IEEE Engineering in Medicine and Biology Magazine},
  author = {Liu, Zhongming and Ding, Lei and He, Bin},
  month = jul,
  year = {2006},
  keywords = {biomedical MRI,Brain modeling,clinical studies,EEG,Electroencephalography,fMRI,functional MRI,Humans,Magnetic heads,magnetic resonance imaging,Magnetic sensors,Magnetoencephalography,MEG,MRI,multimodal neuroimaging,Neuroimaging,neurophysiology,neuroscience,Scalp,Solid modeling,Tensile stress},
  pages = {46--53},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MSPAWZ8V\\1657787.html:text/html}
}

@inproceedings{Baillet2004,
  title = {Electromagnetic Brain Imaging Using {{BrainStorm}}},
  doi = {10.1109/ISBI.2004.1398622},
  abstract = {Electromagnetic brain imaging consists of the mapping of neural generators of magnetic fields and electric potentials measured outside the head using magnetoencephalography (MEG) and electroencephalography (EEC), respectively. Here we report on a collaborative project dedicated to the development and distribution of BrainStorm, a software suite for MEG-EEG data modeling and visualization, with integration of MRI information. BrainStorm is developed in Matlab, which ensures OS portability and facilitated interoperability with many other research software resources. Distribution of BrainStorm is managed under GNU licensing, with no charge to users.},
  timestamp = {2016-10-20T13:37:10Z},
  booktitle = {{{IEEE International Symposium}} on {{Biomedical Imaging}}: {{Nano}} to {{Macro}}, 2004},
  author = {Baillet, S. and Masher, J. C. and Leahy, R. M.},
  month = apr,
  year = {2004},
  keywords = {bioelectric potentials,biomedical MRI,Brain,BrainStorm,Collaborative software,data modeling,Data models,data visualisation,data visualization,Electric potential,electric potentials,Electric variables measurement,Electroencephalography,electromagnetic brain imaging,Electromagnetic fields,Electromagnetic measurements,interoperability,Magnetic field measurement,Magnetic fields,Magnetic heads,Magnetoencephalography,mathematics computing,Matlab,medical computing,neural generator mapping,open systems,OS portability,software portability},
  pages = {652--655 Vol. 1},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F9CQZ2JF\\1398622.html:text/html}
}

@article{David2002,
  title = {Estimation of Neural Dynamics from {{MEG}}/{{EEG}} Cortical Current Density Maps: Application to the Reconstruction of Large-Scale Cortical Synchrony},
  volume = {49},
  issn = {0018-9294},
  shorttitle = {Estimation of Neural Dynamics from {{MEG}}/{{EEG}} Cortical Current Density Maps},
  doi = {10.1109/TBME.2002.802013},
  abstract = {There is a growing interest in elucidating the role of specific patterns of neural dynamics-such as transient synchronization between distant cell assemblies-in brain functions. Magnetoencephalography (MEG)/electroencephalography (EEG) recordings consist in the spatial integration of the activity from large and multiple remotely located populations of neurons. Massive diffusive effects and poor signal-to-noise ratio (SNR) preclude the proper estimation of indices related to cortical dynamics from nonaveraged MEG/EEG surface recordings. Source localization from MEG/EEG surface recordings with its excellent time resolution could contribute to a better understanding of the working brain. We propose a robust and original approach to the MEG/EEG distributed inverse problem to better estimate neural dynamics of cortical sources. For this, the surrogate data method is introduced in the MEG/EEG inverse problem framework. We apply this approach on nonaveraged data with poor SNR using the minimum norm estimator and find source localization results weakly sensitive to noise. Surrogates allow the reduction of the source space in order to reconstruct MEG/EEG data with reduced biases in both source localization and time-series dynamics. Monte Carlo simulations and results obtained from real MEG data indicate it is possible to estimate noninvasively an important part of cortical source locations and dynamic and, therefore, to reveal brain functional networks.},
  timestamp = {2016-10-20T13:38:55Z},
  number = {9},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {David, O. and Garnero, L. and Cosmelli, D. and Varela, F. J.},
  month = sep,
  year = {2002},
  keywords = {Action Potentials,algorithms,Assembly,brain functional networks,Brain Mapping,cellular biophysics,Cerebral cortex,Computer Simulation,cortical dynamics,cortical source locations,current density,Electroencephalography,Electromagnetic fields,Electrophysiology,Humans,inverse problems,large-scale cortical synchrony reconstruction,Large-scale systems,Magnetoencephalography,massive diffusive effects,medical signal processing,MEG/EEG cortical current density maps,Models; Neurological,Models; Statistical,Monte Carlo Method,Monte Carlo methods,Monte Carlo simulations,Neurons,neurophysiology,poor signal-to-noise rati,remotely located neuron populations,Reproducibility of Results,Robustness,Sensitivity and Specificity,signal reconstruction,Signal resolution,Signal to noise ratio,stochastic processes,time series,time-series dynamics},
  pages = {975--987},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\Z5JBAQRU\\1028421.html:text/html}
}

@inproceedings{Maurer2012,
  title = {{{EEG}}/{{MEG}} Artifact Suppression for Improved Neural Activity Estimation},
  doi = {10.1109/ACSSC.2012.6489311},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) measurements can be used to monitor neural activity, that is generally characterized using current or magnetic dipole source models with time-varying amplitude, position, and moment parameters. The EEG/MEG measurements, however, often contain artifacts that do not originate from the brain. These artifacts can include patient movement, normal heart electrical activity, muscle and eye movement, or equipment and environmental clutter. In this paper, we propose a novel neural activity estimation approach that integrates particle filtering with the probabilistic data association filter in order to validate neural measurements and suppress artifacts before estimating neural activity. Simulations using synthetic data with this approach demonstrate high performance in suppressing artifacts and tracking neural activity; results for real data are also presented.},
  timestamp = {2016-10-20T13:41:00Z},
  booktitle = {2012 {{Conference Record}} of the {{Forty Sixth Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}} ({{ASILOMAR}})},
  author = {Maurer, A. and Miao, L. and Zhang, J. J. and Kovvali, N. and Papandreou-Suppappola, A. and Chakrabarti, C.},
  month = nov,
  year = {2012},
  keywords = {bioelectric potentials,biomechanics,Brain,cardiology,current dipole source models,EEG artifact suppression,Electroencephalography,electroencephalography measurements,environmental clutter,equipment clutter,eye,eye movement,magnetic dipole source models,Magnetoencephalography,magnetoencephalography measurements,medical signal detection,medical signal processing,MEG artifact suppression,moment parameters,muscle,muscle movement,neural activity estimation,neural activity monitoring,neural activity tracking,neural measurements,neural nets,neurophysiology,normal heart electrical activity,particle filtering integration,particle filtering (numerical methods),patient monitoring,patient movement,position parameters,probabilistic data association filter,probability,synthetic data,time-varying amplitude parameters},
  pages = {1646--1650},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S9NJE9H7\\6489311.html:text/html}
}

@inproceedings{Laxminarayan2006,
  title = {Controlling {{Dimensionality}} in a {{Systems Approach}} to {{Dynamic Multimodal Functional Brain Imaging}}},
  doi = {10.1109/ACSSC.2006.356607},
  abstract = {The complementary spatial, temporal and specificity advantages of fMRI, EEG, MEG, PET and DOT for functional brain imaging motivate interest in multimodal functional brain imaging. State-variable dynamical systems modeling of neural activity and its relation to local hemodynamics, further coupled with autonomic physiology offers enhanced spatiotemporal resolution and insight into physiological signals and mechanisms. However, such a model also implies an explosion of state dimension. We discuss strategies for controlling this high dimensionality based on subspace approaches applied to the observed data and the model structure.},
  timestamp = {2016-10-21T13:36:08Z},
  booktitle = {Fortieth {{Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Laxminarayan, S. and Diamond, S. G. and Miller, E. and Tadmor, G. and Boas, D. and Brooks, D. H.},
  month = oct,
  year = {2006},
  keywords = {autonomic physiology,Brain,Control systems,dynamic multimodal functional brain imaging,Electroencephalography,haemodynamics,Hemodynamics,High-resolution imaging,Image Enhancement,Image resolution,medical control systems,medical image processing,Modeling,neural activity,neurophysiology,physiological models,physiological signal,Physiology,Positron emission tomography,Spatiotemporal phenomena,spatiotemporal resolution enhancement,state-variable dynamical system modeling,system dimensionality control,US Department of Transportation},
  pages = {166--170},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3PRKITKJ\\4176536.html:text/html}
}

@inproceedings{Anwar2014,
  title = {Multi-Modal Causality Analysis of Eyes-Open and Eyes-Closed Data from Simultaneously Recorded {{EEG}} and {{MEG}}},
  doi = {10.1109/EMBC.2014.6944211},
  abstract = {Owing to the recent advances in multi-modal data analysis, the aim of the present study was to analyze the functional network of the brain which remained the same during the eyes-open (EO) and eyes-closed (EC) resting task. The simultaneously recorded electroencephalogram (EEG) and magnetoencephalogram (MEG) were used for this study, recorded from five distinct cortical regions of the brain. We focused on the `alpha' functional network, corresponding to the individual peak frequency in the alpha band. The total data set of 120 seconds was divided into three segments of 18 seconds each, taken from start, middle, and end of the recording. This segmentation allowed us to analyze the evolution of the underlying functional network. The method of time-resolved partial directed coherence (tPDC) was used to assess the causality. This method allowed us to focus on the individual peak frequency in the `alpha' band (7-13 Hz). Because of the significantly higher power in the recorded EEG in comparison to MEG, at the individual peak frequency of the alpha band, results rely only on EEG. The MEG was used only for comparison. Our results show that different regions of the brain start to `disconnect' from one another over the course of time. The driving signals, along with the feedback signals between different cortical regions start to recede over time. This shows that, with the course of rest, brain regions reduce communication with each another.},
  timestamp = {2016-10-20T13:50:46Z},
  booktitle = {2014 36th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Anwar, A. R. and Mideska, K. G. and Hellriegel, H. and Hoogenboom, N. and Krause, H. and Schnitzler, A. and Deuschl, G. and Raethjen, J. and Heute, U. and Muthuraman, M.},
  month = aug,
  year = {2014},
  keywords = {alpha functional network,Bidirectional control,brain functional network,brain models,cortical regions,EEG,electroencephalogram,Electroencephalography,eye,eyes-closed data,eyes-open data,feedback signals,magnetoencephalogram,Magnetoencephalography,Mathematical model,medical signal processing,MEG,multimodal causality analysis,signal segmentation,time-frequency analysis,time-resolved partial directed coherence,Time series analysis},
  pages = {2825--2828},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KAWFZQCF\\6944211.html:text/html}
}

@article{Antelis2013,
  title = {{{DYNAMO}}: {{Concurrent}} Dynamic Multi-Model Source Localization Method for {{EEG}} and/or {{MEG}}},
  volume = {212},
  issn = {0165-0270},
  shorttitle = {{{DYNAMO}}},
  doi = {10.1016/j.jneumeth.2012.09.017},
  abstract = {This work presents a new dipolar method to estimate the neural sources from separate or combined EEG and MEG data. The novelty lies in the simultaneous estimation and integration of neural sources from different dynamic models with different parameters, leading to a dynamic multi-model solution for the EEG/MEG source localization problem. The first key aspect of this method is defining the source model as a dipolar dynamic system, which allows for the estimation of the probability distribution of the sources within the Bayesian filter estimation framework. A second important aspect is the consideration of several banks of filters that simultaneously estimate and integrate the neural sources of different models. A third relevant aspect is that the final probability estimate is a result of the probabilistic integration of the neural sources of numerous models. Such characteristics lead to a new approach that does not require a prior definition neither of the number of sources or of the underlying temporal dynamics, allowing for the specification of multiple initial prior estimates. The method was validated by three sensor modalities with simulated data designed to impose difficult estimation situations, and with real EEG data recorded in a feedback error-related potential paradigm. On the basis of these evaluations, the method was able to localize the sources with high accuracy.},
  timestamp = {2016-10-21T13:44:55Z},
  number = {1},
  urldate = {2016-10-20},
  journal = {Journal of Neuroscience Methods},
  author = {Antelis, Javier M. and Minguez, Javier},
  month = jan,
  year = {2013},
  keywords = {EEG,IMM,MEG,Multi-model,Source localization},
  pages = {28--42},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZHUGWC2B\\S0165027012003895.html:text/html}
}

@article{Chowdhury,
  title = {Complex Patterns of Spatially Extended Generators of Epileptic Activity: {{Comparison}} of Source Localization Methods {{cMEM}} and 4-{{ExSo}}-{{MUSIC}} on {{High Resolution EEG}} and {{MEG}} Data},
  issn = {1053-8119},
  shorttitle = {Complex Patterns of Spatially Extended Generators of Epileptic Activity},
  doi = {10.1016/j.neuroimage.2016.08.044},
  abstract = {Electric Source Imaging (ESI) and Magnetic Source Imaging (MSI) of EEG and MEG signals are widely used to determine the origin of interictal epileptic discharges during the pre-surgical evaluation of patients with epilepsy. Epileptic discharges are detectable on EEG/ MEG scalp recordings only when associated with a spatially extended cortical generator of several square centimeters, therefore it is essential to assess the ability of source localization methods to recover such spatial extent.

In this study we evaluated two source localization methods that have been developed for localizing spatially extended sources using EEG/MEG data: coherent Maximum Entropy on the Mean (cMEM) and 4th order Extended Source Multiple Signal Classification (4-ExSo-MUSIC). In order to propose a fair comparison of the performances of the two methods in MEG versus EEG, this study considered realistic simulations of simultaneous EEG/MEG acquisitions taking into account an equivalent number of channels in EEG (257 electrodes) and MEG (275 sensors), involving a biophysical computational neural mass model of neuronal discharges and realistically shaped head models. cMEM and 4-ExSo-MUSIC were evaluated for their sensitivity to localize complex patterns of epileptic discharges which includes (a) different locations and spatial extents of multiple synchronous sources, and (b) propagation patterns exhibited by epileptic discharges. Performance of the source localization methods was assessed using a detection accuracy index (Area Under receiver operating characteristic Curve, AUC) and a Spatial Dispersion (SD) metric. Finally, we also presented two examples illustrating the performance of cMEM and 4-ExSo-MUSIC on clinical data recorded using high resolution EEG and MEG.

When simulating single sources at different locations, both 4-ExSo-MUSIC and cMEM exhibited excellent performance (median AUC significantly larger than 0.8 for EEG and MEG), whereas, only for EEG, 4-ExSo-MUSIC showed significantly larger AUC values than cMEM. On the other hand, cMEM showed significantly lower SD values than 4-ExSo-MUSIC for both EEG and MEG. When assessing the impact of the source spatial extent, both methods provided consistent and reliable detection accuracy for a wide range of source spatial extents (source sizes ranging from 3 - 20 cm2 for MEG and 3 - 30 cm2 for EEG). For both EEG and MEG, 4-ExSo-MUSIC localized single source of large signal-to-noise ratio better than cMEM. In the presence of two synchronous sources, cMEM was able to distinguish well the two sources (their location and spatial extent), while 4-ExSo-MUSIC only retrieved one of them. cMEM was able to detect the spatio-temporal propagation patterns of two synchronous activities while 4-ExSo-MUSIC favored the strongest source activity.

Overall, in the context of localizing sources of epileptic discharges from EEG and MEG data, 4-ExSo-MUSIC and cMEM were found accurately sensitive to the location and spatial extent of the sources, with some complementarities. Therefore, they are both eligible for application on clinical data.},
  timestamp = {2016-10-20T14:01:43Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Chowdhury, R. A. and Merlet, I. and Birot, G. and Kobayashi, E. and Nica, A. and Biraben, A. and Wendling, F. and Lina, J. M. and Albera, L. and Grova, C.},
  keywords = {4-ExSo-MUSIC,EEG/MEG source localization,Higher order statistics,Interictal epileptic discharges,Maximum entropy on the mean,Neural mass model},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VT4GCJ7C\\S1053811916304293.html:text/html}
}

@article{Cottereau2015,
  series = {Cutting-edge EEG Methods},
  title = {How to Use {{fMRI}} Functional Localizers to Improve {{EEG}}/{{MEG}} Source Estimation},
  volume = {250},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2014.07.015},
  abstract = {EEG and MEG have excellent temporal resolution, but the estimation of the neural sources that generate the signals recorded by the sensors is a difficult, ill-posed problem. The high spatial resolution of functional MRI makes it an ideal tool to improve the localization of the EEG/MEG sources using data fusion. However, the combination of the two techniques remains challenging, as the neural generators of the EEG/MEG and BOLD signals might in some cases be very different. Here we describe a data fusion approach that was developed by our team over the last decade in which fMRI is used to provide source constraints that are based on functional areas defined individually for each subject. This mini-review describes the different steps that are necessary to perform source estimation using this approach. It also provides a list of pitfalls that should be avoided when doing fMRI-informed EEG/MEG source imaging. Finally, it describes the advantages of using a ROI-based approach for group-level analysis and for the study of sensory systems.},
  timestamp = {2016-10-20T14:01:43Z},
  urldate = {2016-10-20},
  journal = {Journal of Neuroscience Methods},
  author = {Cottereau, Benoit R. and Ales, Justin M. and Norcia, Anthony M.},
  month = jul,
  year = {2015},
  keywords = {data fusion,EEG,fMRI,MEG,source imaging},
  pages = {64--73},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\76Z2695N\\S0165027014002660.html:text/html}
}

@article{David2006,
  title = {Dynamic Causal Modeling of Evoked Responses in {{EEG}} and {{MEG}}},
  volume = {30},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2005.10.045},
  abstract = {Neuronally plausible, generative or forward models are essential for understanding how event-related fields (ERFs) and potentials (ERPs) are generated. In this paper, we present a new approach to modeling event-related responses measured with EEG or MEG. This approach uses a biologically informed model to make inferences about the underlying neuronal networks generating responses. The approach can be regarded as a neurobiologically constrained source reconstruction scheme, in which the parameters of the reconstruction have an explicit neuronal interpretation. Specifically, these parameters encode, among other things, the coupling among sources and how that coupling depends upon stimulus attributes or experimental context. The basic idea is to supplement conventional electromagnetic forward models, of how sources are expressed in measurement space, with a model of how source activity is generated by neuronal dynamics. A single inversion of this extended forward model enables inference about both the spatial deployment of sources and the underlying neuronal architecture generating them. Critically, this inference covers long-range connections among well-defined neuronal subpopulations.

In a previous paper, we simulated ERPs using a hierarchical neural-mass model that embodied bottom-up, top-down and lateral connections among remote regions. In this paper, we describe a Bayesian procedure to estimate the parameters of this model using empirical data. We demonstrate this procedure by characterizing the role of changes in cortico-cortical coupling, in the genesis of ERPs. In the first experiment, ERPs recorded during the perception of faces and houses were modeled as distinct cortical sources in the ventral visual pathway. Category-selectivity, as indexed by the face-selective N170, could be explained by category-specific differences in forward connections from sensory to higher areas in the ventral stream. We were able to quantify and make inferences about these effects using conditional estimates of connectivity. This allowed us to identify where, in the processing stream, category-selectivity emerged.

In the second experiment, we used an auditory oddball paradigm to show that the mismatch negativity can be explained by changes in connectivity. Specifically, using Bayesian model selection, we assessed changes in backward connections, above and beyond changes in forward connections. In accord with theoretical predictions, there was strong evidence for learning-related changes in both forward and backward coupling. These examples show that category- or context-specific coupling among cortical regions can be assessed explicitly, within a mechanistic, biologically motivated inference framework.},
  timestamp = {2016-10-20T14:08:05Z},
  number = {4},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {David, Olivier and Kiebel, Stefan J. and Harrison, Lee M. and Mattout, J{\'e}r{\'e}mie and Kilner, James M. and Friston, Karl J.},
  month = may,
  year = {2006},
  keywords = {Bayesian inference,Causal modeling,Electroencephalography,Magnetoencephalography,Neural networks,Nonlinear dynamics},
  pages = {1255--1272},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S2CI9SRW\\S1053811905008013.html:text/html}
}

@article{Mosher1993,
  title = {Error Bounds for {{EEG}} and {{MEG}} Dipole Source Localization},
  volume = {86},
  issn = {0013-4694},
  doi = {10.1016/0013-4694(93)90043-U},
  abstract = {General formulas are presented for computing a lower bound on localization and moment error for electroencephalographic (EEG) or magnetoencephalographic (MEG) current source dipole models with arbitrary sensor array geometry. Specific EEG and MEG formulas are presented for multiple dipoles in a head model with 4 spherical shells. Localization error bounds are presented for both EEG and MEG for several different sensor configurations. Graphical error contours are presented for 127 sensors covering the upper hemisphere, for both 37 sensors and 127 sensors covering a smaller region, and for the standard 10\textendash{}20 EEG sensor arrangement. Both 1- and 2-dipole cases were examined for all possible dipole orientations and locations within a head quadrant. The results show a strong dependence on absolute dipole location and orientation. The results also show that fusion of the EEG and MEG measurements into a combined model reduces the lower bound. A Monte Carlo simulation was performed to check the tightness of the bounds for a selected case. The simple head model, the low power noise and the few strong dipoles were all selected in this study as optimistic conditions to establish possibly fundamental resolution limits for any localization effort. Results, under these favorable assumptions, show comparable resolutions between the EEG and the MEG models, but accuracy for a single dipole, in either case, appears limited to several millimeters for a single time slice. The lower bounds increase markedly with just 2 dipoles. Observations are given to support the need for full spatiotemporal modeling to improve these lower bounds. All of the simulation results presented can easily be scaled to other instances of noise power and dipole intensity.},
  timestamp = {2016-10-20T14:13:05Z},
  number = {5},
  urldate = {2016-10-20},
  journal = {Electroencephalography and Clinical Neurophysiology},
  author = {Mosher, John C. and Spencer, Michael E. and Leahy, Richard M. and Lewis, Paul S.},
  month = may,
  year = {1993},
  keywords = {Dipole moment error,Dipole source localization error,EEG,MEG,Spatiotemporal source modeling},
  pages = {303--321},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8BUXMDW2\\001346949390043U.html:text/html}
}

@article{Kuuluvainen2014,
  title = {The Neural Basis of Sublexical Speech and Corresponding Nonspeech Processing: {{A}} Combined {{EEG}}\textendash{}{{MEG}} Study},
  volume = {130},
  issn = {0093-934X},
  shorttitle = {The Neural Basis of Sublexical Speech and Corresponding Nonspeech Processing},
  doi = {10.1016/j.bandl.2014.01.008},
  abstract = {We addressed the neural organization of speech versus nonspeech sound processing by investigating preattentive cortical auditory processing of changes in five features of a consonant\textendash{}vowel syllable (consonant, vowel, sound duration, frequency, and intensity) and their acoustically matched nonspeech counterparts in a simultaneous EEG\textendash{}MEG recording of mismatch negativity (MMN/MMNm). Overall, speech\textendash{}sound processing was enhanced compared to nonspeech sound processing. This effect was strongest for changes which affect word meaning (consonant, vowel, and vowel duration) in the left and for the vowel identity change in the right hemisphere also. Furthermore, in the right hemisphere, speech\textendash{}sound frequency and intensity changes were processed faster than their nonspeech counterparts, and there was a trend for speech-enhancement in frequency processing. In summary, the results support the proposed existence of long-term memory traces for speech sounds in the auditory cortices, and indicate at least partly distinct neural substrates for speech and nonspeech sound processing.},
  timestamp = {2016-10-20T14:16:26Z},
  urldate = {2016-10-20},
  journal = {Brain and Language},
  author = {Kuuluvainen, Soila and Nevalainen, P{\"a}ivi and Sorokin, Alexander and Mittag, Maria and Partanen, Eino and Putkinen, Vesa and Sepp{\"a}nen, Miia and K{\"a}hk{\"o}nen, Seppo and Kujala, Teija},
  month = mar,
  year = {2014},
  keywords = {Auditory Cortex,EEG,MEG,MMN,MMNm,Nonspeech,Speech},
  pages = {19--32},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P56Z9X9H\\S0093934X14000133.html:text/html}
}

@article{Chang2015,
  title = {Combined {{MEG}} and {{EEG}} Show Reliable Patterns of Electromagnetic Brain Activity during Natural Viewing},
  volume = {114},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2015.03.066},
  abstract = {Naturalistic stimuli such as movies are increasingly used to engage cognitive and emotional processes during fMRI of brain hemodynamic activity. However, movies have been little utilized during magnetoencephalography (MEG) and EEG that directly measure population-level neuronal activity at a millisecond resolution. Here, subjects watched a 17-min segment from the movie Crash (Lionsgate Films, 2004) twice during simultaneous MEG/EEG recordings. Physiological noise components, including ocular and cardiac artifacts, were removed using the DRIFTER algorithm. Dynamic estimates of cortical activity were calculated using MRI-informed minimum-norm estimation. To improve the signal-to-noise ratio (SNR), principal component analyses (PCA) were employed to extract the prevailing temporal characteristics within each anatomical parcel of the Freesurfer Desikan\textendash{}Killiany cortical atlas. A variety of alternative inter-subject correlation (ISC) approaches were then utilized to investigate the reliability of inter-subject synchronization during natural viewing. In the first analysis, the ISCs of the time series of each anatomical region over the full time period across all subject pairs were calculated and averaged. In the second analysis, dynamic ISC (dISC) analysis, the correlation was calculated over a sliding window of 200 ms with 3.3 ms steps. Finally, in a between-run ISC analysis, the between-run correlation was calculated over the dynamic ISCs of the two different runs after the Fisher z-transformation. Overall, the most reliable activations occurred in occipital/inferior temporal visual and superior temporal auditory cortices as well as in the posterior cingulate, precuneus, pre- and post-central gyri, and right inferior and middle frontal gyri. Significant between-run ISCs were observed in superior temporal auditory cortices and inferior temporal visual cortices. Taken together, our results show that movies can be utilized as naturalistic stimuli in MEG/EEG similarly as in fMRI studies.},
  timestamp = {2016-10-20T14:22:31Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Chang, Wei-Tang and J{\"a}{\"a}skel{\"a}inen, Iiro P. and Belliveau, John W. and Huang, Samantha and Hung, An-Yi and Rossi, Stephanie and Ahveninen, Jyrki},
  month = jul,
  year = {2015},
  pages = {49--56},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KTV3QMSI\\S1053811915002621.html:text/html}
}

@article{Sohrabpour,
  title = {Imaging Brain Source Extent from {{EEG}}/{{MEG}} by Means of an Iteratively Reweighted Edge Sparsity Minimization ({{IRES}}) Strategy},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2016.05.064},
  abstract = {Estimating extended brain sources using EEG/MEG source imaging techniques is challenging. EEG and MEG have excellent temporal resolution at millisecond scale but their spatial resolution is limited due to the volume conduction effect. We have exploited sparse signal processing techniques in this study to impose sparsity on the underlying source and its transformation in other domains (mathematical domains, like spatial gradient). Using an iterative reweighting strategy to penalize locations that are less likely to contain any source, it is shown that the proposed iteratively reweighted edge sparsity minimization (IRES) strategy can provide reasonable information regarding the location and extent of the underlying sources. This approach is unique in the sense that it estimates extended sources without the need of subjectively thresholding the solution. The performance of IRES was evaluated in a series of computer simulations. Different parameters such as source location and signal-to-noise ratio were varied and the estimated results were compared to the targets using metrics such as localization error (LE), area under curve (AUC) and overlap between the estimated and simulated sources. It is shown that IRES provides extended solutions which not only localize the source but also provide estimation for the source extent. The performance of IRES was further tested in epileptic patients undergoing intracranial EEG (iEEG) recording for pre-surgical evaluation. IRES was applied to scalp EEGs during interictal spikes, and results were compared with iEEG and surgical resection outcome in the patients. The pilot clinical study results are promising and demonstrate a good concordance between noninvasive IRES source estimation with iEEG and surgical resection outcomes in the same patients. The proposed algorithm, i.e. IRES, estimates extended source solutions from scalp electromagnetic signals which provide relatively accurate information about the location and extent of the underlying source.},
  timestamp = {2016-10-20T14:21:45Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Sohrabpour, Abbas and Lu, Yunfeng and Worrell, Gregory and He, Bin},
  keywords = {convex optimization,EEG,Extent,Inverse problem,Iterative reweighting,MEG,Source extent,sparsity},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TDVTMH39\\S1053811916301847.html:text/html}
}

@article{Siems2016,
  title = {Measuring the Cortical Correlation Structure of Spontaneous Oscillatory Activity with {{EEG}} and {{MEG}}},
  volume = {129},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2016.01.055},
  abstract = {Power correlations of orthogonalized signals have recently been introduced for MEG as a powerful tool to non-invasively investigate functional connectivity in the human brain. Little is known about the applicability of this approach to EEG, and how compatible the results are between EEG and MEG. To address this, we systematically compared power correlations of simultaneously recorded and source co-registered 64-channel EEG and 275-channel MEG in resting human subjects. For both modalities, connectivity peaked at around 16 Hz. For this frequency range, seed-based correlation maps showed comparable patterns across modalities, with generally more distinct patterns for MEG. A brain-wide pattern correlation analysis also revealed maximum similarity around 16 Hz. Correcting for different signal-to-noise ratio (SNR) across frequencies and modalities revealed pattern correlation between modalities close to one across a broad frequency range from 1 to 32 Hz and only slightly smaller for higher frequencies. The decrease above 32 Hz likely reflected higher susceptibility to muscle artifacts for EEG than for MEG. Our results show that power correlation of orthogonalized signals is feasible for studying functional connectivity with 64-channel EEG. Furthermore, besides differences in SNR, for frequencies from about 8 to 32 Hz, EEG and MEG measure the same correlation patterns across the entire brain.},
  timestamp = {2016-10-20T14:21:45Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Siems, Marcus and Pape, Anna-Antonia and Hipp, Joerg F. and Siegel, Markus},
  month = apr,
  year = {2016},
  keywords = {EEG,functional connectivity,MEG,Orthogonalization,Power correlation},
  pages = {345--355},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RS8P2KEV\\S1053811916000707.html:text/html}
}

@article{vanDiessen2015,
  title = {Opportunities and Methodological Challenges in {{EEG}} and {{MEG}} Resting State Functional Brain Network Research},
  volume = {126},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2014.11.018},
  abstract = {Electroencephalogram (EEG) and magnetoencephalogram (MEG) recordings during resting state are increasingly used to study functional connectivity and network topology. Moreover, the number of different analysis approaches is expanding along with the rising interest in this research area. The comparison between studies can therefore be challenging and discussion is needed to underscore methodological opportunities and pitfalls in functional connectivity and network studies. In this overview we discuss methodological considerations throughout the analysis pipeline of recording and analyzing resting state EEG and MEG data, with a focus on functional connectivity and network analysis. We summarize current common practices with their advantages and disadvantages; provide practical tips, and suggestions for future research. Finally, we discuss how methodological choices in resting state research can affect the construction of functional networks. When taking advantage of current best practices and avoid the most obvious pitfalls, functional connectivity and network studies can be improved and enable a more accurate interpretation and comparison between studies.},
  timestamp = {2016-10-20T14:26:56Z},
  number = {8},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {{van Diessen}, E. and Numan, T. and {van Dellen}, E. and {van der Kooi}, A. W. and Boersma, M. and Hofman, D. and {van Lutterveld}, R. and {van Dijk}, B. W. and {van Straaten}, E. C. W. and Hillebrand, A. and Stam, C. J.},
  month = aug,
  year = {2015},
  keywords = {EEG,functional connectivity,Functional networks,Graph analysis,MEG,Minimum spanning tree,Resting state},
  pages = {1468--1481},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\I69J68V8\\S1388245714008104.html:text/html}
}

@article{Stefan2009,
  title = {Network Characteristics of Idiopathic Generalized Epilepsies in Combined {{MEG}}/{{EEG}}},
  volume = {85},
  issn = {0920-1211},
  doi = {10.1016/j.eplepsyres.2009.03.015},
  abstract = {Summary
Seven patients with idiopathic generalized epilepsies were investigated using MEG and EEG. Spike\textendash{}wave series were seen in all patients, single spikes in six. For both, source analysis showed most often involvement of frontal, perinsular and subcortical/thalamic areas. In all patients, a unilateral frontal accentuation of activity could be observed. Patients with juvenile myoclonic and myoclonic absence epilepsy presented with localizations mainly in the central and premotor regions versus prefrontal accentuation in the other absence patients. MEG/EEG source localization provides important information concerning regional network involvement in idiopathic generalized epilepsies. As a consequence for a neurophysiological concept of ``generalized'' epileptic seizures and syndromes, a third subgroup with regional activity in bilateral homologous regions can be differentiated next to pure focal and multifocal generalized epilepsies.},
  timestamp = {2016-10-20T14:26:56Z},
  number = {2\textendash{}3},
  urldate = {2016-10-20},
  journal = {Epilepsy Research},
  author = {Stefan, Hermann and Paulini-Ruf, Andrea and Hopfeng{\"a}rtner, R{\"u}diger and Rampp, Stefan},
  month = aug,
  year = {2009},
  keywords = {Epileptic networks,Idiopathic generalized epilepsies,Juvenile myoclonic epilepsy,MEG/EEG source analysis,Myoclonic absences},
  pages = {187--198},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8J6PH5VC\\S0920121109000771.html:text/html}
}

@article{Anzellotti2013,
  title = {Temporal Recruitment of Cortical Network Involved in Reading Epilepsy with Paroxysmal Alexia: {{A}} Combined {{EEG}}/{{MEG}} Study},
  volume = {22},
  issn = {1059-1311},
  shorttitle = {Temporal Recruitment of Cortical Network Involved in Reading Epilepsy with Paroxysmal Alexia},
  doi = {10.1016/j.seizure.2012.11.008},
  timestamp = {2016-10-20T14:36:04Z},
  number = {2},
  urldate = {2016-10-20},
  journal = {Seizure},
  author = {Anzellotti, Francesca and Franciotti, Raffaella and Onofrj, Marco},
  month = mar,
  year = {2013},
  pages = {156--158},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FIABVGN2\\S105913111200297X.html:text/html}
}

@article{Morishige2014,
  title = {Estimation of Hyper-Parameters for a Hierarchical Model of Combined Cortical and Extra-Brain Current Sources in the {{MEG}} Inverse Problem},
  volume = {101},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.07.010},
  abstract = {One of the major obstacles in estimating cortical currents from MEG signals is the disturbance caused by magnetic artifacts derived from extra-cortical current sources such as heartbeats and eye movements. To remove the effect of such extra-brain sources, we improved the hybrid hierarchical variational Bayesian method (hyVBED) proposed by Fujiwara et al. (NeuroImage, 2009). hyVBED simultaneously estimates cortical and extra-brain source currents by placing dipoles on cortical surfaces as well as extra-brain sources. This method requires EOG data for an EOG forward model that describes the relationship between eye dipoles and electric potentials. In contrast, our improved approach requires no EOG and less a priori knowledge about the current variance of extra-brain sources. We propose a new method, ``extra-dipole,'' that optimally selects hyper-parameter values regarding current variances of the cortical surface and extra-brain source dipoles. With the selected parameter values, the cortical and extra-brain dipole currents were accurately estimated from the simulated MEG data. The performance of this method was demonstrated to be better than conventional approaches, such as principal component analysis and independent component analysis, which use only statistical properties of MEG signals. Furthermore, we applied our proposed method to measured MEG data during covert pursuit of a smoothly moving target and confirmed its effectiveness.},
  timestamp = {2016-10-20T14:36:04Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Morishige, Ken-ichi and Yoshioka, Taku and Kawawaki, Dai and Hiroe, Nobuo and Sato, Masa-aki and Kawato, Mitsuo},
  month = nov,
  year = {2014},
  keywords = {Covert pursuit,denoising,Hierarchical Bayesian method,MEG},
  pages = {320--336},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9SMXH5NJ\\S1053811914005825.html:text/html}
}

@article{Vorwerk2014,
  title = {A Guideline for Head Volume Conductor Modeling in {{EEG}} and {{MEG}}},
  volume = {100},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.06.040},
  abstract = {For accurate EEG/MEG source analysis it is necessary to model the head volume conductor as realistic as possible. This includes the distinction of the different conductive compartments in the human head. In this study, we investigated the influence of modeling/not modeling the conductive compartments skull spongiosa, skull compacta, cerebrospinal fluid (CSF), gray matter, and white matter and of the inclusion of white matter anisotropy on the EEG/MEG forward solution. Therefore, we created a highly realistic 6-compartment head model with white matter anisotropy and used a state-of-the-art finite element approach. Starting from a 3-compartment scenario (skin, skull, and brain), we subsequently refined our head model by distinguishing one further of the above-mentioned compartments. For each of the generated five head models, we measured the effect on the signal topography and signal magnitude both in relation to a highly resolved reference model and to the model generated in the previous refinement step. We evaluated the results of these simulations using a variety of visualization methods, allowing us to gain a general overview of effect strength, of the most important source parameters triggering these effects, and of the most affected brain regions. Thereby, starting from the 3-compartment approach, we identified the most important additional refinement steps in head volume conductor modeling. We were able to show that the inclusion of the highly conductive CSF compartment, whose conductivity value is well known, has the strongest influence on both signal topography and magnitude in both modalities. We found the effect of gray/white matter distinction to be nearly as big as that of the CSF inclusion, and for both of these steps we identified a clear pattern in the spatial distribution of effects. In comparison to these two steps, the introduction of white matter anisotropy led to a clearly weaker, but still strong, effect. Finally, the distinction between skull spongiosa and compacta caused the weakest effects in both modalities when using an optimized conductivity value for the homogenized compartment. We conclude that it is highly recommendable to include the CSF and distinguish between gray and white matter in head volume conductor modeling. Especially for the MEG, the modeling of skull spongiosa and compacta might be neglected due to the weak effects; the simplification of not modeling white matter anisotropy is admissible considering the complexity and current limitations of the underlying modeling approach.},
  timestamp = {2016-10-20T14:36:04Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Vorwerk, Johannes and Cho, Jae-Hyun and Rampp, Stefan and Hamer, Hajo and Kn{\"o}sche, Thomas R. and Wolters, Carsten H.},
  month = oct,
  year = {2014},
  keywords = {EEG,FEM,Forward problem,MEG,Tissue conductivity anisotropy,volume conductor modeling},
  pages = {590--607},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SR9M39MV\\S1053811914005175.html:text/html}
}

@article{Dubarry2014,
  title = {Simultaneous Recording of {{MEG}}, {{EEG}} and Intracerebral {{EEG}} during Visual Stimulation: {{From}} Feasibility to Single-Trial Analysis},
  volume = {99},
  issn = {1053-8119},
  shorttitle = {Simultaneous Recording of {{MEG}}, {{EEG}} and Intracerebral {{EEG}} during Visual Stimulation},
  doi = {10.1016/j.neuroimage.2014.05.055},
  abstract = {Electroencephalography (EEG), magnetoencephalography (MEG), and intracerebral stereotaxic EEG (SEEG) are the three neurophysiological recording techniques, which are thought to capture the same type of brain activity. Still, the relationships between non-invasive (EEG, MEG) and invasive (SEEG) signals remain to be further investigated. In early attempts at comparing SEEG with either EEG or MEG, the recordings were performed separately for each modality. However such an approach presents substantial limitations in terms of signal analysis. The goal of this technical note is to investigate the feasibility of simultaneously recording these three signal modalities (EEG, MEG and SEEG), and to provide strategies for analyzing this new kind of data. Intracerebral electrodes were implanted in a patient with intractable epilepsy for presurgical evaluation purposes. This patient was presented with a visual stimulation paradigm while the three types of signals were simultaneously recorded. The analysis started with a characterization of the MEG artifact caused by the SEEG equipment. Next, the average evoked activities were computed at the sensor level, and cortical source activations were estimated for both the EEG and MEG recordings; these were shown to be compatible with the spatiotemporal dynamics of the SEEG signals. In the average time\textendash{}frequency domain, concordant patterns between the MEG/EEG and SEEG recordings were found below the 40 Hz level. Finally, a fine-grained coupling between the amplitudes of the three recording modalities was detected in the time domain, at the level of single evoked responses. Importantly, these correlations have shown a high level of spatial and temporal specificity. These findings provide a case for the ability of trimodal recordings (EEG, MEG, and SEEG) to reach a greater level of specificity in the investigation of brain signals and functions.},
  timestamp = {2016-10-20T14:36:04Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Dubarry, Anne-Sophie and Badier, Jean-Michel and Tr{\'e}buchon-Da Fonseca, Agn{\`e}s and Gavaret, Martine and Carron, Romain and Bartolomei, Fabrice and Li{\'e}geois-Chauvel, Catherine and R{\'e}gis, Jean and Chauvel, Patrick and Alario, F. -Xavier and B{\'e}nar, Christian -G.},
  month = oct,
  year = {2014},
  keywords = {EEG,MEG,SEEG,Simultaneous recordings,Single-trial analysis},
  pages = {548--558},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5QJXT66U\\S1053811914004200.html:text/html}
}

@article{Kahkonen2005,
  title = {{{MEG}} and {{TMS}} Combined with {{EEG}} for Mapping Alcohol Effects},
  volume = {37},
  issn = {0741-8329},
  doi = {10.1016/j.alcohol.2006.03.003},
  abstract = {Magnetoencephalography (MEG) is a noninvasive method of studying magnetic fields from outside the skull that are generated by at least partially synchronized neuronal populations in the brain. The advantage of MEG over electroencephalography (EEG) is the transparency of the skull, scalp, and brain tissue to the magnetic fields, which facilitates easy localization of the cortical activity. In MEG, alcohol increased the relative power of the alpha rhythm and reduced the relative power of beta activity in parieto-occipital regions. In contrast, no changes were observed in EEG, indicating that these methods differently detect alcohol's action on the cortex. Furthermore, MEG and EEG also differently detected the effects of alcohol on cognition. Alcohol reduced magnetic and electric auditory N1 and mismatch negativity amplitudes. P3a amplitudes were also reduced in EEG but not in MEG, suggesting that different cortical areas are responsible for alcohol's action on involuntary attention. Transcranial magnetic stimulation (TMS) provides new possibilities for studying localized changes in the electrical properties of the human cortex, especially when combined with EEG. Different cortical areas can be stimulated and the subsequent brain activity can be measured, yielding information about cortical excitability and connectivity. Alcohol modulates EEG responses evoked by motor-cortex TMS, the effects being largest at the right prefrontal cortex (assessed by minimum-norm estimation), meaning that alcohol changed the functional connectivity between motor and prefrontal cortices. Furthermore, alcohol decreases amplitudes of EEG responses after the left prefrontal stimulation of anterior parts of the cortex, which may be associated with the decrease of prefrontal cortical excitability. Taken together, MEG and TMS combined with EEG provide new insight into the focal actions of alcohol on the cortex with a temporal resolution of milliseconds, giving information different from that given by other brain imaging modalities.},
  timestamp = {2016-10-20T14:44:48Z},
  number = {3},
  urldate = {2016-10-20},
  journal = {Alcohol},
  author = {K{\"a}hk{\"o}nen, Seppo},
  month = nov,
  year = {2005},
  keywords = {Alcohol,electroencephalography (EEG),magnetoencephalography (MEG),Transcranial magnetic stimulation (TMS)},
  pages = {129--133},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PS5XDVBT\\S0741832906000425.html:text/html}
}

@article{Lew2013,
  title = {Effects of Sutures and Fontanels on {{MEG}} and {{EEG}} Source Analysis in a Realistic Infant Head Model},
  volume = {76},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2013.03.017},
  abstract = {In infants, the fontanels and sutures as well as conductivity of the skull influence the volume currents accompanying primary currents generated by active neurons and thus the associated electroencephalography (EEG) and magnetoencephalography (MEG) signals. We used a finite element method (FEM) to construct a realistic model of the head of an infant based on MRI images. Using this model, we investigated the effects of the fontanels, sutures and skull conductivity on forward and inverse EEG and MEG source analysis. Simulation results show that MEG is better suited than EEG to study early brain development because it is much less sensitive than EEG to distortions of the volume current caused by the fontanels and sutures and to inaccurate estimates of skull conductivity. Best results will be achieved when MEG and EEG are used in combination.},
  timestamp = {2016-10-20T14:44:48Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Lew, Seok and Sliva, Danielle D. and Choe, Myong-sun and Grant, P. Ellen and Okada, Yoshio and Wolters, Carsten H. and H{\"a}m{\"a}l{\"a}inen, Matti S.},
  month = aug,
  year = {2013},
  keywords = {EEG,FEM,Fontanel,MEG,source analysis,Suture},
  pages = {282--293},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FHQEN24E\\S1053811913002590.html:text/html}
}

@article{Nevalainen2015,
  title = {Evaluation of Somatosensory Cortical Processing in Extremely Preterm Infants at Term with {{MEG}} and {{EEG}}},
  volume = {126},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2014.05.036},
  abstract = {Objective
Prior studies on extremely preterm infants have reported long-term prognostic value of absent secondary somatosensory cortex (SII) responses in magnetoencephalography (MEG) at term. The present work (i) further examines the potential added value of SII responses in neonatal neurological evaluation of preterm infants, and (ii) tests whether SII responses are detectable in routine neonatal electroencephalogram complemented with median nerve stimulation (EEG-SEP).
Methods
Altogether 29 infants born \&lt;28 gestational weeks underwent MEG, MRI, and neonatal neurological examination at term age, and Hempel neurological examination at 2-years corrected age. Term-age EEG-SEP was available for seven infants.
Results
While in neonatal neurological examination severely abnormal finding predicted unfavorable outcome in 2/2 infants, outcome was unfavorable also in 3/9 (33\%) moderately abnormal and in 5/18 (28\%) mildly abnormal/normal infants. Of these eight infants four had unilaterally absent SII responses in MEG, compared with only two of the 24 infants with favorable outcome. Furthermore, SII responses (when present in MEG) were also usually detectable in EEG-SEP.
Conclusions
Complementing clinical EEG recording with SEP holds promise for valuable extension of neonatal neurophysiological assessment.
Significance
Multimodal study of EEG and sensory evoked responses is informative, safe, and cheap, and it can be readily performed at bedside.},
  timestamp = {2016-10-20T14:44:48Z},
  number = {2},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Nevalainen, P{\"a}ivi and Rahkonen, Petri and Pihko, Elina and Lano, Aulikki and Vanhatalo, Sampsa and Andersson, Sture and Autti, Taina and Valanne, Leena and Mets{\"a}ranta, Marjo and Lauronen, Leena},
  month = feb,
  year = {2015},
  keywords = {magnetoencephalography (MEG),Preterm infant,Secondary somatosensory cortex (SII),Somatosensory evoked fields (SEFs),Somatosensory evoked potentials (SEPs)},
  pages = {275--283},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RJ3WNPQI\\S1388245714003216.html:text/html}
}

@article{Quandt2012,
  title = {Single Trial Discrimination of Individual Finger Movements on One Hand: {{A}} Combined {{MEG}} and {{EEG}} Study},
  volume = {59},
  issn = {1053-8119},
  shorttitle = {Single Trial Discrimination of Individual Finger Movements on One Hand},
  doi = {10.1016/j.neuroimage.2011.11.053},
  abstract = {It is crucial to understand what brain signals can be decoded from single trials with different recording techniques for the development of Brain\textendash{}Machine Interfaces. A specific challenge for non-invasive recording methods are activations confined to small spatial areas on the cortex such as the finger representation of one hand. Here we study the information content of single trial brain activity in non-invasive MEG and EEG recordings elicited by finger movements of one hand. We investigate the feasibility of decoding which of four fingers of one hand performed a slight button press. With MEG we demonstrate reliable discrimination of single button presses performed with the thumb, the index, the middle or the little finger (average over all subjects and fingers 57\%, best subject 70\%, empirical guessing level: 25.1\%). EEG decoding performance was less robust (average over all subjects and fingers 43\%, best subject 54\%, empirical guessing level 25.1\%). Spatiotemporal patterns of amplitude variations in the time series provided best information for discriminating finger movements. Non-phase-locked changes of mu and beta oscillations were less predictive. Movement related high gamma oscillations were observed in average induced oscillation amplitudes in the MEG but did not provide sufficient information about the finger's identity in single trials. Importantly, pre-movement neuronal activity provided information about the preparation of the movement of a specific finger. Our study demonstrates the potential of non-invasive MEG to provide informative features for individual finger control in a Brain\textendash{}Machine Interface neuroprosthesis.},
  timestamp = {2016-10-20T14:44:48Z},
  number = {4},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Quandt, F. and Reichert, C. and Hinrichs, H. and Heinze, H. J. and Knight, R. T. and Rieger, J. W.},
  month = feb,
  year = {2012},
  keywords = {Brain–Machine Interface,Electroencephalography,Finger decoding,High-gamma oscillations,Magnetoencephalography,Motor cortex},
  pages = {3316--3324},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IFEMD5GM\\S1053811911013358.html:text/html}
}

@article{Yoshinaga2002,
  series = {Recent advances in human brain mapping},
  title = {Benefit of Combined Use of {{EEG}} and {{MEG}} Dipole},
  volume = {1232},
  issn = {0531-5131},
  doi = {10.1016/S0531-5131(01)00825-1},
  abstract = {Purpose: In this study, we tried to show that EEG and MEG are clinically complementary to each other and that a combination of both technologies is useful for the precise diagnosis of epileptic focus. Subjects and methods: We recorded EEGs and MEGs simultaneously and analyzed dipoles in five patients suffering from intractable localization-related epilepsy with ages ranging from 13 to 19 years. MEG dipoles were analyzed using a BTI Magnes 148-channel magnetometer. EEG dipoles were analyzed using a realistically shaped four-layered head model (scalp\textendash{}skull\textendash{}liquor\textendash{}brain) built from 2.5-mm-slice MRI images. Results: (1) In two of five patients, MEG could not detect any epileptiform discharges, while EEG showed clear spikes. However, dipoles estimated from the MEG data correspond to the early phase of EEG spikes clustered at a location close to that of the EEG-detected dipole. (2) In two of five patients, EEG showed only intermittent high-voltage slow waves (HVSs) without definite spikes. However, MEG showed clear epileptiform discharges preceding these EEG-detected HVSs. Dipoles estimated for these EEG-detected HVSs were located at a location close to that of the MEG-detected dipoles. Conclusion: Simultaneous recording of MEG and EEG with dipole modeling is more useful for accurate determination of epileptic focus than either technique alone.},
  timestamp = {2016-10-20T14:47:12Z},
  urldate = {2016-10-20},
  journal = {International Congress Series},
  author = {Yoshinaga, Harumi and Nakahori, Tomoyuki and Kobayashi, Katsuhiro and Ohtsuka, Yoko and Oka, Eiji and Kitamura, Yoshihiro and Kiriyama, Hideki and Kinugasa, Kazumasa and Miyamoto, Keiichi},
  month = apr,
  year = {2002},
  keywords = {Dipole,EEG,Localization-related epilepsy,MEG},
  pages = {419--426},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\UN85HD2G\\S0531513101008251.html:text/html}
}

@article{Wang2012d,
  title = {Voxel-Based Morphometric {{MRI}} Post-Processing in {{MRI}}-Negative Focal Cortical Dysplasia Followed by Simultaneously Recorded {{MEG}} and Stereo-{{EEG}}},
  volume = {100},
  issn = {0920-1211},
  doi = {10.1016/j.eplepsyres.2012.02.011},
  abstract = {Summary
We aim to report on the usefulness of a voxel-based morphometric MRI post-processing technique in detecting subtle epileptogenic structural lesions. The MRI post-processing technique was implemented in a morphometric analysis program (MAP), in a 30-year-old male with pharmacoresistant focal epilepsy and negative MRI. MAP gray\textendash{}white matter junction file facilitated the identification of a suspicious structural lesion in the right frontal opercular area. The electrophysiological data by simultaneously recorded stereo-EEG and MEG confirmed the epileptogenicity of the underlying subtle structural abnormality. The patient underwent a limited right frontal opercular resection, which completely included the area detected by MAP. Surgical pathology revealed focal cortical dysplasia (FCD) type IIb. Postoperatively the patient has been seizure-free for 2 years. This study demonstrates that MAP has promise in increasing the diagnostic yield of MRI reading in challenging patients with ``non-lesional'' MRIs. The clinical relevance and epileptogenicity of MAP abnormalities in patients with epilepsy have not been investigated systematically; therefore it is important to confirm their pertinence by performing electrophysiological recordings. When confirmed to be epileptogenic, such MAP abnormalities may reflect an underlying subtle cortical dysplasia whose complete resection can lead to seizure-free outcome.},
  timestamp = {2016-10-20T14:54:33Z},
  number = {1\textendash{}2},
  urldate = {2016-10-20},
  journal = {Epilepsy Research},
  author = {Wang, Z. I. and Jones, S. E. and Ristic, A. J. and Wong, C. and Kakisaka, Y. and Jin, K. and Schneider, F. and Gonzalez-Martinez, J. A. and Mosher, J. C. and Nair, D. and Burgess, R. C. and Najm, I. M. and Alexopoulos, A. V.},
  month = jun,
  year = {2012},
  keywords = {Epilepsy,Focal cortical dysplasia,Magnetoencephalography,MEG,MRI-negative,Stereo-EEG,Voxel-based morphometric MRI post-processing},
  pages = {188--193},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IX4EKVQI\\S0920121112000472.html:text/html}
}

@article{vanStraaten2013,
  series = {Neural Networks in Psychiatry},
  title = {Structure out of Chaos: {{Functional}} Brain Network Analysis with {{EEG}}, {{MEG}}, and Functional {{MRI}}},
  volume = {23},
  issn = {0924-977X},
  shorttitle = {Structure out of Chaos},
  doi = {10.1016/j.euroneuro.2012.10.010},
  abstract = {The brain is the characteristic of a complex structure. By representing brain function, measured with EEG, MEG, and fMRI, as an abstract network, methods for the study of complex systems can be applied. These network studies have revealed insights in the complex, yet organized, architecture that is evidently present in brain function. We will discuss some technical aspects of formation and assessment of the functional brain networks. Moreover, the results that have been reported in this respect in the last years, in healthy brains as well as in functional brain networks of subjects with a neurological or psychiatrical disease, will be reviewed.},
  timestamp = {2016-10-20T14:54:33Z},
  number = {1},
  urldate = {2016-10-20},
  journal = {European Neuropsychopharmacology},
  author = {{van Straaten}, Elisabeth C. W. and Stam, Cornelis J.},
  month = jan,
  year = {2013},
  keywords = {EEG,fMRI,Functional brain networks,graph theory,MEG},
  pages = {7--18},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HXI2USGC\\S0924977X12002891.html:text/html}
}

@article{Kahkonen2001,
  title = {Effects of {{Haloperidol}} on {{Selective Attention}}: {{A Combined Whole}}-{{Head MEG}} and {{High}}-{{Resolution EEG Study}}},
  volume = {25},
  issn = {0893-133X},
  shorttitle = {Effects of {{Haloperidol}} on {{Selective Attention}}},
  doi = {10.1016/S0893-133X(01)00255-X},
  abstract = {We used 122-channel magnetoencephalography (MEG) and 64-channel electroencephalogrphy (EEG) simultaneously to study the effects of dopaminergic transmission on human selective attention in a randomized, double-blind placebo-controlled cross-over design. A single dose of dopamine D2 receptor antagonist haloperidol (2 mg) or placebo was given orally to 12 right-handed healthy volunteers 3 hours before measurement. In a dichotic selective attention task, subjects were presented with two trains of standard (700 Hz to the left ear, 1,100 Hz to the right ear) and deviant (770 and 1,210 Hz, respectively) tones. Subjects were instructed to count the tones presented to one ear; whereas, the tones presented to the other ear were to be ignored. Haloperidol significantly attenuated processing negativity (PN), an event-related potential (ERP) component elicited by selectively attended standard tones at 300\textendash{}500 ms after stimulus presentation. These results, indicating impaired selective attention by a blockade of dopamine D2 receptors, were further accompanied with increased mismatch negativity (MMN), elicited by involuntary detection of task-irrelevant deviants. Taken together, haloperidol seemed to induce functional changes in neural networks accounting for both selective and involuntary attention, suggesting modulation of these functions by dopamine D2 receptors.},
  timestamp = {2016-10-20T14:54:33Z},
  number = {4},
  urldate = {2016-10-20},
  journal = {Neuropsychopharmacology},
  author = {K{\"a}hk{\"o}nen, Seppo and Ahveninen, Jyrki and J{\"a}{\"a}skel{\"a}inen, Iiro P and Kaakkola, Seppo and N{\"a}{\"a}t{\"a}nen, Risto and Huttunen, Juha and Pekkonen, Eero},
  month = oct,
  year = {2001},
  keywords = {Auditory,D2 receptors,electroencephalography (EEG),Haloperidol,magnetoencephalography (MEG),Selective attention},
  pages = {498--504},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3FTVWV8D\\S0893133X0100255X.html:text/html}
}

@article{Aydin2014,
  series = {Abstracts of the 30th International Congress of Clinical Neurophysiology (ICCN) of the IFCN, March 20\textendash{}23, Berlin, Germany},
  title = {P806: {{Effects}} of Spike Averaging on {{EEG}}, {{MEG}} and Combined {{EEG}}/{{MEG}} Source Analysis of Epileptic Activity},
  volume = {125, Supplement 1},
  issn = {1388-2457},
  shorttitle = {P806},
  doi = {10.1016/S1388-2457(14)50842-5},
  timestamp = {2016-10-21T13:59:40Z},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Aydin, {\"U}. and Vorwerk, J. and Duempelmann, M. and Kuepper, P. and Kugel, H. and Wellmer, J. and Kellinghaus, C. and Haueisen, J. and Rampp, S. and Stefan, H. and Wolters, C.},
  month = jun,
  year = {2014},
  pages = {S258},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\R7TXA7IF\\S1388245714508425.html:text/html}
}

@article{Nikulin2011,
  title = {A Novel Method for Reliable and Fast Extraction of Neuronal {{EEG}}/{{MEG}} Oscillations on the Basis of Spatio-Spectral Decomposition},
  volume = {55},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2011.01.057},
  abstract = {Neuronal oscillations have been shown to underlie various cognitive, perceptual and motor functions in the brain. However, studying these oscillations is notoriously difficult with EEG/MEG recordings due to a massive overlap of activity from multiple sources and also due to the strong background noise. Here we present a novel method for the reliable and fast extraction of neuronal oscillations from multi-channel EEG/MEG/LFP recordings. The method is based on a linear decomposition of recordings: it maximizes the signal power at a peak frequency while simultaneously minimizing it at the neighboring, surrounding frequency bins. Such procedure leads to the optimization of signal-to-noise ratio and allows extraction of components with a characteristic ``peaky'' spectral profile, which is typical for oscillatory processes. We refer to this method as spatio-spectral decomposition (SSD). Our simulations demonstrate that the method allows extraction of oscillatory signals even with a signal-to-noise ratio as low as 1:10. The SSD also outperformed conventional approaches based on independent component analysis. Using real EEG data we also show that SSD allows extraction of neuronal oscillations (e.g., in alpha frequency range) with high signal-to-noise ratio and with the spatial patterns corresponding to central and occipito-parietal sources. Importantly, running time for SSD is only a few milliseconds, which clearly distinguishes it from other extraction techniques usually requiring minutes or even hours of computational time. Due to the high accuracy and speed, we suggest that SSD can be used as a reliable method for the extraction of neuronal oscillations from multi-channel electrophysiological recordings.},
  timestamp = {2016-10-20T14:59:47Z},
  number = {4},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Nikulin, Vadim V. and Nolte, Guido and Curio, Gabriel},
  month = apr,
  year = {2011},
  keywords = {Alpha,EEG,MEG,Oscillations,Synchronization},
  pages = {1528--1535},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F7RBFIJ8\\S1053811911000930.html:text/html}
}

@article{Thonnessen2008,
  title = {Optimized Mismatch Negativity Paradigm Reflects Deficits in Schizophrenia Patients: {{A}} Combined {{EEG}} and {{MEG}} Study},
  volume = {77},
  issn = {0301-0511},
  shorttitle = {Optimized Mismatch Negativity Paradigm Reflects Deficits in Schizophrenia Patients},
  doi = {10.1016/j.biopsycho.2007.10.009},
  abstract = {Mismatch negativity (MMN) and its neuromagnetic analog (MMNm) are event-related brain responses elicited by changes in a sequence of auditory events and indexes early cognitive processing. It consistently detects neural processing deficits in schizophrenia. So far MMN is assessed with different methods (electroencephalography, EEG; magnetoencephalography, MEG) and with different paradigms: the ``traditional'' oddball design with rare deviants (20\%) or the ``optimum'' design with 50\% deviants varying in one of five parameters each. These MMN measures may not reflect one unitary mechanism which is equally affected in schizophrenia. We compared both designs in 12 patients with schizophrenia and controls using MEG and EEG. Automated, observer-independent data analysis rendered the procedures suitable for clinical applications. The optimum design was fastest to detect MMN and MEG had the best signal-to-noise ratio. In addition MMN was mostly reduced in schizophrenia if measured with MEG in the optimum paradigm. Optimized paradigms improve sensitivity and speed for the detection of schizophrenia endophenotypes. Dysfunctions in this disorder may lie primarily in the fast and automatic encoding of stimulus features at the auditory cortex.},
  timestamp = {2016-10-20T15:00:20Z},
  number = {2},
  urldate = {2016-10-20},
  journal = {Biological Psychology},
  author = {Th{\"o}nnessen, H. and Zvyagintsev, M. and Harke, K. C. and Boers, F. and Dammers, J. and Norra, Ch. and Mathiak, K.},
  month = feb,
  year = {2008},
  keywords = {Auditory processing,Clinical magnetoencephalography,Mismatch negativity (MMN),schizophrenia},
  pages = {205--216},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8WZUUNF2\\S0301051107001810.html:text/html}
}

@article{Leistner2007,
  title = {Combined {{MEG}} and {{EEG}} Methodology for Non-Invasive Recording of Infraslow Activity in the Human Cortex},
  volume = {118},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2007.08.015},
  abstract = {Objective
Periinfarct depolarisation and spreading depression represent key mechanisms of neuronal injury after stroke. Changes in cortical electrical potentials and magnetic fields in the very low frequency range are relevant parameters to characterize these events, which up to now have only been recorded invasively. In this study, we proved whether a non-invasive combined MEG/EEG recording technique is able to quantitatively monitor cortical infraslow activity in humans.
Methods
We used repetitive very slow and slow right finger movements as a physiological motor activation paradigm to induce cortical infraslow activity. Infraslow fields were recorded over the left hemisphere using a modulation-based MEG technique. EEG was performed using 16 standard Ag-Cl electrodes that covered the left motor cortex.
Results
We recorded stable focal motor-related infraslow magnetic field changes in seven out of seven subjects. We also found correlating infraslow electrical potential changes in three out of seven subjects. Slow finger movements generated significantly stronger field and potential changes than very slow movements.
Conclusions
This study demonstrates the technical feasibility of combined non-invasive electrical potential and magnetic field measurements to localize and quantitatively monitor physiological, low amplitude, infraslow cortical activity in humans. This specific combination of simultaneous recording techniques allows to benefit from the specific physical advantages of each method.
Significance
This combined non-invasive MEG\textendash{}EEG methodology is able to provide important information on infraslow neuronal activity originating from tangentially and radially oriented sources. Moreover, this dual approach has the potential to separate neuronal from non-neuronal DC-sources, e.g., radially to the head orientated DC-currents across the skin/scalp/skull/dura occurring during cerebral hypercapnia or hypoxia.},
  timestamp = {2016-10-20T15:03:31Z},
  number = {12},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Leistner, Stefanie and Sander, Tilmann and Burghoff, Martin and Curio, Gabriel and Trahms, Lutz and Mackert, Bruno-Marcel},
  month = dec,
  year = {2007},
  keywords = {DC-Electroencephalography,DC-Magnetoencephalography,Direct current,Finger movement,Infraslow neuronal activation,Motor cortex},
  pages = {2774--2780},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HSK4WDP5\\S1388245707004105.html:text/html}
}

@article{Bast2007,
  title = {Combined {{EEG}} and {{MEG}} Analysis of Early Somatosensory Evoked Activity in Children and Adolescents with Focal Epilepsies},
  volume = {118},
  issn = {1388-2457},
  doi = {10.1016/j.clinph.2007.03.037},
  abstract = {Objective
The study aimed to evaluate differences between EEG and MEG analysis of early somatosensory evoked activity in patients with focal epilepsies in localizing eloquent areas of the somatosensory cortex.
Methods
Twenty-five patients (12 male, 13 female; age 4\textendash{}25 years, mean 11.7 years) were included. Syndromes were classified as symptomatic in 17, idiopathic in 2 and cryptogenic in 6 cases. 10 patients presented with malformations of cortical development (MCD). 122 channel MEG and simultaneous 33-channel EEG were recorded during tactile stimulation of the thumb (sampling rate 769 Hz, band-pass 0.3\textendash{}260 Hz). Forty-four hemispheres were analyzed. Hemispheres were classified as type I: normal (15), II: central structural lesion (16), III: no lesion, but central epileptic discharges (ED, 8), IV: lesion or ED outside the central region (5). Analysis of both sides including one normal and one type II or III hemisphere was possible in 15 patients. Recordings were repeated in 18 hemispheres overall. Averaged data segments were filtered (10\textendash{}250 Hz) and analyzed off-line with BESA. Latencies and amplitudes of N20 and P30 were analyzed. A regional source was fitted for localizing S1 by MRI co-registration. Orientation of EEG N20 was calculated from a single dipole model.
Results
EEG and MEG lead to comparable good results in all normal hemispheres. Only EEG detected N20/P30 in 3 hemispheres of types II/III while MEG showed no signal. N20 dipoles had a more radial orientation in these cases. MEG added information in one hemisphere, when EEG source analysis of a clear N20 was not possible because of a low signal-to-noise ratio. Overall N20 dipoles had a more radial orientation in type II when compared to type I hemispheres (p = 0.01). Further N20/P30 parameters (amplitudes, latencies, localization related to central sulcus) showed no significant differences between affected and normal hemispheres. Early somatosensory evoked activity was preserved within the visible lesion in 5 of the 10 patients with MCD.
Conclusions
MEG should be combined with EEG when analyzing tactile evoked activities in hemispheres with a central structural lesion or ED focus.
Significance
At time, MEG analysis is frequently applied without simultaneous EEG. Our results clearly show that EEG may be superior under specific circumstances and combination is necessary when analyzing activity from anatomically altered cortex.},
  timestamp = {2016-10-20T15:07:05Z},
  number = {8},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Bast, T. and Wright, T. and Boor, R. and Harting, I. and Feneberg, R. and Rupp, A. and Hoechstetter, K. and Rating, D. and Baumg{\"a}rtner, U.},
  month = aug,
  year = {2007},
  keywords = {Epilepsy,SEF,SEP,Somatosensory Cortex,source analysis},
  pages = {1721--1735},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\W9XZPMAE\\S1388245707001964.html:text/html}
}

@article{Jerbi2011,
  series = {NUM{\'E}RO SP{\'E}CIAL : LE CERVEAU DANS TOUS SES {\'E}TATS},
  title = {Inferring Hand Movement Kinematics from {{MEG}}, {{EEG}} and Intracranial {{EEG}}: {{From}} Brain-Machine Interfaces to Motor Rehabilitation},
  volume = {32},
  issn = {1959-0318},
  shorttitle = {Inferring Hand Movement Kinematics from {{MEG}}, {{EEG}} and Intracranial {{EEG}}},
  doi = {10.1016/j.irbm.2010.12.004},
  abstract = {The ability to use electrophysiological brain signals to decode various parameters of voluntary movement is a central question in Brain Machine Interface (BMI) research. Invasive BMI systems can successfully decode movement trajectories from the spiking activity of neurons in primary motor cortex and posterior parietal cortex. It has long been assumed that non-invasive techniques do not provide sufficient signal resolution to decode the kinematics of complex time-varying movements. This view stems from the hypothesis that movement parameters such as direction, position, velocity, or acceleration are primarily encoded by neuronal firing in motor cortex. Consequently, the fact that such signals cannot be detected using non-invasive techniques such as Electroencephalography (EEG) or Magnetoencephalography (MEG) has led to the claim that fine movement properties cannot be decoded with non-invasive methods. However, this view has been proven wrong by numerous studies in recent years. First, a growing body of research over the last decade has shown that the local field potential (LFP) signal, which represents the summed activity of a neuronal population, can encode movement parameters at a level comparable to unit recordings. These findings were confirmed in humans by the successful use of electrocorticography (ECoG) to achieve continuous movement decoding via invasive human BMI approaches. Very recently, a number of non-invasive studies were able to provide striking evidence that even surface-level MEG or EEG data can contain sufficient information on hand movement in order to infer movement direction and hand kinematics from brain signals recorded using non-invasive methods. Here we provide a brief review of this recent literature and discuss its importance on the future of BMI research and its implications on the development of novel motor rehabilitation strategies.},
  timestamp = {2016-10-20T15:11:12Z},
  number = {1},
  urldate = {2016-10-20},
  journal = {IRBM},
  author = {Jerbi, K. and Vidal, J. R. and Mattout, J. and Maby, E. and Lecaignard, F. and Ossandon, T. and Hamam{\'e}, C. M. and Dalal, S. S. and Bouet, R. and Lachaux, J. -P. and Leahy, R. M. and Baillet, S. and Garnero, L. and Delpuech, C. and Bertrand, O.},
  month = feb,
  year = {2011},
  keywords = {Brain-computer interface (BCI),Brain-machine interface (BMI),Décodage du mouvement,ECoG,EEG,Hand velocity,ICM non-invasive,Interface cerveau-machine,MEG,Motor rehabilitation,Movement decoding,Neural prosthesisés,Non-invasive BMI,Prothèse neuronale,Rééducation motrice,SEEG,Vitesse de la main},
  pages = {8--18},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QZ969KSD\\S1959031811000054.html:text/html}
}

@article{Pataraia2005,
  title = {Combined {{MEG}}/{{EEG}} Analysis of the Interictal Spike Complex in Mesial Temporal Lobe Epilepsy},
  volume = {24},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2004.09.031},
  abstract = {We studied the functional organization of the interictal spike complex in 30 patients with mesial temporal lobe epilepsy (MTLE) using combined magnetoencephalography (MEG)/electroencephalography (EEG) recordings. Spikes could be recorded in 14 patients (47\%) during the 2- to 3-h MEG/EEG recording session. The MEG and EEG spikes were subjected to separate dipole analyses; the MEG spike dipole localizations were superimposed on MRI scans. All spike dipoles could be localized to the temporal lobe with a clear preponderance in the medial region. Based on dipole orientations in MEG, patients could be classified into two groups: patients with anterior medial vertical (AMV) dipoles, suggesting epileptic activity in the mediobasal temporal lobe and patients with anterior medial horizontal (AMH) dipoles, indicating involvement of the temporal pole and the anterior parts of the lateral temporal lobe. Whereas patients with AMV dipoles had strictly unitemporal interictal and ictal EEG changes during prolonged video-EEG monitoring, 50\% of patients with AMH dipoles showed evidence of bitemporal affection on interictal and ictal EEG. Nine patients underwent epilepsy surgery so far. Whereas all five patients with AMV dipoles became completely seizure-free postoperatively (Class Ia), two out of four patients with AMH dipoles experienced persistent auras (Class Ib). This difference, however, was not statistically significant. We therefore conclude that combined MEG/EEG dipole modeling can identify subcompartments of the temporal lobe involved in epileptic activity and may be helpful to differentiate between subtypes of mesial temporal lobe epilepsy noninvasively.},
  timestamp = {2016-10-20T15:25:09Z},
  number = {3},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Pataraia, Ekaterina and Lindinger, Gerald and Deecke, Lueder and Mayer, Dagmar and Baumgartner, Christoph},
  month = feb,
  year = {2005},
  keywords = {Dipole modeling,Electroencephalography,Magnetoencephalography,Mesial temporal lobe epilepsy},
  pages = {607--614},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\E9ZT5RNP\\S1053811904005592.html:text/html}
}

@article{Shahbazi2015,
  title = {Self-{{Consistent MUSIC}}: {{An}} Approach to the Localization of True Brain Interactions from {{EEG}}/{{MEG}} Data},
  volume = {112},
  issn = {1053-8119},
  shorttitle = {Self-{{Consistent MUSIC}}},
  doi = {10.1016/j.neuroimage.2015.02.054},
  abstract = {MUltiple SIgnal Classification (MUSIC) is a standard localization method which is based on the idea of dividing the vector space of the data into two subspaces: signal subspace and noise subspace. The brain, divided into several grid points, is scanned entirely and the grid point with the maximum consistency with the signal subspace is considered as the source location. In one of the MUSIC variants called Recursively Applied and Projected MUSIC (RAP-MUSIC), multiple iterations are proposed in order to decrease the location estimation uncertainties introduced by subspace estimation errors. In this paper, we suggest a new method called Self-Consistent MUSIC (SC-MUSIC) which extends RAP-MUSIC to a self-consistent algorithm. This method, SC-MUSIC, is based on the idea that the presence of several sources has a bias on the localization of each source. This bias can be reduced by projecting out all other sources mutually rather than iteratively. While the new method is applicable in all situations when MUSIC is applicable we will study here the localization of interacting sources using the imaginary part of the cross-spectrum due to the robustness of this measure to the artifacts of volume conduction. For an odd number of sources this matrix is rank deficient similar to covariance matrices of fully correlated sources. In such cases MUSIC and RAP-MUSIC fail completely while the new method accurately localizes all sources. We present results of the method using simulations of odd and even number of interacting sources in the presence of different noise levels. We compare the method with three other source localization methods: RAP-MUSIC, dipole fit and MOCA (combined with minimum norm estimate) through simulations. SC-MUSIC shows substantial improvement in the localization accuracy compared to these methods. We also show results for real MEG data of a single subject in the resting state. Four sources are localized in the sensorimotor area at f = 11 Hz which is the expected region for the idle rhythm.},
  timestamp = {2016-10-20T15:29:26Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Shahbazi, Forooz and Ewald, Arne and Nolte, Guido},
  month = may,
  year = {2015},
  pages = {299--309},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2XGGJ57X\\S105381191500155X.html:text/html}
}

@article{Ewald2014,
  title = {Wedge {{MUSIC}}: {{A}} Novel Approach to Examine Experimental Differences of Brain Source Connectivity Patterns from {{EEG}}/{{MEG}} Data},
  volume = {101},
  issn = {1053-8119},
  shorttitle = {Wedge {{MUSIC}}},
  doi = {10.1016/j.neuroimage.2014.07.011},
  abstract = {We introduce a novel method to estimate bivariate synchronization, i.e. interacting brain sources at a specific frequency or band, from MEG or EEG data robust to artifacts of volume conduction. The data driven calculation is solely based on the imaginary part of the cross-spectrum as opposed to the imaginary part of coherency. In principle, the method quantifies how strong a synchronization between a distinct pair of brain sources is present in the data. As an input of the method all pairs of pre-defined locations inside the brain can be used which is computationally exhaustive. In contrast to that, reference sources can be used that have been identified by any source reconstruction technique in a prior analysis step. We introduce different variants of the method and evaluate the performance in simulations. As a particular advantage of the proposed methodology, we demonstrate that the novel approach is capable of investigating differences in brain source interactions between experimental conditions or with respect to a certain baseline. For measured data, we first show the application on resting state MEG data where we find locally synchronized sources in the motor-cortex based on the sensorimotor idle rhythms. Finally, we show an example on EEG motor imagery data where we contrast hand and foot movements. Here, we also find local interactions in the expected brain areas.},
  timestamp = {2016-10-20T15:33:02Z},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Ewald, Arne and Avarvand, Forooz Shahbazi and Nolte, Guido},
  month = nov,
  year = {2014},
  keywords = {Class differences,Electroencephalography,functional connectivity,Imaginary part of coherency,Magnetoencephalography,Source localization,Volume conduction},
  pages = {610--624},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P92BG5GE\\S1053811914005837.html:text/html}
}

@article{Huang2007,
  title = {A Novel Integrated {{MEG}} and {{EEG}} Analysis Method for Dipolar Sources},
  volume = {37},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.06.002},
  abstract = {The ability of magnetoencephalography (MEG) to accurately localize neuronal currents and obtain tangential components of the source is largely due to MEG's insensitivity to the conductivity profile of the head tissues. However, MEG cannot reliably detect the radial component of the neuronal current. In contrast, the localization accuracy of electroencephalography (EEG) is not as good as MEG, but EEG can detect both the tangential and radial components of the source. In the present study, we investigated the conductivity dependence in a new approach that combines MEG and EEG to accurately obtain, not only the location and tangential components, but also the radial component of the source. In this approach, the source location and tangential components are obtained from MEG alone, and optimal conductivity values of the EEG model are estimated by best-fitting EEG signal, while precisely matching the tangential components of the source in EEG and MEG. Then, the radial components are obtained from EEG using the previously estimated optimal conductivity values. Computer simulations testing this integrated approach demonstrated two main findings. First, there are well-organized optimal combinations of the conductivity values that provide an accurate fit to the combined MEG and EEG data. Second, the radial component, in addition to the location and tangential components, can be obtained with high accuracy without needing to know the precise conductivity profile of the head. We then demonstrated that this new approach performed reliably in an analysis of the 20-ms component from human somatosensory responses elicited by electric median-nerve stimulation.},
  timestamp = {2016-10-20T15:33:30Z},
  number = {3},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Huang, Ming-Xiong and Song, Tao and Hagler Jr., Donald J. and Podgorny, Igor and Jousmaki, Veikko and Cui, Li and Gaa, Kathleen and Harrington, Deborah L. and Dale, Anders M. and Lee, Roland R. and Elman, Jeff and Halgren, Eric},
  month = sep,
  year = {2007},
  keywords = {Conductivity,EEG,integration,Inverse problem,Median-nerve stimulation,MEG,Radial,Skull,Somatosensory evoked field,Somatosensory evoked potential,Tangential},
  pages = {731--748},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PTNPABNN\\S1053811907005186.html:text/html}
}

@article{Wennberg2011,
  title = {{{EEG}} and {{MEG}} in Mesial Temporal Lobe Epilepsy: {{Where}} Do the Spikes Really Come From?},
  volume = {122},
  issn = {1388-2457},
  shorttitle = {{{EEG}} and {{MEG}} in Mesial Temporal Lobe Epilepsy},
  doi = {10.1016/j.clinph.2010.11.019},
  abstract = {Objective
There is persistent debate as to whether or not EEG and MEG recordings in patients with mesial temporal lobe epilepsy (MTLE) can detect mesial temporal interictal epileptiform discharges (spikes), and this issue is particularly relevant for source localization studies. With the aim of providing direct evidence pertinent to this debate we present detailed examples of the intracranial sources of spikes recorded with EEG and MEG in MTLE.
Methods
Spikes recorded in five different patients with MTLE during intracranial EEG (n = 2), intraoperative electrocorticography (ECOG; n = 1), combined scalp-intracranial EEG (n = 2) and combined EEG-MEG (n = 1) were analyzed and the intracranial sources of the spike foci were matched with their corresponding extracranial EEG and/or MEG fields. EEG and MEG dipole source localization was performed on six independent spike foci identified in one representative patient with bilateral MTLE.
Results
Spikes with an electrical field maximal at F7/8, F9/10 $\geqslant$ T3/4 were generated in the anterolateral temporal neocortex. The absence of coincident spiking at mesial locations indicated that these were not propagated from or to the hippocampus. Spikes with an electrical field maximal at T3/4 $\geqslant$ T9/10 were generated in the lateral temporal neocortex and likewise did not involve the hippocampus. Individual spikes generated in the mesiobasal temporal neocortex, including the fusiform gyrus, were difficult to detect with EEG (low amplitude diphasic waves most apparent after spike averaging at T3/4, T9/10 $\geqslant$ T5/6, P9/10) and only slightly more identifiable with MEG. Spikes generated within and confined to the mesial temporal structures, as confirmed by intracranial recordings, could not be detected with EEG or MEG. Notably, such spikes could not be detected even at intracranial recording sites on the lateral surface of the temporal lobe.
Conclusions
We present detailed evidence in a small case series showing that typical anterior temporal spikes recorded with EEG and MEG in MTLE arose from the anterolateral temporal neocortex and were neither propagated from nor to the hippocampus. Mid temporal EEG spikes were localized to the lateral temporal neocortex. Intracranially detected mesial temporal spikes were not detected with EEG or MEG.
Significance
The spikes recorded with EEG and MEG in MTLE are localized to neocortical foci, and not to the mesial temporal structures. Current noninvasive EEG and MEG source localization studies cannot accurately identify true mesial temporal spikes.},
  timestamp = {2016-10-20T15:40:44Z},
  number = {7},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Wennberg, Richard and Valiante, Taufik and Cheyne, Douglas},
  month = jul,
  year = {2011},
  keywords = {Dipole mapping,Electrocorticography (ECOG),Electroencephalography,Epilepsy surgery,Intracranial EEG,Magnetoencephalography,Source localization},
  pages = {1295--1313},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IU6I34HZ\\S1388245711000046.html:text/html}
}

@article{Quaedflieg2011,
  title = {Effects of Remifentanil on Processing of Auditory Stimuli: {{A}} Combined {{MEG}}/{{EEG}} Study},
  volume = {500, Supplement},
  issn = {0304-3940},
  shorttitle = {Effects of Remifentanil on Processing of Auditory Stimuli},
  doi = {10.1016/j.neulet.2011.05.178},
  timestamp = {2016-10-20T15:40:44Z},
  urldate = {2016-10-20},
  journal = {Neuroscience Letters},
  author = {Quaedflieg, Conny W. E. M. and M{\"u}nte, Sinikka and Sambeth, Anke and Kalso, Eija},
  month = jul,
  year = {2011},
  pages = {e38},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ISQIHHX7\\S0304394011008044.html:text/html}
}

@article{Otsubo2004,
  series = {Frontiers in Human Brain Topology. Proceedings of ISBET 2004},
  title = {Dipole Source Localization of Epileptic Discharges in {{EEG}} and {{MEG}}},
  volume = {1270},
  issn = {0531-5131},
  doi = {10.1016/j.ics.2004.05.053},
  abstract = {The paper compares EEG with MEG dipoles to assess the importance of both the MEG and EEG for patients with intractable epilepsy. The combination of EEG and MEG dipole analysis provides more accurate and comprehensive information about epileptic activities than either method used alone and produces a full picture of not only epileptic discharges but epileptic neural behaviors in the brain.},
  timestamp = {2016-10-20T15:46:07Z},
  urldate = {2016-10-20},
  journal = {International Congress Series},
  author = {Otsubo, Hiroshi and Ochi, Ayako and Sakamoto, Ryota and Iida, Koji},
  month = aug,
  year = {2004},
  keywords = {Dipole source localization,EEG,Epileptic discharge,MEG,Simultaneous recording},
  pages = {56--60},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DA2DKIME\\S0531513104010878.html:text/html}
}

@article{Gao2016,
  title = {Current Source Mapping by Spontaneous {{MEG}} and {{ECoG}} in Piglets Model},
  volume = {23},
  issn = {1746-8094},
  doi = {10.1016/j.bspc.2015.07.008},
  abstract = {The previous research reveals the presence of relatively strong spatial correlations from spontaneous activity over cortex in Electroencephalography (EEG) and Magnetoencephalography (MEG) measurement. A critical obstacle in MEG current source mapping is that strong background activity masks the relatively weak local information. In this paper, the hypothesis is that the dominant components of this background activity can be captured by the first Principal Component (PC) after employing Principal Component Analysis (PCA), thus discarding the first PC before the back projection would enhance the exposure of the information carried by a subset of sensors that reflects the local neuronal activity. By detecting MEG signals densely (one measurement per 2 \texttimes{} 2 mm2) in three piglets neocortical models over an area of 18 \texttimes{} 26 mm2 with a special shape of lesion by means of a $\mu$SQUID, this basic idea was demonstrated by the fact that a strong activity could be imaged in the lesion region after removing the first PC in Delta, Theta and Alpha band, while the original recordings did not show such activity clearly. Thus, the PCA decomposition can be employed to expose the local activity, which is around the lesion in the piglets' neocortical models, by removing the dominant components of the background activity.},
  timestamp = {2016-10-20T15:46:07Z},
  urldate = {2016-10-20},
  journal = {Biomedical Signal Processing and Control},
  author = {Gao, Lin and Wang, Jue and Stephen, Julia and Zhang, Tongsheng},
  month = jan,
  year = {2016},
  keywords = {Background activity,Lesion,MEG,Piglet neocortical model,Principle Component Analysis},
  pages = {76--84},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2NUW7UW4\\S1746809415001330.html:text/html}
}

@article{Bast2010,
  series = {Abstracts of ICCN 2010: 29th International Congress of Clinical Neurophysiology},
  title = {S10-2 {{Combined EEG}} and {{MEG}} Analysis for Epilepsy},
  volume = {121, Supplement 1},
  issn = {1388-2457},
  doi = {10.1016/S1388-2457(10)60081-8},
  timestamp = {2016-10-20T15:46:07Z},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Bast, T.},
  month = oct,
  year = {2010},
  pages = {S20--S21},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XUH9GR3N\\S1388245710600818.html:text/html}
}

@article{LopesdaSilva2013,
  title = {{{EEG}} and {{MEG}}: {{Relevance}} to {{Neuroscience}}},
  volume = {80},
  issn = {0896-6273},
  shorttitle = {{{EEG}} and {{MEG}}},
  doi = {10.1016/j.neuron.2013.10.017},
  abstract = {To understand dynamic cognitive processes, the high time resolution of EEG/MEG is invaluable. EEG/MEG signals can play an important role in providing measures of functional and effective connectivity in the brain. After a brief description of the foundations and basic methodological aspects of EEG/MEG signals, the relevance of the signals to obtain novel insights into the neuronal mechanisms underlying cognitive processes is surveyed, with emphasis on neuronal oscillations (ultra-slow, theta, alpha, beta, gamma, and HFOs) and combinations of oscillations. Three main functional roles of brain oscillations are put in evidence: (1) coding specific information, (2) setting and modulating brain attentional states, and (3) assuring the communication between neuronal populations such that specific dynamic workspaces may be created. The latter form the material core of cognitive functions.},
  timestamp = {2016-10-20T15:46:07Z},
  number = {5},
  urldate = {2016-10-20},
  journal = {Neuron},
  author = {{Lopes~da~Silva}, Fernando},
  month = dec,
  year = {2013},
  pages = {1112--1128},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CVBXDCCA\\S0896627313009203.html:text/html}
}

@article{Kahkonen2009,
  series = {Papers of the 22nd ECNP Congress},
  title = {P.1.e.013 {{Lorazepam}} Impairs Processing of Frequency Changes Andnovel Sounds: Combined {{MEG}}/{{EEG}} Study},
  volume = {19, Supplement 3},
  issn = {0924-977X},
  shorttitle = {P.1.e.013 {{Lorazepam}} Impairs Processing of Frequency Changes Andnovel Sounds},
  doi = {10.1016/S0924-977X(09)70457-2},
  timestamp = {2016-10-20T15:53:03Z},
  urldate = {2016-10-20},
  journal = {European Neuropsychopharmacology},
  author = {Kahkonen, S. and Pekkonen, E. and Horn, P. and Huttunen, J. and Kivisaari, R. and Korostenskaja, M.},
  month = sep,
  year = {2009},
  pages = {S307--S308},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DGRQPB8R\\S0924977X09704572.html:text/html}
}

@article{Babiloni2004a,
  series = {Proceedings of the International School on Magnetic Resonance and Brain FunctionFrontiers of Brain Functional MRI and Electrophysiological MethodsProceedings of the International School on Magnetic Resonance and Brain Function},
  title = {Multimodal Integration of {{EEG}}, {{MEG}} and {{fMRI}} Data for the Solution of the Neuroimage Puzzle},
  volume = {22},
  issn = {0730-725X},
  doi = {10.1016/j.mri.2004.10.007},
  abstract = {In this paper, advanced methods for the modeling of human cortical activity from combined high-resolution electroencephalography (EEG), magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) data are presented. These methods include a subject's multicompartment head model (scalp, skull, dura mater, cortex) constructed from magnetic resonance images, multidipole source model and regularized linear inverse source estimates of cortical current density. Determination of the priors in the resolution of the linear inverse problem was performed with the use of information from the hemodynamic responses of the cortical areas as revealed by block-designed (strength of activated voxels) fMRI. Examples of the application of these methods to the estimation of the time varying cortical current density activity in selected region of interest (ROI) are presented for movement-related high-resolution EEG data.},
  timestamp = {2016-10-20T15:53:03Z},
  number = {10},
  urldate = {2016-10-20},
  journal = {Magnetic Resonance Imaging},
  author = {Babiloni, Fabio and Mattia, Donetella and Babiloni, Claudio and Astolfi, Laura and Salinari, Serenella and Basilisco, Alessandra and Rossini, Paolo Maria and Marciani, Maria Grazia and Cincotti, Febo},
  month = dec,
  year = {2004},
  keywords = {Linear inverse source estimate,MEG and fMRI integration,Movement-related potentials,Multimodal EEG},
  pages = {1471--1476},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\68TVIFD5\\S0730725X04002929.html:text/html}
}

@article{Stam2005,
  title = {Nonlinear Dynamical Analysis of {{EEG}} and {{MEG}}: {{Review}} of an Emerging Field},
  volume = {116},
  issn = {1388-2457},
  shorttitle = {Nonlinear Dynamical Analysis of {{EEG}} and {{MEG}}},
  doi = {10.1016/j.clinph.2005.06.011},
  abstract = {Many complex and interesting phenomena in nature are due to nonlinear phenomena. The theory of nonlinear dynamical systems, also called `chaos theory', has now progressed to a stage, where it becomes possible to study self-organization and pattern formation in the complex neuronal networks of the brain. One approach to nonlinear time series analysis consists of reconstructing, from time series of EEG or MEG, an attractor of the underlying dynamical system, and characterizing it in terms of its dimension (an estimate of the degrees of freedom of the system), or its Lyapunov exponents and entropy (reflecting unpredictability of the dynamics due to the sensitive dependence on initial conditions). More recently developed nonlinear measures characterize other features of local brain dynamics (forecasting, time asymmetry, determinism) or the nonlinear synchronization between recordings from different brain regions.

Nonlinear time series has been applied to EEG and MEG of healthy subjects during no-task resting states, perceptual processing, performance of cognitive tasks and different sleep stages. Many pathologic states have been examined as well, ranging from toxic states, seizures, and psychiatric disorders to Alzheimer's, Parkinson's and Cre1utzfeldt-Jakob's disease. Interpretation of these results in terms of `functional sources' and `functional networks' allows the identification of three basic patterns of brain dynamics: (i) normal, ongoing dynamics during a no-task, resting state in healthy subjects; this state is characterized by a high dimensional complexity and a relatively low and fluctuating level of synchronization of the neuronal networks; (ii) hypersynchronous, highly nonlinear dynamics of epileptic seizures; (iii) dynamics of degenerative encephalopathies with an abnormally low level of between area synchronization. Only intermediate levels of rapidly fluctuating synchronization, possibly due to critical dynamics near a phase transition, are associated with normal information processing, whereas both hyper\textemdash{}as well as hyposynchronous states result in impaired information processing and disturbed consciousness.},
  timestamp = {2016-10-20T15:53:03Z},
  number = {10},
  urldate = {2016-10-20},
  journal = {Clinical Neurophysiology},
  author = {Stam, C. J.},
  month = oct,
  year = {2005},
  keywords = {chaos,complexity,EEG,MEG,Nonlinear dynamics,self-organization,Time series analysis},
  pages = {2266--2301},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\W872VWXV\\S1388245705002403.html:text/html}
}

@article{Bijma2004,
  title = {The Coupled Dipole Model: An Integrated Model for Multiple {{MEG}}/{{EEG}} Data Sets},
  volume = {23},
  issn = {1053-8119},
  shorttitle = {The Coupled Dipole Model},
  doi = {10.1016/j.neuroimage.2004.06.038},
  abstract = {Often MEG/EEG is measured in a few slightly different conditions to investigate the functionality of the human brain. This kind of data sets show similarities, though are different for each condition. When solving the inverse problem (IP), performing the source localization, one encounters the problem that this IP is ill-posed: constraints are necessary to solve and stabilize the solution to the IP. Moreover, a substantial amount of data is needed to avoid a signal to noise ratio (SNR) that is too poor for source localizations.

In the case of similar conditions, this common information can be exploited by analyzing the data sets simultaneously. The here proposed coupled dipole model (CDM) provides an integrated method in which these similarities between conditions are used to solve and stabilize the inverse problem. The coupled dipole model is applicable when data sets contain common sources or common source time functions.

The coupled dipole model uses a set of common sources and a set of common source time functions (STFs) to model all conditions in one single model. The data of each condition are mathematically described as a linear combination of these common spatial and common temporal components. This linear combination is specified in a coupling matrix for each data set.

The coupled dipole model was applied in two simulation studies and in one experimental study. The simulations show that the errors in the estimated spatial and temporal parameters decrease compared to the standard separate analyses. A decrease in position error of a factor of 10 was shown for the localization of two nearby sources. In the experimental application, the coupled dipole model was shown to be necessary to obtain a plausible solution in at least 3 of 15 conditions investigated. Moreover, using the CDM, a direct comparison between parameters in different conditions is possible, whereas in separate models, the scaling of the amplitude parameters varies in general from data set to data set.},
  timestamp = {2016-10-20T16:07:55Z},
  number = {3},
  urldate = {2016-10-20},
  journal = {NeuroImage},
  author = {Bijma, Fetsje and {de Munck}, Jan C. and B{\"o}cker, Koen B. E. and Huizenga, Hilde M. and Heethaar, Rob M.},
  month = nov,
  year = {2004},
  keywords = {Component model,Coupled dipole model,Integrated model,MEG/EEG source analysis,Spatiotemporal covariance,Trilinear model,Visual evoked field},
  pages = {890--904},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\N6MEKJ8M\\S1053811904003441.html:text/html}
}

@article{Calhoun2016,
  series = {Brain Connectivity in Psychopathology},
  title = {Multimodal {{Fusion}} of {{Brain Imaging Data}}: {{A Key}} to {{Finding}} the {{Missing Link}}(s) in {{Complex Mental Illness}}},
  volume = {1},
  issn = {2451-9022},
  shorttitle = {Multimodal {{Fusion}} of {{Brain Imaging Data}}},
  doi = {10.1016/j.bpsc.2015.12.005},
  abstract = {It is becoming increasingly clear that combining multimodal brain imaging data provides more information for individual subjects by exploiting the rich multimodal information that exists. However, the number of studies that do true multimodal fusion (i.e., capitalizing on joint information among modalities) is still remarkably small given the known benefits. In part, this is because multimodal studies require broader expertise in collecting, analyzing, and interpreting the results than do unimodal studies. In this article, we start by introducing the basic reasons why multimodal data fusion is important and what it can do and, importantly, how it can help us avoid wrong conclusions and help compensate for imperfect brain imaging studies. We also discuss the challenges that need to be confronted for such approaches to be more widely applied by the community. We then provide a review of the diverse studies that have used multimodal data fusion (primarily focused on psychosis) as well as provide an introduction to some of the existing analytic approaches. Finally, we discuss some up-and-coming approaches to multimodal fusion including deep learning and multimodal classification that show considerable promise. Our conclusion is that multimodal data fusion is rapidly growing, but it is still underutilized. The complexity of the human brain coupled with the incomplete measurement provided by existing imaging technology makes multimodal fusion essential to mitigate misdirection and hopefully provide a key to finding the missing link(s) in complex mental illness.},
  timestamp = {2016-10-20T16:18:45Z},
  number = {3},
  urldate = {2016-10-20},
  journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
  author = {Calhoun, Vince D. and Sui, Jing},
  month = may,
  year = {2016},
  keywords = {brain function,Connectivity,data fusion,independent component analysis,Psychosis,schizophrenia},
  pages = {230--244},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\P3IE4KR8\\S2451902216000598.html:text/html}
}

@article{Liao2013,
  title = {A New Wavelet Transform to Sparsely Represent Cortical Current Densities for {{EEG}}/{{MEG}} Inverse Problems},
  volume = {111},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2013.04.015},
  abstract = {The present study investigated the use of transform sparseness of cortical current density on human brain surface to improve electroencephalography/magnetoencephalography (EEG/MEG) inverse solutions. Transform sparseness was assessed by evaluating compressibility of cortical current densities in transform domains. To do that, a structure compression method from computer graphics was first adopted to compress cortical surface structure, either regular or irregular, into hierarchical multi-resolution meshes. Then, a new face-based wavelet method based on generated multi-resolution meshes was proposed to compress current density functions defined on cortical surfaces. Twelve cortical surface models were built by three EEG/MEG softwares and their structural compressibility was evaluated and compared by the proposed method. Monte Carlo simulations were implemented to evaluate the performance of the proposed wavelet method in compressing various cortical current density distributions as compared to other two available vertex-based wavelet methods. The present results indicate that the face-based wavelet method can achieve higher transform sparseness than vertex-based wavelet methods. Furthermore, basis functions from the face-based wavelet method have lower coherence against typical EEG and MEG measurement systems than vertex-based wavelet methods. Both high transform sparseness and low coherent measurements suggest that the proposed face-based wavelet method can improve the performance of L1-norm regularized EEG/MEG inverse solutions, which was further demonstrated in simulations and experimental setups using MEG data. Thus, this new transform on complicated cortical structure is promising to significantly advance EEG/MEG inverse source imaging technologies.},
  timestamp = {2016-10-21T09:15:11Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Computer Methods and Programs in Biomedicine},
  author = {Liao, Ke and Zhu, Min and Ding, Lei},
  month = aug,
  year = {2013},
  keywords = {compressed sensing,Compressibility,EEG/MEG,Inverse problem,Sparseness,Wavelet transform},
  pages = {376--388},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\M6SJ5VR3\\S0169260713001375.html:text/html}
}

@article{Gavaret2015,
  title = {High-Resolution {{EEG}} ({{HR}}-{{EEG}}) and Magnetoencephalography ({{MEG}})},
  volume = {45},
  issn = {0987-7053},
  doi = {10.1016/j.neucli.2014.11.011},
  abstract = {Summary
High-resolution EEG (HR-EEG) and magnetoencephalography (MEG) allow the recording of spontaneous or evoked electromagnetic brain activity with excellent temporal resolution. Data must be recorded with high temporal resolution (sampling rate) and high spatial resolution (number of channels). Data analyses are based on several steps with selection of electromagnetic signals, elaboration of a head model and use of algorithms in order to solve the inverse problem. Due to considerable technical advances in spatial resolution, these tools now represent real methods of ElectroMagnetic Source Imaging. HR-EEG and MEG constitute non-invasive and complementary examinations, characterized by distinct sensitivities according to the location and orientation of intracerebral generators. In the presurgical assessment of drug-resistant partial epilepsies, HR-EEG and MEG can characterize and localize interictal activities and thus the irritative zone. HR-EEG and MEG often yield significant additional data that are complementary to other presurgical investigations and particularly relevant in MRI-negative cases. Currently, the determination of the epileptogenic zone and functional brain mapping remain rather less well-validated indications. In France, in 2014, HR-EEG is now part of standard clinical investigation of epilepsy, while MEG remains a research technique.},
  timestamp = {2016-10-21T09:15:11Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Neurophysiologie Clinique/Clinical Neurophysiology},
  author = {Gavaret, M. and Maillard, L. and Jung, J.},
  month = mar,
  year = {2015},
  keywords = {EEG haute résolution,Épilepsie,Epilepsyés,HR-EEG,Localisation de source,MEG,Source localization},
  pages = {105--111},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9X8I9RNR\\S0987705314002093.html:text/html}
}

@incollection{Henson2010,
  series = {IFMBE Proceedings},
  title = {Multimodal {{Integration}}: {{Constraining MEG Localization}} with {{EEG}} and {{fMRI}}},
  copyright = {\textcopyright{}2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-12196-8 978-3-642-12197-5},
  shorttitle = {Multimodal {{Integration}}},
  abstract = {I review recent methodological developments for multimodal integration of MEG, EEG and fMRI data within a Parametric Empirical Bayesian framework [1]. More specifically, I describe two ways to incorporate multimodal data during distributed MEG/EEG source reconstruction under linear Gaussian assumptions: 1) the simultaneous inversion of EEG and MEG data using a common generative model [2], and 2) the addition of spatial priors from fMRI data when inverting MEG or EEG data [3]. In the former, the addition of EEG data was shown to increase the conditional precision of source estimates relative to MEG alone; in the latter, the inclusion of each suprathreshold cluster in the fMRI data as a separate spatial prior was shown to increase the Bayesian model evidence for MEG and EEG reconstruction. The former is an example of ``symmetric'' integration, or ``fusion'', in which a single generative model of all data modalities is inverted; the latter is an example of ``asymmetric'' integration, in which the data from one modality is used to inform inversion of another. I will conclude by considering whether symmetric fusion of MEG/EEG and fMRI data is worthwhile.},
  language = {en},
  timestamp = {2016-10-21T13:44:33Z},
  number = {28},
  urldate = {2016-10-21},
  booktitle = {17th {{International Conference}} on {{Biomagnetism Advances}} in {{Biomagnetism}} \textendash{} {{Biomag2010}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Henson, Richard N. A.},
  editor = {Supek, Selma and Su{\v s}ac, Ana},
  year = {2010},
  keywords = {Biomedical engineering,Biophysics and Biological Physics,EEG,fMRI,fusion,Image Processing and Computer Vision,Imaging / Radiology,MEG,multimodal},
  pages = {97--100},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QZEI4E4D\\978-3-642-12197-5_18.html:text/html},
  doi = {10.1007/978-3-642-12197-5_18}
}

@article{Jung2011,
  title = {An Improved Technique to Consider Mismatches between {{fMRI}} and {{EEG}}/{{MEG}} Sources for {{fMRI}} Constrained {{EEG}}/{{MEG}} Source Imaging},
  volume = {1},
  issn = {2093-9868},
  doi = {10.1007/s13534-011-0002-2},
  abstract = {PurposeAlthough fMRI constrained EEG/MEG source imaging can enhance spatiotemporal resolutions of functional neuroimaging, it has been reported that hard fMRI constraint can result in misidentification of neuronal sources if severe mismatches exist between fMRI activations and EEG/MEG sources. In our previous works, we proposed an approach to address this issue, which automatically adjusts the strength of fMRI constraints by considering the mismatch level. The previous studies proved to be useful particularly when one wants to obtain actual EEG/MEG source locations and uses fMRI prior information as an auxiliary tool to enhance focality of the distributed sources. However, some loss of concentration in the reconstructed images was inevitable when distinct mismatches existMethodsIn this study, instead of automatically extending the prior activation regions, we classified and labeled distinct prior activation regions, compared each of the fMRI prior activation regions with each of the thresholded EEG/MEG activation regions, excluded the matched EEG/MEG activation regions, and produced modified prior activation regions having smaller areas than the extended prior activation regions obtained using the conventional approach.ResultsA series of realistic EEG simulations, assuming fMRI invisible and discrepancy sources, showed that the improved approach not only compensated for the distortions due to the mismatched activations, but also maintained the high spatial resolution of the fMRI constrained EEG/MEG source imaging.ConclusionsOur simulation results demonstrated that the proposed technique can be a promising option to deal with mismatches between fMRI and EEG/MEG sources in the fMRI constrained EEG/MEG source imaging.},
  language = {en},
  timestamp = {2016-10-21T09:33:33Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Biomedical Engineering Letters},
  author = {Jung, Young-Jin and Im, Chang-Hwan},
  month = feb,
  year = {2011},
  pages = {32--41},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\24HRQF6A\\s13534-011-0002-2.html:text/html}
}

@incollection{Ahlfors2014,
  title = {{{MEG}} and {{Multimodal Integration}}},
  copyright = {\textcopyright{}2014 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-33044-5 978-3-642-33045-2},
  abstract = {Functional brain imaging methods provide measures of various physiological processes with a range of spatial and temporal scales. Because the sensitivity properties of the imaging modalities differ, combining multimodal data is expected to provide more information about the brain activity than is available by a single method. In direct data fusion, multimodal data can be described as complementary or supportive. Complementary modalities have the same type of sources, such as electroencephalography (EEG) and magnetoencephalography (MEG), which are both generated by cortical primary currents, but with different sensitivity characteristics. Combination of EEG and MEG data can resolve ambiguities in data from only one of the modalities. In a supportive role data from one imaging modality guides the analysis and interpretation of another modality. Structural magnetic resonance imaging (MRI) provides supportive data for MEG source estimation, e.g., by indicating allowable locations and orientations of MEG source currents. Functional MRI (fMRI) can be used in a supportive role to suggest a likely source distribution for MEG among multiple alternatives. MEG and fMRI can also be considered complementary if the different source types, i.e., primary currents for MEG and blood oxygenation level dependent (BOLD) contrast for fMRI, are both derived from a common physiological model.},
  language = {en},
  timestamp = {2016-10-21T09:38:44Z},
  urldate = {2016-10-21},
  booktitle = {Magnetoencephalography},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ahlfors, Seppo P.},
  editor = {Supek, Selma and Aine, Cheryl J.},
  year = {2014},
  keywords = {Biomedical engineering,data fusion,electroencephalography (EEG),functional magnetic resonance imaging (fMRI),Image Processing and Computer Vision,magnetoencephalography (MEG),Medical and Radiation Physics,multimodal,Neuroradiology,Neurosciences},
  pages = {183--198},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\D4VJTUHG\\978-3-642-33045-2_7.html:text/html},
  doi = {10.1007/978-3-642-33045-2_7}
}

@incollection{Goncalves2009,
  series = {Computational Methods in Applied Sciences},
  title = {Multimodality in {{Brain Imaging}}: {{Methodologic Aspects}} and {{Applications}}},
  copyright = {\textcopyright{}2009 Springer Science +Business Media B.V.},
  isbn = {978-1-4020-9085-1 978-1-4020-9086-8},
  shorttitle = {Multimodality in {{Brain Imaging}}},
  abstract = {The human brain is probably the most sophisticated result of evolution and its existence has allowed the human species to shape its environment in a definitive way. Though the structure and function of the brain are very complex, together they make the brain attain a remarkable degree of effectiveness. The human brain controls the central nervous system (CNS), the peripheral nervous system (PNS) and it regulates virtually all human activity [1, 24]. Different types of activity are controlled by different elements of the central-peripheral nervous systems. Involuntary functions such as heart rate control, respiration or digestion are unconsciously controlled through a part of the peripheral nervous system which is the autonomic nervous system [1, 24] whereas complex mental activities such as thought, reason or abstraction are consciously controlled.},
  language = {en},
  timestamp = {2016-10-21T09:42:00Z},
  number = {13},
  urldate = {2016-10-21},
  booktitle = {Advances in {{Computational Vision}} and {{Medical Image Processing}}},
  publisher = {{Springer Netherlands}},
  author = {Gon{\c c}alves, S{\'o}nia I.},
  editor = {Tavares, Jo{\~a}o Manuel R. S. and Jorge, R. M. Natal},
  year = {2009},
  keywords = {Appl.Mathematics/Computational Methods of Engineering,Biomedical engineering,Computer Imaging; Vision; Pattern Recognition and Graphics,Image Processing and Computer Vision,Imaging / Radiology,Simulation and Modeling},
  pages = {93--103},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\26GGK86E\\978-1-4020-9086-8_5.html:text/html},
  doi = {10.1007/978-1-4020-9086-8_5}
}

@article{Velmurugan2016,
  title = {Combined {{MEG}}\textendash{}{{EEG}} Source Localisation in Patients with Sub-Acute Sclerosing Pan-Encephalitis},
  volume = {37},
  issn = {1590-1874, 1590-3478},
  doi = {10.1007/s10072-016-2571-4},
  abstract = {To study the genesis and propagation patterns of periodic complexes (PCs) associated with myoclonic jerks in sub-acute sclerosing pan-encephalitis (SSPE) using magnetoencephalography (MEG) and electroencephalography (EEG). Simultaneous recording of MEG (306 channels) and EEG (64 channels) in five patients of SSPE (M:F = 3:2; age 10.8 $\pm$ 3.2 years; symptom-duration 6.2 $\pm$ 10 months) was carried out using Elekta Neuromag\textregistered{} TRIUX\texttrademark{} system. Qualitative analysis of 80\textendash{}160 PCs per patient was performed. Ten isomorphic classical PCs with significant field topography per patient were analysed at the `onset' and at `earliest significant peak' of the burst using discrete and distributed source imaging methods. MEG background was asymmetrical in 2 and slow in 3 patients. Complexes were periodic (3) or quasi-periodic (2), occurring every 4\textendash{}16 s and varied in morphology among patients. Mean source localization at onset of bursts using discrete and distributed source imaging in magnetic source imaging (MSI) was in thalami and or insula (50 and 50 \%, respectively) and in electric source imaging (ESI) was also in thalami and or insula (38 and 46 \%, respectively). Mean source localization at the earliest rising phase of peak in MSI was in peri-central gyrus (49 and 42 \%) and in ESI it was in frontal cortex (52 and 56 \%). Further analysis revealed that PCs were generated in thalami and or insula and thereafter propagated to anterolateral surface of the cortices (viz. sensori-motor cortex and frontal cortex) to same side as that of the onset. This novel MEG\textendash{}EEG based case series of PCs provides newer insights for understanding the plausible generators of myoclonus in SSPE and patterns of their propagation.},
  language = {en},
  timestamp = {2016-10-21T14:04:40Z},
  number = {8},
  urldate = {2016-10-21},
  journal = {Neurological Sciences},
  author = {Velmurugan, J. and Sinha, Sanjib and Nagappa, Madhu and Mariyappa, N. and Bindu, P. S. and Ravi, G. S. and Hazra, Nandita and Thennarasu, K. and Ravi, V. and Taly, A. B. and Satishchandra, P.},
  month = apr,
  year = {2016},
  pages = {1221--1231},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RAGTPM6E\\s10072-016-2571-4.html:text/html}
}

@incollection{Pitolli2010,
  series = {Lecture Notes in Computer Science},
  title = {Neuroelectric {{Current Localization}} from {{Combined EEG}}/{{MEG Data}}},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-27412-1 978-3-642-27413-8},
  abstract = {EEG/MEG devices record external signals which are generated by the neuronal electric activity of the brain. The localization of the neuronal sources requires the solution of the neuroelectromagnetic inverse problem which is highly ill-posed and ill-conditioned. We provide an iterative thresholding algorithm for recovering neuroeletric current densities within the brain through combined EEG/MEG data. We use a joint sparsity constraint to promote solutions localized in small brain area, assuming that the vector components of the current densities possess the same sparse spatial pattern. At each iteration step, the EEG/MEG forward problem is numerically solved by a Galerkin boundary element method. Some numerical experiments on the localization of current dipole sources are also given. The numerical results show that joint sparsity constraints outperform classical regularization methods based on quadratic constraints.},
  language = {en},
  timestamp = {2016-10-21T13:46:31Z},
  number = {6920},
  urldate = {2016-10-21},
  booktitle = {Curves and {{Surfaces}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Pitolli, Francesca},
  editor = {Boissonnat, Jean-Daniel and Chenin, Patrick and Cohen, Albert and Gout, Christian and Lyche, Tom and Mazure, Marie-Laurence and Schumaker, Larry},
  month = jun,
  year = {2010},
  keywords = {Computer-Aided Engineering (CAD; CAE) and Design,Computer Graphics,Computer Imaging; Vision; Pattern Recognition and Graphics,Discrete Mathematics in Computer Science,Galerkin boundary element method,Image Processing and Computer Vision,Simulation and Modeling,Source reconstruction,sparse representation,Thresholded iteration},
  pages = {562--574},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KCBS7KIK\\978-3-642-27413-8_37.html:text/html},
  doi = {10.1007/978-3-642-27413-8_37}
}

@incollection{Zavala-Fernandez2007,
  series = {Lecture Notes in Computer Science},
  title = {Multi-Modal {{ICA Exemplified}} on {{Simultaneously Measured MEG}} and {{EEG Data}}},
  copyright = {\textcopyright{}2007 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-74493-1 978-3-540-74494-8},
  abstract = {A multi-modal linear mixing model is suggested for simultaneously measured MEG and EEG data. On the basis of this model an ICA decomposition is calculated for a combined MEG and EEG signal vector using the TDSEP algorithm. A single modality demixing procedure is developed to classify ICA components to be multi-modality sources detected by EEG and MEG simultaneously or to be single mode sources. Under this premise, data from 10 subjects are analysed and four exemplary types of sources are selected. We found that these sources represent physically meaningful multi- and single-mode signals: Alpha oscillations, heart activity, eye blinks, and slow signal drifts.},
  language = {en},
  timestamp = {2016-10-21T09:56:45Z},
  number = {4666},
  urldate = {2016-10-21},
  booktitle = {Independent {{Component Analysis}} and {{Signal Separation}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Zavala-Fernandez, Heriberto and Sander, Tilmann H. and Burghoff, Martin and Orglmeister, Reinhold and Trahms, Lutz},
  editor = {Davies, Mike E. and James, Christopher J. and Abdallah, Samer A. and Plumbley, Mark D.},
  month = sep,
  year = {2007},
  keywords = {Algorithm Analysis and Problem Complexity,Coding and Information Theory,Computation by Abstract Devices,Data Mining and Knowledge Discovery,Signal; Image and Speech Processing,Statistics and Computing/Statistics Programs},
  pages = {673--680},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HRAGABSS\\978-3-540-74494-8_84.html:text/html},
  doi = {10.1007/978-3-540-74494-8_84}
}

@incollection{Pflieger2000,
  title = {Superadditive {{Information}} from {{Simultaneous MEG}}/{{EEG Data}}},
  copyright = {\textcopyright{}2000 Springer Science+Business Media New York},
  isbn = {978-1-4612-7066-9 978-1-4612-1260-7},
  abstract = {Although MEG and EEG measurement modalities are empirically distinct, there is a theoretically unified statistical-biophysical framework for posing and (in a limited sense) solving the electromagnetic inverse problem. In addition, biomagnetic and bioelectric measurements are differentially sensitive to the same type of intracranial signals, i.e., macroscopic ionic source currents. Since MEG and EEG share a common foundation and probably provide complementary information about the same kind of source signals, a truly integrated electromagnetic source imaging (EMSI) modality appears theoretically immanent. In practice, EMSI requires combined analysis of MEG and EEG acquired (in the ideal case) simultaneously.},
  language = {en},
  timestamp = {2016-10-21T13:32:29Z},
  urldate = {2016-10-21},
  booktitle = {Biomag 96},
  publisher = {{Springer New York}},
  author = {Pflieger, M. E. and Simpson, G. V. and Ahlfors, S. P. and Ilmoniemi, R. J.},
  editor = {Aine, Cheryl J. and Stroink, Gerhard and Wood, Charles C. and Okada, Yoshio and Swithenby, Stephen J.},
  year = {2000},
  keywords = {Physics; general},
  pages = {1154--1157},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KQFUFKGG\\978-1-4612-1260-7_282.html:text/html},
  doi = {10.1007/978-1-4612-1260-7_282}
}

@incollection{Schwartz2000,
  title = {Correlation {{Between Depth}} and {{MEG}}/{{EEG Surface Recordings}} in {{Auditory Cortex}}. {{Contribution}} of {{Numerical Simulations}}},
  copyright = {\textcopyright{}2000 Springer Science+Business Media New York},
  isbn = {978-1-4612-7066-9 978-1-4612-1260-7},
  abstract = {The main purpose of our study is to combine three different methods of brain activity recordings: depth recordings (stereotactic EEG: SEEG) and surface recordings (EEG/MEG) in order to improve localizations of the sources of the brain activity. We present here the first step of this study. We built a model of a region of interest (ROI) within the brain based on MRI information. Our goal was to obtain an accurate description of the surface of the ROI. According to this model, we defined realistic dipole layers in respect to the anatomy (orientation, shape, extension). By activating those dipole layers we produced SEEG, EEG and MEG theoretical signals. In order to validate this concept on a physiological point of view, we chose the auditory cortex as the ROI. We compared the simulations with in vivo signals (Auditory evoked potential (AEP)) for which Li{\'e}geois-Chauvel et al [1] studied the localization of the middle latency components using SEEG. We performed the validation by taking into account the auditory cortex anatomy and built the simulations for each patient studied. We then compared the theoretical results with the real SEEG data. Furthermore we used those simulations to test the reliability of several MEG sources modeling algorithms.},
  language = {en},
  timestamp = {2016-10-21T10:09:17Z},
  urldate = {2016-10-21},
  booktitle = {Biomag 96},
  publisher = {{Springer New York}},
  author = {Schwartz, D. and Badier, J. M. and Scarabin, J. M. and Li{\'e}geois-Chauvel, C.},
  editor = {Aine, Cheryl J. and Stroink, Gerhard and Wood, Charles C. and Okada, Yoshio and Swithenby, Stephen J.},
  year = {2000},
  keywords = {Physics; general},
  pages = {1170--1173},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CV93R6IV\\978-1-4612-1260-7_286.html:text/html},
  doi = {10.1007/978-1-4612-1260-7_286}
}

@article{Cohen1991,
  title = {{{EEG}} versus {{MEG}} Localization Accuracy: {{Theory}} and Experiment},
  volume = {4},
  issn = {0896-0267, 1573-6792},
  shorttitle = {{{EEG}} versus {{MEG}} Localization Accuracy},
  doi = {10.1007/BF01132766},
  abstract = {SummaryWe first review the theoretical and computer modelling studies concerning localization accuracy of EEG and MEG, both separately and together; the source is here a dipole. The results show that, of the three causes of localization errors, noise and head modelling errors have about the same effect on EEG and MEG localization accuracies, while the results for measurement placement errors are inconclusive. Thus, these results to date show no significant superiority of MEG over EEG localization accuracy. Secondly, we review the experimental findings, where there are again localization accuracy studies of EEG and MEG both separately and together. The most significant EEG-only study was due to dipoles implanted in the heads of patients, and produced an average localization error of 20 mm. Various MEG-only studies gave an average error of 2\textendash{}3 mm in saline spheres and 4\textendash{}8 mm in saline-filled skulls. In the one study where EEG and MEG localization were directly compared in the same actual head, again using dipoles implanted in patients, the average EEG and MEG errors of localization were 10 and 8 mm respectively. The MEG error was later confirmed by a similar (but MEG-only) experiment in another study, using a more elaborate MEG system. In summary, both theory and experiment suggests that the MEG offers no significant advantage over the EEG in the task of localizing a dipole source. The main use of the MEG, therefore, should be based on the proven feature that the MEG signal from a radial source is highly suppressed, allowing it to complement the EEG in selecting between competing source configurations. A secondary useful feature is that it handles source modelling errors differently than does the EEG, allowing it to help clarify non-dipolar extended sources.},
  language = {en},
  timestamp = {2016-10-21T12:39:49Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Cohen, David and Cuffin, B. Neil},
  year = {1991},
  pages = {95--103},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\47RZUHBB\\BF01132766.html:text/html}
}

@incollection{Poghosyan2015,
  title = {Personalized {{Management}} of {{Epilepsy Through Smart Use}} of {{EEG}} and {{Detailed MEG Analysis}}},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  isbn = {978-3-319-20048-4 978-3-319-20049-1},
  abstract = {ARMOR aspires to use all available information from diverse sources to enhance the management of epilepsy in terms of diagnosis, home monitoring and evaluating the efficacy of treatment. In this chapter we introduce a novel approach aimed at achieving this goal through smart use of available data. The primary focus of the approach is to use magnetoencephalography (MEG) or high-density electroencephalography (hdEEG), when they are available, to improve the effectiveness of routine electroencephalography (EEG) tests. In a nutshell, we acknowledge that with the currently available hardware it is not possible to record MEG or hdEEG on a routine basis, so we want to use one or few such measurements to develop a personalized neurophysiological model of the epileptic condition and then use the model to derive specific biomarkers, which can be measured with simple and more easily available techniques such as few-channel EEG, electrocardiogram (ECG), electrodermal response (EDR) etc. Thus the goal is to use MEG or hdEEG for background reference and a base for development of biomarkers that can address specific clinical questions for a specific patient using simpler devices, which can be easily used in the home environment, such as few EEG electrodes. Although as stated above the primary focus of the approach is the smart use of advanced neurophysiological techniques, such as MEG or hdEEG, the methods developed here can be used with simpler data, such as low-density EEG (e.g. 21-electrode 10\textendash{}20 system), which is largely available for most epilepsy patients, to significantly improve the EEG setup for further routine measurements.},
  language = {en},
  timestamp = {2016-10-21T10:15:59Z},
  urldate = {2016-10-21},
  booktitle = {Cyberphysical {{Systems}} for {{Epilepsy}} and {{Related Brain Disorders}}},
  publisher = {{Springer International Publishing}},
  author = {Poghosyan, Vahe and Ioannides, Andreas A.},
  editor = {Voros, Nikolaos S. and Antonopoulos, Christos P.},
  year = {2015},
  keywords = {Biomedical engineering,Biometrics,Circuits and Systems,Health Informatics,Neurosciences,Signal; Image and Speech Processing},
  pages = {255--270},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7494X5W2\\978-3-319-20049-1_13.html:text/html},
  doi = {10.1007/978-3-319-20049-1_13}
}

@article{Wieringa1993,
  title = {The Estimation of a Realistic Localization of Dipole Layers within the Brain Based on Functional ({{EEG}}, {{MEG}}) and Structural ({{MRI}}) Data: {{A}} Preliminary Note},
  volume = {5},
  issn = {0896-0267, 1573-6792},
  shorttitle = {The Estimation of a Realistic Localization of Dipole Layers within the Brain Based on Functional ({{EEG}}, {{MEG}}) and Structural ({{MRI}}) Data},
  doi = {10.1007/BF01128685},
  language = {en},
  timestamp = {2016-10-21T12:39:12Z},
  number = {4},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Wieringa, H. J. and Peters, M. J. and da Silva, F. H. Lopes},
  year = {1993},
  pages = {327--330},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SP78664H\\BF01128685.html:text/html}
}

@incollection{Cheyne2000,
  title = {{{EEG}} and {{MEG Source Analysis}} of {{Somatosensory Evoked Responses}} to {{Mechanical Stimulation}} of the {{Fingers}}},
  copyright = {\textcopyright{}2000 Springer Science+Business Media New York},
  isbn = {978-1-4612-7066-9 978-1-4612-1260-7},
  abstract = {Previous MEG and EEG studies have successfully utilized electrical stimulation of the digits in order to study generators of early responses in the human primary somatosensory cortex [1\textendash{}3]. Natural stimulation of the somatosensory system using vibratory pulses to the tips of the digits has been found to elicit qualitatively different responses in the EEG [4, 5] presumably due to the activation of different neuronal pathways. Similar responses have been observed in the MEG to transient mechanical stimuli producing large responses at latencies of 50 msec, corresponding to the electrical P50 response [6, 7]. A more recent study [8] also noted MEG responses at 70 msec latencies in some subjects resembling the N70 component described by H{\"a}m{\"a}l{\"a}inen and co-workers [4]. The earlier, P50 response is presumed to reflect activation of primary sensory cortex (SI). A study of mechanically evoked epidural and single unit responses in waking monkeys [5] found that this component was associated with a period of inhibitory input to neurons in areas 3b and 1, whereas the later, slow component (N70) was not associated with activity in SI and appeared to arise from other cortical areas, such as SII. The magnetically recorded P50 (P50m) appears to be the largest and most consistent MEG event recorded during transient tactile stimulation in humans and its potential application for somatotopic mapping studies [6, 7] warrants further investigation of its neural generation. Moreover, the marked orthogonality of the EEG and MEG topographies of the P50 response makes it an ideal candidate for the comparison and/or combination of EEG and MEG localization methods. The current study compared separate high-density, 32 channel EEG and 143 channel MEG recordings in two subjects using identical stimulation paradigms in order to compare dipole source locations in somatosensory cortex obtained separately for each method. In addition, 3-dimensional MRI was obtained for both subjects in order to constrain source model information as well as aid in the integration of coordinate systems for the MEG and EEG source models.},
  language = {en},
  timestamp = {2016-10-21T10:21:21Z},
  urldate = {2016-10-21},
  booktitle = {Biomag 96},
  publisher = {{Springer New York}},
  author = {Cheyne, D. and Roberts, L. E. and Gaetz, W. and Bosnyak, D. and Weinberg, H. and Johnson, B. and Nahmias, C. and Deecke, L.},
  editor = {Aine, Cheryl J. and Stroink, Gerhard and Wood, Charles C. and Okada, Yoshio and Swithenby, Stephen J.},
  year = {2000},
  keywords = {Physics; general},
  pages = {1130--1133},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TRZBSGWM\\978-1-4612-1260-7_276.html:text/html},
  doi = {10.1007/978-1-4612-1260-7_276}
}

@incollection{Lutterveld2012,
  title = {Neurophysiological {{Research}}: {{EEG}} and {{MEG}}},
  copyright = {\textcopyright{}2012 Springer Science+Business Media, LLC},
  isbn = {978-1-4614-0958-8 978-1-4614-0959-5},
  shorttitle = {Neurophysiological {{Research}}},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) are two techniques that distinguish themselves from other neuroimaging methodologies through their ability to directly measure brain-related activity and their excellent temporal resolution. A large body of research has applied these techniques to investigate auditory hallucinations. Across a variety of approaches, the left superior temporal cortex is consistently reported to be involved in this symptom. Moreover, there is increasing evidence that a failure in corollary discharge, i.e., a neural signal originating in frontal speech areas that indicates to sensory areas that forthcoming thought is self-generated, may underlie the experience of auditory hallucinations.},
  language = {en},
  timestamp = {2016-10-21T10:22:23Z},
  urldate = {2016-10-21},
  booktitle = {Hallucinations},
  publisher = {{Springer New York}},
  author = {van Lutterveld, Remko and Ford, Judith M.},
  editor = {Blom, Jan Dirk and Sommer, Iris E. C.},
  year = {2012},
  keywords = {Neurobiology,Neurosciences},
  pages = {283--295},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3HRGPNW8\\978-1-4614-0959-5_21.html:text/html},
  doi = {10.1007/978-1-4614-0959-5_21}
}

@article{Ahlfors2015,
  title = {Modeling the Effect of Dendritic Input Location on {{MEG}} and {{EEG}} Source Dipoles},
  volume = {53},
  issn = {0140-0118, 1741-0444},
  doi = {10.1007/s11517-015-1296-5},
  abstract = {The cerebral sources of magneto- and electroencephalography (MEG, EEG) signals can be represented by current dipoles. We used computational modeling of realistically shaped passive-membrane dendritic trees of pyramidal cells from the human cerebral cortex to examine how the spatial distribution of the synaptic inputs affects the current dipole. The magnitude of the total dipole moment vector was found to be proportional to the vertical location of the synaptic input. The dipole moment had opposite directions for inputs above and below a reversal point located near the soma. Inclusion of shunting-type inhibition either suppressed or enhanced the current dipole, depending on whether the excitatory and inhibitory synapses were on the same or opposite side of the reversal point. Relating the properties of the macroscopic current dipoles to dendritic current distributions can help to provide means for interpreting MEG and EEG data in terms of synaptic connection patterns within cortical areas.},
  language = {en},
  timestamp = {2016-10-24T16:31:49Z},
  number = {9},
  urldate = {2016-10-21},
  journal = {Medical and Biological Engineering and Computing},
  author = {Ahlfors, Seppo P. and Wreh, Christopher},
  month = apr,
  year = {2015},
  pages = {879--887},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7KEPCHAH\\s11517-015-1296-5.html:text/html}
}

@article{Schwartz1999,
  title = {Evaluation of a {{New MEG}}-{{EEG Spatio}}-{{Temporal Localization Approach Using}} a {{Realistic Source Model}}},
  volume = {11},
  issn = {0896-0267, 1573-6792},
  doi = {10.1023/A:1022206603596},
  abstract = {This paper introduces a new technique for the localization of brain electromagnetic activity: a spatio-temporal fit (SPTF). This algorithm uses some properties of the principal component analysis and makes no assumptions about the number of sources to be located. It was applied to both simulated and real MEG/EEG signals and was compared to the well-known moving dipole fit (MDF) technique. For the simulations, we constructed extended sources, rather than single dipoles, that respected realistic anatomical and temporal properties. From these, we generated, under different noise conditions, MEG and EEG signals from which localization was performed. The real signals were auditory evoked fields. Firstly, it appeared that the SPTF was able to separate simultaneously activated sources even on strongly noisy signals while, most of the time, the MDF failed to give a clear description of the source configuration. Secondly, although we used the same head model to both generate the signals and locate the sources, localization for EEG was inferior to that for MEG. In conclusion, since in all test conditions the SPTF is found to be far superior to MDF, we suggest the use SPTF for the localization of equivalent dipoles.},
  language = {en},
  timestamp = {2016-10-21T12:38:50Z},
  number = {4},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Schwartz, D. P. and Badier, J. M. and Bihou{\'e}, P. and Bouliou, A.},
  year = {1999},
  pages = {279--289},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QQJ6R7MB\\A1022206603596.html:text/html}
}

@article{Klamer2014,
  title = {Differences {{Between MEG}} and {{High}}-{{Density EEG Source Localizations Using}} a {{Distributed Source Model}} in {{Comparison}} to {{fMRI}}},
  volume = {28},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-014-0405-3},
  abstract = {Electroencephalography (EEG) and magnetoencephalography (MEG) are widely used to localize brain activity and their spatial resolutions have been compared in several publications. While most clinical studies demonstrated higher accuracy of MEG source localization, simulation studies suggested a more accurate EEG than MEG localization for the same number of channels. However, studies comparing real MEG and EEG data with equivalent number of channels are scarce. We investigated 14 right-handed healthy subjects performing a motor task in MEG, high-density-(hd-) EEG and fMRI as well as a somatosensory task in MEG and hd-EEG and compared source analysis results of the evoked brain activity between modalities with different head models. Using individual head models, hd-EEG localized significantly closer to the anatomical reference point obtained by fMRI than MEG. Source analysis results were least accurate for hd-EEG based on a standard head model. Further, hd-EEG and MEG localized more medially than fMRI. Localization accuracy of electric source imaging is dependent on the head model used with more accurate results obtained with individual head models. If this is taken into account, EEG localization can be more accurate than MEG localization for the same number of channels.},
  language = {en},
  timestamp = {2016-10-21T11:16:00Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Klamer, Silke and Elshahabi, Adham and Lerche, Holger and Braun, Christoph and Erb, Michael and Scheffler, Klaus and Focke, Niels K.},
  month = oct,
  year = {2014},
  pages = {87--94},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JID9PAKD\\s10548-014-0405-3.html:text/html}
}

@article{Ahlfors2010,
  title = {Sensitivity of {{MEG}} and {{EEG}} to {{Source Orientation}}},
  volume = {23},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-010-0154-x},
  abstract = {An important difference between magnetoencephalography (MEG) and electroencephalography (EEG) is that MEG is insensitive to radially oriented sources. We quantified computationally the dependency of MEG and EEG on the source orientation using a forward model with realistic tissue boundaries. Similar to the simpler case of a spherical head model, in which MEG cannot see radial sources at all, for most cortical locations there was a source orientation to which MEG was insensitive. The median value for the ratio of the signal magnitude for the source orientation of the lowest and the highest sensitivity was 0.06 for MEG and 0.63 for EEG. The difference in the sensitivity to the source orientation is expected to contribute to systematic differences in the signal-to-noise ratio between MEG and EEG.},
  language = {en},
  timestamp = {2016-10-21T11:16:51Z},
  number = {3},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Ahlfors, Seppo P. and Han, Jooman and Belliveau, John W. and H{\"a}m{\"a}l{\"a}inen, Matti S.},
  month = jul,
  year = {2010},
  pages = {227--232},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AF22KJK5\\s10548-010-0154-x.html:text/html}
}

@incollection{Mosher2000,
  title = {Subspace {{Angles}}: {{A Metric}} for {{Comparisons}} in {{EEG}} and {{MEG}}},
  copyright = {\textcopyright{}2000 Springer Science+Business Media New York},
  isbn = {978-1-4612-7066-9 978-1-4612-1260-7},
  shorttitle = {Subspace {{Angles}}},
  abstract = {In forward head modeling, various approximations are made in order to keep the problem tractable. Simplifications can yield models ranging from simple spherical models to multi-tessellated arbitrary surfaces in a boundary element model (BEM). Spherical head models differ in the number of shells and the assumed conductivities. Other assumptions in the BEM include the choice of basis sets, such as constant, linear, or quadratic variations of the voltages across the individual areal elements, or the selection of error-weighting method, such as collocation, Galerkin, or ``direct'' methods. Numerical versus analytic integration can also yield numerical differences. These differences in parameters and approximations can yield models whose external fields (EEG potentials or MEG magnetic fields) differ for the same internal source configuration. Quantitative measures are needed to determine if these differences are significant.},
  language = {en},
  timestamp = {2016-10-21T11:19:04Z},
  urldate = {2016-10-21},
  booktitle = {Biomag 96},
  publisher = {{Springer New York}},
  author = {Mosher, J. C.},
  editor = {Aine, Cheryl J. and Stroink, Gerhard and Wood, Charles C. and Okada, Yoshio and Swithenby, Stephen J.},
  year = {2000},
  keywords = {Physics; general},
  pages = {302--305},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7U5DU8UH\\978-1-4612-1260-7_73.html:text/html},
  doi = {10.1007/978-1-4612-1260-7_73}
}

@article{Dannhauer2012,
  title = {Spatio-Temporal {{Regularization}} in {{Linear Distributed Source Reconstruction}} from {{EEG}}/{{MEG}}: {{A Critical Evaluation}}},
  volume = {26},
  issn = {0896-0267, 1573-6792},
  shorttitle = {Spatio-Temporal {{Regularization}} in {{Linear Distributed Source Reconstruction}} from {{EEG}}/{{MEG}}},
  doi = {10.1007/s10548-012-0263-9},
  abstract = {The high temporal resolution of EEG/MEG data offers a way to improve source reconstruction estimates which provide insight into the spatio-temporal involvement of neuronal sources in the human brain. In this work, we investigated the performance of spatio-temporal regularization (STR) in a current density approach using a systematic comparison to simple ad hoc or post hoc filtering of the data or of the reconstructed current density, respectively. For the used STR approach we implemented a frequency-specific constraint to penalize solutions outside a narrow frequency band of interest. The widely used sLORETA algorithm was adapted for STR and generally used for source reconstruction. STR and filtering approaches were evaluated with respect to spatial localization error and spatial dispersion, as well as to correlation of original and reconstructed source time courses in single source and two source scenarios with fixed source locations and oscillating source waveforms. We used extensive computer simulations and tested all algorithms with different parameter settings (noise levels and regularization parameters) for EEG data. To verify our results, we also used data from MEG phantom measurements. For the investigated scenarios, we did not find any evidence that STR-based methods outperform purely spatial algorithms applied to temporally filtered data. Furthermore, the results show very clearly that the performance of STR depends very much on the choice of regularization parameters.},
  language = {en},
  timestamp = {2016-10-21T11:20:40Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Dannhauer, Moritz and L{\"a}mmel, Eric and Wolters, Carsten H. and Kn{\"o}sche, Thomas R.},
  month = oct,
  year = {2012},
  pages = {229--246},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QUW4Q4KE\\s10548-012-0263-9.html:text/html}
}

@incollection{Papadopoulo2010,
  series = {IFMBE Proceedings},
  title = {The {{Adjoint Method}} for {{General EEG}} and {{MEG Sensor}}-{{Based Lead Field Equations}}},
  copyright = {\textcopyright{}2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-12196-8 978-3-642-12197-5},
  abstract = {The inverse source problem in electroencephalography (EEG) and magnetoencephalography (MEG) generally uses a lead field, which relates any source in the brain to its measurements at the sensors. For complex geometries, there is no analytical formula of the lead field. The common approach is to numerically compute the value of the lead field for a finite number of point sources (dipoles). There are several drawbacks: the model of the source space is fixed (a set of dipoles) and the computation can be expensive for as much as 10 000 dipoles. The common idea to bypass these problems is to compute the lead field from a sensor point of view. We show how the adjoint method can be used to derive general EEG and MEG sensor-based lead field equations. Within a simple framework, we provide a complete review of the explicit lead field equations, and we are able to extend these equations to non-point like sensors.},
  language = {en},
  timestamp = {2016-10-21T11:21:10Z},
  number = {28},
  urldate = {2016-10-21},
  booktitle = {17th {{International Conference}} on {{Biomagnetism Advances}} in {{Biomagnetism}} \textendash{} {{Biomag2010}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Papadopoulo, Th{\'e}odore and Vallagh{\'e}, Sylvain and Clerc, Maureen},
  editor = {Supek, Selma and Su{\v s}ac, Ana},
  year = {2010},
  keywords = {Adjoint method,Biomedical engineering,Biophysics and Biological Physics,Electroencephalography,Forward problem,Image Processing and Computer Vision,Imaging / Radiology,Magnetoencephalography},
  pages = {105--108},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MFPCPGHA\\978-3-642-12197-5_20.html:text/html},
  doi = {10.1007/978-3-642-12197-5_20}
}

@article{Meij2001,
  title = {The {{Existence}} of {{Two Sources}} in {{Rolandic Epilepsy}}: {{Confirmation}} with {{High Resolution EEG}}, {{MEG}} and {{fMRI}}},
  volume = {13},
  issn = {0896-0267, 1573-6792},
  shorttitle = {The {{Existence}} of {{Two Sources}} in {{Rolandic Epilepsy}}},
  doi = {10.1023/A:1011128729215},
  abstract = {In benign rolandic epilepsy seizure semiology suggests that the epileptic focus resides in the lower sensorimotor cortex. Previous studies involving dipole modeling based on 32 channel EEG have confirmed this localization. These studies have also suggested that two distinct dipole sources are required to adequately describe the typical interictal spikes. Since in benign epilepsy invasive validation is prohibited, this study tries to further establish these results using a multi-modal approach, involving 32 channel EEG, high resolution 84 channel EEG, 151 channel MEG and fMRI. From one patient interictal spikes were recorded and analyzed using the MUSIC algorithm in a realistic volume conductor model. In an fMRI experiment the same patient performed voluntary tongue movements, thus mimicking a typical seizure. Results show that EEG, MEG and fMRI localization converge on the same area in the lower part of the sensorimotor cortex, and that high resolution EEG clearly reveals two distinct sources, one in the post- and one in the pre-central cortex.},
  language = {en},
  timestamp = {2016-10-21T12:38:08Z},
  number = {4},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {van der Meij, W. and Huiskamp, G. J. M. and Rutten, G. J. M. and Wieneke, G. H. and van Huffelen, A. C. and van Nieuwenhuizen, O.},
  year = {2001},
  pages = {275--282},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XN4MCJK6\\A1011128729215.html:text/html}
}

@article{Zanow2004,
  title = {{{ASA}}-{{Advanced Source Analysis}} of {{Continuous}} and {{Event}}-{{Related EEG}}/{{MEG Signals}}},
  volume = {16},
  issn = {0896-0267, 1573-6792},
  doi = {10.1023/B:BRAT.0000032867.41555.d0},
  abstract = {Sophisticated analysis methods for EEG and MEG play a key role in the better understanding of brain functions as measured by high-density EEG and MEG. Being commercially available since 1996, the ASA software (ANT Software BV, Enschede, Netherlands) has been gaining growing popularity among clinical and cognitive researchers. With the following article, we present an overview on the currently available functionality of the software and provide examples of its application.},
  language = {en},
  timestamp = {2016-10-21T12:37:30Z},
  number = {4},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Zanow, Frank and Kn{\"o}sche, Thomas R.},
  year = {2004},
  pages = {287--290},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\BXKDFMJI\\BBRAT.0000032867.41555.html:text/html}
}

@article{Bertrand2001,
  title = {{{MRI Prior Computation}} and {{Parallel Tempering Algorithm}}: {{A Probabilistic Resolution}} of the {{MEG}}/{{EEG Inverse Problem}}},
  volume = {14},
  issn = {0896-0267, 1573-6792},
  shorttitle = {{{MRI Prior Computation}} and {{Parallel Tempering Algorithm}}},
  doi = {10.1023/A:1012567806745},
  abstract = {Since the MEG inverse problem is ill-posed and admits many possible solutions, it is not possible to give it a single " true" answer. Therefore, we propose here to use a specific probabilistic algorithm to map the full probability distribution of the MEG sources with Markov Chain Monte Carlo methods. Using a Bayesian approach, the probability of the MEG solutions is expressed as the product of the likelihood by the prior probability. To compute the prior and constrain the MEG inverse problem resolution, MRI data are also acquired and automatically processed to determine the brain position and volume. We then use Parallel Tempering algorithm to estimate the full posterior probability and determine the likely solutions of the inverse problem. We illustrate the method with results obtained from the analysis of somatosensory data. This illustrates both the MRI processing for the prior computation, and how the knowledge of the full posterior probability distribution can be used to estimate the position of the sources, as well as their likely extension.},
  language = {en},
  timestamp = {2016-10-21T12:37:06Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Bertrand, C{\'e}dric and Hamada, Yasukazu and Kado, Hisashi},
  year = {2001},
  pages = {57--68},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RBGCZ4CP\\A1012567806745.html:text/html}
}

@incollection{Louis2003,
  title = {Spatio-{{Temporal Current Density Reconstruction}} from {{EEG}}-/{{MEG}}-{{Data}}},
  copyright = {\textcopyright{}2003 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-62914-3 978-3-642-55753-8},
  abstract = {The determination of the sources of electric activity inside the brain from electric measurements on the surface of the head is known to be an ill-posed problem. In this paper a new algorithm which takes temporal a-priori information into account is described and compared to existing algorithms as Tikhonov-Phillips. There are further applications in medical and technical fields as the determination of electrical sources in the living heart and the determination of acoustic sources.},
  language = {en},
  timestamp = {2016-10-21T11:24:57Z},
  urldate = {2016-10-21},
  booktitle = {Mathematics \textemdash{} {{Key Technology}} for the {{Future}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Louis, Alfred K. and Schmitt, Uwe and Darvas, Felix and B{\"u}chner, Helmut and Fuchs, Manfred},
  editor = {J{\"a}ger, Willi and Krebs, Hans-Joachim},
  year = {2003},
  keywords = {Appl.Mathematics/Computational Methods of Engineering,Computational Science and Engineering,Computer Imaging; Vision; Pattern Recognition and Graphics,Math. Applications in Chemistry,Statistics for Life Sciences; Medicine; Health Sciences,visualization},
  pages = {472--482},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\WNKZSBQJ\\978-3-642-55753-8_38.html:text/html},
  doi = {10.1007/978-3-642-55753-8_38}
}

@article{Malmivuo2011,
  title = {Comparison of the {{Properties}} of {{EEG}} and {{MEG}} in {{Detecting}} the {{Electric Activity}} of the {{Brain}}},
  volume = {25},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-011-0202-1},
  abstract = {Since the detection of the first biomagnetic signals in 1963 there has been continuous discussion on the properties and relative merits of bioelectric and biomagnetic measurements. In this review article it is briefly discussed the early history of this controversy. Then the theory of the independence and interdependence of bioelectric and biomagnetic signals is explained, and a clinical study on ECG and MCG that strongly supports this theory is presented. The spatial resolutions of EEG and MEG are compared in detail, and the issue of the maximum number of electrodes in EEG is also discussed. Finally, some special properties of EEG and MEG methods are described. In brief, the conclusion is that EEG and MEG are only partially independent and their spatial resolutions are about the same. Recording both of them brings some additional information on the bioelectric activity of the brain. These two methods have certain unique properties that make either of them more beneficial in certain applications.},
  language = {en},
  timestamp = {2016-10-21T11:26:03Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Malmivuo, Jaakko},
  month = sep,
  year = {2011},
  pages = {1--19},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\AIPJNIHR\\s10548-011-0202-1.html:text/html}
}

@incollection{Mattout2003,
  series = {Lecture Notes in Computer Science},
  title = {Localization {{Estimation Algorithm}} ({{LEA}}): {{A Supervised Prior}}-{{Based Approach}} for {{Solving}} the {{EEG}}/{{MEG Inverse Problem}}},
  copyright = {\textcopyright{}2003 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-40560-3 978-3-540-45087-0},
  shorttitle = {Localization {{Estimation Algorithm}} ({{LEA}})},
  abstract = {Localizing and quantifying the sources of ElectroEncephalo-Graphy (EEG) and MagnetoEncephaloGraphy (MEG) measurements is an ill-posed inverse problem, whose solution requires a spatial regularization involving both anatomical and functional priors. The distributed source model enables the introduction of such constraints. However, the resulting solution is unstable since the equation system one has to solve is badly conditioned and under-determined. We propose an original approach for solving the inverse problem, that allows to deal with a better-determined system and to temper the influence of priors according to their consistency with the measured EEG/MEG data. This Localization Estimation Algorithm (LEA) estimates the amplitude of a selected subset of sources, which are localized based on a prior distribution of activation probability. LEA is evaluated through numerical simulations and compared to a classical Weighted Minimum Norm estimation.},
  language = {en},
  timestamp = {2016-10-21T11:34:18Z},
  number = {2732},
  urldate = {2016-10-21},
  booktitle = {Information {{Processing}} in {{Medical Imaging}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mattout, J{\'e}r{\'e}mie and P{\'e}l{\'e}grini-Issac, M{\'e}lanie and Bellio, Anne and Daunizeau, Jean and Benali, Habib},
  editor = {Taylor, Chris and Noble, J. Alison},
  month = jul,
  year = {2003},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Health Informatics,Image Processing and Computer Vision,Imaging / Radiology,Science; general},
  pages = {536--547},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PKXHN932\\978-3-540-45087-0_45.html:text/html},
  doi = {10.1007/978-3-540-45087-0_45}
}

@article{Tian2013,
  title = {{{EEG}}/{{MEG Source Reconstruction}} with {{Spatial}}-{{Temporal Two}}-{{Way Regularized Regression}}},
  volume = {11},
  issn = {1539-2791, 1559-0089},
  doi = {10.1007/s12021-013-9193-2},
  abstract = {In this work, we propose a spatial-temporal two-way regularized regression method for reconstructing neural source signals from EEG/MEG time course measurements. The proposed method estimates the dipole locations and amplitudes simultaneously through minimizing a single penalized least squares criterion. The novelty of our methodology is the simultaneous consideration of three desirable properties of the reconstructed source signals, that is, spatial focality, spatial smoothness, and temporal smoothness. The desirable properties are achieved by using three separate penalty functions in the penalized regression framework. Specifically, we impose a roughness penalty in the temporal domain for temporal smoothness, and a sparsity-inducing penalty and a graph Laplacian penalty in the spatial domain for spatial focality and smoothness. We develop a computational efficient multilevel block coordinate descent algorithm to implement the method. Using a simulation study with several settings of different spatial complexity and two real MEG examples, we show that the proposed method outperforms existing methods that use only a subset of the three penalty functions.},
  language = {en},
  timestamp = {2016-10-21T11:34:46Z},
  number = {4},
  urldate = {2016-10-21},
  journal = {Neuroinformatics},
  author = {Tian, Tian Siva and Huang, Jianhua Z. and Shen, Haipeng and Li, Zhimin},
  month = jul,
  year = {2013},
  pages = {477--493},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PQF52W5C\\s12021-013-9193-2.html:text/html}
}

@incollection{David2001,
  series = {Lecture Notes in Computer Science},
  title = {A {{New Approach}} to the {{MEG}}/{{EEG Inverse Problem}} for the {{Recovery}} of {{Cortical Phase}}-{{Synchrony}}},
  copyright = {\textcopyright{}2001 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-42245-7 978-3-540-45729-9},
  abstract = {Little has been done yet to study the synchronization properties of the sources estimated from the MEG/EEG inverse problem, despite the growing interest in the role of phase relations in brain functions. In order to achieve this aim, we propose a novel approach to the MEG/EEG inverse problem based on some regularization using spectral priors: The MEG/EEG raw data are filtered in a frequency band of interest and blurred with some specific ``regularization noise'' prior to the inversion process. This formalism uses non quadratic regularization and a deterministic optimization algorithm. We proceed to Monte Carlo simulations using the chaotic R{\"o}ssler oscillators to model the neural generators. Our results demonstate that it is possible to reveal some phase-locking between brain sources with great accuracy following the computation of the inverse problem based on scalp MEG/EEG measurements.},
  language = {en},
  timestamp = {2016-10-21T12:51:18Z},
  number = {2082},
  urldate = {2016-10-21},
  booktitle = {Information {{Processing}} in {{Medical Imaging}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {David, Olivier and Garnero, Line and Varela, Francisco J.},
  editor = {Insana, Michael F. and Leahy, Richard M.},
  month = jun,
  year = {2001},
  keywords = {Artificial Intelligence (incl. Robotics),Health Informatics,Image Processing and Computer Vision,Imaging / Radiology,Internal Medicine,Pattern Recognition},
  pages = {272--285},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\GA3S9BV8\\3-540-45729-1_29.html:text/html},
  doi = {10.1007/3-540-45729-1_29}
}

@article{Silva1991,
  title = {Source Localization of {{EEG}} versus {{MEG}}: {{Empirical}} Comparison Using Visually Evoked Responses and Theoretical Considerations},
  volume = {4},
  issn = {0896-0267, 1573-6792},
  shorttitle = {Source Localization of {{EEG}} versus {{MEG}}},
  doi = {10.1007/BF01132770},
  abstract = {SummaryTheoretically, the information we can obtain about the functional localization of a source of brain activity from the scalp, for instance evoked by a sensory stimulus, is the same whether one uses EEG or MEG recordings. However, the nature of the sources and, especially of the volume conductor, poses constraints such that appreciable differences between both types of data may exist. We present here empirical and theoretical data that illustrate which are the main constraints and to what extent they may affect electric potential and magnetic field maps. The empirical data consists of visual evoked potential and magnetic fields to the appearance of a checkerboard pattern (half-visual field stimulation). The concept of equivalent dipole is presented and its limitations are discussed. It is considered that the concept of equivalent dipole (ED) yields only an approximate description of the activity of a patch of cortex. A main difference between EEG and MEG recordings is the fact that radially oriented dipoles can hardly be seen in the MEG in contrast with the EEG. Accordingly, a weak tangential dipole component is difficult to distinguish in the EEG if a strong radial component is also present. However, a combination of both methods can give useful complementary information in such cases. A factor that influences largely such differences is the model of volume conductor used. A four concentric spheres model, as commonly used for solving the inverse problem of source localization, causes appreciable errors when EEG data are used but much less in case of the MEG. The use of a model consisting of eccentric spheres fitting the four compartments, brain, CSF, skull and scalp, provides a better approximation of the real geometry of the head and allows to obtain comparable results for visual evoked potentials and magnetic fields. It is emphasized that for precise localization of EDs, especially based on EEG recordings, a realistic model of the different compartments of the head is necessary. The latter must be tailor made to a given subject using MRI-scans, in view of the large variability in head geometry between subjects.},
  language = {en},
  timestamp = {2016-10-21T12:36:35Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {da Silva, F. H. Lopes and Wieringa, H. J. and Peters, M. J.},
  year = {1991},
  pages = {133--142},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\6Z4I5J8Q\\BF01132770.html:text/html}
}

@incollection{Babiloni2004b,
  series = {Bioelectric Engineering},
  title = {Multimodal {{Imaging}} from {{Neuroelectromagnetic}} and {{Functional Magnetic Resonance Recordings}}},
  copyright = {\textcopyright{}2005 Kluwer Academic/Plenum Publishers, New York},
  isbn = {978-0-306-48112-3 978-0-387-49963-5},
  abstract = {Human neocortical processes involve temporal and spatial scales spanning several orders of magnitude, from the rapidly shifting somatosensory processes characterized by a temporal scale of milliseconds and a spatial scales of few square millimeters to the memory processes, involving time periods of seconds and spatial scale of square centimeters. Information about the brain activity can be obtained by measuring different physical variables arising from the brain processes, such as the increase in consumption of oxygen by the neural tissues or a variation of the electric potential over the scalp surface. All these variables are connected in direct or indirect way to the neural ongoing processes, and each variable has its own spatial and temporal resolution. The different neuroimaging techniques are then confined to the spatiotemporal resolution offered by the monitored variables. For instance, it is known from physiology that the temporal resolution of the hemodynamic deoxyhemoglobin increase/decrease lies in the range of 1\textendash{}2 seconds, while its spatial resolution is generally observable with the current imaging techniques at few mm scale. Today, no neuroimaging method allows a spatial resolution on a mm scale and a temporal resolution on a msec scale. Hence, it is of interest to study the possibility to integrate the information offered by the different physiological variables in a unique mathematical context. This operation is called the ``multimodal integration'' of variable X and Y, when the X variable has typically particular appealing spatial resolution property (mm scale) and the Y variable has particular attractive temporal properties (on a ms scale). Nevertheless, the issue of several temporal and spatial domains is critical in the study of the brain functions, since different properties could become observable, depending on the spatio-temporal scales at which the brain processes are measured.},
  language = {en},
  timestamp = {2016-10-21T11:39:21Z},
  urldate = {2016-10-21},
  booktitle = {Modeling and {{Imaging}} of {{Bioelectrical Activity}}},
  publisher = {{Springer US}},
  author = {Babiloni, Fabio and Cincotti, Febo},
  editor = {He, Bin},
  year = {2004},
  keywords = {Biomedical engineering,Imaging / Radiology,Neuroradiology,Neurosciences},
  pages = {251--280},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\D2Q2BRTW\\978-0-387-49963-5_8.html:text/html},
  doi = {10.1007/978-0-387-49963-5_8}
}

@article{Ueno1990,
  title = {Modeling and Source Localization of {{MEG}} Activities},
  volume = {3},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/BF01128872},
  abstract = {SummaryDuring the past decade, substantial advances in the understanding of the functional organization of the human brain have been made through the technique of MEG topographic mapping. Most of these investigations were concerned with the estimation and localization of sources which were modeled as single current dipoles positioned in a semi-infinite volume conductor with homogeneous conductivity. However, the sources in the brain are complex, and the head as a volume conductor consists of different materials with different electrical conductivities. The influence of these inhomogeneities on the MEG topography is studied by a computer simulation, modeling the sources as single or multiple dipoles located in inhomogeneous volume conductors. The computer simulation suggests some important aspects in estimation of source localization. The sources of MEG activities in human subject during sleep are also studied. A comparison of simulated MEG topographic patterns with measured data suggests that the sources of K-complexes can be modeled by two current dipoles. Sources for delta waves are analyzed by the FFT technique. The results show that the frequency distributions are different for delta waves measured by MEG and EEG techniques, leading us to conclude that at least two different sources are present. The MEG measurements have an advantage to provide important information concerning brain function which cannot be obtained using the EEG measurements.},
  language = {en},
  timestamp = {2016-10-21T11:49:20Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Ueno, Shoogo and Iramina, Keiji},
  year = {1990},
  pages = {151--165},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZHSKHB7V\\BF01128872.html:text/html}
}

@article{Murro1995,
  title = {Precision of Dipole Localization in a Spherical Volume Conductor: {{A}} Comparison of Referential {{EEG}}, Magnetoencephalography and Scalp Current Density Methods},
  volume = {8},
  issn = {0896-0267, 1573-6792},
  shorttitle = {Precision of Dipole Localization in a Spherical Volume Conductor},
  doi = {10.1007/BF01199775},
  abstract = {SummaryIn this study, we determined the influence of dipole orientation, dipole location, and number of recording sites on the precision of dipole localization in a spherical volume conductor. We compared localization from referential EEG (R-EEG), scalp current density EEG (SCD-EEG) and magnetoencephalography (MEG). Dipole orientation had a small influence on the precision of dipole localization for R-EEG and SCD-EEG. Dipole location relative to the recording sites, dipole depth, and number of recording channels strongly influenced the precision of dipole localization. Assuming equal signal to noise conditions for each recording method, MEG and SCD-EEG had a similar precision for dipole localization of a single tangential dipole source and both methods were more precise than R-EEG. However, SCD-EEG was inferior to MEG for distinguishing a single tangential current source from a pair of deeper radial current sources. In summary, these results suggest that the MEG will be most useful for localization of multiple simultaneous dipole sources.},
  language = {en},
  timestamp = {2016-10-21T11:50:22Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Murro, Anthony M. and Smith, Joseph R. and King, Don W. and Park, Young D.},
  year = {1995},
  pages = {119--125},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RQ9CFXZ9\\BF01199775.html:text/html}
}

@article{Hughes1977,
  title = {Relationship of the Magnetoencephalogram to Abnormal Activity in the Electroencephalogram},
  volume = {217},
  issn = {0340-5354, 1432-1459},
  doi = {10.1007/BF00312921},
  abstract = {SummaryThe EEG and also the MEG were recorded simultaneously from 10 patients with various types of EEG abnormalities; the characteristics of the MEG and its relationship to the EEG were investigated with the use of a digital computer. Examples are shown in which the EEG activity is poorly represented in the MEG and include one configuration of a slow wave with a slow descending limb (intermixed with a more sinusoidal waveform), posterior slow waves and the wave of the 3/sec spike and wave complex. Magnetic theory would suggest that these patterns are associated with dipolar sources oriented radially or perpendicularly to the outer surface of the cortex and each pattern is discussed in relation to this possibility. Examples are shown in which a given pattern is well represented in both MEG and EEG recordings. These include eye-blink artifact, slow waves from a tumor, diffuse theta activity, sinusoidal anterior delta rhythms and the spike of the 3/sec spike and wave complex. Magnetic theory would suggest that these patterns are likely associated with a dipolar source oriented tangentially or parallel to the cortex's surface and each pattern was appropriately discussed. An example is shown of a horizontally oriented delta EEG focus and its corresponding longitudinally oriented magnetic field. The few examples of activity recorded better in the MEG than EEG include some rare instances of slow waves associated with a known tumor, alpha activity in a patient with diffuse delta rhythms and the harmonic components of the 3/sec spike and wave complex. The MEG, as recorded with this particular magnetometer, tending to show diffuse, non-localizable changes associated with localized EEG abnormalities may have its maximum value in helping to determine the dipolar orientation of various EEG waveforms by simultaneously recording the MEG and EEG and also occasionally in its ability to record activity that is not found in the EEG.},
  language = {en},
  timestamp = {2016-10-21T12:36:14Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Journal of Neurology},
  author = {Hughes, John R. and Cohen, J. and Mayman, C. I. and Scholl, M. L. and Hendrix, D. E.},
  year = {1977},
  pages = {79--93},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9PU5QHQF\\BF00312921.html:text/html}
}

@incollection{He2013,
  title = {Electrophysiological {{Mapping}} and {{Neuroimaging}}},
  copyright = {\textcopyright{}2013 Springer Science+Business Media New York},
  isbn = {978-1-4614-5226-3 978-1-4614-5227-0},
  abstract = {Although electrical activity recorded from the exposed cerebral cortex of a monkey was reported in 1875 [1], it was not until 1929 that Hans Berger, a psychiatrist in Jena, Germany, first recorded noninvasively rhythmic electrical activity from the human scalp [2], which has subsequently known as electroencephalography (EEG). Since then, EEG has become an important tool for probing brain electrical activity and aiding in clinical diagnosis of neurological disorders, due to its excellent temporal resolution in the order of msec. The first recording of magnetic fields from the human brain was reported in 1972 by David Cohen at the Massachusetts Institute of Technology [3], which led to the development of magnetoencephalography (MEG). Like EEG, MEG also enjoys high temporal resolution in detecting brain electrical activity. EEG and MEG have become two prominent methods for noninvasive assessment of brain electrical activity, providing unsurpassed temporal resolution, in neuroscience research and clinical applications such as epilepsy or sleeping disorders.},
  language = {en},
  timestamp = {2016-10-21T11:52:07Z},
  urldate = {2016-10-21},
  booktitle = {Neural {{Engineering}}},
  publisher = {{Springer US}},
  author = {He, Bin and Ding, Lei},
  editor = {He, Bin},
  year = {2013},
  keywords = {Biomedical engineering,Biophysics and Biological Physics,Imaging / Radiology,Neurosciences},
  pages = {499--543},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HHUW4FX2\\978-1-4614-5227-0_12.html:text/html},
  doi = {10.1007/978-1-4614-5227-0_12}
}

@article{Kakisaka2013,
  title = {Sensitivity of Scalp 10\textendash{}20 {{EEG}} and Magnetoencephalography},
  volume = {15},
  issn = {1950-6945, 1950-6945},
  doi = {10.1684/epd.2013.0554},
  abstract = {Although previous studies have investigated the sensitivity of electroencephalography (EEG) and magnetoencephalography (MEG) to detect spikes by comparing simultaneous recordings, there are no published reports that focus on the relationship between spike dipole orientation or sensitivity of scalp EEG/MEG and the ``gold standard'' of intracranial recording (stereotactic EEG). We evaluated two patients with focal epilepsy; one with lateral temporal focus and the other with insular focus. Two MEG recordings were performed for both patients, each recorded simultaneously with initially scalp EEG, based on international 10\textendash{}20 electrode placement with additional electrodes for anterior temporal regions, and subsequently stereotactic EEG. Localisation of MEG spike dipoles from both studieswas concordant and all MEG spikes were detected by stereotactic EEG. For the patient with lateral temporal epilepsy, spike sensitivity of MEG and scalp EEG (relative to stereotactic EEG) was 55 and 0\%, respectively. Of note, in this case, MEG spike dipoles were oriented tangentially to scalp surface in a tight cluster; the angle of the spike dipole to the vertical line was 3.6 degrees. For the patient with insular epilepsy, spike sensitivity of MEG and scalp EEG (relative to stereotactic EEG) was 83 and 44\%, respectively; the angle of the spike dipole to the vertical line was 45.3 degrees. For the patient with lateral temporal epilepsy, tangential spikes from the lateral temporal cortex were difficult to detect based on scalp 10\textendash{}20 EEG and for the patient with insular epilepsy, it was possible to evaluate operculum insular sources using MEG.We believe that these findings may be important for the interpretation of clinical EEG and MEG.},
  language = {en},
  timestamp = {2016-10-21T11:53:50Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Epileptic Disorders},
  author = {Kakisaka, Yosuke and Alkawadri, Rafeed and Wang, Zhong I. and Enatsu, Rei and Mosher, John C. and Dubarry, Anne-Sophie and Alexopoulos, Andreas V. and Burgess, Richard C.},
  month = apr,
  year = {2013},
  pages = {27--31},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\HGFQPP9F\\epd.2013.html:text/html}
}

@incollection{Baillet2010,
  series = {IFMBE Proceedings},
  title = {Academic {{Software Toolboxes}} for the {{Analysis}} of {{MEG Data}}},
  copyright = {\textcopyright{}2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-12196-8 978-3-642-12197-5},
  abstract = {Free MEG and EEG data analysis software packages springing from academic research are now widely used in published work. These toolboxes and applications are typically developed by or in close contact with researchers addressing cognitive or clinical neuroscience questions. Thus they often contain the latest methodological developments from the research community. It is therefore vital to educate MEG researchers and make them aware of the new possibilities offered by these toolboxes. The aim of this paper is to illustrate the characteristics and advantages of the various toolboxes to users and developers alike. We present each toolbox with their key features and target audience.},
  language = {en},
  timestamp = {2016-10-21T11:54:36Z},
  number = {28},
  urldate = {2016-10-21},
  booktitle = {17th {{International Conference}} on {{Biomagnetism Advances}} in {{Biomagnetism}} \textendash{} {{Biomag2010}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Baillet, S. and Tadel, F. and Leahy, R. M. and Mosher, J. C. and Delorme, A. and Makeig, S. and Oostenveld, R. and H{\"a}m{\"a}l{\"a}inen, M. and Dalal, S. S. and Zumer, J. and Clerc, M. and Wolters, C. H. and Kiebel, S. and Jensen, O.},
  editor = {Supek, Selma and Su{\v s}ac, Ana},
  year = {2010},
  keywords = {Biomedical engineering,Biophysics and Biological Physics,Connectivity,data analysis,forward modeling,Image Processing and Computer Vision,Imaging,Imaging / Radiology,mapping,MEG,source analysis},
  pages = {101--104},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NJ2J873A\\978-3-642-12197-5_19.html:text/html},
  doi = {10.1007/978-3-642-12197-5_19}
}

@article{Buchner1994,
  title = {Source Analysis of Median Nerve and Finger Stimulated Somatosensory Evoked Potentials: {{Multichannel}} Simultaneous Recording of Electric and Magnetic Fields Combined with 3d-{{MR}} Tomography},
  volume = {6},
  issn = {0896-0267, 1573-6792},
  shorttitle = {Source Analysis of Median Nerve and Finger Stimulated Somatosensory Evoked Potentials},
  doi = {10.1007/BF01211175},
  abstract = {SummaryAt the current state of technology, multichannel simultaneous recording of combined electric potentials and magnetic fields should constitute the most powerful tool for separation and localization of focal brain activity. We performed an explorative study of multichannel simultaneous electric SEPs and magnetically recorded SEFs. MEG only sees tangentially oriented sources, while EEG signals include the entire activity of the brain. These characteristics were found to be very useful in separating multiple sources with overlap of activity in time. The electrically recorded SEPs were adequately modelled by three equivalent dipoles located: (1) in the region of the brainstem, modelling the P14 peak at the scalp, (2) a tangentially oriented dipole, modelling the N20-P20 and N30-P30 peaks, and part of the P45, and (3) a radially oriented dipole, modelling the P22 peak and part of the P45, both located in the region of the somatosensory cortex. Magnetically recorded SEFs were adequately modelled by a single equivalent dipole, modelling the N20-P20 and N30-P30 peaks, located close to the posterior bank of the central sulcus, in area 3b (mean deviation: 3 mm). The tangential sources in the electrical data were located 6 mm on average from the area 3b. MEG and EEG was able to locate the sources of finger stimulated SEFs in accordance with the somatotopic arrangement along the central fissure. A combined analysis demonstrated that MEG can provide constraints to the orientation and location of sources and helps to stabilize the inverse solution in a multiple-source model of the EEG.},
  language = {en},
  timestamp = {2016-10-21T13:51:43Z},
  number = {4},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Buchner, Helmut and Fuchs, Manfred and Wischmann, Hans-Aloys and D{\"o}ssel, Olaf and Ludwig, Irene and Knepper, Achim and Berg, Patrick},
  year = {1994},
  pages = {299--310},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\C7ZUWC7N\\BF01211175.html:text/html}
}

@article{Jmail2016,
  title = {Comparison of {{Brain Networks During Interictal Oscillations}} and {{Spikes}} on {{Magnetoencephalography}} and {{Intracerebral EEG}}},
  volume = {29},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-016-0501-7},
  abstract = {Electromagnetic source localization in electroencephalography (EEG) and magnetoencephalography (MEG) allows finding the generators of transient interictal epileptiform discharges (`interictal spikes'). In intracerebral EEG (iEEG), oscillatory activity (above 30 Hz) has also been shown to be a marker of neuronal dysfunction. Still, the difference between networks involved in transient and oscillatory activities remains largely unknown. Our goal was thus to extract and compare the networks involved in interictal oscillations and spikes, and to compare the non-invasive results to those obtained directly within the brain. In five patients with both MEG and iEEG recordings, we computed correlation graphs across regions, for (1) interictal spikes and (2) epileptic oscillations around 30 Hz. We show that the corresponding networks can involve a widespread set of regions (average of 10 per patient), with only partial overlap (38 \% of the total number of regions in MEG, 50 \% in iEEG). The non-invasive results were concordant with intracerebral recordings (79 \% for the spikes and 50 \% for the oscillations). We compared our interictal results to iEEG ictal data. The regions labeled as seizure onset zone (SOZ) belonged to interictal networks in a large proportion of cases: 75 \% (resp. 58 \%) for spikes and 58 \% (resp. 33 \%) for oscillations in iEEG (resp. MEG). A subset of SOZ regions were detected by one type of discharges but not the other (25 \% for spikes and 8 \% for oscillations). Our study suggests that spike and oscillatory activities involve overlapping but distinct networks, and are complementary for presurgical mapping.},
  language = {en},
  timestamp = {2016-10-21T11:57:56Z},
  number = {5},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Jmail, Nawel and Gavaret, Martine and Bartolomei, F. and Chauvel, P. and Badier, Jean-Michel and B{\'e}nar, Christian-G.},
  month = jun,
  year = {2016},
  pages = {752--765},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9GX52EBC\\s10548-016-0501-7.html:text/html}
}

@article{Iramina1996,
  title = {Source Estimation of Spontaneous {{MEG}} Activity and Auditory Evoked Responses in Normal Subjects during Sleep},
  volume = {8},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/BF01184788},
  abstract = {SummaryThis study focuses on source estimation of spontaneous MEG activity and auditory evoked responses during sleep. Sources of K-complexes and auditory evoked responses were investigated by magnetoencephalograph (MEG) and electroencephalograph (EEG) measurements, simultaneously. Sources of K-complexes during stage 2 sleep were investigated. The MEG results suggested that the sources of K-complexes can be modeled by two current dipoles. Dipoles for the K-complexes were estimated to be located 5 mm away from the sources of the N100 components of auditory evoked responses during wakefulness. Sources of auditory evoked responses during each sleep stage were also investigated to clarify the origins of the K-complex, the vertex sharp transient, and delta waves. Estimated dipoles for the N100 component for each sleep stage were estimated to be at slightly different locations in the auditory area. Based upon results of the MEG measurements and the EEG topographies, sources of the N330 component can be modeled by multiple current dipoles, which are seen to be distributed diffusely throughout the cerebral cortex.},
  language = {en},
  timestamp = {2016-10-21T12:35:27Z},
  number = {3},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Iramina, Keiji and Ueno, Shoogo},
  year = {1996},
  pages = {297--301},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SFPMA4KZ\\BF01184788.html:text/html}
}

@incollection{Li2005,
  series = {Lecture Notes in Computer Science},
  title = {{{EEG Source Localization}} for {{Two Dipoles}} in the {{Brain Using}} a {{Combined Method}}},
  copyright = {\textcopyright{}2005 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-26972-4 978-3-540-31693-0},
  abstract = {Estimating the correct location of electric current source with the brain from electroencephalographic (EEG) recordings is a challenging analytic and computational problem. Specifically, there is no unique solution and solutions do not depend continuously on the data. This is an inverse problem from EEG to dipole source. In this paper we consider a method combining backpropagation neural network (BPNN) with nonlinear least square (NLS) method for source localization. For inverse problem, the BP neural network and the NLS method has its own advantage and disadvantage, so we use the BPNN to supply the initial value to the NLS method and then get the final result, here we select the Powell algorithm to do the NLS calculating. All these work are for the fast and accurate dipole source localization. The main purpose of using this combined method is to localize two dipole sources when they are locating at the same region of the brain. The following investigations are presented to show that this combined method used in this paper is an advanced approach for two dipole sources localization with high accuracy and fast calculating.},
  language = {en},
  timestamp = {2016-10-21T11:59:42Z},
  number = {3578},
  urldate = {2016-10-21},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} - {{IDEAL}} 2005},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Li, Zhuoming and Zhang, Yu and Zhang, Qinyu and Akutagawa, Masatake and Nagashino, Hirofumi and Shichijo, Fumio and Kinouchi, Yohsuke},
  editor = {Gallagher, Marcus and Hogan, James P. and Maire, Frederic},
  month = jul,
  year = {2005},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computers and Society,Database Management,Information Storage and Retrieval,Information Systems Applications (incl. Internet)},
  pages = {171--178},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FAD3THVT\\11508069_23.html:text/html},
  doi = {10.1007/11508069_23}
}

@article{Bradshaw2001,
  title = {Spatial {{Filter Approach}} for {{Comparison}} of the {{Forward}} and {{Inverse Problems}} of {{Electroencephalography}} and {{Magnetoencephalography}}},
  volume = {29},
  issn = {0090-6964, 1573-9686},
  doi = {10.1114/1.1352641},
  abstract = {We present an analysis of the relative information content of cortical current source reconstructions from electroencephalogram (EEG) and magnetoencephalogram (MEG) forward calculations by examining the spatial filters that relate the internal sources with the externally measured electric potentials and magnetic fields. The forward spatial filters are seen to be low-pass functions of spatial frequency and spatial resolution degrades in external measurements. Inverse spatial filters may be used to reconstruct cortical sources from external data, but since they are high-pass functions of spatial frequency, they must be regularized to avoid instabilities caused by noise at higher spatial frequencies. The regularization process limits the spatial resolution of source reconstructions. EEG forward spatial filters fall off at lower spatial frequencies than MEG filters; hence, there is less information available in higher spatial frequencies resulting in lower spatial resolution in inverse reconstructions. The tangential component of the magnetic field provides even higher spatial resolution than can be obtained using the radial component. An accompanying article examines the surface Laplacian for both the EEG and the MEG. \textcopyright{} 2001 Biomedical Engineering Society.PAC01: 8710+e, 0230Zz, 8719Nn},
  language = {en},
  timestamp = {2016-10-21T12:34:30Z},
  number = {3},
  urldate = {2016-10-21},
  journal = {Annals of Biomedical Engineering},
  author = {Bradshaw, L. A. and Wijesinghe, R. S. and Wikswo, J. P.},
  year = {2001},
  pages = {214--226},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JD5DDQHS\\1.html:text/html}
}

@article{Ding2012c,
  title = {Inverse Source Imaging Methods in Recovering Distributed Brain Sources},
  volume = {2},
  issn = {2093-9868, 2093-985X},
  doi = {10.1007/s13534-012-0047-x},
  abstract = {Over the past decades tremendous efforts have been made in developing functional neuroimaging techniques to better understand human brain functions in both normal and diseased states. Towards this goal, it is essential to develop a technique that can noninvasively image human brain activity with high spatial and temporal resolution. Electroencephalography (EEG) and magnetoencephalography (MEG) are important tools for studying the human brain's large-scale neuronal dynamics, thanks to their millisecond temporal resolution. However, EEG and MEG are limited in providing spatial information concerning the location of active sources in the brain. Localizing the sources of EEG/MEG dynamics can be achieved by the so-called electrophysiological source imaging techniques. Recently, there has been a growing interest in source imaging techniques in recovering distributed brain sources. Such distributed source imaging techniques have been advanced in many aspects, including the forward modeling and the inverse imaging, and have been shown promising in many neuroscience and clinical applications. This paper reviews the basic principles, recent advancements and applications of the distributed source imaging techniques.},
  language = {en},
  timestamp = {2016-10-21T12:04:55Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Biomedical Engineering Letters},
  author = {Ding, Lei and Yuan, Han},
  month = mar,
  year = {2012},
  pages = {2--7},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\38XPFAZH\\s13534-012-0047-x.html:text/html}
}

@incollection{Ghaemmaghami2015,
  series = {Lecture Notes in Computer Science},
  title = {Movie {{Genre Classification}} by {{Exploiting MEG Brain Signals}}},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  isbn = {978-3-319-23230-0 978-3-319-23231-7},
  abstract = {Genre classification is an essential part of multimedia content recommender systems. In this study, we provide experimental evidence for the possibility of performing genre classification based on brain recorded signals. The brain decoding paradigm is employed to classify magnetoencephalography (MEG) data presented in [1] to four genre classes: Comedy, Romantic, Drama, and Horror. Our results show that: 1) there is a significant correlation between audio-visual features of movies and corresponding brain signals specially in the visual and temporal lobes; 2) the genre of movie clips can be classified with an accuracy significantly over the chance level using the MEG signal. On top of that we show that the combination of multimedia features and MEG-based features achieves the best accuracy. Our study provides a primary step towards user-centric media content retrieval using brain signals.},
  language = {en},
  timestamp = {2016-10-21T12:05:54Z},
  number = {9279},
  urldate = {2016-10-21},
  booktitle = {Image {{Analysis}} and {{Processing}} \textemdash{} {{ICIAP}} 2015},
  publisher = {{Springer International Publishing}},
  author = {Ghaemmaghami, Pouya and Abadi, Mojtaba Khomami and Kia, Seyed Mostafa and Avesani, Paolo and Sebe, Nicu},
  editor = {Murino, Vittorio and Puppo, Enrico},
  month = sep,
  year = {2015},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Brain decoding,Computer Graphics,Genre classification,Image Processing and Computer Vision,MEG,Multimedia content retrieval,Pattern Recognition,signal processing},
  pages = {683--693},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S4GMGNEB\\978-3-319-23231-7_61.html:text/html},
  doi = {10.1007/978-3-319-23231-7_61}
}

@article{Zimozdra2015,
  title = {Combining {{Electric}} and {{Magnetic Data}} to {{Solve Inverse Problems}} of {{Electroencephalography}}},
  volume = {27},
  issn = {1046-283X, 1573-837X},
  doi = {10.1007/s10598-015-9300-3},
  abstract = {The article examines two main methods of recording the activity of cerebral neuron sources \textendash{} electroand magneto-encephalography. A spherical and an ellipsoidal model of the head are considered. EEG and MEG data are shown to depend on the initial parameters (source position and orientation). The two methods can be combined for solving the inverse problem of electroencephalography.},
  language = {en},
  timestamp = {2016-10-21T14:01:13Z},
  number = {1},
  urldate = {2016-10-21},
  journal = {Computational Mathematics and Modeling},
  author = {Zimozdra, R. E.},
  month = dec,
  year = {2015},
  pages = {20--25},
  annote = {read},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XUMDT3HI\\s10598-015-9300-3.html:text/html}
}

@incollection{Jiang2007,
  series = {Lecture Notes in Computer Science},
  title = {Multiple {{Signal Classification Based}} on {{Genetic Algorithm}} for {{MEG Sources Localization}}},
  copyright = {\textcopyright{}2007 Springer Berlin Heidelberg},
  isbn = {978-3-540-72392-9 978-3-540-72393-6},
  abstract = {How to locate the neural activation sources effectively and precisely from the magnetoencephalographic (MEG) recording is a critical issue for the clinical neurology and brain functions research. Multiple signal classification (MUSIC) algorithm and recursive MUSIC algorithm are widely used to locate multiple dipolar sources from the MEG data. The drawback of these algorithms is that they run very slowly when scanning a three-dimensional head volume globally. In order to solve this problem, a novel MEG sources localization scheme based on genetic algorithm (GA) is proposed. First, this scheme uses the property of global optimum of GA to estimate the rough source location. Then, combined with grids in small area, the accurate dipolar source localization is performed. Furthermore, we introduce the adaptive crossover and mutation probability, two-point crossover operator, periodical substitution and niche strategies to overcome the disadvantage of GA which falls into local optimum occasionally. Experimental results show that the proposed scheme can improve the speed of source localization greatly and its accuracy is satisfactory.},
  language = {en},
  timestamp = {2016-10-21T12:10:03Z},
  number = {4492},
  urldate = {2016-10-21},
  booktitle = {Advances in {{Neural Networks}} \textendash{} {{ISNN}} 2007},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jiang, Chenwei and Ma, Jieming and Wang, Bin and Zhang, Liming},
  editor = {Liu, Derong and Fei, Shumin and Hou, Zengguang and Zhang, Huaguang and Sun, Changyin},
  month = jun,
  year = {2007},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Computer Communication Networks,Discrete Mathematics in Computer Science,Pattern Recognition},
  pages = {1133--1139},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QFWX9VKJ\\978-3-540-72393-6_134.html:text/html},
  doi = {10.1007/978-3-540-72393-6_134}
}

@article{Dai2011,
  title = {Source {{Connectivity Analysis}} from {{MEG}} and Its {{Application}} to {{Epilepsy Source Localization}}},
  volume = {25},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-011-0211-0},
  abstract = {We report an approach to perform source connectivity analysis from MEG, and initially evaluate this approach to interictal MEG to localize epileptogenic foci and analyze interictal discharge propagations in patients with medically intractable epilepsy. Cortical activities were reconstructed from MEG using individual realistic geometry boundary element method head models. Directional connectivity among cortical regions of interest was then estimated using directed transfer function. The MEG source connectivity analysis method was implemented in the eConnectome software, which is open-source and freely available at http://econnectome.umn.edu. As an initial evaluation, the method was applied to study MEG interictal spikes from five epilepsy patients. Estimated primary epileptiform sources were consistent with surgically resected regions, suggesting the feasibility of using cortical source connectivity analysis from interictal MEG for potential localization of epileptiform activities.},
  language = {en},
  timestamp = {2016-10-21T12:11:29Z},
  number = {2},
  urldate = {2016-10-21},
  journal = {Brain Topography},
  author = {Dai, Yakang and Zhang, Wenbo and Dickens, Deanna L. and He, Bin},
  month = nov,
  year = {2011},
  pages = {157--166},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\24B3DIUK\\s10548-011-0211-0.html:text/html}
}

@incollection{Ioannides2015,
  title = {Source-{{Estimation}} from {{Non}}-Invasive {{Recordings}} of {{Brain Electrical Activity}} in {{Sleep}} and {{Epilepsy}}},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  isbn = {978-3-319-20048-4 978-3-319-20049-1},
  abstract = {Epilepsy and sleep are characterized by spontaneously occurring large graphoelements in electroencephalography (EEG) and magnetoencephalography (MEG) recordings, such as ictal and interictal epileptiform discharges in epilepsy and K-complexes (KC) in sleep. Localization of the neural sources of these graphoelements, which is of immense clinical and research importance, requires application of electromagnetic source analysis methods. A number of such methods are available; however their ability to localize widespread synchronous cortical sources, such as the sources of KCs and widespread epileptiform discharges, is contested. Here, we used KC as an exemplar of large graphoelements with such sources to test the performance of a diverse set of commonly employed source analysis methods. We analyzed segments of sleep MEG data with clear KCs using equivalent current dipole models, beamformer methods, linear distributed source methods, and a non-linear distributed source method\textemdash{}magnetic field tomography (MFT). MFT provided the most robust and steady localization across KCs, which was also highly consistent with the intracranial findings: strong and widespread activations were reliably found in superior aspects of bilateral frontal cortex. Conversely, the localizations provided by the other methods were very variable across KCs and were all inconsistent with the intracranial findings: in many cases, the KCs were incorrectly localized in deep medial brain structures. Our current and earlier results showing the excellent localization accuracy of MFT for focal as well as extended brain sources and the smart uses of MEG and EEG in epilepsy, demonstrate that the MFT analysis of MEG signals may be a powerful tool for the studies of epilepsy, epilepsy monitoring and in pre-surgical evaluation of patients.},
  language = {en},
  timestamp = {2016-10-21T12:12:03Z},
  urldate = {2016-10-21},
  booktitle = {Cyberphysical {{Systems}} for {{Epilepsy}} and {{Related Brain Disorders}}},
  publisher = {{Springer International Publishing}},
  author = {Ioannides, Andreas A. and Liu, Lichan and Poghosyan, Vahe and Hamandi, Khalid and Kostopoulos, George K.},
  editor = {Voros, Nikolaos S. and Antonopoulos, Christos P.},
  year = {2015},
  keywords = {Biomedical engineering,Biometrics,Circuits and Systems,Health Informatics,Neurosciences,Signal; Image and Speech Processing},
  pages = {61--86},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NSEWH4P3\\978-3-319-20049-1_4.html:text/html},
  doi = {10.1007/978-3-319-20049-1_4}
}

@incollection{Mosher1999a,
  series = {Lecture Notes in Computer Science},
  title = {{{MEG Source Imaging Using Multipolar Expansions}}},
  copyright = {\textcopyright{}1999 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-66167-2 978-3-540-48714-2},
  abstract = {We describe the use of truncated multipolar expansions for producing dynamic images of cortical neural activation from measurements of the magnetoencephalogram. We use a signal-subspace method to find the locations of a set of multipolar sources, each of which represents a region of activity in the cerebral cortex. Our method builds up an estimate of the sources in a recursive manner, i.e. we first search for point current dipoles, then magnetic dipoles, and finally first order multipoles. The dynamic behavior of these sources is then computed using a linear fit to the spatiotemporal data. The final step in the procedure is to map each of the multipolar sources into an equivalent distributed source on the cortical surface. The method is demonstrated through a Monte Carlo simulation.},
  language = {en},
  timestamp = {2016-10-21T12:13:16Z},
  number = {1613},
  urldate = {2016-10-21},
  booktitle = {Information {{Processing}} in {{Medical Imaging}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mosher, John C. and Leahy, Richard M. and Shattuck, David W. and Baillet, Sylvain},
  editor = {Kuba, Attila and {\v S}{\'a}amal, Martin and Todd-Pokropek, Andrew},
  month = jun,
  year = {1999},
  keywords = {Artificial Intelligence (incl. Robotics),Health Informatics,Image Processing and Computer Vision,Internal Medicine,Multimedia Information Systems},
  pages = {15--28},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VFN5NJZ6\\3-540-48714-X_2.html:text/html},
  doi = {10.1007/3-540-48714-X_2}
}

@article{Hoey2000,
  title = {Influence of Measurement Noise and Electrode Mislocalisation on {{EEG}} Dipole-Source Localisation},
  volume = {38},
  issn = {0140-0118, 1741-0444},
  doi = {10.1007/BF02347049},
  abstract = {Measurement noise in the electro-encephalogram (EEG) and inaccurate formation about the locations of the EEG electrodes on the head induce localisation errors in the results of EEG dipole source analysis. These errors are studied by performing dipole source localisation for simulated electrode potentials in a spherical head model, for a range of different dipole locations and for two different numbers (27 and 148) of electrodes. Dipole source localisation is performed by iteratively minimising the residual energy (RE), using the simplex algorithm. The ratio of the dipole localisation error (cm) to the noise level (\%) of Gaussian measurement noise amounts to 0.15 cm/\% and 0.047 cm/\% for the 27 and 148 electrode configurations, respectively, for a radial dipole with 40\% eccentricity The localisation error due to noise can be reduced by taking into account multiple time instants of the measured potentials. In the case of random displacements of the EEG electrodes, the ratio of dipole localisation errors to electrode location errors amounts to 0.78 cm-1 cm and 0.27 cm-1 cm for the 27 and 148 electrode configurations, respectively. It is concluded that it is important to reduce the measurement noise, and particularly the electrode mislocalisation, as the influence of the latter is not reduced by taking into account multiple time instants.},
  language = {en},
  timestamp = {2016-10-21T12:34:04Z},
  number = {3},
  urldate = {2016-10-21},
  journal = {Medical and Biological Engineering and Computing},
  author = {Hoey, G. Van and Vanrumste, B. and D'Hav{\'e}, M. and de Walle, R. Van and Lemahieu, I. and Boon, P.},
  year = {2000},
  pages = {287--296},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NBH3R9QD\\BF02347049.html:text/html}
}

@incollection{Ma2006,
  series = {Lecture Notes in Computer Science},
  title = {Multiple {{Signal Classification Based}} on {{Chaos Optimization Algorithm}} for {{MEG Sources Localization}}},
  copyright = {\textcopyright{}2006 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-34482-7 978-3-540-34483-4},
  abstract = {How to localize the neural activation sources effectively and precisely from the magnetoencephalographic (MEG) recording is a critical issue for the clinical neurology and the study on brain functions. Multiple signal classification (MUSIC) algorithm and its extension referred to as recursive MUSIC algorithm are widely used to localize multiple dipolar sources from the MEG data. The drawback of these algorithms is that they run very slowly when scanning a three-dimensional head volume globally. In order to solve this problem, a novel MEG source localization method based on chaos optimization algorithm is proposed. This method uses the property of ergodicity of chaos to estimate the rough source location. Then combining with grids in small area, the accurate dipolar source localization is performed. Experimental results show that this method can improve the speed of source localization greatly and its accuracy is satisfactory.},
  language = {en},
  timestamp = {2016-10-21T12:20:28Z},
  number = {3973},
  urldate = {2016-10-21},
  booktitle = {Advances in {{Neural Networks}} - {{ISNN}} 2006},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ma, Jie-Ming and Wang, Bin and Cao, Yang and Zhang, Li-Ming},
  editor = {Wang, Jun and Yi, Zhang and Zurada, Jacek M. and Lu, Bao-Liang and Yin, Hujun},
  month = may,
  year = {2006},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Computation by Abstract Devices,Computer Communication Networks,Discrete Mathematics in Computer Science,Pattern Recognition},
  pages = {600--605},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\E2585N55\\11760191_88.html:text/html},
  doi = {10.1007/11760191_88}
}

@incollection{He2005,
  series = {Bioelectric Engineering},
  title = {Electrophysiological {{Neuroimaging}}},
  copyright = {\textcopyright{}2005 Kluwer Academic/Plenum Publishers},
  isbn = {978-0-306-48609-8 978-0-306-48610-4},
  abstract = {Although electrical activity recorded from the exposed cerebral cortex of a monkey was reported in 1875 (Caton, 1875), it was not until 1929 that Hans Berger, a psychiatrist in Jena, Germany, first recorded rhythmic electrical activity from the human head (Berger, 1929). Since then, the electroencephalogram (EEG) has become one of the most prominent methods for noninvasive examination of brain activity. Tremendous effort has been made in order to describe the phenomena of the EEG in normal individuals and in those with various diseases. In particular, the EEG has been demonstrated to be a valuable tool for both researchers and clinicians in the fields of sleep physiology and epilepsy, although other applications are also promising, such as in the fields of psychiatry and psychophysiology.},
  language = {en},
  timestamp = {2016-10-21T12:22:00Z},
  urldate = {2016-10-21},
  booktitle = {Neural {{Engineering}}},
  publisher = {{Springer US}},
  author = {He, Bin and Lian, Jie},
  editor = {He, Bin},
  year = {2005},
  keywords = {Biomedical engineering,Biomedicine general,Biophysics and Biological Physics,Imaging / Radiology,Neurosciences},
  pages = {221--261},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\264CT6EK\\0-306-48610-5_7.html:text/html},
  doi = {10.1007/0-306-48610-5_7}
}

@incollection{Mosher2000a,
  title = {Comparison of the {{Constant}} and {{Linear Boundary Element Method}} for {{EEG}} and {{MEG Forward Modeling}}},
  copyright = {\textcopyright{}2000 Springer Science+Business Media New York},
  isbn = {978-1-4612-7066-9 978-1-4612-1260-7},
  abstract = {We present a comparison of boundary element methods for solving the forward problem in EEG and MEG. We use the method of weighted residuals and focus on the collocation and Galerkin forms for constant and linear basis functions. We also examine the effect of the isolated skull approach for reducing numerical errors due to the low conductivity of the skull. We demonstrate the improvement that a linear Galerkin approach may yield in solving the forward problem.},
  language = {en},
  timestamp = {2016-10-21T12:25:01Z},
  urldate = {2016-10-21},
  booktitle = {Biomag 96},
  publisher = {{Springer New York}},
  author = {Mosher, J. C. and Chang, C. H. and Leahy, R. M.},
  editor = {Aine, Cheryl J. and Stroink, Gerhard and Wood, Charles C. and Okada, Yoshio and Swithenby, Stephen J.},
  year = {2000},
  keywords = {Physics; general},
  pages = {306--309},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\B4BZICNP\\978-1-4612-1260-7_74.html:text/html},
  doi = {10.1007/978-1-4612-1260-7_74}
}

@incollection{Munck1999,
  series = {Springer Series in Synergetics},
  title = {The {{Spatial Distribution}} of {{Spontaneous EEG}} and {{MEG}}},
  copyright = {\textcopyright{}1999 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-64219-7 978-3-642-60007-4},
  abstract = {A subject sitting at rest with electrodes attached to his or her head produces EEG-signals, even when he or she is not performing a specific task. These signals are irregular and it is impossible to predict how the signal will evolve in time with only the knowledge of their behavior in the past. This impossibility does not imply that the signals are completely random, because often characteristic rhythms can be observed when the condition of the subject is changed. A well known example is the appearance of the alpha-rhythm when the subject closes his eyes (e.g. Lopes da Silva, 1987). The precise shape of these EEG signals can not be predicted and the exact knowledge is not very useful. A meaningful description of spontaneous EEG signals can only be given in terms of their statistical properties. The same is true for the behavior of the EEG signals in the space domain. EEG maps can vary from very regular and dipolar patters to very irregular patterns that do not easily reveal where the underlying generators are located. To characterize the maps of spontaneous EEG one has to use statistics.},
  language = {en},
  timestamp = {2016-10-21T12:26:09Z},
  urldate = {2016-10-21},
  booktitle = {Analysis of {{Neurophysiological Brain Functioning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Munck, Jan C. De and Dijk, Bob W. Van},
  editor = {Uhl, Dr Christian},
  year = {1999},
  keywords = {Biophysics and Biological Physics,Mathematical and Computational Biology,Neurology,Physiological; Cellular and Medical Topics,Statistical Physics; Dynamical Systems and Complexity},
  pages = {202--228},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8Q96NM7Z\\978-3-642-60007-4_11.html:text/html},
  doi = {10.1007/978-3-642-60007-4_11}
}

@article{Gribonval2015,
  title = {Sparse and {{Spurious}}: {{Dictionary Learning With Noise}} and {{Outliers}}},
  volume = {61},
  issn = {0018-9448},
  shorttitle = {Sparse and {{Spurious}}},
  doi = {10.1109/TIT.2015.2472522},
  abstract = {A popular approach within the signal processing and machine learning communities consists in modeling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. This paper considers the case of over-complete dictionaries, noisy signals, and possible outliers, thus extending the previous work limited to noiseless settings and/or undercomplete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity, and the number of observations.},
  timestamp = {2016-10-24T16:27:37Z},
  number = {11},
  journal = {IEEE Transactions on Information Theory},
  author = {Gribonval, R. and Jenatton, R. and Bach, F.},
  month = nov,
  year = {2015},
  keywords = {audio processing,Clustering algorithms,Complexity theory,concave programming,Dictionaries,dictionary learning,Encoding,Image coding,Image processing,learning (artificial intelligence),linear codes,Machine learning,machine learning community,Noise measurement,nonconvex procedure,sample complexity,signal processing,Signal representations,Sparse coding,sparse dictionary learning,Sparse matrices,sparse signal probabilistic model,structured learning},
  pages = {6298--6319},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\X5JPZG2M\\7222421.html:text/html}
}

@article{Ubaru2016,
  title = {Improving the {{Incoherence}} of a {{Learned Dictionary}} via {{Rank Shrinkage}}},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00907},
  abstract = {This letter considers the problem of dictionary learning for sparse signal representation whose atoms have low mutual coherence. To learn such dictionaries, at each step, we first updated the dictionary using the method of optimal directions (MOD) and then applied a dictionary rank shrinkage step to decrease its mutual coherence. In the rank shrinkage step, we first compute a rank 1 decomposition of the column-normalized least squares estimate of the dictionary obtained from the MOD step. We then shrink the rank of this learned dictionary by transforming the problem of reducing the rank to a nonnegative garrotte estimation problem and solving it using a path-wise coordinate descent approach. We establish theoretical results that show that the rank shrinkage step included will reduce the coherence of the dictionary, which is further validated by experimental results. Numerical experiments illustrating the performance of the proposed algorithm in comparison to various other well-known dictionary learning algorithms are also presented.},
  timestamp = {2016-11-14T13:52:28Z},
  urldate = {2016-11-14},
  journal = {Neural Computation},
  author = {Ubaru, Shashanka and Seghouane, Abd-Krim and Saad, Yousef},
  month = oct,
  year = {2016},
  pages = {1--23},
  file = {Neural Computation Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\PEEV5TTU\\NECO_a_00907.html:text/html}
}

@article{Aydin2014a,
  title = {Combining {{EEG}} and {{MEG}} for the {{Reconstruction}} of {{Epileptic Activity Using}} a {{Calibrated Realistic Volume Conductor Model}}},
  volume = {9},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0093154},
  abstract = {To increase the reliability for the non-invasive determination of the irritative zone in presurgical epilepsy diagnosis, we introduce here a new experimental and methodological source analysis pipeline that combines the complementary information in EEG and MEG, and apply it to data from a patient, suffering from refractory focal epilepsy. Skull conductivity parameters in a six compartment finite element head model with brain anisotropy, constructed from individual MRI data, are estimated in a calibration procedure using somatosensory evoked potential (SEP) and field (SEF) data. These data are measured in a single run before acquisition of further runs of spontaneous epileptic activity. Our results show that even for single interictal spikes, volume conduction effects dominate over noise and need to be taken into account for accurate source analysis. While cerebrospinal fluid and brain anisotropy influence both modalities, only EEG is sensitive to skull conductivity and conductivity calibration significantly reduces the difference in especially depth localization of both modalities, emphasizing its importance for combining EEG and MEG source analysis. On the other hand, localization differences which are due to the distinct sensitivity profiles of EEG and MEG persist. In case of a moderate error in skull conductivity, combined source analysis results can still profit from the different sensitivity profiles of EEG and MEG to accurately determine location, orientation and strength of the underlying sources. On the other side, significant errors in skull modeling are reflected in EEG reconstruction errors and could reduce the goodness of fit to combined datasets. For combined EEG and MEG source analysis, we therefore recommend calibrating skull conductivity using additionally acquired SEP/SEF data.},
  timestamp = {2016-11-14T13:14:03Z},
  number = {3},
  urldate = {2016-11-14},
  journal = {PLoS ONE},
  author = {Aydin, {\"U}mit and Vorwerk, Johannes and K{\"u}pper, Philipp and Heers, Marcel and Kugel, Harald and Galka, Andreas and Hamid, Laith and Wellmer, J{\"o}rg and Kellinghaus, Christoph and Rampp, Stefan and Wolters, Carsten Hermann},
  month = mar,
  year = {2014},
  pmid = {24671208},
  pmcid = {PMC3966892}
}

@article{Aydin2015,
  title = {Combined {{EEG}}/{{MEG}} Can Outperform Single Modality {{EEG}} or {{MEG}} Source Reconstruction in Presurgical Epilepsy Diagnosis},
  volume = {10},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118753},
  abstract = {We investigated two important means for improving source reconstruction in presurgical epilepsy diagnosis. The first investigation is about the optimal choice of the number of epileptic spikes in averaging to (1) sufficiently reduce the noise bias for an accurate determination of the center of gravity of the epileptic activity and (2) still get an estimation of the extent of the irritative zone. The second study focuses on the differences in single modality EEG (80-electrodes) or MEG (275-gradiometers) and especially on the benefits of combined EEG/MEG (EMEG) source analysis. Both investigations were validated with simultaneous stereo-EEG (sEEG) (167-contacts) and low-density EEG (ldEEG) (21-electrodes). To account for the different sensitivity profiles of EEG and MEG, we constructed a six-compartment finite element head model with anisotropic white matter conductivity, and calibrated the skull conductivity via somatosensory evoked responses. Our results show that, unlike single modality EEG or MEG, combined EMEG uses the complementary information of both modalities and thereby allows accurate source reconstructions also at early instants in time (epileptic spike onset), i.e., time points with low SNR, which are not yet subject to propagation and thus supposed to be closer to the origin of the epileptic activity. EMEG is furthermore able to reveal the propagation pathway at later time points in agreement with sEEG, while EEG or MEG alone reconstructed only parts of it. Subaveraging provides important and accurate information about both the center of gravity and the extent of the epileptogenic tissue that neither single nor grand-averaged spike localizations can supply.},
  language = {ENG},
  timestamp = {2016-11-14T13:27:27Z},
  number = {3},
  journal = {PloS One},
  author = {Aydin, {\"U}mit and Vorwerk, Johannes and D{\"u}mpelmann, Matthias and K{\"u}pper, Philipp and Kugel, Harald and Heers, Marcel and Wellmer, J{\"o}rg and Kellinghaus, Christoph and Haueisen, Jens and Rampp, Stefan and Stefan, Hermann and Wolters, Carsten H.},
  year = {2015},
  keywords = {Action Potentials,Adolescent,Brain Mapping,Cerebral cortex,Electroencephalography,Epilepsy,Female,Humans,Magnetoencephalography,Preoperative Period},
  pages = {e0118753},
  annote = {read},
  pmid = {25761059},
  pmcid = {PMC4356563}
}

@article{Ebersole2010,
  title = {Combining {{MEG}} and {{EEG}} Source Modeling in Epilepsy Evaluations},
  volume = {27},
  issn = {1537-1603},
  doi = {10.1097/WNP.0b013e318201ffc4},
  abstract = {This article reviews the relative strengths and weaknesses of MEG and EEG source modeling for localization of epileptogenic foci. Proper interpretation of these dipole models requires an appreciation for the limitations of each technique and an understanding of the character of the cortical sources that can generate epileptiform transients identifiable in recordings of spontaneous cerebral activity. MEG is sensitive to smaller sources, is not altered by the skull and scalp, requires a simpler head model, and provides more accurate localization, but it is insensitive to radial sources. EEG requires larger sources, is attenuated and smeared by the skull/scalp, requires a more complicated head model, and provides less accurate localization; however, and most importantly, it is sensitive to all source orientations. In conclusion, the case is made that maximal clinical information is obtained when simultaneous MEG and EEG are both subjected to source modeling, either individually or in a combined fashion.},
  language = {ENG},
  timestamp = {2016-11-14T13:53:13Z},
  number = {6},
  journal = {Journal of Clinical Neurophysiology: Official Publication of the American Electroencephalographic Society},
  author = {Ebersole, John S. and Ebersole, Susan M.},
  month = dec,
  year = {2010},
  keywords = {Animals,Brain,Brain Mapping,Electroencephalography,Epilepsy,Humans,Magnetoencephalography},
  pages = {360--371},
  pmid = {21076318}
}

@article{Yoshinaga2002a,
  title = {Benefit of Simultaneous Recording of {{EEG}} and {{MEG}} in Dipole Localization},
  volume = {43},
  issn = {0013-9580},
  abstract = {PURPOSE: In this study, we tried to show that EEG and magnetoencephalography (MEG) are clinically complementary to each other and that a combination of both technologies is useful for the precise diagnosis of epileptic focus.
METHODS: We recorded EEGs and MEGs simultaneously and analyzed dipoles in seven patients with intractable localization-related epilepsy. MEG dipoles were analyzed by using a BTI Magnes 148-channel magnetometer. EEG dipoles were analyzed by using a realistically shaped four-layered head model (scalp-skull-fluid-brain) built from 2.0-mm slice magnetic resonance imaging (MRI) images.
RESULTS: (a) In two of seven patients, MEG could not detect any epileptiform discharges, whereas EEG showed clear spikes. However, dipoles estimated from the MEG data corresponding to the early phase of EEG spikes clustered at a location close to that of the EEG-detected dipole. (b) In two of seven patients, EEG showed only intermittent high-voltage slow waves (HVSs) without definite spikes. However, MEG showed clear epileptiform discharges preceding these EEG-detected HVSs. Dipoles estimated for these EEG-detected HVSs were located at a location close to that of the MEG-detected dipoles. (c) Based on the agreement of the results of these two techniques, surgical resection was performed in one patient with good results.
CONCLUSIONS: Dipole modeling of epileptiform activity by MEG and EEG sometimes provides information not obtainable with either modality used alone.},
  language = {ENG},
  timestamp = {2016-11-14T13:53:22Z},
  number = {8},
  journal = {Epilepsia},
  author = {Yoshinaga, Harumi and Nakahori, Tomoyuki and Ohtsuka, Yoko and Oka, Eiji and Kitamura, Yoshihiro and Kiriyama, Hideki and Kinugasa, Kazumasa and Miyamoto, Keiichi and Hoshida, Toru},
  month = aug,
  year = {2002},
  keywords = {Adolescent,Adult,Brain,Electroencephalography,Epilepsy,Humans,Magnetoencephalography,Male,Time Factors},
  pages = {924--928},
  pmid = {12181013}
}

@article{Ko1998,
  title = {Source Localization Determined by Magnetoencephalography and Electroencephalography in Temporal Lobe Epilepsy: Comparison with Electrocorticography: Technical Case Report},
  volume = {42},
  issn = {0148-396X},
  shorttitle = {Source Localization Determined by Magnetoencephalography and Electroencephalography in Temporal Lobe Epilepsy},
  abstract = {OBJECTIVE AND IMPORTANCE: Source modeling by magnetoencephalography (MEG) and electroencephalography (EEG) may be useful techniques for noninvasive localization of epileptogenic zones for surgery in patients with partial seizures.
CLINICAL PRESENTATION: Simultaneous recordings of MEG and EEG, obtained in two patients, were coregistered on each patient's magnetic resonance image for direct comparison of these two methods with intracranial electrocorticography.
TECHNIQUE: The average difference between MEG and EEG for localization of the same interictal spikes was approximately 2 cm in one patient and 3.8 cm in the other patient. One patient experienced a complex partial seizure during testing, which permitted comparison between interictal and ictal source localization by both MEG and EEG. The EEG ictal localization differed from the interictal one, whereas the MEG ictal and interictal localizations were more similar. In this patient, the MEG interictal source seemed to localize close to the ictal source, whereas EEG did not. The patients underwent temporal lobectomy after electrocorticography, and the results were compared with the findings of MEG and EEG. Although the results of both techniques agreed with the findings of electrocorticography, in one patient the MEG localization seemed to be more accurate. Both patients experienced good surgical outcomes.
CONCLUSION: Both MEG and EEG source localization can add useful and complementary information for epilepsy surgery evaluation. MEG seemed to be more accurate than EEG, especially when comparing interictal versus ictal localization. Further study is needed to evaluate the validity of source localization as useful noninvasive techniques to localize the epileptogenic zone.},
  language = {ENG},
  timestamp = {2016-11-14T13:44:16Z},
  number = {2},
  journal = {Neurosurgery},
  author = {Ko, D. Y. and Kufta, C. and Scaffidi, D. and Sato, S.},
  month = feb,
  year = {1998},
  keywords = {Adolescent,Adult,Cerebral cortex,Electroencephalography,Epilepsy; Temporal Lobe,Female,Follow-Up Studies,Humans,Magnetoencephalography,Male},
  pages = {414--421; discussion 421--422},
  pmid = {9482198}
}

@article{Alikhanian2013,
  title = {Adaptive Cluster Analysis Approach for Functional Localization Using Magnetoencephalography},
  volume = {7},
  issn = {1662-4548},
  doi = {10.3389/fnins.2013.00073},
  abstract = {In this paper we propose an agglomerative hierarchical clustering Ward's algorithm in tandem with the Affinity Propagation algorithm to reliably localize active brain regions from magnetoencephalography (MEG) brain signals. Reliable localization of brain areas with MEG has been difficult due to variations in signal strength, and the spatial extent of the reconstructed activity. The proposed approach to resolve this difficulty is based on adaptive clustering on reconstructed beamformer images to find locations that are consistently active across different participants and experimental conditions with high spatial resolution. Using data from a human reaching task, we show that the method allows more accurate and reliable localization from MEG data alone without using functional magnetic resonance imaging (fMRI) or any other imaging techniques.},
  timestamp = {2016-11-14T14:51:34Z},
  urldate = {2016-11-14},
  journal = {Frontiers in Neuroscience},
  author = {Alikhanian, Hooman and Crawford, J. Douglas and DeSouza, Joseph F. X. and Cheyne, Douglas O. and Blohm, Gunnar},
  month = may,
  year = {2013},
  annote = {read},
  pmid = {23675314},
  pmcid = {PMC3653128}
}

@article{Heers2014,
  title = {Spatial Correlation of Hemodynamic Changes Related to Interictal Epileptic Discharges with Electric and Magnetic Source Imaging},
  volume = {35},
  issn = {1097-0193},
  doi = {10.1002/hbm.22482},
  abstract = {INTRODUCTION: Blood oxygenation level-dependent (BOLD) signal changes at the time of interictal epileptic discharges (IEDs) identify their associated vascular/hemodynamic responses. BOLD activations and deactivations can be found within the epileptogenic zone but also at a distance. Source imaging identifies electric (ESI) and magnetic (MSI) sources of IEDs, with the advantage of a higher temporal resolution. Therefore, the objective of our study was to evaluate the spatial concordance between ESI/MSI and BOLD responses for similar IEDs.
METHODS: Twenty-one patients with similar IEDs in simultaneous electroencephalogram/functional magnetic resonance imaging (EEG/fMRI) and in simultaneous EEG/magnetoencephalogram (MEG) recordings were studied. IEDs in EEG/fMRI acquisition were analyzed in an event-related paradigm within a general linear model (GLM). ESI/MSI of averaged IEDs was performed using the Maximum Entropy on the Mean. We assessed the spatial concordance between ESI/MSI and clusters of BOLD activations/deactivations with surface-based metrics.
RESULTS: ESI/MSI were concordant with one BOLD cluster for 20/21 patients (concordance with activation: 14/21 patients, deactivation: 6/21 patients, no concordance: 1/21 patients; concordance with MSI only: 3/21, ESI only: 2/21). These BOLD clusters exhibited in 19/20 cases the most significant voxel. BOLD clusters that were spatially concordant with ESI/MSI were concordant with IEDs from invasive recordings in 8/11 patients (activations: 5/8, deactivations: 3/8).
CONCLUSION: As the results of BOLD, ESI and MSI are often concordant, they reinforce our confidence in all of them. ESI and MSI confirm the most significant BOLD cluster within BOLD maps, emphasizing the importance of these clusters for the definition of the epileptic focus.},
  language = {ENG},
  timestamp = {2016-11-22T10:24:34Z},
  number = {9},
  journal = {Human Brain Mapping},
  author = {Heers, Marcel and Hedrich, Tanguy and An, Dongmei and Dubeau, Fran{\c c}ois and Gotman, Jean and Grova, Christophe and Kobayashi, Eliane},
  month = sep,
  year = {2014},
  keywords = {Adult,Brain,Brain Mapping,Cerebrovascular Circulation,EEG-functional magnetic resonance imaging (EEG/fMRI),electric source imaging,electroencephalogram (EEG),Electroencephalography,Epilepsies; Partial,focal epilepsy,Hemodynamics,Humans,Intracranial EEG,Linear Models,magnetic resonance imaging,magnetic source imaging,magnetoencephalogram (MEG),Magnetoencephalography,Oxygen,Retrospective Studies,Signal Processing; Computer-Assisted,Young Adult},
  pages = {4396--4414},
  pmid = {24615912}
}

@article{Dassios2007,
  title = {On the Complementarity of Electroencephalography and Magnetoencephalography},
  volume = {23},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/23/6/016},
  abstract = {We show that for the spherical model of the brain, the part of the neuronal current that generates the electric potential (and therefore the electric field) lives in the orthogonal complement of the part of the current that generates the magnetic potential (and therefore the magnetic induction field). This means that for a continuously distributed neuronal current, information missing in the electroencephalographic data is precisely information that is available in the magnetoencephalographic data, and vice versa. In this way, the notion of complementarity between the imaging techniques of electroencephalography and magnetoencephalography is mathematically defined. Using this notion of complementarity and expanding the neuronal current in terms of vector spherical harmonics, which by definition provide the angular dependence of the current, we show that if the electric and the magnetic potentials in the exterior of the head are given, then we can determine certain moments of the functions which provide the radial dependence of the neuronal current. In addition to the above notion of complementarity, we also present a notion of unification of electroencephalography and magnetoencephalography by showing that they are governed respectively by the scalar and the vector invariants of a unified dyadic field describing electromagnetoencephalography.},
  language = {en},
  timestamp = {2016-11-22T10:55:34Z},
  number = {6},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Dassios, G. and Fokas, A. S. and Hadjiloizi, D.},
  year = {2007},
  pages = {2541--2549},
  annote = {read}
}

@article{Hunold2016,
  title = {{{EEG}} and {{MEG}}: Sensitivity to Epileptic Spike Activity as Function of Source Orientation and Depth},
  volume = {37},
  issn = {0967-3334},
  shorttitle = {{{EEG}} and {{MEG}}},
  doi = {10.1088/0967-3334/37/7/1146},
  abstract = {Simultaneous electroencephalography (EEG) and magnetoencephalography (MEG) recordings of neuronal activity from epileptic patients reveal situations in which either EEG or MEG or both modalities show visible interictal spikes. While different signal-to-noise ratios (SNRs) of the spikes in EEG and MEG have been reported, a quantitative relation of spike source orientation and depth as well as the background brain activity to the SNR has not been established. We investigated this quantitative relationship for both dipole and patch sources in an anatomically realistic cortex model. Altogether, 5600 dipole and 3300 patch sources were distributed on the segmented cortical surfaces of two volunteers. The sources were classified according to their quantified depths and orientations, ranging from 20 mm to 60 mm below the skin surface and radial and tangential, respectively. The source time-courses mimicked an interictal spike, and the simulated background activity emulated resting activity. Simulations were conducted with individual three-compartment boundary element models. The SNR was evaluated for 128 EEG, 102 MEG magnetometer, and 204 MEG gradiometer channels. For superficial dipole and superficial patch sources, EEG showed higher SNRs for dominantly radial orientations, and MEG showed higher values for dominantly tangential orientations. Gradiometers provided higher SNR than magnetometers for superficial sources, particularly for those with dominantly tangential orientations. The orientation dependent difference in SNR in EEG and MEG gradually changed as the sources were located deeper, where the interictal spikes generated higher SNRs in EEG compared to those in MEG for all source orientations. With deep sources, the SNRs in gradiometers and magnetometers were of the same order. To better detect spikes, both EEG and MEG should be used.},
  language = {en},
  timestamp = {2016-11-22T12:19:56Z},
  number = {7},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Hunold, A. and Funke, M. E. and Eichardt, R. and Stenroos, M. and Haueisen, J.},
  year = {2016},
  pages = {1146}
}

@article{Im2005,
  title = {Anatomically Constrained Dipole Adjustment ({{ANACONDA}}) for Accurate {{MEG}}/{{EEG}} Focal Source Localizations},
  volume = {50},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/50/20/012},
  abstract = {This paper proposes an alternative approach to enhance localization accuracy of MEG and EEG focal sources. The proposed approach assumes anatomically constrained spatio-temporal dipoles, initial positions of which are estimated from local peak positions of distributed sources obtained from a pre-execution of distributed source reconstruction. The positions of the dipoles are then adjusted on the cortical surface using a novel updating scheme named cortical surface scanning. The proposed approach has many advantages over the conventional ones: (1) as the cortical surface scanning algorithm uses spatio-temporal dipoles, it is robust with respect to noise; (2) it requires no a priori information on the numbers and initial locations of the activations; (3) as the locations of dipoles are restricted only on a tessellated cortical surface, it is physiologically more plausible than the conventional ECD model. To verify the proposed approach, it was applied to several realistic MEG/EEG simulations and practical experiments. From the several case studies, it is concluded that the anatomically constrained dipole adjustment (ANACONDA) approach will be a very promising technique to enhance accuracy of focal source localization which is essential in many clinical and neurological applications of MEG and EEG.},
  language = {en},
  timestamp = {2016-11-22T12:21:45Z},
  number = {20},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Im, Chang-Hwan and Jung, Hyun-Kyo and Fujimaki, Norio},
  year = {2005},
  pages = {4931}
}

@article{Liao2012,
  title = {Sparse Imaging of Cortical Electrical Current Densities via Wavelet Transforms},
  volume = {57},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/57/21/6881},
  abstract = {While the cerebral cortex in the human brain is of functional importance, functions defined on this structure are difficult to analyze spatially due to its highly convoluted irregular geometry. This study developed a novel L1-norm regularization method using a newly proposed multi-resolution face-based wavelet method to estimate cortical electrical activities in electroencephalography (EEG) and magnetoencephalography (MEG) inverse problems. The proposed wavelets were developed based on multi-resolution models built from irregular cortical surface meshes, which were realized in this study too. The multi-resolution wavelet analysis was used to seek sparse representation of cortical current densities in transformed domains, which was expected due to the compressibility of wavelets, and evaluated using Monte Carlo simulations. The EEG/MEG inverse problems were solved with the use of the novel L1-norm regularization method exploring the sparseness in the wavelet domain. The inverse solutions obtained from the new method using MEG data were evaluated by Monte Carlo simulations too. The present results indicated that cortical current densities could be efficiently compressed using the proposed face-based wavelet method, which exhibited better performance than the vertex-based wavelet method. In both simulations and auditory experimental data analysis, the proposed L1-norm regularization method showed better source detection accuracy and less estimation errors than other two classic methods, i.e. weighted minimum norm (wMNE) and cortical low-resolution electromagnetic tomography (cLORETA). This study suggests that the L1-norm regularization method with the use of face-based wavelets is a promising tool for studying functional activations of the human brain.},
  language = {en},
  timestamp = {2016-11-22T12:22:34Z},
  number = {21},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Liao, Ke and Zhu, Min and Ding, Lei and Valette, S{\'e}bastien and Zhang, Wenbo and Dickens, Deanna},
  year = {2012},
  pages = {6881}
}

@article{Cho2013a,
  title = {Localization of Epileptogenic Zones in {{Lennox}}\textendash{}{{Gastaut}} Syndrome Using Frequency Domain Source Imaging of Intracranial Electroencephalography: A Preliminary Investigation},
  volume = {34},
  issn = {0967-3334},
  shorttitle = {Localization of Epileptogenic Zones in {{Lennox}}\textendash{}{{Gastaut}} Syndrome Using Frequency Domain Source Imaging of Intracranial Electroencephalography},
  doi = {10.1088/0967-3334/34/2/247},
  abstract = {Although intracranial electroencephalography (iEEG) has been widely used to localize epileptogenic zones in epilepsy, visual inspection of iEEG recordings does not always result in a favorable surgical outcome, especially in secondary generalized epilepsy such as Lennox\textendash{}Gastaut syndrome (LGS). Various computational iEEG analysis methods have recently been introduced to confirm the visual inspection results. Of these methods, high gamma oscillation in iEEG has attracted interest because a series of studies have reported a close relationship between epileptogenic zones and cortical areas with high gamma oscillation. Meanwhile, frequency domain source imaging of EEG and MEG oscillations has proven to be a useful auxiliary tool for identifying rough locations of epileptogenic zones. To the best of our knowledge, however, frequency domain source imaging of high gamma iEEG oscillations has not been studied. In this study, we investigated whether the iEEG-based frequency domain source imaging of high gamma oscillation (60\textendash{}100 Hz) would be a useful supplementary tool for identifying epileptogenic zones in patients with secondary generalized epilepsy. The method was applied to three successfully operated on LGS patients, whose iEEG contained some ictal events with distinct high gamma oscillations before seizure onset. The resultant cortical source distributions were compared with surgical resection areas and with high gamma spectral power distributions on the intracranial sensor plane. While the results of the sensor-level analyses contained many spurious activities, the results of frequency domain source imaging coincided better with the surgical resection areas, suggesting that the frequency domain source imaging of iEEG high gamma oscillations might help enhance the accuracy of pre-surgical evaluations of patients with secondary generalized epilepsy.},
  language = {en},
  timestamp = {2016-11-22T12:24:06Z},
  number = {2},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Cho, Jae-Hyun and Kang, Hoon-Chul and Jung, Young-Jin and Kim, Jeong-Youn and Kim, Heung Dong and Yoon, Dae Sung and {Yong-Ho Lee} and Im, Chang-Hwan},
  year = {2013},
  pages = {247}
}

@article{Migliorelli2016,
  title = {Influence of Metallic Artifact Filtering on {{MEG}} Signals for Source Localization during Interictal Epileptiform Activity},
  volume = {13},
  issn = {1741-2552},
  doi = {10.1088/1741-2560/13/2/026029},
  abstract = {Objective. Medical intractable epilepsy is a common condition that affects 40\% of epileptic patients that generally have to undergo resective surgery. Magnetoencephalography (MEG) has been increasingly used to identify the epileptogenic foci through equivalent current dipole (ECD) modeling, one of the most accepted methods to obtain an accurate localization of interictal epileptiform discharges (IEDs). Modeling requires that MEG signals are adequately preprocessed to reduce interferences, a task that has been greatly improved by the use of blind source separation (BSS) methods. MEG recordings are highly sensitive to metallic interferences originated inside the head by implanted intracranial electrodes, dental prosthesis, etc and also coming from external sources such as pacemakers or vagal stimulators. To reduce these artifacts, a BSS-based fully automatic procedure was recently developed and validated, showing an effective reduction of metallic artifacts in simulated and real signals (Migliorelli et al 2015 J. Neural Eng. 12 [http://dx.doi.org/10.1088/1741-2560/12/4/046001] 046001 ). The main objective of this study was to evaluate its effects in the detection of IEDs and ECD modeling of patients with focal epilepsy and metallic interference. Approach. A comparison between the resulting positions of ECDs was performed: without removing metallic interference; rejecting only channels with large metallic artifacts; and after BSS-based reduction. Measures of dispersion and distance of ECDs were defined to analyze the results. Main results. The relationship between the artifact-to-signal ratio and ECD fitting showed that higher values of metallic interference produced highly scattered dipoles. Results revealed a significant reduction on dispersion using the BSS-based reduction procedure, yielding feasible locations of ECDs in contrast to the other two approaches. Significance. The automatic BSS-based method can be applied to MEG datasets affected by metallic artifacts as a processing step to improve the localization of epileptic foci.},
  language = {en},
  timestamp = {2016-11-22T12:25:32Z},
  number = {2},
  urldate = {2016-11-22},
  journal = {Journal of Neural Engineering},
  author = {Migliorelli, Carolina and Alonso, Joan F. and Romero, Sergio and Ma{\~n}anas, Miguel A. and Nowak, Rafa{\l} and Russi, Antonio},
  year = {2016},
  pages = {026029}
}

@article{Huizenga2001,
  title = {Simultaneous {{MEG}} and {{EEG}} Source Analysis},
  volume = {46},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/46/7/301},
  abstract = {A method is described to derive source and conductivity estimates in a simultaneous MEG and EEG source analysis. In addition the covariance matrix of the estimates is derived. Simulation studies with a concentric spheres model and a more realistic boundary element model indicate that this method has several advantages, even if only a few EEG sensors are added to a MEG configuration. First, a simultaneous analysis profits from the `preferred' location directions of MEG and EEG. Second, deep sources can be estimated quite accurately, which is an advantage compared to MEG. Third, superficial sources profit from accurate MEG location and from accurate EEG moment. Fourth, the radial source component can be estimated, which is an advantage compared to MEG. Fifth, the conductivities can be estimated. It is shown that conductivity estimation gives a substantial increase in precision, even if the conductivities are not identified appropriately. An illustrative analysis of empirical data supports these findings.},
  language = {en},
  timestamp = {2016-11-22T12:26:30Z},
  number = {7},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Huizenga, H. M. and van Zuijen, T. L. and Heslenfeld, D. J. and Molenaar, P. C. M.},
  year = {2001},
  pages = {1737}
}

@article{Peng2006,
  title = {The Unique Determination of the Primary Current by {{MEG}} and {{EEG}}},
  volume = {51},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/51/21/012},
  abstract = {In this paper, we use a more realistic head model with ovoid geometry for approximation of a human head. By inverting the Geselowitz equation, some analytic results on the inverse MEG problem are presented in homogenous ovoid geometry. On one hand, some information about the components of primary current is shown by the decomposition of primary current in different coordinates in the case of a special MEG sensor position. On the other hand, in the general case, using decomposition of the primary current in spherical coordinates, we show that two scalar functions which specify the tangential part of the primary current can be uniquely determined with the assumption that two scalar functions are conjugate and harmonic in terms of two variables. Hence, the tangential part of the current can be completely known from the two scalar functions. Moreover, we obtain the unique determination of the primary current by combining MEG with EEG.},
  language = {en},
  timestamp = {2016-11-22T12:30:06Z},
  number = {21},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Peng, Li and Cheng, Jin and Jin, Lu},
  year = {2006},
  pages = {5565}
}

@article{Nara2007,
  title = {Direct Reconstruction Algorithm of Current Dipoles for Vector Magnetoencephalography and Electroencephalography},
  volume = {52},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/52/13/014},
  abstract = {This paper presents a novel algorithm to reconstruct parameters of a sufficient number of current dipoles that describe data (equivalent current dipoles, ECDs, hereafter) from radial/vector magnetoencephalography (MEG) with and without electroencephalography (EEG). We assume a three-compartment head model and arbitrary surfaces on which the MEG sensors and EEG electrodes are placed. Via the multipole expansion of the magnetic field, we obtain algebraic equations relating the dipole parameters to the vector MEG/EEG data. By solving them directly, without providing initial parameter guesses and computing forward solutions iteratively, the dipole positions and moments projected onto the xy -plane (equatorial plane) are reconstructed from a single time shot of the data. In addition, when the head layers and the sensor surfaces are spherically symmetric, we show that the required data reduce to radial MEG only. This clarifies the advantage of vector MEG/EEG measurements and algorithms for a generally-shaped head and sensor surfaces. In the numerical simulations, the centroids of the patch sources are well localized using vector/radial MEG measured on the upper hemisphere. By assuming the model order to be larger than the actual dipole number, the resultant spurious dipole is shown to have a much smaller strength magnetic moment (about 0.05 times smaller when the SNR = 16 dB), so that the number of ECDs is reasonably estimated. We consider that our direct method with greatly reduced computational cost can also be used to provide a good initial guess for conventional dipolar/multipolar fitting algorithms.},
  language = {en},
  timestamp = {2016-11-22T12:32:53Z},
  number = {13},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Nara, Takaaki and Oohama, Junji and Hashimoto, Masaru and Takeda, Tsunehiro and Ando, Shigeru},
  year = {2007},
  pages = {3859}
}

@article{Cohen1987,
  title = {A Method for Combining {{MEG}} and {{EEG}} to Determine the Sources},
  volume = {32},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/32/1/013},
  abstract = {A three-step method is presented which combines an electroencephalographic (MEG) and (EEG) map over the head to solve the inverse problem (to determine the sources). This method uses the feature that the MEG does not see a radial source, but only a tangential source, while the EEG sees both. A first test is also made of the method, using computer simulation, and the results presented. The purpose of the test is to see if the method is valid with noisy MEG and EEG data, and when some modelling errors are present; a single dipole source was used in a spherical head. It was found that the method works well when the RMS noise at each map location is 5\% of the maximum MEG and EEG (readily attained in practice), but breaks down when the noise is 10\% (quite noisy data). The modelling errors involved grid size, head radius and distance to the MEG coil, and were studied only through the first step of the method; with errors in a reasonable range, this limited test again worked well.},
  language = {en},
  timestamp = {2016-11-22T12:37:58Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Cohen, D. and Cuffin, B. N.},
  year = {1987},
  pages = {85}
}

@article{Vallaghe2009,
  title = {The Adjoint Method for General {{EEG}} and {{MEG}} Sensor-Based Lead Field Equations},
  volume = {54},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/54/1/009},
  abstract = {Most of the methods for the inverse source problem in electroencephalography (EEG) and magnetoencephalography (MEG) use a lead field as an input. The lead field is the function which relates any source in the brain to its measurements at the sensors. For complex geometries, there is no analytical formula of the lead field. The common approach is to numerically compute the value of the lead field for a finite number of point sources (dipoles). There are several drawbacks: the model of the source space is fixed (a set of dipoles), and the computation can be expensive for as much as 10 000 dipoles. The common idea to bypass these problems is to compute the lead field from a sensor point of view. In this paper, we use the adjoint method to derive general EEG and MEG sensor-based lead field equations. Within a simple framework, we provide a complete review of the explicit lead field equations, and we are able to extend these equations to non-pointlike sensors.},
  language = {en},
  timestamp = {2016-11-22T12:39:51Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Vallagh{\'e}, Sylvain and Papadopoulo, Th{\'e}odore and Clerc, Maureen},
  year = {2009},
  pages = {135}
}

@article{Kybic2005,
  title = {Fast Multipole Acceleration of the {{MEG}}/{{EEG}} Boundary Element Method},
  volume = {50},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/50/19/018},
  abstract = {The accurate solution of the forward electrostatic problem is an essential first step before solving the inverse problem of magneto- and electroencephalography (MEG/EEG). The symmetric Galerkin boundary element method is accurate but cannot be used for very large problems because of its computational complexity and memory requirements. We describe a fast multipole-based acceleration for the symmetric boundary element method (BEM). It creates a hierarchical structure of the elements and approximates far interactions using spherical harmonics expansions. The accelerated method is shown to be as accurate as the direct method, yet for large problems it is both faster and more economical in terms of memory consumption.},
  language = {en},
  timestamp = {2016-11-22T12:41:21Z},
  number = {19},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Kybic, Jan and Clerc, Maureen and Faugeras, Olivier and Keriven, Renaud and Papadopoulo, Th{\'e}o},
  year = {2005},
  pages = {4695}
}

@article{Gencer2004,
  title = {Sensitivity of {{EEG}} and {{MEG}} Measurements to Tissue Conductivity},
  volume = {49},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/49/5/004},
  abstract = {Monitoring the electrical activity inside the human brain using electrical and magnetic field measurements requires a mathematical head model. Using this model the potential distribution in the head and magnetic fields outside the head are computed for a given source distribution. This is called the forward problem of the electro-magnetic source imaging. Accurate representation of the source distribution requires a realistic geometry and an accurate conductivity model. Deviation from the actual head is one of the reasons for the localization errors. In this study, the mathematical basis for the sensitivity of voltage and magnetic field measurements to perturbations from the actual conductivity model is investigated. Two mathematical expressions are derived relating the changes in the potentials and magnetic fields to conductivity perturbations. These equations show that measurements change due to secondary sources at the perturbation points. A finite element method (FEM) based formulation is developed for computing the sensitivity of measurements to tissue conductivities efficiently. The sensitivity matrices are calculated for both a concentric spheres model of the head and a realistic head model. The rows of the sensitivity matrix show that the sensitivity of a voltage measurement is greater to conductivity perturbations on the brain tissue in the vicinity of the dipole, the skull and the scalp beneath the electrodes. The sensitivity values for perturbations in the skull and brain conductivity are comparable and they are, in general, greater than the sensitivity for the scalp conductivity. The effects of the perturbations on the skull are more pronounced for shallow dipoles, whereas, for deep dipoles, the measurements are more sensitive to the conductivity of the brain tissue near the dipole. The magnetic measurements are found to be more sensitive to perturbations near the dipole location. The sensitivity to perturbations in the brain tissue is much greater when the primary source is tangential and it decreases as the dipole depth increases. The resultant linear system of equations can be used to update the initially assumed conductivity distribution for the head. They may be further exploited to image the conductivity distribution of the head from EEG and/or MEG measurements. This may be a fast and promising new imaging modality.},
  language = {en},
  timestamp = {2016-11-22T12:42:32Z},
  number = {5},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Gen{\c c}er, Nevzat G. and Acar, Can E.},
  year = {2004},
  pages = {701}
}

@article{Jonmohamadi2016,
  title = {Source-Space {{ICA}} for {{MEG}} Source Imaging},
  volume = {13},
  issn = {1741-2552},
  doi = {10.1088/1741-2560/13/1/016005},
  abstract = {Objective . One of the most widely used approaches in electroencephalography/magnetoencephalography (MEG) source imaging is application of an inverse technique (such as dipole modelling or sLORETA) on the component extracted by independent component analysis (ICA) (sensor-space ICA + inverse technique). The advantage of this approach over an inverse technique alone is that it can identify and localize multiple concurrent sources. Among inverse techniques, the minimum-variance beamformers offer a high spatial resolution. However, in order to have both high spatial resolution of beamformer and be able to take on multiple concurrent sources, sensor-space ICA + beamformer is not an ideal combination. Approach . We propose source-space ICA for MEG as a powerful alternative approach which can provide the high spatial resolution of the beamformer and handle multiple concurrent sources. The concept of source-space ICA for MEG is to apply the beamformer first and then singular value decomposition + ICA. In this paper we have compared source-space ICA with sensor-space ICA both in simulation and real MEG. The simulations included two challenging scenarios of correlated/concurrent cluster sources. Main Results . Source-space ICA provided superior performance in spatial reconstruction of source maps, even though both techniques performed equally from a temporal perspective. Real MEG from two healthy subjects with visual stimuli were also used to compare performance of sensor-space ICA and source-space ICA. We have also proposed a new variant of minimum-variance beamformer called weight-normalized linearly-constrained minimum-variance with orthonormal lead-field. Significance . As sensor-space ICA-based source reconstruction is popular in EEG and MEG imaging, and given that source-space ICA has superior spatial performance, it is expected that source-space ICA will supersede its predecessor in many applications.},
  language = {en},
  timestamp = {2016-11-22T12:44:46Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Journal of Neural Engineering},
  author = {Jonmohamadi, Yaqub and Jones, Richard D.},
  year = {2016},
  pages = {016005}
}

@article{Pursiainen2008,
  title = {{{EEG}}/{{MEG}} Forward Simulation through h- and p-Type Finite Elements},
  volume = {124},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/124/1/012041},
  abstract = {Electro/Magnetoencephalography (EEG/MEG) is a non-invasive imaging modality, in which a primary current density generated by the neural activity in the brain is to be reconstructed from external electric potential/magnetic field measurements. This work focuses on effective and accurate simulation of the EEG/MEG forward model through the h- and p-versions of the finite element method ( h - and p -FEM). The goal is to compare the effectiveness of these two versions in forward simulation. Both h- and p-type forward simulations are described and implemented, and the technical solutions found are discussed. These include, for example, suitable ways to generate a finite element mesh for a real head geometry through the use of different element types. Performances of the two implemented forward simulation types are compared by measuring directly the forward modeling error, as well as by computing reconstructions through a regularized FOCUSS (FOCal Underdetermined System Solver) algorithm. The results obtained suggest that the p -type performs better in terms of the forward modeling error. However, both types perform well in regularized FOCUSS reconstruction.},
  language = {en},
  timestamp = {2016-11-22T12:45:57Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Journal of Physics: Conference Series},
  author = {Pursiainen, S.},
  year = {2008},
  pages = {012041}
}

@article{Im2003,
  title = {Assessment Criteria for {{MEG}}/{{EEG}} Cortical Patch Tests},
  volume = {48},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/48/15/320},
  abstract = {To validate newly developed methods or implemented software for magnetoencephalography/electroencephalography (MEG/EEG) source localization problems, many researchers have used human skull phantom experiments or artificially constructed forward data sets. Between the two methods, the use of an artificial data set constructed with forward calculation attains superiority over the use of a human skull phantom in that it is simple to implement, adjust and control various conditions. Nowadays, for the forward calculation, especially for the cortically distributed source models, generating artificial activation patches on a brain cortical surface has been popularized instead of activating some point dipole sources. However, no well-established assessment criterion to validate the reconstructed results quantitatively has yet been introduced. In this paper, we suggest some assessment criteria to compare and validate the various MEG/EEG source localization techniques or implemented software applied to the cortically distributed source model. Four different criteria can be used to measure accuracy, degrees of focalization, noise-robustness, existence of spurious sources and so on. To verify the usefulness of the proposed criteria, four different results from two different noise conditions and two different reconstruction techniques were compared for several patches. The simulated results show that the new criteria can provide us with a reliable index to validate the MEG/EEG source localization techniques.},
  language = {en},
  timestamp = {2016-11-22T12:46:35Z},
  number = {15},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Im, Chang-Hwan and An, Kwang-Ok and Jung, Hyun-Kyo and Kwon, Hyukchan and Lee, Yong-Ho},
  year = {2003},
  pages = {2561}
}

@article{Stok1987,
  title = {Inverse Solutions Based on {{MEG}} and {{EEG}} Applied to Volume Conductor Analysis},
  volume = {32},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/32/1/015},
  abstract = {An inverse solution computer program, using a single current dipole in a selected volume conductor, calculates an equivalent dipole from a magneto- or electroencephalographic distribution. The program is used to evaluate several volume conductor models of the head by using one model when generating the distribution and another when calculating the equivalent dipole. Sources of errors in the equivalent dipole, namely uncertainties in the model parameters (e.g. conductivities) and noise in the MEG or EEG distribution, are investigated in the same way. A realistically shaped model of the head is introduced to investigate the extent to which sphere-shaped models can be used.},
  language = {en},
  timestamp = {2016-11-22T12:47:38Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Stok, C. J. and Meijs, J. W. H. and Peters, M. J.},
  year = {1987},
  pages = {99}
}

@article{Pruis1993,
  title = {A Comparison of Different Numerical Methods for Solving the Forward Problem in {{EEG}} and {{MEG}}},
  volume = {14},
  issn = {0967-3334},
  doi = {10.1088/0967-3334/14/4A/001},
  abstract = {In view of the complexity of the conductivity and the geometry of the human head, a numerical method would appear to be necessary for the adequate calculation of the electric potential and the magnetic induction generated by electric sources within the brain. Four numerical methods that could be used for solving this problem are the finite-difference method, the finite-element method, the boundary-element method, and the finite-volume method. These methods could be used to calculate the electric potential and the magnetic induction directly. Alternatively, they could be applied to compute the electric potential or the electric field and the magnetic induction could then be determined by numerical integration of the Biot-Savart law. The four numerical methods are briefly reviewed. Thereafter the relative merits of the methods and the various options for using them to solve the EEG and MEG problem are evaluated.},
  language = {en},
  timestamp = {2016-11-22T12:49:20Z},
  number = {4A},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Pruis, G. W. and Gilding, B. H. and Peters, M. J.},
  year = {1993},
  pages = {A1}
}

@article{Guy1993,
  title = {{{MEG}} and {{EEG}} in Epilepsy: Is There a Difference?},
  volume = {14},
  issn = {0967-3334},
  shorttitle = {{{MEG}} and {{EEG}} in Epilepsy},
  doi = {10.1088/0967-3334/14/4A/018},
  abstract = {Interictal epileptiform activity recorded by scalp EEG, foramen ovale electrodes and MEG is discussed. Gross differences in waveform between the electric and magnetic records are discussed in the light of intracranial depth recordings.},
  language = {en},
  timestamp = {2016-11-22T12:49:47Z},
  number = {4A},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Guy, C. N. and Walker, S. and Alarcon, G. and Binnie, C. D. and Chesterman, P. and Fenwick, P. and Smith, S.},
  year = {1993},
  pages = {A99}
}

@article{Berg1991,
  title = {Dipole Modelling of Eye Activity and Its Application to the Removal of Eye Artefacts from the {{EEG}} and {{MEG}}},
  volume = {12},
  issn = {0143-0815},
  doi = {10.1088/0143-0815/12/A/010},
  abstract = {The spatio-temporal dipole model approach has been used to identify the difference dipoles arising from changes in the ocular dipoles due to eye movements and blinks. Based on these results a method has been developed to remove eye artefacts from electrical or magnetic data. The method avoids distortions due to the head model by determining the spatial distribution of the signals from the eyes empirically. Using simultaneous modelling of the EEG or MEG activity with dipole sources distributed within the head together with the empirically determined spatial eye components, the eye activity can be estimated and removed from the EEG or MEG. This greatly reduces the distortion to the topography that is a concomitant of previous eye artefact correction methods. The advantages of the method are illustrated using simulated and real electrical data.},
  language = {en},
  timestamp = {2016-11-22T12:50:49Z},
  number = {A},
  urldate = {2016-11-22},
  journal = {Clinical Physics and Physiological Measurement},
  author = {Berg, P. and Scherg, M.},
  year = {1991},
  pages = {49}
}

@article{Volegov2004,
  title = {Noise-Free Magnetoencephalography Recordings of Brain Function},
  volume = {49},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/49/10/020},
  abstract = {Perhaps the greatest impediment to acquiring high-quality magnetoencephalography (MEG) recordings is the ubiquitous ambient magnetic field noise. We have designed and built a whole-head MEG system using a helmet-like superconducting imaging surface (SIS) surrounding the array of superconducting quantum interference device (SQUID) magnetometers used to measure the MEG signal. We previously demonstrated that the SIS passively shields the SQUID array from ambient magnetic field noise, independent of frequency, by 25\textendash{}60 dB depending on sensor location. SQUID 'reference sensors' located on the outside of the SIS helmet measure ambient magnetic fields in very close proximity to the MEG magnetometers while being nearly perfectly shielded from all sources in the brain. The fact that the reference sensors measure no brain signal yet are located in close proximity to the MEG sensors enables very accurate estimation and subtraction of the ambient field noise contribution to the MEG sensors using an adaptive algorithm. We have demonstrated total ambient noise reduction factors in excess of 10 6 ($>$120 dB). The residual noise for most MEG SQUID channels is at or near the intrinsic SQUID noise floor, typically 2\textendash{}3 f T Hz -1/2 . We are recording MEG signals with greater signal-to-noise than equivalent EEG measurements.},
  language = {en},
  timestamp = {2016-11-22T12:51:56Z},
  number = {10},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Volegov, P. and Matlachov, A. and Mosher, J. and Espy, M. A. and Jr, R. H. Kraus},
  year = {2004},
  pages = {2117}
}

@article{Ding2011a,
  title = {Sparse Cortical Current Density Imaging in Motor Potentials Induced by Finger Movement},
  volume = {8},
  issn = {1741-2552},
  doi = {10.1088/1741-2560/8/3/036008},
  abstract = {Predominant components in electro- or magneto-encephalography (EEG/MEG) are scalp projections of synchronized neuronal electrical activity distributed over cortical structures. Reconstruction of cortical sources underlying EEG/MEG can thus be achieved with the use of the cortical current density (CCD) model. We have developed a sparse electromagnetic source imaging method based on the CCD model, named as the variation-based cortical current density (VB-SCCD) algorithm, and have shown that it has much enhanced performance in reconstructing extended cortical sources in simulations (Ding 2009 Phys. Med. Biol . 54 2683\textendash{}97). The present study aims to evaluate the performance of VB-SCCD, for the first time, using experimental data obtained from six participants. The results indicate that the VB-SCCD algorithm is able to successfully reveal spatially distributed cortical sources behind motor potentials induced by visually cued repetitive finger movements, and their dynamic patterns, with millisecond resolution. These findings of motor sources and cortical systems are supported by the physiological knowledge of motor control and evidence from various neuroimaging studies with similar experiments. Furthermore, our present results indicate the improvement of cortical source resolvability of VB-SCCD, as compared with two other classical algorithms. The proposed solver embedded in VB-SCCD is able to handle large-scale computational problems, which makes the use of high-density CCD models possible and, thus, reduces model misspecifications. The present results suggest that VB-SCCD provides high resolution source reconstruction capability and is a promising tool for studying complicated dynamic systems of brain activity for basic neuroscience and clinical neuropsychiatric research.},
  language = {en},
  timestamp = {2016-11-22T12:53:13Z},
  number = {3},
  urldate = {2016-11-22},
  journal = {Journal of Neural Engineering},
  author = {Ding, Lei and Ni, Ying and Sweeney, John and He, Bin},
  year = {2011},
  pages = {036008}
}

@article{Goncalves2000a,
  title = {The Application of Electrical Impedance Tomography to Reduce Systematic Errors in the {{EEG}} Inverse Problem - a Simulation Study},
  volume = {21},
  issn = {0967-3334},
  doi = {10.1088/0967-3334/21/3/304},
  abstract = {In this paper we propose a new method, using the principles of electrical impedance tomography (EIT), to correct for the systematic errors in the inverse problem (IP) of electroencephalography (EEG) that arise from the wrong specification of the electrical conductivities of the head compartments. By injecting known currents into pairs of electrodes and measuring the resulting potential differences recorded from the other electrodes, the equivalent conductivities of brain ($\sigma$ 3 ), skull ($\sigma$ 2 ) and scalp ($\sigma$ 1 ) can be estimated. Since the geometry of the head is assumed to be known, the electrical conductivities remain as the only unknown parameters to be estimated. These conductivities can then be used in the inverse problem of EEG. The simulations performed in this study, using a three-layer sphere to model the head, prove the feasibility of the method, theoretically. Even in the presence of simulated noise with a value of signal-to-noise ratio (SNR) equal to 10, estimations of the electrical conductivities within 5\% of the true values were obtained. Simulations showed the existence of a strong relation between errors in the skull thickness and the EIT estimated conductivities. If the skull thickness is wrongly specified, for example overestimated by a factor of two, the conductivity determined by EIT is also overestimated by a factor of two. Simulations showed that this compensation effect also works in the inverse problem of EEG. Application of the proposed method reduces systematic errors in the dipole localization, up to an amount of 1 cm. However it proved to be ineffective to decrease the dipole strength error.},
  language = {en},
  timestamp = {2016-11-22T12:53:49Z},
  number = {3},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Gon{\c c}alves, S. and de Munck, J. C. and Heethaar, R. M. and da Silva, F. H. Lopes and van Dijk, B. W.},
  year = {2000},
  pages = {379}
}

@article{Jun2006,
  title = {Improving Source Detection and Separation in a Spatiotemporal {{Bayesian}} Inference Dipole Analysis},
  volume = {51},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/51/10/004},
  abstract = {Most existing spatiotemporal multi-dipole approaches for MEG/EEG source localization assume that the dipoles are active for the full time range being analysed. If the actual time range of activity of sources is significantly shorter than the time range being analysed, the detectability, localization and time-course determination of such sources may be adversely affected, especially for weak sources. In order to improve detectability and reconstruction of such sources, it is natural to add active time range information (starting time point and ending time point of source activation) for each candidate source as unknown parameters in the analysis. However, this adds additional nonlinear free parameters that could burden the analysis and could be unfeasible for some methods. Recently, we described a spatiotemporal Bayesian inference multi-dipole analysis for the MEG/EEG inverse problem. This approach treated the number of dipoles as a free parameter, produced realistic uncertainty estimates using a Markov chain Monte Carlo numerical sampling of the posterior distribution and included a method to reduce the unwanted effects of local minima. In this paper, our spatiotemporal Bayesian inference multi-dipole analysis is extended to incorporate active time range parameters of starting and stopping time points. The properties of this analysis in comparison to the previous one without active time range parameters are demonstrated through extensive studies using both simulated and empirical MEG data.},
  language = {en},
  timestamp = {2016-11-22T12:55:24Z},
  number = {10},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Jun, Sung C. and George, John S. and Plis, Sergey M. and Ranken, Doug M. and Schmidt, David M. and Wood, C. C.},
  year = {2006},
  pages = {2395}
}

@article{Dassios2009,
  title = {On the Non-Uniqueness of the Inverse Problem Associated with Electroencephalography},
  volume = {25},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/25/11/115012},
  abstract = {We present here a quantitative characterization of the non-uniqueness for the inverse problem of electroencephalography (EEG). First, we identify the singular support of the electric potential generated by a dipolar current which is fired inside the spherical model of the brain. Next, we extend this result to a continuously distributed neuronal current and we derive the equivalent Green's integral representation. Then, using the Hansen representation of the current, we show that among the three scalar representation functions, only two are needed to represent the observed electric potential on the surface or outside the head. The scalar function that is missed by the EEG recordings is exactly the one that is recorded by magnetoencephalography (MEG). Finally, the solution of the inverse EEG problem is reduced to a specific moment problem, which is exactly solved under the minimum-current assumption.},
  language = {en},
  timestamp = {2016-11-22T12:56:14Z},
  number = {11},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Dassios, G. and Hadjiloizi, D.},
  year = {2009},
  pages = {115012}
}

@article{Plis2007,
  title = {Probabilistic Forward Model for Electroencephalography Source Analysis},
  volume = {52},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/52/17/014},
  abstract = {Source localization by electroencephalography (EEG) requires an accurate model of head geometry and tissue conductivity. The estimation of source time courses from EEG or from EEG in conjunction with magnetoencephalography (MEG) requires a forward model consistent with true activity for the best outcome. Although MRI provides an excellent description of soft tissue anatomy, a high resolution model of the skull (the dominant resistive component of the head) requires CT, which is not justified for routine physiological studies. Although a number of techniques have been employed to estimate tissue conductivity, no present techniques provide the noninvasive 3D tomographic mapping of conductivity that would be desirable. We introduce a formalism for probabilistic forward modeling that allows the propagation of uncertainties in model parameters into possible errors in source localization. We consider uncertainties in the conductivity profile of the skull, but the approach is general and can be extended to other kinds of uncertainties in the forward model. We and others have previously suggested the possibility of extracting conductivity of the skull from measured electroencephalography data by simultaneously optimizing over dipole parameters and the conductivity values required by the forward model. Using Cramer\textendash{}Rao bounds, we demonstrate that this approach does not improve localization results nor does it produce reliable conductivity estimates. We conclude that the conductivity of the skull has to be either accurately measured by an independent technique, or that the uncertainties in the conductivity values should be reflected in uncertainty in the source location estimates.},
  language = {en},
  timestamp = {2016-11-22T12:56:57Z},
  number = {17},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Plis, Sergey M. and George, John S. and Jun, Sung C. and Ranken, Doug M. and Volegov, Petr L. and Schmidt, David M.},
  year = {2007},
  pages = {5309}
}

@article{Fokas2012,
  title = {Electro-Magneto-Encephalography for the Three-Shell Model: Minimal {{L}} 2 -Norm in Spherical Geometry},
  volume = {28},
  issn = {0266-5611},
  shorttitle = {Electro-Magneto-Encephalography for the Three-Shell Model},
  doi = {10.1088/0266-5611/28/3/035010},
  abstract = {The problem of determining a continuously distributed neuronal current inside the brain within the framework of the three-shell model was analysed in Fokas (2009 J. R. Soc. Interface 6 479\textendash{}88), where it was shown that the simultaneous use of electro-encephalography and magneto-encephalography yields information about two of the three scalar functions specifying the interior current. In particular, for the spherical and ellipsoidal geometries, it is possible to determine the angular parts of these two functions, as well as to obtain certain explicit constraints satisfied by their radial parts. The complete determination of the radial parts of these two functions, as well as the determination of the third unknown function, requires some a priori assumption about the current. One such possible assumption is that the current minimizes the L 2 -norm. It is shown here that in the case of spherical geometry this assumption yields a unique and explicit formula for the current.},
  language = {en},
  timestamp = {2016-11-22T12:57:53Z},
  number = {3},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Fokas, A. S. and Kurylev, Y.},
  year = {2012},
  pages = {035010}
}

@article{Jun2002,
  title = {Fast Accurate {{MEG}} Source Localization Using a Multilayer Perceptron Trained with Real Brain Noise},
  volume = {47},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/47/14/312},
  abstract = {Iterative gradient methods such as Levenberg\textendash{}Marquardt (LM) are in widespread use for source localization from electroencephalographic (EEG) and magnetoencephalographic (MEG) signals. Unfortunately, LM depends sensitively on the initial guess, necessitating repeated runs. This, combined with LM's high per-step cost, makes its computational burden quite high. To reduce this burden, we trained a multilayer perceptron (MLP) as a real-time localizer. We used an analytical model of quasistatic electromagnetic propagation through a spherical head to map randomly chosen dipoles to sensor activities according to the sensor geometry of a 4D Neuroimaging Neuromag-122 MEG system, and trained a MLP to invert this mapping in the absence of noise or in the presence of various sorts of noise such as white Gaussian noise, correlated noise, or real brain noise. A MLP structure was chosen to trade off computation and accuracy. This MLP was trained four times, with each type of noise. We measured the effects of initial guesses on LM performance, which motivated a hybrid MLP-start-LM method, in which the trained MLP initializes LM. We also compared the localization performance of LM, MLPs, and hybrid MLP-start-LMs for realistic brain signals. Trained MLPs are much faster than other methods, while the hybrid MLP-start-LMs are faster and more accurate than fixed-4-start-LM. In particular, the hybrid MLP-start-LM initialized by a MLP trained with the real brain noise dataset is 60 times faster and is comparable in accuracy to random-20-start-LM, and this hybrid system (localization error: 0.28 cm, computation time: 36 ms) shows almost as good performance as optimal-1-start-LM (localization error: 0.23 cm, computation time: 22 ms), which initializes LM with the correct dipole location. MLPs trained with noise perform better than the MLP trained without noise, and the MLP trained with real brain noise is almost as good an initial guesser for LM as the correct dipole location.},
  language = {en},
  timestamp = {2016-11-22T12:59:08Z},
  number = {14},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Jun, Sung Chan and Pearlmutter, Barak A. and Nolte, Guido},
  year = {2002},
  pages = {2547}
}

@article{Riera1998,
  title = {{{EEG}}-Distributed Inverse Solutions for a Spherical Head Model},
  volume = {14},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/14/4/016},
  abstract = {The theoretical study of the minimum norm solution to the MEG inverse problem has been carried out in previous papers for the particular case of spherical symmetry. However, a similar study for the EEG is remarkably more difficult due to the very complicated nature of the expression relating the voltage differences on the scalp to the primary current density (PCD) even for this simple symmetry. This paper introduces the use of the electric lead field (ELF) on the dyadic formalism in the spherical coordinate system to overcome such a drawback using an expansion of the ELF in terms of longitudinal and orthogonal vector fields. This approach allows us to represent EEG Fourier coefficients on a 2-sphere in terms of a current multipole expansion. The choice of a suitable basis for the Hilbert space of the PCDs on the brain region allows the current multipole moments to be related by spatial transfer functions to the PCD spectral coefficients. Properties of the most used distributed inverse solutions are explored on the basis of these results. Also, a part of the ELF null space is completely characterized and those spherical components of the PCD which are possible silent candidates are discussed.},
  language = {en},
  timestamp = {2016-11-22T12:59:43Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Riera, J. J. and Fuentes, M. E. and Vald{\'e}s, P. A. and Oh{\'a}rriz, Y.},
  year = {1998},
  pages = {1009}
}

@article{Ermer2001,
  title = {Rapidly Recomputable {{EEG}} Forward Models for Realistic Head Shapes},
  volume = {46},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/46/4/324},
  abstract = {With the increasing availability of surface extraction techniques for magnetic resonance and x-ray computed tomography images, realistic head models can be readily generated as forward models in the analysis of electroencephalography (EEG) and magnetoencephalography (MEG) data. Inverse analysis of this data, however, requires that the forward model be computationally efficient. We propose two methods for approximating the EEG forward model using realistic head shapes. The `sensor-fitted sphere' approach fits a multilayer sphere individually to each sensor, and the `three-dimensional interpolation' scheme interpolates using a grid on which a numerical boundary element method (BEM) solution has been precomputed. We have characterized the performance of each method in terms of magnitude and subspace error metrics, as well as computational and memory requirements. We have also made direct performance comparisons with traditional spherical models. The approximation provided by the interpolative scheme had an accuracy nearly identical to full BEM, even within 3 mm of the inner skull surface. Forward model computation during inverse procedures was approximately 30 times faster than for a traditional three-shell spherical model. Cast in this framework, high-fidelity numerical solutions currently viewed as computationally prohibitive for solving the inverse problem (e.g. linear Galerkin BEM) can be rapidly recomputed in a highly efficient manner. The sensor-fitting method has a similar one-time cost to the BEM method, and while it produces some improvement over a standard three-shell sphere, its performance does not approach that of the interpolation method. In both methods, there is a one-time cost associated with precomputing the forward solution over a set of grid points.},
  language = {en},
  timestamp = {2016-11-22T13:00:25Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Ermer, John J. and Mosher, John C. and Baillet, Sylvain and Leahy, Richard M.},
  year = {2001},
  pages = {1265}
}

@article{Pursiainen2012,
  title = {Complete Electrode Model in {{EEG}}: Relationship and Differences to the Point Electrode Model},
  volume = {57},
  issn = {0031-9155},
  shorttitle = {Complete Electrode Model in {{EEG}}},
  doi = {10.1088/0031-9155/57/4/999},
  abstract = {In electroencephalography (EEG) source analysis, a primary current density generated by the neural activity of the brain is reconstructed from external electrode voltage measurements. This paper focuses on accurate and effective simulations of EEG through the complete electrode model (CEM). The CEM allows for the incorporation of the electrode size, shape and effective contact impedance into the forward simulation. Both neural currents in the brain and shunting currents between the electrodes and the skin can affect the measured voltages in the CEM. The goal of this study was to investigate the CEM by comparing it with the point electrode model (PEM), which is the current standard electrode model for EEG. We used a three-dimensional, realistic and high-resolution finite element head model as the reference computational domain in the comparison. The PEM could be formulated as a limit of the CEM, in which the effective impedance of each electrode goes to infinity and the size tends to zero. Numerical results concerning the forward and inverse errors and electrode voltage strengths with different impedances and electrode sizes are presented. Based on the results obtained, limits for extremely high and low impedance values of the shunting currents are suggested.},
  language = {en},
  timestamp = {2016-11-22T13:06:44Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Pursiainen, S. and Lucka, F. and Wolters, C. H.},
  year = {2012},
  pages = {999}
}

@article{Rodriguez2012,
  title = {Inverse Source Problems for Eddy Current Equations},
  volume = {28},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/28/1/015006},
  abstract = {We study the inverse source problem for the eddy current approximation of Maxwell equations. As for the full system of Maxwell equations, we show that a volume current source cannot be uniquely identified by knowledge of the tangential components of the electromagnetic fields on the boundary, and we characterize the space of non-radiating sources. On the other hand, we prove that the inverse source problem has a unique solution if the source is supported on the boundary of a subdomain or if it is the sum of a finite number of dipoles. We address the applicability of this result for the localization of brain activity from electroencephalography and magnetoencephalography measurements.},
  language = {en},
  timestamp = {2016-11-22T13:07:37Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Rodr{\'\i}guez, Ana Alonso and Cama{\~n}o, Jessika and Valli, Alberto},
  year = {2012},
  pages = {015006}
}

@article{Munck1991,
  title = {Symmetry Considerations in the Quasi-Static Approximation of Volume Conductor Theory},
  volume = {36},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/36/4/009},
  abstract = {In living subjects electromagnetic signals are generated which can be measured electrically with electrodes and normal amplifiers or magnetically, by means of SQUID-magnetometers. The former technique is called EEG (electro-encephalography), the latter MEG (magneto-encephalography). Since the electromagnetic field patterns are dependent on physiological processes inside the body, a study of the electromagnetic field can help to understand these physiological processes. Some theoretical problems which are posed by such a study are considered. The way in which the electromagnetic field is generated is discussed. At a microscopic level, there are different physical mechanisms responsible for the generation of the electromagnetic field. Inside the brain, there are the synaptic interactions of neurons which produce the EEG and MEG. Cardiac potentials and cardiac magnetic induction are generated by the synchronous polarization of cardiac muscle cells.},
  language = {en},
  timestamp = {2016-11-22T13:08:20Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {de Munck, J. C. and van Dijk, B. W.},
  year = {1991},
  pages = {521}
}

@article{Tanzer2005,
  title = {Representation of Bioelectric Current Sources Using {{Whitney}} Elements in the Finite Element Method},
  volume = {50},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/50/13/004},
  abstract = {Bioelectric current sources of magneto- and electroencephalograms (MEG, EEG) are usually modelled with discrete delta-function type current dipoles, despite the fact that the currents in the brain are naturally continuous throughout the neuronal tissue. In this study, we represent bioelectric current sources in terms of Whitney-type elements in the finite element method (FEM) using a tetrahedral mesh. The aim is to study how well the Whitney elements can reproduce the potential and magnetic field patterns generated by a point current dipole in a homogeneous conducting sphere. The electric potential is solved for a unit sphere model with isotropic conductivity and magnetic fields are calculated for points located on a cap outside the sphere. The computed potential and magnetic field are compared with analytical solutions for a current dipole. Relative difference measures between the FEM and analytical solutions are less than 1\%, suggesting that Whitney elements as bioelectric current sources are able to produce the same potential and magnetic field patterns as the point dipole sources.},
  language = {en},
  timestamp = {2016-11-22T13:08:50Z},
  number = {13},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Tanzer, I. O{\u g}uz and J{\"a}rvenp{\"a}{\"a}, Seppo and Nenonen, Jukka and Somersalo, Erkki},
  year = {2005},
  pages = {3023}
}

@article{Scherg1993,
  title = {Somatosensory Evoked Potentials and Magnetic Fields: Separation of Multiple Source Activities},
  volume = {14},
  issn = {0967-3334},
  shorttitle = {Somatosensory Evoked Potentials and Magnetic Fields},
  doi = {10.1088/0967-3334/14/4A/006},
  abstract = {Median nerve somatosensory evoked potentials (SEP) and magnetic fields (SEF) were recorded in two subjects with multichannel devices. Single-moving- and multiple-stationary-dipole models were compared with the brain electric source analysis program of Scherg. Subcortical sources, reflecting the afferent neural volley when entering the brainstem and leaving the thalamus, were found only in the SEP. The analysis of SEF and SEP revealed a minimum of four overlapping source activities in the region of the contralateral post- and precentral cortical projection areas. Two sources in the depth of the central sulcus could not be resolved unambiguously. The third, more superficial source, which probably reflects activation of area 1, was better defined in the source analysis of the SEP, because dipole orientation was close to radial. The fourth source was more posterior. Several problems observed in the analysis of the present MEG and EEG data suggest that the simultaneous measurement and analysis of multichannel EEG and MEG data will substantially increase spatio-temporal resolution.},
  language = {en},
  timestamp = {2016-11-22T13:09:28Z},
  number = {4A},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Scherg, M. and Buchner, H.},
  year = {1993},
  pages = {A35}
}

@article{Korber2016,
  title = {{{SQUIDs}} in Biomagnetism: A Roadmap towards Improved Healthcare},
  volume = {29},
  issn = {0953-2048},
  shorttitle = {{{SQUIDs}} in Biomagnetism},
  doi = {10.1088/0953-2048/29/11/113001},
  abstract = {Globally, the demand for improved health care delivery while managing escalating costs is a major challenge. Measuring the biomagnetic fields that emanate from the human brain already impacts the treatment of epilepsy, brain tumours and other brain disorders. This roadmap explores how superconducting technologies are poised to impact health care. Biomagnetism is the study of magnetic fields of biological origin. Biomagnetic fields are typically very weak, often in the femtotesla range, making their measurement challenging. The earliest in vivo human measurements were made with room-temperature coils. In 1963, Baule and McFee (1963 Am . Heart J . 55 [http://dx.doi.org/10.1016/0002-8703(63)90075-9] 95-6 ) reported the magnetic field produced by electric currents in the heart (`magnetocardiography'), and in 1968, Cohen (1968 Science 161 [http://dx.doi.org/10.1126/science.161.3843.784] 784-6 ) described the magnetic field generated by alpha-rhythm currents in the brain (`magnetoencephalography'). Subsequently, in 1970, Cohen et al (1970 Appl. Phys. Lett. 16 [http://dx.doi.org/10.1063/1.1653195] 278\textendash{}80 ) reported the recording of a magnetocardiogram using a Superconducting QUantum Interference Device (SQUID). Just two years later, in 1972, Cohen (1972 Science 175 [http://dx.doi.org/10.1126/science.175.4022.664] 664\textendash{}6 ) described the use of a SQUID in magnetoencephalography. These last two papers set the scene for applications of SQUIDs in biomagnetism, the subject of this roadmap. The SQUID is a combination of two fundamental properties of superconductors. The first is flux quantization\textemdash{}the fact that the magnetic flux $\Phi$ in a closed superconducting loop is quantized in units of the magnetic flux quantum, $\Phi$ 0 $\equiv$ h /2 e , $\approx$ 2.07 \texttimes{} 10 -15 Tm 2 (Deaver and Fairbank 1961 Phys. Rev. Lett. 7 [http://dx.doi.org/10.1103/PhysRevLett.7.43] 43\textendash{}6 , Doll R and N{\"a}bauer M 1961 Phys. Rev. Lett. 7 [http://dx.doi.org/10.1103/PhysRevLett.7.51] 51\textendash{}2 ). Here, h is the Planck constant and e the elementary charge. The second property is the Josephson effect, predicted in 1962 by Josephson (1962 Phys. Lett. 1 [http://dx.doi.org/10.1016/0031-9163(62)91369-0] 251\textendash{}3 ) and observed by Anderson and Rowell (1963 Phys. Rev. Lett. 10 [http://dx.doi.org/10.1103/PhysRevLett.10.230] 230\textendash{}2 ) in 1963. The Josephson junction consists of two weakly coupled superconductors separated by a tunnel barrier or other weak link. A tiny electric current is able to flow between the superconductors as a supercurrent, without developing a voltage across them. At currents above the `critical current' (maximum supercurrent), however, a voltage is developed. In 1964, Jaklevic et al (1964 Phys. Rev. Lett. 12 [http://dx.doi.org/10.1103/PhysRevLett.12.159] 159\textendash{}60 ) observed quantum interference between two Josephson junctions connected in series on a superconducting loop, giving birth to the dc SQUID. The essential property of the SQUID is that a steady increase in the magnetic flux threading the loop causes the critical current to oscillate with a period of one flux quantum. In today's SQUIDs, using conventional semiconductor readout electronics, one can typically detect a change in $\Phi$ corresponding to 10 -6 $\Phi$ 0 in one second. Although early practical SQUIDs were usually made from bulk superconductors, for example, niobium or Pb-Sn solder blobs, today's devices are invariably made from thin superconducting films patterned with photolithography or even electron lithography. An extensive description of SQUIDs and their applications can be found in the SQUID Handbooks (Clarke and Braginski 2004 Fundamentals and Technology of SQUIDs and SQUID Systems vol I [http://dx.doi.org/10.1002/3527603646] (Weinheim, Germany: Wiley-VCH), Clarke and Braginski 2006 Applications of SQUIDs and SQUID Systems vol II [http://dx.doi.org/10.1002/9783527609956] (Weinheim, Germany: Wiley-VCH)). The roadmap begins (chapter 1) with a brief review of the state-of-the-art of SQUID-based magnetometers and gradiometers for biomagnetic measurements. The magnetic field noise referred to the pick-up loop is typically a few fT Hz -1/2 , often limited by noise in the metallized thermal insulation of the dewar rather than by intrinsic SQUID noise. The authors describe a pathway to achieve an intrinsic magnetic field noise as low as 0.1 fT Hz -1/2 , approximately the Nyquist noise of the human body. They also descibe a technology to defeat dewar noise. Chapter 2 reviews the neuroscientific and clinical use of magnetoencephalography (MEG), by far the most widespread application of biomagnetism with systems containing typically 300 sensors cooled to liquid-helium temperature, 4.2 K. Two important clinical applications are presurgical mapping of focal epilepsy and of eloquent cortex in brain-tumor patients. Reducing the sensor-to-brain separation and the system noise level would both improve spatial resolution. The very recent commercial innovation that replaces the need for frequent manual transfer of liquid helium with an automated system that collects and liquefies the gas and transfers the liquid to the dewar will make MEG systems more accessible. A highly promising means of placing the sensors substantially closer to the scalp for MEG is to use high-transition-temperature (high- T c ) SQUID sensors and flux transformers (chapter 3). Operation of these devices at liquid-nitrogen temperature, 77 K, enables one to minimize or even omit metallic thermal insulation between the sensors and the dewar. Noise levels of a few fT Hz -1/2 have already been achieved, and lower values are likely. The dewars can be made relatively flexible, and thus able to be placed close to the skull irrespective of the size of the head, potentially providing higher spatial resolution than liquid-helium based systems. The successful realization of a commercial high- T c MEG system would have a major commercial impact. Chapter 4 introduces the concept of SQUID-based ultra-low-field magnetic resonance imaging (ULF MRI) operating at typically several kHz, some four orders of magnitude lower than conventional, clinical MRI machines. Potential advantages of ULF MRI include higher image contrast than for conventional MRI, enabling methodologies not currently available. Examples include screening for cancer without a contrast agent, imaging traumatic brain injury (TBI) and degenerative diseases such as Alzheimer's, and determining the elapsed time since a stroke. The major current problem with ULF MRI is that its signal-to-noise ratio (SNR) is low compared with high-field MRI. Realistic solutions to this problem are proposed, including implementing sensors with a noise level of 0.1 fT Hz -1/2 . A logical and exciting prospect (chapter 5) is to combine MEG and ULF MRI into a single system in which both signal sources are detected with the same array of SQUIDs. A prototype system is described. The combination of MEG and ULF MRI allows one to obtain structural images of the head concurrently with the recording of brain activity. Since all MEG images require an MRI to determine source locations underlying the MEG signal, the combined modality would give a precise registration of the two images; the combination of MEG with high-field MRI can produce registration errors as large as 5 mm. The use of multiple sensors for ULF MRI increases both the SNR and the field of view. Chapter 6 describes another potentially far-reaching application of ULF MRI, namely neuronal current imaging (NCI) of the brain. Currently available neuronal imaging techniques include MEG, which is fast but has relatively poor spatial resolution, perhaps 10 mm, and functional MRI (fMRI) which has a millimeter resolution but is slow, on the order of seconds, and furthermore does not directly measure neuronal signals. NCI combines the ability of direct measurement of MEG with the spatial precision of MRI. In essence, the magnetic fields generated by neural currents shift the frequency of the magnetic resonance signal at a location that is imaged by the three-dimensional magnetic field gradients that form the basis of MRI. The currently achieved sensitivity of NCI is not quite sufficient to realize its goal, but it is close. The realization of NCI would represent a revolution in functional brain imaging. Improved techniques for immunoassay are always being sought, and chapter 7 introduces an entirely new topic, magnetic nanoparticles for immunoassay. These particles are bio-funtionalized, for example with a specific antibody which binds to its corresponding antigen, if it is present. Any resulting changes in the properties of the nanoparticles are detected with a SQUID. For liquid-phase detection, there are three basic methods: AC susceptibility, magnetic relaxation and remanence measurement. These methods, which have been successfully implemented for both in vivo and ex vivo applications, are highly sensitive and, although further development is required, it appears highly likely that at least some of them will be commercialized. Chapter 8 concludes the roadmap with an assessment of the commercial market for MEG systems. Despite the huge advances that have been realized since MEG was first introduced, the number of commercial systems deployed around the world remains small, around 250 units employing about 50 000 SQUIDs. The slow adoption of this technology is undoubtedly in part due to the high cost, not least because of the need to surround the entire system in an expensive magnetically shielded room. Nonetheless, the recent introduction of automatically refilling liquid-helium systems, the ongoing reduction in sensor noise, the potential availability of high- T c SQUID systems, the availability of new and better software and the combination of MEG with ULF MRI all have the potential to increase the market size in the not-so-distant future. In particular, there is a great and growing need for better noninvasive technologies to measure brain function. There are hundreds of millions of people in the world who suffer from brain disorders such as epilepsy, stroke, dementia or depression. The enormous cost to society of these diseases can be reduced by earlier and more accurate detection and diagnosis. Once the challenges outlined in this roadmap have been met and the outstanding problems have been solved, the potential demand for SQUID-based health technology can be expected to increase by ten- if not hundred-fold.},
  language = {en},
  timestamp = {2016-11-22T13:10:21Z},
  number = {11},
  urldate = {2016-11-22},
  journal = {Superconductor Science and Technology},
  author = {K{\"o}rber, Rainer and Storm, Jan-Hendrik and Seton, Hugh and M{\"a}kel{\"a}, Jyrki P. and Paetau, Ritva and Parkkonen, Lauri and {Christoph Pfeiffer} and Riaz, Bushra and Schneiderman, Justin F. and Dong, Hui and Hwang, Seong-min and You, Lixing and Inglis, Ben and {John Clarke} and Espy, Michelle A. and Ilmoniemi, Risto J. and Magnelind, Per E. and Matlashov, Andrei N. and Nieminen, Jaakko O. and Volegov, Petr L. and Zevenhoven, Koos C. J. and H{\"o}fner, Nora and Burghoff, Martin and Enpuku, Keiji and Yang, S. Y. and Chieh, Jen-Jei and {Jukka Knuutila} and Laine, Petteri and Nenonen, Jukka},
  year = {2016},
  pages = {113001}
}

@article{Stefan1993,
  title = {Multichannel Magneto-Electroencephalography Recordings of Interictal and Ictal Activity},
  volume = {14},
  issn = {0967-3334},
  doi = {10.1088/0967-3334/14/4A/020},
  abstract = {A lobar or even a intralobar congruence was found when comparing the findings of magnetic source localization with presurgical evaluation (EEG, MRI and intraoperative ECoG) in temporal lobe epilepsy. The first dipolar activity that can be recognized during a spike-wave event ((primary focal epileptic activity (PFA))) was localized in temporal neocortical or mesial regions. Further centres of epileptic activity could be localized by the method of spike averaging by correlation. This was interpreted as propagation of the electric activity. The comparison of interictal and ictal MEG localization results showed congruency in a patient with temporal lobe epilepsy. The combination of MEG and MRI helps to build a bridge between morphological and functional localization. MEG can serve as a pointer to discrete lesions in MRI.},
  language = {en},
  timestamp = {2016-11-22T13:11:52Z},
  number = {4A},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Stefan, H. and Abraham-Fuchs, K. and Schneider, S. and Schuler, P. and Huk, W. J.},
  year = {1993},
  pages = {A109}
}

@article{Wessel2011,
  title = {Biosignal 2010: {{Advanced}} Technologies in Intensive Care and Sleep Medicine},
  volume = {32},
  issn = {0967-3334},
  shorttitle = {Biosignal 2010},
  doi = {10.1088/0967-3334/32/11/E01},
  abstract = {This focus section of Physiological Measurement follows the international conference Biosignal 2010: Advanced technologies in intensive care and sleep medicine. The conference was hosted at the Humboldt-Universit{\"a}t zu Berlin and the Charit{\'e}-Universit{\"a}tsmedizin Berlin on occasion of their bicentennial and tricentennial, respectively. The event offered an interdisciplinary platform for biomedical engineers, mathematicians, physicists and physicians to develop solutions for monitoring problems in intensive care and sleep medicine. The main objectives of Biosignal 2010 were to provide a forum for the discussion of research results and new scientific knowledge, promote personal contact and synergism, advance interaction between academia and industry and facilitate the exchange of information on new processes and equipment. The focus section papers in this issue originated from discussions and feedback during the conference. First, Lehnertz (2011) reviews time series analysis techniques to reveal coupling directions from neurophysiological data. Limitations of currently available methods such as limited time series length and required stationarity are discussed and recent developments in the analysis of transient data are described. Lenz et al (2011) present a simulation study investigating the possible combined analysis of simultaneously measured electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) data to detect causal relations in the brain, considered as a directed, task specific network with vertices and directed edges. Their results indicate that the convolution effect of the fMRI forward model imposes a challenge for parameter estimation and reduces the influence of fMRI in combined EEG\textendash{}fMRI models. Sander et al (2011) study stroke patients with lesions related to the motor system using magnetoencephalography (MEG) and electromyography (EMG). MEG\textendash{}EMG coupling phenomena were analyzed using the imaginary part of coherency and are attributed to cortico-muscular coupling driving the muscles, demonstrating the feasibility of electrophysiological and motor function analysis. Next in the focus section, De Silva and Schier (2011) investigate different wavelet methods for de-noising and tracking temporal variations of the auditory brainstem response (ABR). An ensemble based approach is successfully applied to the extraction of a fully featured ABR, enabling significant reduction of the clinical test time. Lee et al (2011) present an algorithm of respiratory rate extraction using a particle filter (PF), which is applicable to both photoplethysmogram (PPG) and electrocardiogram (ECG) signals. Their method is able to accurately extract respiratory rates for both metronome and spontaneous breathing, even during strenuous exercises. This can be an attractive feature for applications since it can be calculated online. Magagnin et al (2011) discover that non-stationarities significantly distort short-term spectral, symbolic and entropy heart rate variability (HRV) measures towards sympathetic overestimation. Therefore, they analyze HRV series recorded in healthy subjects during uncontrolled daily activities typical of 24 h Holter recordings and during predetermined levels of robotic-assisted treadmill-based physical exercise. Milde et al (2011) perform a methodological study to investigate the interaction of HRV, respiratory movements (RM) and systolic arterial blood pressure (sABP). Using time-variant partial directed coherence the respiratory sinus arrhythmia (RSA) and the Traube\textendash{}Hering\textendash{}Mayer components can be separated from the data and therefore discriminate RM and RSA. Concluding the focus section, Maier et al (2011) describe steps towards subject-specific classification in ECG-based detection of sleep apnea. The database comprises simultaneous recordings of polysomnograms (PSGs) and 8-lead Holter-ECGs. They also show that including a measure of body position in the ECG analysis can increase the apnea detection rate. Moorman et al (2011) review their previous work aiming at the early diagnosis of sepsis in premature newborn infants based on statistical signal processing and non-linear dynamics. Precursors of this bacterial infection are reduced variability and transient heart rate decelerations from hours to days prior to clinical signs of illness. They find that measurements of standard deviation, sample asymmetry and sample entropy are highly related to imminent clinical illness. Their approach provides predictive monitoring information for clinicians which evidently saves lives. Finally, Huhle et al (2012) investigate the degree of hypnosis and nociception during general anaesthesia based on heart rate variability. While standard time domain parameters lacked specificity, one symbolic dynamics parameter seems to be specifically influenced by changes in depth of hypnosis and another one appears to be adequate for specific monitoring of nociception. These retrospective results need to be confirmed in future studies, however. (Please note that this paper does not appear in this focus section and will be published in a subsequent issue.) The different problems addressed in this focus section underline the complexity of the human system. The authors demonstrate that data analyses and modelling methods may have the ability to lead to significant medical improvements. Patients as well as the whole of society will benefit from the rapid utilization of this potential in clinical practice. Therefore, our goal should always be the development of highly-sophisticated methods that improve individual diagnostics while decreasing patient risks. Acknowledgment We thank the German research foundation DFG for supporting the BIOSIGNAL 2010 conference (KU837/32-1). Niels Wessel, J{\"u}rgen Kurths, Hagen Malberg and Thomas Penzel Guest Editors References De Silva A C and Schier M A 2011 Evaluation of wavelet techniques in rapid extraction of ABR variations from underlying EEG Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S03] 1747\textendash{}61 Huhle R et al 2012 Effects of hypnosis and nociception on heart rate variability during general anaesthesia Physiol. Meas. 33 to be published Lee J et al 2011 Respiratory rate extraction from pulse oximeter and electrocardiographic recordings Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S04] 1763\textendash{}73 Lehnertz K 2011 Assessing directed interactions from neurophysiologic signals\textemdash{}an overview Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/R01] 1715\textendash{}24 Lenz M et al 2011 Joint EEG/fMRI state space model for the detection of directed interactions in human brains\textemdash{}a simulation study Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S01] 1725\textendash{}36 Magagnin V et al 2011 Non-stationarities significantly distort short-term spectral, symbolic and entropy heart rate variability indices Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S05] 1775\textendash{}86 Maier C et al 2011 Steps towards subject-specific classification in ECG-based detection of sleep apnea Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S07] 1807\textendash{}19 Milde T et al 2011 Time-variant partial directed coherence in analysis of the cardiovascular system. A methodological study Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S06] 1787\textendash{}805 Moorman J R et al 2011 Cardiovascular oscillations at the bedside: early diagnosis of neonatal sepsis using heart rate characteristics monitoring Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S08] 1821\textendash{}32 Sander T H et al 2011 Characterization of motor and somatosensory function for stroke patients Physiol. Meas. 32 [http://iopscience.iop.org/0967-3334/32/11/S02] 1737\textendash{}4},
  language = {en},
  timestamp = {2016-11-22T13:12:25Z},
  number = {11},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Wessel, Niels and Kurths, J{\"u}rgen and Malberg, Hagen and Penzel, Thomas},
  year = {2011}
}

@article{Croce2016,
  title = {{{EEG}}\textendash{}{{fMRI Bayesian}} Framework for Neural Activity Estimation: A Simulation Study},
  volume = {13},
  issn = {1741-2552},
  shorttitle = {{{EEG}}\textendash{}{{fMRI Bayesian}} Framework for Neural Activity Estimation},
  doi = {10.1088/1741-2560/13/6/066017},
  abstract = {Objective. Due to the complementary nature of electroencephalography (EEG) and functional magnetic resonance imaging (fMRI), and given the possibility of simultaneous acquisition, the joint data analysis can afford a better understanding of the underlying neural activity estimation. In this simulation study we want to show the benefit of the joint EEG\textendash{}fMRI neural activity estimation in a Bayesian framework. Approach. We built a dynamic Bayesian framework in order to perform joint EEG\textendash{}fMRI neural activity time course estimation. The neural activity is originated by a given brain area and detected by means of both measurement techniques. We have chosen a resting state neural activity situation to address the worst case in terms of the signal-to-noise ratio. To infer information by EEG and fMRI concurrently we used a tool belonging to the sequential Monte Carlo (SMC) methods: the particle filter (PF). Main results. First, despite a high computational cost, we showed the feasibility of such an approach. Second, we obtained an improvement in neural activity reconstruction when using both EEG and fMRI measurements. Significance. The proposed simulation shows the improvements in neural activity reconstruction with EEG\textendash{}fMRI simultaneous data. The application of such an approach to real data allows a better comprehension of the neural dynamics.},
  language = {en},
  timestamp = {2016-11-22T13:12:59Z},
  number = {6},
  urldate = {2016-11-22},
  journal = {Journal of Neural Engineering},
  author = {Croce, Pierpaolo and Basti, Alessio and Marzetti, Laura and Zappasodi, Filippo and Gratta, Cosimo Del},
  year = {2016},
  pages = {066017}
}

@article{Im2007a,
  title = {An {{EEG}}-Based Real-Time Cortical Rhythmic Activity Monitoring System},
  volume = {28},
  issn = {0967-3334},
  doi = {10.1088/0967-3334/28/9/011},
  abstract = {In the present study, we introduce an electroencephalography (EEG)-based, real-time, cortical rhythmic activity monitoring system which can monitor spatiotemporal changes of cortical rhythmic activity on a subject's cortical surface, not on the subject's scalp surface, with a high temporal resolution. In the monitoring system, a frequency domain inverse operator is preliminarily constructed, considering the subject's anatomical information and sensor configurations, and then the spectral current power at each cortical vertex is calculated for the Fourier transforms of successive sections of continuous data, when a particular frequency band is given. A preliminary offline simulation study using four sets of artifact-free, eye-closed, resting EEG data acquired from two dementia patients and two normal subjects demonstrates that spatiotemporal changes of cortical rhythmic activity can be monitored at the cortical level with a maximal delay time of about 200 ms, when 18 channel EEG data are analyzed under a Pentium4 3.4 GHz environment. The first pilot system is applied to two human experiments\textemdash(1) cortical alpha rhythm changes induced by opening and closing eyes and (2) cortical mu rhythm changes originated from the arm movements\textemdash{}and demonstrated the feasibility of the developed system.},
  language = {en},
  timestamp = {2016-11-22T13:14:15Z},
  number = {9},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Im, Chang-Hwan and Hwang, Han-Jeong and Che, Huije and Lee, Seunghwan},
  year = {2007},
  pages = {1101}
}

@article{Penna2014,
  title = {Impact of {{SQUIDs}} on Functional Imaging in Neuroscience},
  volume = {27},
  issn = {0953-2048},
  doi = {10.1088/0953-2048/27/4/044004},
  abstract = {This paper provides an overview on the basic principles and applications of magnetoencephalography (MEG), a technique that requires the use of many SQUIDs and thus represents one of the most important applications of superconducting electronics. Since the development of the first SQUID magnetometers, it was clear that these devices could be used to measure the ultra-low magnetic signals associated with the bioelectric activity of the neurons of the human brain. Forty years on from the first measurement of magnetic alpha rhythm by David Cohen, MEG has become a fundamental tool for the investigation of brain functions. The simple localization of cerebral sources activated by sensory stimulation performed in the early years has been successively expanded to the identification of the sequence of neuronal pool activations, thus decrypting information of the hierarchy underlying cerebral processing. This goal has been achieved thanks to the development of complex instrumentation, namely whole head MEG systems, allowing simultaneous measurement of magnetic fields all over the scalp with an exquisite time resolution. The latest trends in MEG, such as the study of brain networks, i.e. how the brain organizes itself in a coherent and stable way, are discussed. These sound applications together with the latest technological developments aimed at implementing systems able to record MEG signals and magnetic resonance imaging (MRI) of the head with the same set-up pave the way to high performance systems for brain functional investigation in the healthy and the sick population.},
  language = {en},
  timestamp = {2016-11-22T13:14:51Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Superconductor Science and Technology},
  author = {Penna, Stefania Della and Pizzella, Vittorio and Romani, Gian Luca},
  year = {2014},
  pages = {044004}
}

@article{Ahadzi2004,
  title = {Neuromagnetic Field Strength Outside the Human Head Due to Impedance Changes from Neuronal Depolarization},
  volume = {25},
  issn = {0967-3334},
  doi = {10.1088/0967-3334/25/1/040},
  abstract = {The holy grail of neuroimaging would be to have an imaging system, which could image neuronal electrical activity over milliseconds. One way to do this would be by imaging the impedance changes associated with ion channels opening in neuronal membranes in the brain during activity. In principle, we could measure this change by using electrical impedance tomography (EIT) but it is close to its threshold of detectability. With the inherent limitation in the use of electrodes, we propose a new scheme based on recording the magnetic field resulting from an injected current with superconducting quantum interference devices (SQUIDs), used in magnetoencephalography (MEG). We have performed a feasibility study using computer simulation. The head was modelled as concentric spheres to mimic the scalp, skull, cerebrospinal fluid and brain using the finite element method. The magnetic field 1 cm away from the scalp was estimated. An impedance change of 1\% in a 2 cm radius volume in the brain was modelled as the region of depolarization. A constant current of 100 $\mathrm{\mu}$A was injected into the head from diametrically opposite electrodes. The model predicts that the standing magnetic field is about 10 pT and changed by about 3 fT (0.03\%) on depolarization. The spectral noise density in a typical MEG system in the frequency band 1\textendash{}100 Hz is about 7 fT, so this places the change at the limit of detectability. This is similar to electrical recording, as in conventional EIT systems, but there may be advantages to MEG in that the magnetic field directly traverses the skull and instrumentation errors from the electrode\textendash{}skin interface will be obviated.},
  language = {en},
  timestamp = {2016-11-22T13:19:41Z},
  number = {1},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {Ahadzi, G. M. and Liston, A. D. and Bayford, R. H. and Holder, D. S.},
  year = {2004},
  pages = {365}
}

@article{Xu2004a,
  title = {An Alternative Subspace Approach to {{EEG}} Dipole Source Localization},
  volume = {49},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/49/2/010},
  abstract = {In the present study, we investigate a new approach to electroencephalography (EEG) three-dimensional (3D) dipole source localization by using a non-recursive subspace algorithm called FINES. In estimating source dipole locations, the present approach employs projections onto a subspace spanned by a small set of particular vectors (FINES vector set) in the estimated noise-only subspace instead of the entire estimated noise-only subspace in the case of classic MUSIC. The subspace spanned by this vector set is, in the sense of principal angle, closest to the subspace spanned by the array manifold associated with a particular brain region. By incorporating knowledge of the array manifold in identifying FINES vector sets in the estimated noise-only subspace for different brain regions, the present approach is able to estimate sources with enhanced accuracy and spatial resolution, thus enhancing the capability of resolving closely spaced sources and reducing estimation errors. The present computer simulations show, in EEG 3D dipole source localization, that compared to classic MUSIC, FINES has (1) better resolvability of two closely spaced dipolar sources and (2) better estimation accuracy of source locations. In comparison with RAP-MUSIC, FINES' performance is also better for the cases studied when the noise level is high and/or correlations among dipole sources exist.},
  language = {en},
  timestamp = {2016-11-22T13:20:35Z},
  number = {2},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Xu, Xiao-Liang and Xu, Bobby and He, Bin},
  year = {2004},
  pages = {327}
}

@article{Sengul2012,
  title = {An Extended {{Kalman}} Filtering Approach for the Estimation of Human Head Tissue Conductivities by Using {{EEG}} Data: A Simulation Study},
  volume = {33},
  issn = {0967-3334},
  shorttitle = {An Extended {{Kalman}} Filtering Approach for the Estimation of Human Head Tissue Conductivities by Using {{EEG}} Data},
  doi = {10.1088/0967-3334/33/4/571},
  abstract = {In this study, we propose an extended Kalman filter approach for the estimation of the human head tissue conductivities in vivo by using electroencephalogram (EEG) data. Since the relationship between the surface potentials and conductivity distribution is nonlinear, the proposed algorithm first linearizes the system and applies extended Kalman filtering. By using a three-compartment realistic head model obtained from the magnetic resonance images of a real subject, a known dipole assumption and 32 electrode positions, the performance of the proposed method is tested in simulation studies and it is shown that the proposed algorithm estimates the tissue conductivities with less than 1\% error in noiseless measurements and less than 5\% error when the signal-to-noise ratio is 40 dB or higher. We conclude that the proposed extended Kalman filter approach successfully estimates the tissue conductivities in vivo .},
  language = {en},
  timestamp = {2016-11-22T13:21:25Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Physiological Measurement},
  author = {{\c S}eng{\"u}l, G. and Baysal, U.},
  year = {2012},
  pages = {571}
}

@article{Calvetti2011,
  title = {Bayesian Mixture Models for Source Separation in {{MEG}}},
  volume = {27},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/27/11/115001},
  abstract = {This paper discusses the problem of imaging electromagnetic brain activity from measurements of the induced magnetic field outside the head. This imaging modality, magnetoencephalography (MEG), is known to be severely ill posed, and in order to obtain useful estimates for the activity map, complementary information needs to be used to regularize the problem. In this paper, a particular emphasis is on finding non-superficial focal sources that induce a magnetic field that may be confused with noise due to external sources and with distributed brain noise. The data are assumed to come from a mixture of a focal source and a spatially distributed possibly virtual source; hence, to differentiate between those two components, the problem is solved within a Bayesian framework, with a mixture model prior encoding the information that different sources may be concurrently active. The mixture model prior combines one density that favors strongly focal sources and another that favors spatially distributed sources, interpreted as clutter in the source estimation. Furthermore, to address the challenge of localizing deep focal sources, a novel depth sounding algorithm is suggested, and it is shown with simulated data that the method is able to distinguish between a signal arising from a deep focal source and a clutter signal.},
  language = {en},
  timestamp = {2016-11-22T13:22:39Z},
  number = {11},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Calvetti, Daniela and Homa, Laura and Somersalo, Erkki},
  year = {2011},
  pages = {115001}
}

@article{Nolte2003,
  title = {The Magnetic Lead Field Theorem in the Quasi-Static Approximation and Its Use for Magnetoencephalography Forward Calculation in Realistic Volume Conductors},
  volume = {48},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/48/22/002},
  abstract = {The equation for the magnetic lead field for a given magnetoencephalography (MEG) channel is well known for arbitrary frequencies $\omega$ but is not directly applicable to MEG in the quasi-static approximation. In this paper we derive an equation for $\omega$ = 0 starting from the very definition of the lead field instead of using Helmholtz's reciprocity theorems. The results are (a) the transpose of the conductivity times the lead field is divergence-free, and (b) the lead field differs from the one in any other volume conductor by a gradient of a scalar function. Consequently, for a piecewise homogeneous and isotropic volume conductor, the lead field is always tangential at the outermost surface. Based on this theoretical result, we formulated a simple and fast method for the MEG forward calculation for one shell of arbitrary shape: we correct the corresponding lead field for a spherical volume conductor by a superposition of basis functions, gradients of harmonic functions constructed here from spherical harmonics, with coefficients fitted to the boundary conditions. The algorithm was tested for a prolate spheroid of realistic shape for which the analytical solution is known. For high order in the expansion, we found the solutions to be essentially exact and for reasonable accuracies much fewer multiplications are needed than in typical implementations of the boundary element methods. The generalization to more shells is straightforward.},
  language = {en},
  timestamp = {2016-11-22T13:23:38Z},
  number = {22},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Nolte, Guido},
  year = {2003},
  pages = {3637}
}

@article{Ding2009,
  title = {Reconstructing Cortical Current Density by Exploring Sparseness in the Transform Domain},
  volume = {54},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/54/9/006},
  abstract = {In the present study, we have developed a novel electromagnetic source imaging approach to reconstruct extended cortical sources by means of cortical current density (CCD) modeling and a novel EEG imaging algorithm which explores sparseness in cortical source representations through the use of L1-norm in objective functions. The new sparse cortical current density (SCCD) imaging algorithm is unique since it reconstructs cortical sources by attaining sparseness in a transform domain (the variation map of cortical source distributions). While large variations are expected to occur along boundaries (sparseness) between active and inactive cortical regions, cortical sources can be reconstructed and their spatial extents can be estimated by locating these boundaries. We studied the SCCD algorithm using numerous simulations to investigate its capability in reconstructing cortical sources with different extents and in reconstructing multiple cortical sources with different extent contrasts. The SCCD algorithm was compared with two L2-norm solutions, i.e. weighted minimum norm estimate (wMNE) and cortical LORETA. Our simulation data from the comparison study show that the proposed sparse source imaging algorithm is able to accurately and efficiently recover extended cortical sources and is promising to provide high-accuracy estimation of cortical source extents.},
  language = {en},
  timestamp = {2016-11-22T13:24:43Z},
  number = {9},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Ding, Lei},
  year = {2009},
  pages = {2683}
}

@article{Dassios2005,
  title = {On the Non-Uniqueness of the Inverse {{MEG}} Problem},
  volume = {21},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/21/2/L01},
  abstract = {It has recently been shown by Fokas and coworkers that if the brain is approximated by a homogeneous sphere, magnetoencephalographic measurements determine only the moments of one of the three scalar functions specifying the electrochemically generated current in the brain. In this letter, we show that this is a generic limitation of MEG. Indeed, this indeterminancy persists in the general case that the sphere is replaced by a starlike conductor.},
  language = {en},
  timestamp = {2016-11-22T13:26:06Z},
  number = {2},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Dassios, G. and Fokas, A. S. and Kariotou, F.},
  year = {2005},
  pages = {L1}
}

@article{Nolte2005,
  title = {Analytic Expansion of the {{EEG}} Lead Field for Realistic Volume Conductors},
  volume = {50},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/50/16/010},
  abstract = {EEG forward calculation in realistic volume conductors using the boundary element method suffers from the fact that the solutions become inaccurate for superficial sources. Here we propose to correct an analytical approximation of the respective lead fields with series of spherical harmonics with respect to multiple expansion points. The necessary correction depends very much on the chosen analytical approximation. We constructed the latter such that the correction can be modelled adequately within the chosen basis. Simulations for a 3-shell prolate spheroid demonstrate the accurate modelling of the lead fields. Explicit comparison with analytically known solutions was done for the 3-shell spherical volume conductor showing that relative errors are mostly far below 1\% even for the most superficial sources placed directly on the innermost surface.},
  language = {en},
  timestamp = {2016-11-22T13:27:09Z},
  number = {16},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Nolte, Guido and Dassios, George},
  year = {2005},
  pages = {3807}
}

@article{Riera2006,
  title = {A Theoretical Formulation of the Electrophysiological Inverse Problem on the Sphere},
  volume = {51},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/51/7/007},
  abstract = {The construction of three-dimensional images of the primary current density (PCD) produced by neuronal activity is a problem of great current interest in the neuroimaging community, though being initially formulated in the 1970s. There exist even now enthusiastic debates about the authenticity of most of the inverse solutions proposed in the literature, in which low resolution electrical tomography (LORETA) is a focus of attention. However, in our opinion, the capabilities and limitations of the electro and magneto encephalographic techniques to determine PCD configurations have not been extensively explored from a theoretical framework, even for simple volume conductor models of the head. In this paper, the electrophysiological inverse problem for the spherical head model is cast in terms of reproducing kernel Hilbert spaces (RKHS) formalism, which allows us to identify the null spaces of the implicated linear integral operators and also to define their representers . The PCD are described in terms of a continuous basis for the RKHS, which explicitly separates the harmonic and non-harmonic components. The RKHS concept permits us to bring LORETA into the scope of the general smoothing splines theory. A particular way of calculating the general smoothing splines is illustrated, avoiding a brute force discretization prematurely. The Bayes information criterion is used to handle dissimilarities in the signal/noise ratios and physical dimensions of the measurement modalities, which could affect the estimation of the amount of smoothness required for that class of inverse solution to be well specified. In order to validate the proposed method, we have estimated the 3D spherical smoothing splines from two data sets: electric potentials obtained from a skull phantom and magnetic fields recorded from subjects performing an experiment of human faces recognition.},
  language = {en},
  timestamp = {2016-11-22T13:29:34Z},
  number = {7},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Riera, Jorge J. and Vald{\'e}s, Pedro A. and Tanabe, Kunio and Kawashima, Ryuta},
  year = {2006},
  pages = {1737}
}

@article{Ding2006,
  title = {{{3D}} Source Localization of Interictal Spikes in Epilepsy Patients with {{MRI}} Lesions},
  volume = {51},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/51/16/011},
  abstract = {The present study aims to accurately localize epileptogenic regions which are responsible for epileptic activities in epilepsy patients by means of a new subspace source localization approach, i.e. first principle vectors (FINE), using scalp EEG recordings. Computer simulations were first performed to assess source localization accuracy of FINE in the clinical electrode set-up. The source localization results from FINE were compared with the results from a classic subspace source localization approach, i.e. MUSIC, and their differences were tested statistically using the paired t -test. Other factors influencing the source localization accuracy were assessed statistically by ANOVA. The interictal epileptiform spike data from three adult epilepsy patients with medically intractable partial epilepsy and well-defined symptomatic MRI lesions were then studied using both FINE and MUSIC. The comparison between the electrical sources estimated by the subspace source localization approaches and MRI lesions was made through the coregistration between the EEG recordings and MRI scans. The accuracy of estimations made by FINE and MUSIC was also evaluated and compared by R 2 statistic, which was used to indicate the goodness-of-fit of the estimated sources to the scalp EEG recordings. The three-concentric-spheres head volume conductor model was built for each patient with three spheres of different radii which takes the individual head size and skull thickness into consideration. The results from computer simulations indicate that the improvement of source spatial resolvability and localization accuracy of FINE as compared with MUSIC is significant when simulated sources are closely spaced, deep, or signal-to-noise ratio is low in a clinical electrode set-up. The interictal electrical generators estimated by FINE and MUSIC are in concordance with the patients' structural abnormality, i.e. MRI lesions, in all three patients. The higher R 2 values achieved by FINE than MUSIC indicate that FINE provides a more satisfactory fitting of the scalp potential measurements than MUSIC in all patients. The present results suggest that FINE provides a useful brain source imaging technique, from clinical EEG recordings, for identifying and localizing epileptogenic regions in epilepsy patients with focal partial seizures. The present study may lead to the establishment of a high-resolution source localization technique from scalp-recorded EEGs for aiding presurgical planning in epilepsy patients.},
  language = {en},
  timestamp = {2016-11-22T13:30:53Z},
  number = {16},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Ding, Lei and Worrell, Gregory A. and Lagerlund, Terrence D. and He, Bin},
  year = {2006},
  pages = {4047}
}

@article{Im2006,
  title = {A Technique to Consider Mismatches between {{fMRI}} and {{EEG}}/{{MEG}} Sources for {{fMRI}}-Constrained {{EEG}}/{{MEG}} Source Imaging: A Preliminary Simulation Study},
  volume = {51},
  issn = {0031-9155},
  shorttitle = {A Technique to Consider Mismatches between {{fMRI}} and {{EEG}}/{{MEG}} Sources for {{fMRI}}-Constrained {{EEG}}/{{MEG}} Source Imaging},
  doi = {10.1088/0031-9155/51/23/004},
  abstract = {fMRI-constrained EEG/MEG source imaging can be a powerful tool in studying human brain functions with enhanced spatial and temporal resolutions. Recent studies on the combination of fMRI and EEG/MEG have suggested that fMRI prior information could be readily implemented by simply imposing different weighting factors to cortical sources overlapping with the fMRI activations. It has been also reported, however, that such a hard constraint may cause severe distortions or elimination of meaningful EEG/MEG sources when there are distinct mismatches between the fMRI activations and the EEG/MEG sources. If one wants to obtain the actual EEG/MEG source locations and uses the fMRI prior information as just an auxiliary tool to enhance focality of the distributed EEG/MEG sources, it is reasonable to weaken the strength of fMRI constraint when severe mismatches between fMRI and EEG/MEG sources are observed. The present study suggests an efficient technique to automatically adjust the strength of fMRI constraint according to the mismatch level. The use of the proposed technique rarely affects the results of conventional fMRI-constrained EEG/MEG source imaging if no major mismatch between the two modalities is detected; while the new results become similar to those of typical EEG/MEG source imaging without fMRI constraint if the mismatch level is significant. A preliminary simulation study using realistic EEG signals demonstrated that the proposed technique can be a promising tool to selectively apply fMRI prior information to EEG/MEG source imaging.},
  language = {en},
  timestamp = {2016-11-22T13:33:00Z},
  number = {23},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Im, Chang-Hwan and Lee, Soo Yeol},
  year = {2006},
  pages = {6005}
}

@article{Wolters2004a,
  title = {Efficient Computation of Lead Field Bases and Influence Matrix for the {{FEM}}-Based {{EEG}} and {{MEG}} Inverse Problem},
  volume = {20},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/20/4/007},
  abstract = {The inverse problem in electro- and magneto-encephalography (EEG/MEG) aims at reconstructing the underlying current distribution in the human brain using potential differences and/or magnetic fluxes that are measured non-invasively directly, or at a close distance, from the head surface. The simulation of EEG and MEG fields for a given dipolar source in the brain using a volume-conduction model of the head is called the forward problem. The finite element (FE) method, used for the forward problem, is able to realistically model tissue conductivity inhomogeneities and anisotropies, which is crucial for an accurate reconstruction of the current distribution. So far, the computational complexity is quite large when using the necessary high resolution FE models. In this paper we will extend the concept of the EEG lead field basis to the MEG and present algorithms for their efficient computation. Exploiting the fact that the number of sensors is generally much smaller than the number of reasonable dipolar sources, our lead field approach will speed up the state-of-the-art forward approach by a factor of more than 100 for a realistic choice of the number of sensors and sources. Our approaches can be applied to inverse reconstruction algorithms in both continuous and discrete source parameter space for EEG and MEG. In combination with algebraic multigrid solvers, the presented approach leads to a highly efficient solution of FE-based source reconstruction problems.},
  language = {en},
  timestamp = {2016-11-22T13:34:34Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Wolters, C. H. and Grasedyck, L. and Hackbusch, W.},
  year = {2004},
  pages = {1099}
}

@article{Stenroos2012,
  title = {Bioelectromagnetic Forward Problem: Isolated Source Approach Revis(It)Ed},
  volume = {57},
  issn = {0031-9155},
  shorttitle = {Bioelectromagnetic Forward Problem},
  doi = {10.1088/0031-9155/57/11/3517},
  abstract = {Electro- and magnetoencephalography (EEG and MEG) are non-invasive modalities for studying the electrical activity of the brain by measuring voltages on the scalp and magnetic fields outside the head. In the forward problem of EEG and MEG, the relationship between the neural sources and resulting signals is characterized using electromagnetic field theory. This forward problem is commonly solved with the boundary-element method (BEM). The EEG forward problem is numerically challenging due to the low relative conductivity of the skull. In this work, we revise the isolated source approach (ISA) that enables the accurate, computationally efficient BEM solution of this problem. The ISA is formulated for generic basis and weight functions that enable the use of Galerkin weighting. The implementation of the ISA-formulated linear Galerkin BEM (LGISA) is first verified in spherical geometry. Then, the LGISA is compared with conventional Galerkin and symmetric BEM approaches in a realistic 3-shell EEG/MEG model. The results show that the LGISA is a state-of-the-art method for EEG/MEG forward modeling: the ISA formulation increases the accuracy and decreases the computational load. Contrary to some earlier studies, the results show that the ISA increases the accuracy also in the computation of magnetic fields.},
  language = {en},
  timestamp = {2016-11-22T13:36:01Z},
  number = {11},
  urldate = {2016-11-22},
  journal = {Physics in Medicine and Biology},
  author = {Stenroos, M. and Sarvas, J.},
  year = {2012},
  pages = {3517}
}

@article{Fokas2012a,
  title = {Electro-Magneto-Encephalography for the Three-Shell Model: Numerical Implementation via Splines for Distributed Current in Spherical Geometry},
  volume = {28},
  issn = {0266-5611},
  shorttitle = {Electro-Magneto-Encephalography for the Three-Shell Model},
  doi = {10.1088/0266-5611/28/3/035009},
  abstract = {The basic inverse problems for the functional imaging techniques of electroencephalography (EEG) and magnetoencephalography (MEG) consist in estimating the neuronal current in the brain from the measurement of the electric potential on the scalp and of the magnetic field outside the head. Here we present a rigorous derivation of the relevant formulae for a three-shell spherical model in the case of independent as well as simultaneous MEG and EEG measurements. Furthermore, we introduce an explicit and stable technique for the numerical implementation of these formulae via splines. Numerical examples are presented using the locations and the normal unit vectors of the real 102 magnetometers and 70 electrodes of the Elekta Neuromag (R) system. These results may have useful implications for the interpretation of the reconstructions obtained via the existing approaches.},
  language = {en},
  timestamp = {2016-11-22T13:37:16Z},
  number = {3},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Fokas, A. S. and Hauk, O. and Michel, V.},
  year = {2012},
  pages = {035009}
}

@article{Clerc2012,
  title = {Source Localization Using Rational Approximation on Plane Sections},
  volume = {28},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/28/5/055018},
  abstract = {In functional neuroimaging, a crucial problem is to localize active sources within the brain non-invasively, from knowledge of electromagnetic measurements outside the head. Identification of point sources from boundary measurements is an ill-posed inverse problem. In the case of electroencephalography (EEG), measurements are only available at electrode positions, the number of sources is not known in advance and the medium within the head is inhomogeneous. This paper presents a new method for EEG source localization, based on rational approximation techniques in the complex plane. The method is used in the context of a nested sphere head model, in combination with a cortical mapping procedure. Results on simulated data prove the applicability of the method in the context of realistic measurement configurations.},
  language = {en},
  timestamp = {2016-11-22T13:39:56Z},
  number = {5},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Clerc, M. and Leblond, J. and Marmorat, J.-P. and Papadopoulo, T.},
  year = {2012},
  pages = {055018}
}

@article{Sorrentino2014,
  title = {Bayesian Multi-Dipole Modelling of a Single Topography in {{MEG}} by Adaptive Sequential {{Monte Carlo}} Samplers},
  volume = {30},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/30/4/045010},
  abstract = {In this paper, we develop a novel Bayesian approach to the problem of estimating neural currents in the brain from a fixed distribution of magnetic field (called topography ), measured by magnetoencephalography. Differently from recent studies that describe inversion techniques, such as spatio-temporal regularization/filtering, in which neural dynamics always plays a role, we face here a purely static inverse problem. Neural currents are modelled as an unknown number of current dipoles, whose state space is described in terms of a variable-dimension model. Within the resulting Bayesian framework, we set up a sequential Monte Carlo sampler to explore the posterior distribution. An adaptation technique is employed in order to effectively balance the computational cost and the quality of the sample approximation. Then, both the number and the parameters of the unknown current dipoles are simultaneously estimated. The performance of the method is assessed by means of synthetic data, generated by source configurations containing up to four dipoles. Eventually, we describe the results obtained by analysing data from a real experiment, involving somatosensory evoked fields, and compare them to those provided by three other methods.},
  language = {en},
  timestamp = {2016-11-22T13:41:02Z},
  number = {4},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Sorrentino, Alberto and Luria, Gianvittorio and Aramini, Riccardo},
  year = {2014},
  pages = {045010}
}

@article{Pursiainen2012a,
  title = {Raviart\textendash{}{{Thomas}}-Type Sources Adapted to Applied {{EEG}} and {{MEG}}: Implementation and Results},
  volume = {28},
  issn = {0266-5611},
  shorttitle = {Raviart\textendash{}{{Thomas}}-Type Sources Adapted to Applied {{EEG}} and {{MEG}}},
  doi = {10.1088/0266-5611/28/6/065013},
  abstract = {This paper studies numerically electroencephalography and magnetoencephalography (EEG and MEG), two non-invasive imaging modalities in which external measurements of the electric potential and the magnetic field are, respectively, utilized to reconstruct the primary current density (neuronal activity) of the human brain. The focus is on adapting a Raviart\textendash{}Thomas-type source model to meet the needs of EEG and MEG applications. The goal is to construct a model that provides an accurate approximation of dipole source currents and can be flexibly applied to different reconstruction strategies as well as to realistic computation geometries. The finite element method is applied in the simulation of the data. Least-squares fit interpolation is used to establish Cartesian source directions, which guarantee that the recovered current field is minimally dependent on the underlying finite element mesh. Implementation is explained in detail and made accessible, e.g., by using quadrature-free formulae and the Gaussian one-point rule in numerical integration. Numerical results are presented concerning, for example, the iterative alternating sequential inverse algorithm as well as resolution, smoothness and local refinement of the finite element mesh. Both spherical and pseudo-realistic head models, as well as real MEG data, are utilized in the numerical experiments.},
  language = {en},
  timestamp = {2016-11-22T14:01:50Z},
  number = {6},
  urldate = {2016-11-22},
  journal = {Inverse Problems},
  author = {Pursiainen, S.},
  year = {2012},
  pages = {065013}
}

@article{Hamalainen1993,
  title = {Magnetoencephalography$\backslash$char22\{\}theory, Instrumentation, and Applications to Noninvasive Studies of the Working Human Brain},
  volume = {65},
  doi = {10.1103/RevModPhys.65.413},
  abstract = {Magnetoencephalography (MEG) is a noninvasive technique for investigating neuronal activity in the living human brain. The time resolution of the method is better than 1 ms and the spatial discrimination is, under favorable circumstances, 2-3 mm for sources in the cerebral cortex. In MEG studies, the weak 10 fT-1 pT magnetic fields produced by electric currents flowing in neurons are measured with multichannel SQUID (superconducting quantum interference device) gradiometers. The sites in the cerebral cortex that are activated by a stimulus can be found from the detected magnetic-field distribution, provided that appropriate assumptions about the source render the solution of the inverse problem unique. Many interesting properties of the working human brain can be studied, including spontaneous activity and signal processing following external stimuli. For clinical purposes, determination of the locations of epileptic foci is of interest. The authors begin with a general introduction and a short discussion of the neural basis of MEG. The mathematical theory of the method is then explained in detail, followed by a thorough description of MEG instrumentation, data analysis, and practical construction of multi-SQUID devices. Finally, several MEG experiments performed in the authors' laboratory are described, covering studies of evoked responses and of spontaneous activity in both healthy and diseased brains. Many MEG studies by other groups are discussed briefly as well.},
  timestamp = {2016-11-23T13:13:45Z},
  number = {2},
  urldate = {2016-11-23},
  journal = {Reviews of Modern Physics},
  author = {H{\"a}m{\"a}l{\"a}inen, Matti and Hari, Riitta and Ilmoniemi, Risto J. and Knuutila, Jukka and Lounasmaa, Olli V.},
  month = apr,
  year = {1993},
  pages = {413--497},
  file = {APS Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KK5A74JS\\RevModPhys.65.html:text/html}
}

@article{Liu2002,
  title = {Monte {{Carlo}} Simulation Studies of {{EEG}} and {{MEG}} Localization Accuracy},
  volume = {16},
  issn = {1065-9471},
  abstract = {Both electroencephalography (EEG) and magnetoencephalography (MEG) are currently used to localize brain activity. The accuracy of source localization depends on numerous factors, including the specific inverse approach and source model, fundamental differences in EEG and MEG data, and the accuracy of the volume conductor model of the head (i.e., the forward model). Using Monte Carlo simulations, this study removes the effect of forward model errors and theoretically compares the use of EEG alone, MEG alone, and combined EEG/MEG data sets for source localization. Here, we use a linear estimation inverse approach with a distributed source model and a realistic forward head model. We evaluated its accuracy using the crosstalk and point spread metrics. The crosstalk metric for a specified location on the cortex describes the amount of activity incorrectly localized onto that location from other locations. The point spread metric provides the complementary measure: for that same location, the point spread describes the mis-localization of activity from that specified location to other locations in the brain. We also propose and examine the utility of a "noise sensitivity normalized" inverse operator. Given our particular forward and inverse models, our results show that 1) surprisingly, EEG localization is more accurate than MEG localization for the same number of sensors averaged over many source locations and orientations; 2) as expected, combining EEG with MEG produces the best accuracy for the same total number of sensors; 3) the noise sensitivity normalized inverse operator improves the spatial resolution relative to the standard linear estimation operator; and 4) use of an a priori fMRI constraint universally reduces both crosstalk and point spread.},
  language = {ENG},
  timestamp = {2016-11-23T13:13:38Z},
  number = {1},
  journal = {Human Brain Mapping},
  author = {Liu, Arthur K. and Dale, Anders M. and Belliveau, John W.},
  month = may,
  year = {2002},
  keywords = {algorithms,Artifacts,Bayes Theorem,Brain,Brain Mapping,Electrodes,Electroencephalography,Humans,Image Processing; Computer-Assisted,Magnetoencephalography,Models; Neurological,Monte Carlo Method,Reproducibility of Results},
  pages = {47--62},
  pmid = {11870926}
}

@article{Wolters2006,
  title = {Influence of Tissue Conductivity Anisotropy on {{EEG}}/{{MEG}} Field and Return Current Computation in a Realistic Head Model: {{A}} Simulation and Visualization Study Using High-Resolution Finite Element Modeling},
  volume = {30},
  issn = {1053-8119},
  shorttitle = {Influence of Tissue Conductivity Anisotropy on {{EEG}}/{{MEG}} Field and Return Current Computation in a Realistic Head Model},
  doi = {10.1016/j.neuroimage.2005.10.014},
  abstract = {To achieve a deeper understanding of the brain, scientists, and clinicians use electroencephalography (EEG) and magnetoencephalography (MEG) inverse methods to reconstruct sources in the cortical sheet of the human brain. The influence of structural and electrical anisotropy in both the skull and the white matter on the EEG and MEG source reconstruction is not well understood.

In this paper, we report on a study of the sensitivity to tissue anisotropy of the EEG/MEG forward problem for deep and superficial neocortical sources with differing orientation components in an anatomically accurate model of the human head.

The goal of the study was to gain insight into the effect of anisotropy of skull and white matter conductivity through the visualization of field distributions, isopotential surfaces, and return current flow and through statistical error measures. One implicit premise of the study is that factors that affect the accuracy of the forward solution will have at least as strong an influence over solutions to the associated inverse problem.

Major findings of the study include (1) anisotropic white matter conductivity causes return currents to flow in directions parallel to the white matter fiber tracts; (2) skull anisotropy has a smearing effect on the forward potential computation; and (3) the deeper a source lies and the more it is surrounded by anisotropic tissue, the larger the influence of this anisotropy on the resulting electric and magnetic fields. Therefore, for the EEG, the presence of tissue anisotropy both for the skull and white matter compartment substantially compromises the forward potential computation and as a consequence, the inverse source reconstruction. In contrast, for the MEG, only the anisotropy of the white matter compartment has a significant effect. Finally, return currents with high amplitudes were found in the highly conducting cerebrospinal fluid compartment, underscoring the need for accurate modeling of this space.},
  timestamp = {2016-11-23T13:19:45Z},
  number = {3},
  urldate = {2016-11-23},
  journal = {NeuroImage},
  author = {Wolters, C. H. and Anwander, A. and Tricoche, X. and Weinstein, D. and Koch, M. A. and MacLeod, R. S.},
  month = apr,
  year = {2006},
  keywords = {CSF,EEG,Finite element method,Forward problem,MEG,Return current,Source reconstruction,Tissue conductivity anisotropy,visualization},
  pages = {813--826},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZVT5GXU5\\S1053811905007871.html:text/html}
}

@article{Gullmar2010a,
  title = {Influence of Anisotropic Electrical Conductivity in White Matter Tissue on the {{EEG}}/{{MEG}} Forward and Inverse Solution. {{A}} High-Resolution Whole Head Simulation Study},
  volume = {51},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.02.014},
  abstract = {To investigate the influence of anisotropic electrical conductivity in white matter on the forward and inverse solution in electroencephalography (EEG) and magnetoencephalography (MEG) numerical simulation studies were performed. A high-resolution (1~mm3 isotropic) finite element model of a human head was implemented to study the sensitivity of EEG and MEG source localization. In vivo information on the anisotropy was obtained from magnetic resonance diffusion tensor imaging and included into the model, whereas both a direct transformation and a direct transformation with volume normalization were used to obtain conductivity tensors. Additionally, fixed artificial anisotropy ratios were also used, while considering only the orientation information from DTI, to generate conductivity tensors. Analysis was performed using over 25,000 single dipolar sources covering the full neocortex. Major findings of the study include that EEG is more sensitive to anisotropic conductivities in white matter compared to MEG. Especially with the inverse analysis, we found that sources placed deep in sulci are located more laterally if anisotropic conductivity of white matter tissue is neglected. Overall, the single-source localization errors resulting from a neglect of anisotropy were found to be smaller compared to errors associated with other modeling errors, like misclassified tissue or the use of nonrealistic head models. In contrast to the small localization error we observed significant changes in magnitude and orientation. The latter is important since dipole orientation might be more important than absolute dipole localization in assigning, e.g., epileptic activity to the wall of the affected brain sulcal area. If high-resolution finite element models are used to perform source localization in EEG and MEG experiments and the quality of the measured data permits localization accuracy of 1~mm and below, the influence of anisotropic compartments has to be taken into account.},
  timestamp = {2016-11-23T13:21:59Z},
  number = {1},
  urldate = {2016-11-23},
  journal = {NeuroImage},
  author = {G{\"u}llmar, Daniel and Haueisen, Jens and Reichenbach, J{\"u}rgen R.},
  month = may,
  year = {2010},
  keywords = {Anisotropic conductivity,diffusion tensor imaging,EEG,FEM,MEG,segmentation,Source localization},
  pages = {145--163},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F3QC5G3A\\S1053811910001825.html:text/html}
}

@article{Schimpf1998,
  title = {Realistic Computer Modelling of Electric and Magnetic Fields of Human Head and Torso},
  volume = {24},
  issn = {0167-8191},
  doi = {10.1016/S0167-8191(98)00065-9},
  abstract = {Anatomically realistic computer models are needed for an accurate modeling of electric and magnetic fields of the human head and torso. Their applications are in the emerging field of functional tomography for non-invasive medical diagnostics. Some of these models tend to become very large and require supercomputers for solution. The future supercomputing challenge is to solve these models in a time frame such that patient specific models can be used for online clinical diagnostics and treatment planning. In this paper we present methods and tools for developing anatomically realistic torso and head models, numerical techniques to solve for potentials and currents in the models, and their clinical applications.},
  timestamp = {2016-11-23T13:23:05Z},
  number = {9\textendash{}10},
  urldate = {2016-11-23},
  journal = {Parallel Computing},
  author = {Schimpf, Paul and Haueisen, Jens and Ramon, Ceon and Nowak, Hannes},
  month = sep,
  year = {1998},
  keywords = {Boundary element method,Cardiac defibrillation modeling,Electroencephalography,Finite element method,Magnetocardiography},
  pages = {1433--1460},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZZ7D886J\\S0167819198000659.html:text/html}
}

@article{Haueisen2012,
  title = {Tangential and Radial Epileptic Spike Activity: Different Sensitivity in {{EEG}} and {{MEG}}},
  volume = {29},
  issn = {1537-1603},
  shorttitle = {Tangential and Radial Epileptic Spike Activity},
  doi = {10.1097/WNP.0b013e3182624491},
  abstract = {OBJECTIVE: Observations in epileptic patients show that interictal spikes are sometimes only visible in electroencephalography (EEG) and sometimes only in magnetoencephalography (MEG). This observation cannot readily be explained by the theoretical sensitivities of EEG and MEG based on analytical models. In this context, we aimed to study the directional sensitivity of radial and tangential spike activity in numerical simulations using realistic head models.
METHODS: We calculated the signal-to-noise ratio (SNR) of simulated spikes at varying orientations and with varying background activity in 12 brain regions in 4 volunteers. Different levels of background activity were modeled by adjusting the amplitudes of several thousand dipoles distributed in the cortex.
RESULTS: For a fixed realistic background activity, we found a higher SNR for MEG spikes for spike orientations that deviated not $>$ 30$^\circ$ from the tangential direction. In contrast, we found a higher SNR for EEG spikes that deviated not $>$ 45$^\circ$ from the radial direction. When the radial background activity was selectively increased, the sensitivity of EEG for radially oriented spikes decreased; when the tangential background activity selectively increased, the sensitivity of MEG for tangentially oriented spikes was decreased.
CONCLUSIONS: Our simulations provide a possible explanation for the clinically observed differences in epileptic spike detection between EEG and MEG. Epileptic spike detection can be improved by analyzing a combination of EEG and MEG data.},
  language = {ENG},
  timestamp = {2016-11-23T13:34:10Z},
  number = {4},
  journal = {Journal of Clinical Neurophysiology: Official Publication of the American Electroencephalographic Society},
  author = {Haueisen, Jens and Funke, Michael and G{\"u}llmar, Daniel and Eichardt, Roland},
  month = aug,
  year = {2012},
  keywords = {Action Potentials,Electroencephalography,Epilepsy,Female,Humans,Magnetoencephalography,Male,Sensitivity and Specificity,signal-to-noise ratio},
  pages = {327--332},
  pmid = {22854766}
}

@article{Akhtari2002,
  title = {Conductivities of {{Three}}-{{Layer Live Human Skull}}},
  volume = {14},
  issn = {0896-0267, 1573-6792},
  doi = {10.1023/A:1014590923185},
  abstract = {Electrical conductivities of compact, spongiosum, and bulk layers of the live human skull were determined at varying frequencies and electric fields at room temperature using the four-electrode method. Current, at higher densities that occur in human cranium, was applied and withdrawn over the top and bottom surfaces of each sample and potential drop across different layers was measured. We used a model that considers variations in skull thicknesses to determine the conductivity of the tri-layer skull and its individual anatomical structures. The results indicate that the conductivities of the spongiform (16.2-41.1 milliS/m), the top compact (5.4-7.2 milliS/m) and lower compact (2.8-10.2 milliS/m) layers of the skull have significantly different and inhomogeneous conductivities. The conductivities of the skull layers are frequency dependent in the 10-90 Hz region and are non-ohmic in the 0.45-2.07 A/m2 region. These current densities are much higher than those occurring in human brain.},
  language = {en},
  timestamp = {2016-11-23T14:41:53Z},
  number = {3},
  urldate = {2016-11-23},
  journal = {Brain Topography},
  author = {Akhtari, M. and Bryant, H. C. and Mamelak, A. N. and Flynn, E. R. and Heller, L. and Shih, J. J. and Mandelkem, M. and Matlachov, A. and Ranken, D. M. and Best, E. D. and DiMauro, M. A. and Lee, R. R. and Sutherling, W. W.},
  month = mar,
  year = {2002},
  pages = {151--167},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FG64NPH6\\A1014590923185.html:text/html}
}

@article{Akhtari2003,
  title = {A {{Model}} for {{Frequency Dependence}} of {{Conductivities}} of the {{Live Human Skull}}},
  volume = {16},
  issn = {0896-0267, 1573-6792},
  doi = {10.1023/A:1025658432696},
  abstract = {A mathematical model ($\sigma$($\omega$) $\simeq$ A$\omega\alpha$, where, $\sigma$ $\equiv$ conductivity, $\omega$ =2$\pi$f $\equiv$ applied frequency (Hz), A (amplitude) and $\alpha$ (unitless) $\equiv$ search parameters) was used to fit the frequency dependence of electrical conductivities of compact, spongiosum, and bulk layers of the live and, subsequently, dead human skull samples. The results indicate that the fit of this model to the experimental data is excellent. The ranges of values of A and $\alpha$ were, spongiform (12.0-36.5, 0.0083-0.0549), the top compact (5.02-7.76, -0.137-0.0144), the lower compact (2.31-10.6, 0.0267-0.0452), and the bulk (7.46-10.6, 0.0133-0.0239). The respective values A and $\alpha$ for the respective layers of the dead skull samples were (40.1-89.7, -0.0017-0.0287), (5.53-14.5, -0.0296 - -0.0061), (4.58-15.9, -0.0226-0.0268), and (12.7-25.3, -0.0158-0.0132).},
  language = {en},
  timestamp = {2016-11-23T14:43:13Z},
  number = {1},
  urldate = {2016-11-23},
  journal = {Brain Topography},
  author = {Akhtari, M. and Bryant, H. C. and Emin, D. and Merrifield, W. and Mamelak, A. N. and Flynn, E. R. and Shih, J. J. and Mandelkern, M. and Matlachov, A. and Ranken, D. M. and Best, E. D. and DiMauro, M. A. and Lee, R. R. and Sutherling, W. W.},
  month = sep,
  year = {2003},
  pages = {39--55},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\BBSRSP6D\\A1025658432696.html:text/html}
}

@article{Sadleir2007,
  title = {Modeling {{Skull Electrical Properties}}},
  volume = {35},
  issn = {0090-6964, 1573-9686},
  doi = {10.1007/s10439-007-9343-5},
  abstract = {Accurate representations and measurements of skull electrical conductivity are essential in developing appropriate forward models for applications such as inverse EEG or Electrical Impedance Tomography of the head. Because of its layered structure, it is often assumed that skull is anisotropic, with an anisotropy ratio around 10. However, no detailed investigation of skull anisotropy has been performed. In this paper we investigate four-electrode measurements of conductivities and their relation to tissue anisotropy ratio (ratio of tangential to radial conductivity) in layered or anisotropic biological samples similar to bone. It is shown here that typical values for the thicknesses and radial conductivities of individual skull layers produce tissue with much smaller anisotropy ratios than 10. Moreover, we show that there are very significant differences between the field patterns formed in a three-layered isotropic structure plausible for bone, and those formed assuming that bone is homogeneous and anisotropic. We performed a measurement of conductivity using an electrode configuration sensitive to the distinction between three-layered and homogeneous anisotropic composition and found results consistent with the sample being three-layered. We recommend that the skull be more appropriately represented as three isotropic layers than as homogeneous and anisotropic.},
  language = {en},
  timestamp = {2016-11-23T14:43:48Z},
  number = {10},
  urldate = {2016-11-23},
  journal = {Annals of Biomedical Engineering},
  author = {Sadleir, R. J. and Argibay, A.},
  month = oct,
  year = {2007},
  pages = {1699--1712},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NP7XJRFV\\s10439-007-9343-5.html:text/html}
}

@article{Pohlmeier1997,
  title = {The Influence of Skull-Conductivity Misspecification on Inverse Source Localization in Realistically Shaped Finite Element Head Models},
  volume = {9},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/BF01190384},
  abstract = {SummaryThe electric conductivities of different tissues are important parameters of the head model and their precise knowledge appears to be a prerequisite for the localization of electric sources within the brain. To estimate the error in source localization due to errors in assumed conductivity values, parameter variations on skull conductivities are examined. The skull conductivity was varied in a wide range and, in a second part of this paper, the effect of a nonhomogeneous skull conductivity was examined. An error in conductivity of lower than 20\% appears to be acceptable for fine finite element head models with average discretization errors down to 3mm. Nonhomogeneous skull conductivities, e.g., sutures, yield important mislocalizations especially in the vincinty of electrodes and should be modeled.},
  language = {en},
  timestamp = {2016-11-23T14:44:46Z},
  number = {3},
  urldate = {2016-11-23},
  journal = {Brain Topography},
  author = {Pohlmeier, Robert and Buchner, Helmut and Knoll, Gunter and Rien{\"A}cker, Adrian and Beckmann, Rainer and Pesch, J{\"o}rg},
  month = mar,
  year = {1997},
  pages = {157--162},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\993CT8PQ\\BF01190384.html:text/html}
}

@incollection{Peters2004,
  series = {Bioelectric Engineering},
  title = {The {{Electrical Conductivity}} of {{Living Tissue}}: {{A Parameter}} in the {{Bioelectrical Inverse Problem}}},
  copyright = {\textcopyright{}2005 Kluwer Academic/Plenum Publishers, New York},
  isbn = {978-0-306-48112-3 978-0-387-49963-5},
  shorttitle = {The {{Electrical Conductivity}} of {{Living Tissue}}},
  abstract = {Electrically active cells within the human body generate currents in the tissues surrounding these cells. These currents are called volume currents. The volume currents in turn give rise to potential differences between electrodes attached to the body. When these electrodes are attached to the torso, electrical potential differences generated by the heart are recorded. The recording of these electrical potential differences as a function of time is called an electrocardiogram (ECG). ECG measurements can be used to compute the generators within the heart. This is called the solution of the ECG inverse problem. This solution may be of interest for diagnostic purposes. For instance, it can be used to localize an extra conducting pathway between atria and ventricles. This pathway can then subsequently be removed by radio-frequent ablation through a catheter. When the active cells are situated within the brain and the electrodes are attached to the scalp, the recording of the potential difference measured between two electrodes as a function of time is called an electroencephalogram (EEG). The EEG inverse problem can, for example, be used to localize an epileptic focus as part of the presurgical evaluation. The frequencies involved in electrocardiograms and electroencephalograms are in the range of 1-1000Hz. Therefore, the Maxwell equations can be used in a quasi-static approximation, implicating that capacitive and inductive effects and wave phenomena are ignored as argued by (1967).},
  language = {en},
  timestamp = {2016-11-23T14:45:37Z},
  urldate = {2016-11-23},
  booktitle = {Modeling and {{Imaging}} of {{Bioelectrical Activity}}},
  publisher = {{Springer US}},
  author = {Peters, Maria J. and Stinstra, Jeroen G. and Leveles, Ibolya},
  editor = {He, Bin},
  year = {2004},
  keywords = {Biomedical engineering,Imaging / Radiology,Neuroradiology,Neurosciences},
  pages = {281--319},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QT3ACCWE\\978-0-387-49963-5_9.html:text/html},
  doi = {10.1007/978-0-387-49963-5_9}
}

@article{Dannhauer2011a,
  title = {Modeling of the Human Skull in {{EEG}} Source Analysis},
  volume = {32},
  issn = {1097-0193},
  doi = {10.1002/hbm.21114},
  abstract = {We used computer simulations to investigate finite element models of the layered structure of the human skull in EEG source analysis. Local models, where each skull location was modeled differently, and global models, where the skull was assumed to be homogeneous, were compared to a reference model, in which spongy and compact bone were explicitly accounted for. In both cases, isotropic and anisotropic conductivity assumptions were taken into account. We considered sources in the entire brain and determined errors both in the forward calculation and the reconstructed dipole position. Our results show that accounting for the local variations over the skull surface is important, whereas assuming isotropic or anisotropic skull conductivity has little influence. Moreover, we showed that, if using an isotropic and homogeneous skull model, the ratio between skin/brain and skull conductivities should be considerably lower than the commonly used 80:1. For skull modeling, we recommend (1) Local models: if compact and spongy bone can be identified with sufficient accuracy (e.g., from MRI) and their conductivities can be assumed to be known (e.g., from measurements), one should model these explicitly by assigning each voxel to one of the two conductivities, (2) Global models: if the conditions of (1) are not met, one should model the skull as either homogeneous and isotropic, but with considerably higher skull conductivity than the usual 0.0042 S/m, or as homogeneous and anisotropic, but with higher radial skull conductivity than the usual 0.0042 S/m and a considerably lower radial:tangential conductivity anisotropy than the usual 1:10. Hum Brain Mapp, 2010. \textcopyright{} 2010 Wiley-Liss, Inc.},
  language = {en},
  timestamp = {2016-11-23T15:04:25Z},
  number = {9},
  urldate = {2016-11-23},
  journal = {Human Brain Mapping},
  author = {Dannhauer, Moritz and Lanfer, Benjamin and Wolters, Carsten H. and Kn{\"o}sche, Thomas R.},
  month = sep,
  year = {2011},
  keywords = {EEG,Finite element model,Forward problem,Inverse problem,skull modeling,Source reconstruction,Tissue conductivity anisotropy},
  pages = {1383--1399},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S4X3VEJV\\abstract.html:text/html}
}

@article{Montes-Restrepo2014,
  title = {Influence of {{Skull Modeling Approaches}} on {{EEG Source Localization}}},
  volume = {27},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-013-0313-y},
  abstract = {Electroencephalographic source localization (ESL) relies on an accurate model representing the human head for the computation of the forward solution. In this head model, the skull is of utmost importance due to its complex geometry and low conductivity compared to the other tissues inside the head. We investigated the influence of using different skull modeling approaches on ESL. These approaches, consisting in skull conductivity and geometry modeling simplifications, make use of X-ray computed tomography (CT) and magnetic resonance (MR) images to generate seven different head models. A head model with an accurately segmented skull from CT images, including spongy and compact bone compartments as well as some air-filled cavities, was used as the reference model. EEG simulations were performed for a configuration of 32 and 128 electrodes, and for both noiseless and noisy data. The results show that skull geometry simplifications have a larger effect on ESL than those of the conductivity modeling. This suggests that accurate skull modeling is important in order to achieve reliable results for ESL that are useful in a clinical environment. We recommend the following guidelines to be taken into account for skull modeling in the generation of subject-specific head models: (i) If CT images are available, i.e., if the geometry of the skull and its different tissue types can be accurately segmented, the conductivity should be modeled as isotropic heterogeneous. The spongy bone might be segmented as an erosion of the compact bone; (ii) when only MR images are available, the skull base should be represented as accurately as possible and the conductivity can be modeled as isotropic heterogeneous, segmenting the spongy bone directly from the MR image; (iii) a large number of EEG electrodes should be used to obtain high spatial sampling, which reduces the localization errors at realistic noise levels.},
  language = {en},
  timestamp = {2016-11-24T12:52:25Z},
  number = {1},
  urldate = {2016-11-24},
  journal = {Brain Topography},
  author = {Montes-Restrepo, Victoria and van Mierlo, Pieter and Strobbe, Gregor and Staelens, Steven and Vandenberghe, Stefaan and Hallez, Hans},
  month = jan,
  year = {2014},
  pages = {95--111},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\4SEVVTC9\\s10548-013-0313-y.html:text/html}
}

@article{Haueisen1997,
  title = {Influence of Tissue Resistivities on Neuromagnetic Fields and Electric Potentials Studied with a Finite Element Model of the Head},
  volume = {44},
  issn = {0018-9294},
  doi = {10.1109/10.605429},
  abstract = {Modeling in magnetoencephalography (MEG) and electroencephalography (EEG) requires knowledge of the in vivo tissue resistivities of the head. The aim of this paper is to examine the influence of tissue resistivity changes on the neuromagnetic field and the electric scalp potential. A high-resolution finite element method (FEM) model (452162 elements, 2-mm resolution) of the human head with 13 different tissue types is employed for this purpose. Our main finding was that the magnetic fields are sensitive to changes in the tissue resistivity in the vicinity of the source. In comparison, the electric surface potentials are sensitive to changes in the tissue resistivity in the vicinity of the source and in the vicinity of the position of the electrodes. The magnitude (strength) of magnetic fields and electric surface potentials is strongly influenced by tissue resistivity changes, while the topography is not as strongly influenced. Therefore, an accurate modeling of magnetic field and electric potential strength requires accurate knowledge of tissue resistivities, while for source localization procedures this knowledge might not be a necessity.},
  timestamp = {2016-11-24T12:55:41Z},
  number = {8},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Haueisen, J. and Ramon, C. and Eiselt, M. and Brauer, H. and Nowak, H.},
  month = aug,
  year = {1997},
  keywords = {Adipose Tissue,biomagnetism,Brain,Brain modeling,brain models,Conductivity,electrical resistivity,Electric Impedance,Electric potential,electric potentials,electric potential strength,electric scalp potential,electric surface potentials,Electrodes,Electroencephalography,Electromagnetic fields,Evoked Potentials,finite element analysis,Finite element methods,Head,high-resolution finite element method,Humans,In vivo,Magnetic fields,Magnetic heads,magnetic resonance imaging,Magnetoencephalography,Modeling,Models; Neurological,Muscle; Skeletal,neuromagnetic fields,neurophysiology,resolution,Sensitivity and Specificity,Skull,Source localization,surface potential,Surface Properties,Surface topography,tissue resistivities,tissue types,topography},
  pages = {727--735},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\4I8AUATT\\605429.html:text/html}
}

@article{Tuch2001,
  title = {Conductivity Tensor Mapping of the Human Brain Using Diffusion Tensor {{MRI}}},
  volume = {98},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.171473898},
  abstract = {Knowledge of the electrical conductivity properties of excitable tissues is essential for relating the electromagnetic fields generated by the tissue to the underlying electrophysiological currents. Efforts to characterize these endogenous currents from measurements of the associated electromagnetic fields would significantly benefit from the ability to measure the electrical conductivity properties of the tissue noninvasively. Here, using an effective medium approach, we show how the electrical conductivity tensor of tissue can be quantitatively inferred from the water self-diffusion tensor as measured by diffusion tensor magnetic resonance imaging. The effective medium model indicates a strong linear relationship between the conductivity and diffusion tensor eigenvalues (respectively, $\sigma$ and d) in agreement with theoretical bounds and experimental measurements presented here ($\sigma$/d $\approx$ 0.844 $\pm$ 0.0545 S$\cdot$s/mm3, r2 = 0.945). The extension to other biological transport phenomena is also discussed.},
  language = {en},
  timestamp = {2016-11-24T12:59:20Z},
  number = {20},
  urldate = {2016-11-24},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Tuch, David S. and Wedeen, Van J. and Dale, Anders M. and George, John S. and Belliveau, John W.},
  month = sep,
  year = {2001},
  pages = {11697--11701},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XDRIR445\\11697.html:text/html},
  pmid = {11573005}
}

@article{Lew2009,
  title = {Accuracy and Run-Time Comparison for Different Potential Approaches and Iterative Solvers in Finite Element Method Based {{EEG}} Source Analysis},
  volume = {59},
  issn = {0168-9274},
  doi = {10.1016/j.apnum.2009.02.006},
  abstract = {Accuracy and run-time play an important role in medical diagnostics and research as well as in the field of neuroscience. In Electroencephalography (EEG) source reconstruction, a current distribution in the human brain is reconstructed noninvasively from measured potentials at the head surface (the EEG inverse problem). Numerical modeling techniques are used to simulate head surface potentials for dipolar current sources in the human cortex, the so-called EEG forward problem. In this paper, the efficiency of algebraic multi-grid (AMG), incomplete Cholesky (IC) and Jacobi preconditioners for the conjugate gradient (CG) method are compared for iteratively solving the finite element (FE) method based EEG forward problem. The interplay of the three solvers with a full subtraction approach and two direct potential approaches, the Venant and the partial integration method for the treatment of the dipole singularity is examined. The examination is performed in a four-compartment sphere model with anisotropic skull layer, where quasi-analytical solutions allow for an exact quantification of computational speed versus numerical error. Specifically-tuned constrained Delaunay tetrahedralization (CDT) FE meshes lead to high accuracies for both the full subtraction and the direct potential approaches. Best accuracies are achieved by the full subtraction approach if the homogeneity condition is fulfilled. It is shown that the AMG-CG achieves an order of magnitude higher computational speed than the CG with the standard preconditioners with an increasing gain factor when decreasing mesh size. Our results should broaden the application of accurate and fast high-resolution FE volume conductor modeling in source analysis routine.},
  timestamp = {2016-11-24T13:01:59Z},
  number = {8},
  urldate = {2016-11-24},
  journal = {Applied Numerical Mathematics},
  author = {Lew, S. and Wolters, C. H. and Dierkes, T. and R{\"o}er, C. and MacLeod, R. S.},
  month = aug,
  year = {2009},
  keywords = {Algebraic multi-grid preconditioner,Anisotropic four-layer sphere model,Constrained Delaunay tetrahedralization,Dipole singularity,Electroencephalography,Finite element method,Full subtraction potential approach,Incomplete Cholesky preconditioner,Jacobi preconditioner,Partial integration potential approach,Preconditioned conjugate gradient method,Source reconstruction,Venant potential approach},
  pages = {1970--1988},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SX5RF37C\\S016892740900021X.html:text/html}
}

@article{Rullmann2009,
  title = {{{EEG}} Source Analysis of Epileptiform Activity Using a 1~Mm Anisotropic Hexahedra Finite Element Head Model},
  volume = {44},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2008.09.009},
  abstract = {The major goal of the evaluation in presurgical epilepsy diagnosis for medically intractable patients is the precise reconstruction of the epileptogenic foci, preferably with non-invasive methods. This paper evaluates whether surface electroencephalography (EEG) source analysis based on a 1~mm anisotropic finite element (FE) head model can provide additional guidance for presurgical epilepsy diagnosis and whether it is practically feasible in daily routine. A 1~mm hexahedra FE volume conductor model of the patient's head with special focus on accurately modeling the compartments skull, cerebrospinal fluid (CSF) and the anisotropic conducting brain tissues was constructed using non-linearly co-registered T1-, T2- and diffusion-tensor-magnetic resonance imaging data. The electrodes of intra-cranial EEG (iEEG) measurements were extracted from a co-registered computed tomography image. Goal function scan (GFS), minimum norm least squares (MNLS), standardized low resolution electromagnetic tomography (sLORETA) and spatio-temporal current dipole modeling inverse methods were then applied to the peak of the averaged ictal discharges EEG data. MNLS and sLORETA pointed to a single center of activity. Moving and rotating single dipole fits resulted in an explained variance of more than 97\%. The non-invasive EEG source analysis methods localized at the border of the lesion and at the border of the iEEG electrodes which mainly received ictal discharges. Source orientation was towards the epileptogenic tissue. For the reconstructed superficial source, brain conductivity anisotropy and the lesion conductivity had only a minor influence, whereas a correct modeling of the highly conducting CSF compartment and the anisotropic skull was found to be important. The proposed FE forward modeling approach strongly simplifies meshing and reduces run-time (37~ms for one forward computation in the model with 3.1~million unknowns), corroborating the practical feasibility of the approach.},
  timestamp = {2016-11-24T13:01:59Z},
  number = {2},
  urldate = {2016-11-24},
  journal = {NeuroImage},
  author = {Rullmann, M. and Anwander, A. and Dannhauer, M. and Warfield, S. K. and Duffy, F. H. and Wolters, C. H.},
  month = jan,
  year = {2009},
  keywords = {Cerebrospinal fluid,Diffusion-tensor magnetic resonance imaging,Finite element method,Goal function scan,Minimum norm least squares,Presurgical epilepsy diagnosis,source analysis,Spatio-temporal dipole modeling,Standardized low resolution electromagnetic tomography,Surface- and intra-cranial EEG,Tissue conductivity anisotropy},
  pages = {399--410},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5IMQUFFR\\S1053811908009750.html:text/html}
}

@article{Goncalve2003,
  title = {In Vivo Measurement of the Brain and Skull Resistivities Using an {{EIT}}-Based Method and the Combined Analysis of {{SEF}}/{{SEP}} Data},
  volume = {50},
  issn = {0018-9294},
  doi = {10.1109/TBME.2003.816072},
  abstract = {Results of "in vivo" measurements of the skull and brain resistivities are presented for six subjects. Results are obtained using two different methods, based on spherical head models. The first method uses the principles of electrical impedance tomography (EIT) to estimate the equivalent electrical resistivities of brain ($\rho$brain), skull ($\rho$skull) and skin ($\rho$skin) according to S. Goncalves et al., Physiol. Meas., vol. 21, p. 379-93 (2000).. The second one estimates the same parameters through a combined analysis of the evoked somatosensory cortical response, recorded simultaneously using magnetoencephalography (MEG) and electroencephalography (EEG). The EIT results, obtained with the same relative skull thickness (0.05) for all subjects, show a wide variation of the ratio $\rho$skull/$\rho$brain among subjects (average =72, SD=48\%). However, the $\rho$skull/$\rho$brain ratios of the individual subjects are well reproduced by combined analysis of somatosensory evoked fields (SEF) and somatosensory evoked potentials (SEP). These preliminary results suggest that the $\rho$skull/$\rho$brain variations over subjects cannot be disregarded in the EEG inverse problem (IP) when a spherical model is used. The agreement between EIT and SEF/SEP points to the fact that whatever the source of variability, the proposed EIT-based method skull/$\rho$brain, $\rho$brain, $\rho$skull and $\rho$skin.},
  timestamp = {2016-11-24T13:09:00Z},
  number = {9},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Goncalve, S. and de Munck, J. C. and Verbunt, J. P. A. and Heethaar, R. M. and da Silva, F. H. L.},
  month = sep,
  year = {2003},
  keywords = {Brain,Brain Mapping,brain models,brain resistivity measurement,Conductivity,EEG,EEG inverse problem,Electric Impedance,electric impedance imaging,electric impedance tomography,Electric resistance,electric resistance measurement,electrodiagnostics,Electroencephalography,Evoked Potentials; Somatosensory,evoked somatosensory cortical response,Head,Impedance,inverse problems,In vivo,Magnetic heads,medical signal processing,MEG,Models; Biological,parameter estimation,relative skull thickness,Reproducibility of Results,Sensitivity and Specificity,Skin,Skin Physiology,Skull,skull resistivity measurement,somatosensory evoked fields,somatosensory evoked potentials,somatosensory phenomena,Tomography},
  pages = {1124--1127},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\C3H8NCGU\\1220220.html:text/html}
}

@article{Goncalves2003,
  title = {In Vivo Measurement of the Brain and Skull Resistivities Using an {{EIT}}-Based Method and Realistic Models for the Head},
  volume = {50},
  issn = {0018-9294},
  doi = {10.1109/TBME.2003.812164},
  abstract = {In vivo measurements of equivalent resistivities of skull ($\rho$skull) and brain ($\rho$brain) are performed for six subjects using an electric impedance tomography (EIT)-based method and realistic models for the head. The classical boundary element method (BEM) formulation for EIT is very time consuming. However, the application of the Sherman-Morrison formula reduces the computation time by a factor of 5. Using an optimal point distribution in the BEM model to optimize its accuracy, decreasing systematic errors of numerical origin, is important because cost functions are shallow. Results demonstrate that $\rho$skull/$\rho$brain is more likely to be within 20 and 50 rather than equal to the commonly accepted value of 80. The variation in $\rho$brain (average = 301 $\Omega$ $\cdot$ cm, SD = 13\%) and $\rho$skull (average = 12230 $\Omega$ $\cdot$ cm, SD = 18\%) is decreased by half, when compared with the results using the sphere model, showing that the correction for geometry errors is essential to obtain realistic estimations. However, a factor of 2.4 may still exist between values of $\rho$skull/$\rho$brain corresponding to different subjects. Earlier results show the necessity of calibrating $\rho$brain and $\rho$skull by measuring them in vivo for each subject, in order to decrease errors associated with the electroencephalogram inverse problem. We show that the proposed method is suited to this goal.},
  timestamp = {2016-11-24T13:09:15Z},
  number = {6},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Goncalves, S. I. and de Munck, J. C. and Verbunt, J. P. A. and Bijma, F. and Heethaar, R. M. and da Silva, F. Lopes},
  month = jun,
  year = {2003},
  keywords = {Accuracy,Adult,bioelectric phenomena,boundary element methods,boundary-elements methods,Brain,Brain Mapping,Brain modeling,brain models,brain resistivity,calibration,classical boundary element method,computation time,Computer Simulation,Conductivity,Cost function,cost functions,EIT-based method,electrical resistivity,Electric Impedance,electric impedance imaging,electric impedance tomography,Electric variables measurement,electroencephalogram inverse problem,Electroencephalography,equivalent resistivities,Female,geometry errors,Head,Humans,Impedance measurement,In vivo,in vivo measurement,Male,Models; Biological,optimal point distribution,Performance evaluation,realistic head models,Reproducibility of Results,Sensitivity and Specificity,Sherman-Morrison formula,Skull,skull resistivity,sphere model,systematic errors,Tomography},
  pages = {754--767},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\T5MFWCKF\\1203814.html:text/html}
}

@inproceedings{Goncalves2001,
  title = {In Vivo Measurement of Skull and Brain Resistivities with {{EIT}} Based Method and Analysis of {{SEF}}/{{SEP}} Data},
  volume = {1},
  doi = {10.1109/IEMBS.2001.1019124},
  abstract = {In this paper we present results of the equivalent brain and skull resistivities ($\rho$brain and $\rho$skull) for 6 different subjects using 2 different and independent procedures: an EIT based method and the combined analysis of SEF/SEP data. With the EIT based method known currents are injected into the head and the resulting potential distributions are recorded from scalp electrodes. The conductivities are estimated by fitting the conductivity parameters of a 3-sphere head model onto the measured potentials. With the combined SEP/SEF method, a current source is activated inside the brain using a nervous medianus stimulation. The MEG data is used to determine dipole position and tangential orientation, whereas the simultaneously recorded EEG data is used to find the dipole radial component and the electrical conductivities of the brain and the skull. The results show a large variability in the ratio of skull and brain conductivities $\rho$skull/$\rho$brain over subjects. However, a strong agreement was found between the results of EIT and SEF/SEP methods even though they are quite different, both in theoretical and technical terms. These results indicate that generic conductivity values will result in large systematic errors of EEG inverse modelling. However, the good agreement between the EIT and the SEP/SEF method indicates that the individual's $\rho$skull/$\rho$brain ratio can reliably be determined using the EIT method.},
  timestamp = {2016-11-24T13:09:52Z},
  booktitle = {2001 {{Conference Proceedings}} of the 23rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Goncalves, S. and de Munck, J. C. and Verbunt, J. P. A.},
  year = {2001},
  keywords = {3-sphere head model,bioelectric phenomena,Biomedical measurements,Brain modeling,brain models,brain resistivity measurement,Conductivity,conductivity parameters,current source,data analysis,dipole position,electrical resistivity,electric impedance imaging,Electroencephalography,In vivo,Magnetic heads,Magnetoencephalography,measured potentials,MEG data,nervous medianus stimulation,orthopaedics,Scalp,Skin,Skull,skull resistivity measurement,tangential orientation},
  pages = {1006--1008 vol.1},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\M26V5HIN\\1019124.html:text/html}
}

@article{Iwasaki2005,
  title = {Detection of {{Epileptiform Activity}} by {{Human Interpreters}}: {{Blinded Comparison}} between {{Electroencephalography}} and {{Magnetoencephalography}}},
  volume = {46},
  issn = {1528-1167},
  shorttitle = {Detection of {{Epileptiform Activity}} by {{Human Interpreters}}},
  doi = {10.1111/j.0013-9580.2005.21104.x},
  abstract = {Summary:\hspace{0.6em}Purpose: Objectively to evaluate whether independent spike detection by human interpreters is clinically valid in magnetoencephalography (MEG) and to characterize detection differences between MEG and scalp electroencephalography (EEG). Methods: We simultaneously recorded scalp EEG and MEG data from 43 patients with intractable focal epilepsy. Raw EEG and MEG waveforms were reviewed independently by two experienced epileptologists, one for EEG and one for MEG, blinded to the other modality and to the clinical information. The number and localization of spikes detected by EEG and/or MEG were compared in relation to clinical diagnosis based on postoperative seizure freedom. Results: Interictal spikes were captured in both EEG and MEG in 31, in MEG alone in eight, in EEG alone in one, and in neither modality in three patients. The number of detections ranged widely with no statistical difference between modalities. A median of 25.7\% of total spikes was detectable by both modalities. Spike localization was similarly consistent with the epilepsy diagnosis in 85.2\% (EEG) and 78.1\% (MEG) of the patients. Inaccurate localization occurred only in those cases with very few spikes detected, especially when the detections were in one modality alone. Conclusions: Interictal epileptiform discharges are easily perceived in MEG. Independent spike identification in MEG can provide clinical results comparable, but not superior, to EEG. Many spikes were seen in only one modality or the other; therefore the use of both EEG and MEG may provide additional information.},
  language = {en},
  timestamp = {2016-11-24T13:16:03Z},
  number = {1},
  urldate = {2016-11-24},
  journal = {Epilepsia},
  author = {Iwasaki, Masaki and Pestana, Elia and Burgess, Richard C. and L{\"u}ders, Hans O. and Shamoto, Hiroshi and Nakasato, Nobukazu},
  month = jan,
  year = {2005},
  keywords = {Electroencephalography,Epilepsy,interictal spikes,Magnetoencephalography},
  pages = {59--68},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\9HH7PWD3\\abstract.html:text/html}
}

@article{Oishi2006,
  title = {Single and {{Multiple Clusters}} of {{Magnetoencephalographic Dipoles}} in {{Neocortical Epilepsy}}: {{Significance}} in {{Characterizing}} the {{Epileptogenic Zone}}},
  volume = {47},
  issn = {1528-1167},
  shorttitle = {Single and {{Multiple Clusters}} of {{Magnetoencephalographic Dipoles}} in {{Neocortical Epilepsy}}},
  doi = {10.1111/j.1528-1167.2006.00428.x},
  abstract = {Summary:\hspace{0.6em}Purpose: To characterize the epileptogenic zone in neocortical epilepsy (NE) by using magnetoencephalography (MEG). Methods: We defined and compared locations of single and multiple clusters of equivalent current dipoles (ECDs) for interictal spikes with MRI findings, ictal-onset zones (IOZs) from subdural electroencephalography (SDEEG), resected areas, and postsurgical outcomes of 20 patients who underwent cortical resection for medically intractable NE. Results: Fourteen patients had single clusters; six had multiple clusters. Overlap of clusters and IOZs defined group A (nine patients), in which a single cluster coincided with the IOZ; group B1 (four patients), in which a single cluster was within or partially overlapped the IOZ; group B2 (five patients), in which multiple-cluster sections overlapped IOZs; group C (two patients; one single; one multiple), in which no overlap was seen. More single clusters (nine of 14) than multiple clusters (none of six) coincided with the IOZ (p = 0.014). More patients with single clusters (10 of 14) than patients with multiple clusters (one of six) had seizure-free outcomes (p = 0.049). Eight of nine patients in group A, versus three of 11 in groups B1, B2, and C, achieved seizure-free outcomes (p = 0.0098). Correlations between MRI findings and postsurgical outcomes were not statistically significant; eight of 13 patients with single lesions, one of four with no lesions, and two of three with multifocal lesions had seizure-free outcomes. Conclusions: In neocortical epilepsy, MEG ECD clusters correlated with SDEEG IOZs. Single clusters indicated discrete epileptogenic zones that required complete resection for seizure-free outcome. Multiple clusters necessitated that the multiple or extensive epileptogenic zones be completely identified and delineated by SDEEG.},
  language = {en},
  timestamp = {2016-11-24T13:32:27Z},
  number = {2},
  urldate = {2016-11-24},
  journal = {Epilepsia},
  author = {Oishi, Makoto and Kameyama, Shigeki and Masuda, Hiroshi and Tohyama, Jun and Kanazawa, Osamu and Sasagawa, Mutsuo and Otsubo, Hiroshi},
  month = feb,
  year = {2006},
  keywords = {Epileptogenic zone,Ictal-onset zone,Magnetoencephalography,Neocortical epilepsy,Single and multiple clusters of dipoles},
  pages = {355--364},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IPWER3GN\\abstract.html:text/html}
}

@article{Luders2004,
  title = {Does Magnetoencephalography Add to Scalp Video-{{EEG}} as a Diagnostic Tool in Epilepsy Surgery?},
  volume = {63},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.63.10.1987},
  language = {en},
  timestamp = {2016-11-24T13:36:02Z},
  number = {10},
  urldate = {2016-11-24},
  journal = {Neurology},
  author = {L{\"u}ders, Hans O. and Iwasaki, Masaki},
  month = nov,
  year = {2004},
  pages = {1987--1988},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\TH4WT2FX\\1987.html:text/html},
  pmid = {15557543}
}

@article{Lucka2012,
  title = {Hierarchical {{Bayesian}} Inference for the {{EEG}} Inverse Problem Using Realistic {{FE}} Head Models: {{Depth}} Localization and Source Separation for Focal Primary Currents},
  volume = {61},
  issn = {1053-8119},
  shorttitle = {Hierarchical {{Bayesian}} Inference for the {{EEG}} Inverse Problem Using Realistic {{FE}} Head Models},
  doi = {10.1016/j.neuroimage.2012.04.017},
  abstract = {The estimation of the activity-related ion currents by measuring the induced electromagnetic fields at the head surface is a challenging and severely ill-posed inverse problem. This is especially true in the recovery of brain networks involving deep-lying sources by means of EEG/MEG recordings which is still a challenging task for any inverse method. Recently, hierarchical Bayesian modeling (HBM) emerged as a unifying framework for current density reconstruction (CDR) approaches comprising most established methods as well as offering promising new methods. Our work examines the performance of fully-Bayesian inference methods for HBM for source configurations consisting of few, focal sources when used with realistic, high-resolution finite element (FE) head models. The main foci of interest are the correct depth localization, a well-known source of systematic error of many CDR methods, and the separation of single sources in multiple-source scenarios. Both aspects are very important in the analysis of neurophysiological data and in clinical applications. For these tasks, HBM provides a promising framework and is able to improve upon established CDR methods such as minimum norm estimation (MNE) or sLORETA in many aspects. For challenging multiple-source scenarios where the established methods show crucial errors, promising results are attained. Additionally, we introduce Wasserstein distances as performance measures for the validation of inverse methods in complex source scenarios.},
  timestamp = {2016-11-24T13:38:10Z},
  number = {4},
  urldate = {2016-11-24},
  journal = {NeuroImage},
  author = {Lucka, Felix and Pursiainen, Sampsa and Burger, Martin and Wolters, Carsten H.},
  month = jul,
  year = {2012},
  keywords = {Current density reconstruction,Depth localization,EEG inverse problem,Fully-Bayesian inference,Hierarchical Bayesian modeling,Wasserstein distance},
  pages = {1364--1382},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\4FSWZ47I\\S1053811912003989.html:text/html}
}

@incollection{Fuchs2000,
  title = {Source {{Reconstructions}} by {{Spatial Deviation Scans}}},
  abstract = {Reconstructions of generators of biomagnetic and bioelectric activity are nowadays dominated by single or multiple equivalent current dipole models. These methods suffer from the dependence of their results from the Start values of the nonlinear minimization algorithm that optimizes the dipole positions in order to achieve a minimum of the deviation $\Delta$ between the measured data M and the forward calculated field distribution Lj: $\Delta$2 = $\parallel$ M - Lj $\parallel$2. The lead field matrix L depends on the sensor geometry, the dipole positions, and the volume conductor model. The optimum dipole components j that minimize $\Delta$2 can be calculated by j = (LTL)-1LTM.},
  language = {en},
  timestamp = {2016-11-24T14:22:24Z},
  urldate = {2016-11-24},
  booktitle = {{{SpringerLink}}},
  publisher = {{Springer New York}},
  author = {Fuchs, M. and Wischmann, H.-A. and Wagner, M. and Drenckhahn, R. and K{\"o}hler, Th},
  year = {2000},
  pages = {213--216},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\N22GDPH3\\978-1-4612-1260-7_51.html:text/html},
  doi = {10.1007/978-1-4612-1260-7_51}
}

@article{Mosher1992a,
  title = {Multiple Dipole Modeling and Localization from Spatio-Temporal {{MEG}} Data},
  volume = {39},
  issn = {0018-9294},
  doi = {10.1109/10.141192},
  abstract = {The authors present general descriptive models for spatiotemporal MEG (magnetoencephalogram) data and show the separability of the linear moment parameters and nonlinear location parameters in the MEG problem. A forward model with current dipoles in a spherically symmetric conductor is used as an example: however, other more advanced MEG models, as well as many EEG (electroencephalogram) models, can also be formulated in a similar linear algebra framework. A subspace methodology and computational approach to solving the conventional least-squares problem is presented. A new scanning approach, equivalent to the statistical MUSIC method, is also developed. This subspace method scans three-dimensional space with a one-dipole model, making it computationally feasible to scan the complete head volume.},
  timestamp = {2016-11-24T14:24:14Z},
  number = {6},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Mosher, J. C. and Lewis, P. S. and Leahy, R. M.},
  month = jun,
  year = {1992},
  keywords = {algorithms,biomagnetism,Biomedical measurements,brain models,Computer Simulation,forward model,Humans,Image processing,Laboratories,Least-Squares Analysis,least-squares problem,Linear Models,linear moment parameters,Localization,Magnetic field measurement,magnetoencephalogram,Magnetoencephalography,Models; Neurological,multiple dipole modeling,Multiple signal classification,Neurons,nonlinear location parameters,Power engineering and energy,principal component analysis,signal processing,spherically symmetric conductor,SQUIDs,statistical MUSIC method,subspace method,subspace methodology},
  pages = {541--557},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\I5T959VT\\141192.html:text/html}
}

@article{Vorwerk2012,
  title = {Comparison of {{Boundary Element}} and {{Finite Element Approaches}} to the {{EEG Forward Problem}}},
  volume = {57},
  doi = {10.1515/bmt-2012-4152},
  timestamp = {2016-11-24T14:27:35Z},
  number = {SI-1 Track-O},
  urldate = {2016-11-24},
  journal = {Biomedical Engineering / Biomedizinische Technik},
  author = {Vorwerk, J. and Clerc, M. and Burger, M. and Wolters, C. H.},
  year = {2012},
  pages = {795--798}
}

@article{Wolters2007a,
  title = {Geometry-{{Adapted Hexahedral Meshes Improve Accuracy}} of {{Finite}}-{{Element}}-{{Method}}-{{Based EEG Source Analysis}}},
  volume = {54},
  issn = {0018-9294},
  doi = {10.1109/TBME.2007.890736},
  abstract = {Mesh generation in finite-element- (FE) method-based electroencephalography (EEG) source analysis generally influences greatly the accuracy of the results. It is thus important to determine a meshing strategy well adopted to achieve both acceptable accuracy for potential distributions and reasonable computation times and memory usage. In this paper, we propose to achieve this goal by smoothing regular hexahedral finite elements at material interfaces using a node-shift approach. We first present the underlying theory for two different techniques for modeling a current dipole in FE volume conductors, a subtraction and a direct potential method. We then evaluate regular and smoothed elements in a four-layer sphere model for both potential approaches and compare their accuracy. We finally compute and visualize potential distributions for a tangentially and a radially oriented source in the somatosensory cortex in regular and geometry-adapted three-compartment hexahedra FE volume conductor models of the human head using both the subtraction and the direct potential method. On the average, node-shifting reduces both topography and magnitude errors by more than a factor of 2 for tangential and 1.5 for radial sources for both potential approaches. Nevertheless, node-shifting has to be carried out with caution for sources located within or close to irregular hexahedra, because especially for the subtraction method extreme deformations might lead to larger overall errors. With regard to realistic volume conductor modeling, node-shifted hexahedra should thus be used for the skin and skull compartments while we would not recommend deforming elements at the grey and white matter surfaces.},
  timestamp = {2016-11-24T14:30:59Z},
  number = {8},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Wolters, C. H. and Anwander, A. and Berti, G. and Hartmann, U.},
  month = aug,
  year = {2007},
  keywords = {Action Potentials,algorithms,bioelectric potentials,Brain,Brain Mapping,Brain modeling,brain models,Computer Simulation,Conducting materials,Conductors,deformations,Diagnosis; Computer-Assisted,Dipole,direct potential approach,direct potential method,Distributed computing,EEG,EEG source analysis,Electroencephalography,FE volume conductor models,finite element analysis,Finite element method,finite-element method,Finite element methods,geometry-adapted hexahedra,geometry-adapted meshes,geometry-adapted volume conductor models,grey matter surface,hexahedral meshes,human head,Humans,material interfaces,mesh generation,Models; Neurological,neurophysiology,node-shift approach,node-shifted hexahedra,node-shifting,radially oriented source,realistic head modeling,regular hexahedra,Reproducibility of Results,Sensitivity and Specificity,Skin,skull compartments,smoothing methods,Solid modeling,Somatosensory Cortex,Source reconstruction,subtraction method.,subtraction potential approach,tangential sources,three-compartment hexahedra volume conductor model,visualization,white matter surface},
  pages = {1446--1453},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\2VFT3DFU\\4273626.html:text/html}
}

@article{Hoekema2003,
  title = {Measurement of the {{Conductivity}} of {{Skull}}, {{Temporarily Removed During Epilepsy Surgery}}},
  volume = {16},
  issn = {0896-0267, 1573-6792},
  doi = {10.1023/A:1025606415858},
  abstract = {The conductivity of the human skull plays an important role in source localization of brain activity, because it is low as compared to other tissues in the head. The value usually taken for the conductivity of skull is questionable. In a carefully chosen procedure, in which sterility, a stable temperature, and relative humidity were guaranteed, we measured the (lumped, homogeneous) conductivity of the skull in five patients undergoing epilepsy surgery, using an extended four-point method. Twenty-eight current configurations were used, in each of which the potential due to an applied current was measured. A finite difference model, incorporating the geometry of the skull and the electrode locations, derived from CT data, was used to mimic the measurements. The conductivity values found were ranging from 32 mS/m to 80 mS/m, which is much higher than the values reported in other studies. Causes for these higher conductivity values are discussed.},
  language = {en},
  timestamp = {2016-11-24T14:45:57Z},
  number = {1},
  urldate = {2016-11-24},
  journal = {Brain Topography},
  author = {Hoekema, R. and Wieneke, G. H. and Leijten, F. S. S. and van Veelen, C. W. M. and van Rijen, P. C. and Huiskamp, G. J. M. and Ansems, J. and van Huffelen, A. C.},
  month = sep,
  year = {2003},
  pages = {29--38},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\FHCEE3IA\\A1025606415858.html:text/html}
}

@article{Baumann1997,
  title = {The Electrical Conductivity of Human Cerebrospinal Fluid at Body Temperature},
  volume = {44},
  issn = {0018-9294},
  doi = {10.1109/10.554770},
  abstract = {The electrical conductivity of human cerebrospinal fluid (CSF) from seven patients was measured at both room temperature (25$^\circ$C) and body temperature (37$^\circ$C). Across the frequency range of 10 Hz-10 kHz, room temperature conductivity was 1.45 S/m, but body temperature conductivity was 1.79 S/m, approximately 23\% higher. Modelers of electrical sources in the human brain have underestimated human CSF conductivity by as much as 44\% for nearly two decades, and this should be corrected to increase the accuracy of source localization models.},
  timestamp = {2016-11-24T14:48:30Z},
  number = {3},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Baumann, S. B. and Wozny, D. R. and Kelly, S. K. and Meno, F. M.},
  month = mar,
  year = {1997},
  keywords = {10 Hz to 10 kHz,25 C,37 C,bioelectric phenomena,Biological system modeling,Biomedical measurements,biothermics,body temperature,Brain,Brain modeling,Cerebrospinal fluid,Conductivity measurement,electrical conductivity,electrical sources,Electric Conductivity,Frequency measurement,human brain,human cerebrospinal fluid electrical conductivity,Humans,Magnetic heads,model accuracy,Models; Neurological,patients,Plasma temperature,Platinum,room temperature,source localization models,Temperature distribution},
  pages = {220--223},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\B268NIU4\\554770.html:text/html}
}

@article{Ramon2004,
  title = {Role of {{Soft Bone}}, {{CSF}} and {{Gray Matter}} in {{EEG Simulations}}},
  volume = {16},
  issn = {0896-0267, 1573-6792},
  doi = {10.1023/B:BRAT.0000032859.68959.76},
  abstract = {Effects of soft skull bone, cerebrospinal fluid (CSF) and gray matter on scalp potentials were examined with highly heterogeneous finite element models of an adult male subject. These models were constructed from segmented T1 weighted magnetic resonance images. Models had voxel resolutions of 1x1x3.2 mm with a total of about 1.5 million voxels. The scalp potentials, due to a dipolar source in the motor cortex area, were computed with an adaptive finite element solver. It was found that the scalp potentials were significantly affected by the soft bone, CSF and gray matter tissue boundaries in the models.},
  language = {en},
  timestamp = {2016-11-24T14:49:18Z},
  number = {4},
  urldate = {2016-11-24},
  journal = {Brain Topography},
  author = {Ramon, Ceon and Schimpf, P. and Haueisen, J. and Holmes, M. and Ishimaru, A.},
  month = jun,
  year = {2004},
  pages = {245--248},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RRU9IVM3\\BBRAT.0000032859.68959.html:text/html}
}

@article{Dassios2009a,
  title = {Electro-Magneto-Encephalography for a Three-Shell Model: Dipoles and beyond for the Spherical Geometry},
  volume = {25},
  issn = {0266-5611},
  shorttitle = {Electro-Magneto-Encephalography for a Three-Shell Model},
  doi = {10.1088/0266-5611/25/3/035001},
  abstract = {We consider the inverse problem of identifying the current inside the brain, within the framework of the three-shell spherical model, from either electroencephalographic or magnetoencephalographic measurements, under some a priori assumptions about the nature of the current. In particular, we show that under the assumption that the current is localized within a small sphere of radius $\epsilon$, it is possible to determine explicitly the center of the sphere as well as the characteristics of the current by solving a certain system of linear algebraic equations. In addition, we derive simple algorithms for identifying one or more dipoles. Finally, we present a set of compatibility conditions which can be used to verify whether the given measurements can be described by n dipoles.},
  language = {en},
  timestamp = {2016-11-25T13:01:49Z},
  number = {3},
  urldate = {2016-11-25},
  journal = {Inverse Problems},
  author = {Dassios, G. and Fokas, A. S.},
  year = {2009},
  pages = {035001}
}

@article{Hein2009,
  title = {Approximate Source Conditions for Nonlinear Ill-Posed Problems\textemdash{}chances and Limitations},
  volume = {25},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/25/3/035003},
  abstract = {In the recent past the authors, with collaborators, have published convergence rate results for regularized solutions of linear ill-posed operator equations by avoiding the usual assumption that the solutions satisfy prescribed source conditions. Instead the degree of violation of such source conditions is expressed by distance functions d ( R ) depending on a radius R $\geq$ 0 which is an upper bound of the norm of source elements under consideration. If d ( R ) tends to zero as R $\rightarrow$ $\infty$ an appropriate balancing of occurring regularization error terms yields convergence rates results. This approach was called the method of approximate source conditions, originally developed in a Hilbert space setting. The goal of this paper is to formulate chances and limitations of an application of this method to nonlinear ill-posed problems in reflexive Banach spaces and to complement the field of low order convergence rates results in nonlinear regularization theory. In particular, we are going to establish convergence rates for a variant of Tikhonov regularization. To keep structural nonlinearity conditions simple, we update the concept of degree of nonlinearity in Hilbert spaces to a Bregman distance setting in Banach spaces.},
  language = {en},
  timestamp = {2016-11-25T13:03:26Z},
  number = {3},
  urldate = {2016-11-25},
  journal = {Inverse Problems},
  author = {Hein, Torsten and Hofmann, Bernd},
  year = {2009},
  pages = {035003}
}

@article{Dassios2013,
  title = {The Definite Non-Uniqueness Results for Deterministic {{EEG}} and {{MEG}} Data},
  volume = {29},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/29/6/065012},
  abstract = {The solvability of the inverse problems of electroencephalography and magnetoencephalography has been studied extensively in the literature using a variety of models, including spherical and non-spherical geometries, homogeneous and inhomogeneous head models, and neuronal excitations involving the discrete and continuous distribution of dipoles. Among the important methods used are the methods based on spectral decompositions, physical arguments and integral representation techniques. Regarding the uniqueness of these inverse problems, a general result, independent of the geometry and the homogeneity of the conducting medium, has been obtained recently by the second author. This paper summarizes this result which appears to be mathematically definitive, in the sense that the geometry is arbitrary, the brain is surrounded by shells of varying conductivities, the neuronal current is arbitrary, the data are complete and the proofs are analytical. Furthermore, this paper includes a summary of the main steps of the proofs leading to the above result. In addition, it demonstrates the consistency of this general uniqueness result with all earlier results known in the literature.},
  language = {en},
  timestamp = {2016-11-25T13:05:50Z},
  number = {6},
  urldate = {2016-11-25},
  journal = {Inverse Problems},
  author = {Dassios, George and Fokas, A. S.},
  year = {2013},
  pages = {065012}
}

@article{Fokas2004,
  title = {The Unique Determination of Neuronal Currents in the Brain via Magnetoencephalography},
  volume = {20},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/20/4/005},
  abstract = {The problem of determining the neuronal current inside the brain from measurements of the induced magnetic field outside the head is discussed under the assumption that the space occupied by the brain is approximately spherical. By inverting the Geselowitz equation, the part of the current which can be reconstructed from the measurements is precisely determined. This actually consists of only certain moments of one of the two functions specifying the tangential part of the current. The other function specifying the tangential part of the current as well as the radial part of the current is completely arbitrary. However, it is also shown that with the assumption of energy minimization, the current can be reconstructed uniquely. A numerical implementation of this unique reconstruction is also presented.},
  language = {en},
  timestamp = {2016-11-25T13:07:55Z},
  number = {4},
  urldate = {2016-11-25},
  journal = {Inverse Problems},
  author = {Fokas, A. S. and Kurylev, Y. and Marinakis, V.},
  year = {2004},
  pages = {1067}
}

@incollection{Dassios2006,
  address = {Providence, Rhode Island},
  title = {What Is Recoverable in the Inverse Magnetoencephalography Problem?},
  volume = {408},
  isbn = {978-0-8218-3968-3 978-0-8218-7998-6},
  language = {en},
  timestamp = {2016-11-25T13:20:00Z},
  urldate = {2016-11-25},
  booktitle = {Contemporary {{Mathematics}}},
  publisher = {{American Mathematical Society}},
  author = {Dassios, George},
  editor = {Ammari, Habib and Kang, Hyeonbae},
  year = {2006},
  pages = {181--200}
}

@incollection{Dassios2009b,
  series = {Lecture Notes in Mathematics},
  title = {Electric and {{Magnetic Activity}} of the {{Brain}} in {{Spherical}} and {{Ellipsoidal Geometry}}},
  copyright = {\textcopyright{}2009 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-03443-5 978-3-642-03444-2},
  abstract = {Understanding the functional brain is one of the top challenges of contemporary science. The challenge is connected with the fact that we are trying to understand how an organized structure works and the only means available for this task is the structure itself. Therefore an extremely complicated scientific problem is combined with a hard philosophical problem. Under the given conditions it comes as no surprise that so many, apparently simple, physical and mathematical problems in neuroscience are not generally solved today. One of this problems is the electromagnetic problem of a current field inside an arbitrary conductor. We do understand the physics of this problem, but it is very hard to solve the corresponding mathematical problem if the geometry of the conducting medium diverts from the spherical one. Mathematically, the human brain is an approximately 1.5 L of conductive material in the shape of an ellipsoid with average semiaxes of 6, 6.5 and 9 cm [47]. On the outermost layer of the brain, known as the cerebral cortex, most of the 1011 neurons contained within the brain are distributed. The neurons are the basic elements of this complicated network and each one of them possesses 104 interconnections with neighboring neurons. At each interconnection, also known as a synapse, neurons communicate via the transfer of particular ions, the neurotransmitters [38]. Neurons are electrochemically excited and they are able to fire instantaneous currents giving rise to very weak magnetic fields which can be measured with the SQUID (Superconducting QUantum Interference Device). The SQUID is the most sensitive apparatus ever built. It can measure magnetic fields as small as 10-14 T, a sensitivity that is necessary to measure the 10-15 to 10-13 T fields resulting from brain activity. For comparison we mention that the magnetic fields due to brain activity are about 10-9 of the Earth's average magnetic field, 10-5 of the fluctuations of the Earth's magnetic field, and about 10-3 of the maximum magnetic field generated by the beating heart.},
  language = {en},
  timestamp = {2016-11-25T13:25:19Z},
  number = {1983},
  urldate = {2016-11-25},
  booktitle = {Mathematical {{Modeling}} in {{Biomedical Imaging I}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Dassios, George},
  editor = {Ammari, Habib},
  year = {2009},
  keywords = {Imaging / Radiology,Mathematical and Computational Biology,Ordinary Differential Equations,partial differential equations,Potential Theory},
  pages = {133--202},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QEMB88XI\\978-3-642-03444-2_4.html:text/html},
  doi = {10.1007/978-3-642-03444-2_4}
}

@book{zotero-null-15972,
  title = {Mathematical {{Modeling}} in {{Biomedical Imaging I}} - {{Electrical}} and | {{Habib Ammari}} | {{Springer}}},
  abstract = {This volume gives an introduction to a fascinating research area to applied mathematicians. It is devoted to providing the exposition of promising...},
  timestamp = {2016-11-25T13:26:47Z},
  urldate = {2016-11-25},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3HB52UH6\\9783642034435.html:text/html}
}

@article{Fokas1996,
  title = {Inversion Method for Magnetoencephalography},
  volume = {12},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/12/3/001},
  abstract = {The problem of determining the neural current inside the brain from measurements of the magnetic field outside the head is discussed. In particular the non-uniqueness question is completely resolved. Furthermore, it is shown that with the assumption of energy minimization, the current can be reconstructed uniquely.},
  language = {en},
  timestamp = {2016-11-25T13:29:21Z},
  number = {3},
  urldate = {2016-11-25},
  journal = {Inverse Problems},
  author = {Fokas, A. S. and Gel-fand, I. M. and Kurylev, Y.},
  year = {1996},
  pages = {L9}
}

@article{Jerbi2002,
  title = {On {{MEG}} Forward Modelling Using Multipolar Expansions},
  volume = {47},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/47/4/301},
  abstract = {Magnetoencephalography (MEG) is a non-invasive functional imaging modality based on the measurement of the external magnetic field produced by neural current sources within the brain. The reconstruction of the underlying sources is a severely ill-posed inverse problem typically tackled using either low-dimensional parametric source models, such as an equivalent current dipole (ECD), or high-dimensional minimum-norm imaging techniques. The inability of the ECD to properly represent non-focal sources and the over-smoothed solutions obtained by minimum-norm methods underline the need for an alternative approach. Multipole expansion methods have the advantages of the parametric approach while at the same time adequately describing sources with significant spatial extent and arbitrary activation patterns. In this paper we first present a comparative review of spherical harmonic and Cartesian multipole expansion methods that can be used in MEG. The equations are given for the general case of arbitrary conductors and realistic sensor configurations and also for the special cases of spherically symmetric conductors and radially oriented sensors. We then report the results of computer simulations used to investigate the ability of a first-order multipole model (dipole and quadrupole) to represent spatially extended sources, which are simulated by 2D and 3D clusters of elemental dipoles. The overall field of a cluster is analysed using singular value decomposition and compared to the unit fields of a multipole, centred in the middle of the cluster, using subspace correlation metrics. Our results demonstrate the superior utility of the multipolar source model over ECD models in providing source representations of extended regions of activity.},
  language = {en},
  timestamp = {2016-11-25T13:32:33Z},
  number = {4},
  urldate = {2016-11-25},
  journal = {Physics in Medicine and Biology},
  author = {Jerbi, K. and Mosher, J. C. and Baillet, S. and Leahy, R. M.},
  year = {2002},
  pages = {523}
}

@article{Frank1952,
  title = {Electric {{Potential Produced}} by {{Two Point Current Sources}} in a {{Homogeneous Conducting Sphere}}},
  volume = {23},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.1702037},
  abstract = {The electric potential produced by positive and negative point current sources located in a homogeneous conducting sphere, a problem of interest in the field of electrocardiography, is obtained for arbitrary source locations and separations. Certain special cases of the general solution that are of particular value in both experimental and theoretical electrocardiographic research are also presented.},
  timestamp = {2016-11-25T14:16:50Z},
  number = {11},
  urldate = {2016-11-25},
  journal = {Journal of Applied Physics},
  author = {Frank, Ernest},
  month = nov,
  year = {1952},
  keywords = {Electric currents},
  pages = {1225--1228},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SSPJQCHE\\1.html:text/html}
}

@inproceedings{Lomet2012,
  title = {Model Selection in Block Clustering by the Integrated Classification Likelihood},
  abstract = {Block clustering (or co-clustering) aims at simultaneously partitioning the rows and columns of a data table to reveal homogeneous block structures. This structure can stem from the latent block model which provides a probabilistic modeling of data tables whose block pattern is defined from the row and column classes. For continuous data, each table entry is typically assumed to follow a Gaussian distribution. For a given data table, several candidate models are usually examined: they may differ in the numbers of clusters or in the number of free parameters. Model selection then becomes a critical issue, for which the tools that have been derived for model-based one-way clustering need to be adapted. In one-way clustering, most selection criteria are based on asymptotical considerations that are difficult to render in block clustering due to dual nature of rows and columns. We circumvent this problem by developing a non-asymptotic criterion based on the Integrated Classification Likelihood. This criterion can be computed in closed form once a proper prior distribution has been defined on the parameters. The experimental results show steady performances for medium to large data tables with well-separated and moderately-separated clusters.},
  language = {en},
  timestamp = {2016-12-01T15:26:35Z},
  urldate = {2016-12-01},
  author = {Lomet, Aurore and Govaert, G{\'e}rard and Grandvalet, Yves},
  month = aug,
  year = {2012},
  pages = {519--530},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\CS7I9DIT\\hal-00730829.html:text/html}
}

@article{Wyse2012,
  title = {Block Clustering with Collapsed Latent Block Models},
  volume = {22},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-011-9233-4},
  abstract = {We introduce a Bayesian extension of the latent block model for model-based block clustering of data matrices. Our approach considers a block model where block parameters may be integrated out. The result is a posterior defined over the number of clusters in rows and columns and cluster memberships. The number of row and column clusters need not be known in advance as these are sampled along with cluster memberhips using Markov chain Monte Carlo. This differs from existing work on latent block models, where the number of clusters is assumed known or is chosen using some information criteria. We analyze both simulated and real data to validate the technique.},
  language = {en},
  timestamp = {2016-12-01T15:27:56Z},
  number = {2},
  urldate = {2016-12-01},
  journal = {Statistics and Computing},
  author = {Wyse, Jason and Friel, Nial},
  month = mar,
  year = {2012},
  pages = {415--428},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MBZZMTA2\\s11222-011-9233-4.html:text/html}
}

@article{Lomet2014,
  title = {Model Selection for {{Gaussian}} Latent Block Clustering with the Integrated Classification Likelihood},
  issn = {1862-5347, 1862-5355},
  doi = {10.1007/s11634-013-0161-3},
  abstract = {Block clustering aims to reveal homogeneous block structures in a data table. Among the different approaches of block clustering, we consider here a model-based method: the Gaussian latent block model for continuous data which is an extension of the Gaussian mixture model for one-way clustering. For a given data table, several candidate models are usually examined, which differ for example in the number of clusters. Model selection then becomes a critical issue. To this end, we develop a criterion based on an approximation of the integrated classification likelihood for the Gaussian latent block model, and propose a Bayesian information criterion-like variant following the same pattern. We also propose a non-asymptotic exact criterion, thus circumventing the controversial definition of the asymptotic regime arising from the dual nature of the rows and columns in co-clustering. The experimental results show steady performances of these criteria for medium to large data tables.},
  language = {en},
  timestamp = {2016-12-01T15:29:41Z},
  urldate = {2016-12-01},
  journal = {Advances in Data Analysis and Classification},
  author = {Lomet, Aurore and Govaert, G{\'e}rard and Grandvalet, Yves},
  month = feb,
  year = {2014},
  pages = {1--20},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\ZB96GG3Z\\s11634-013-0161-3.html:text/html}
}

@inproceedings{Lomet2012a,
  title = {An {{Approximation}} of the {{Integrated Classification Likelihood}} for the {{Latent Block Model}}},
  doi = {10.1109/ICDMW.2012.32},
  abstract = {Block clustering (or co-clustering or simultaneous clustering) aims at simultaneously partitioning the rows and columns of a data table to reveal homogeneous block structures. This structure can stem from the latent block model which provides a probabilistic modelling of data tables whose block patterns are defined from the row and column classes. For continuous data, each table entry is typically assumed to follow a Gaussian distribution whose parameters are common to all entries belonging to the same block, that is, sharing the same row and column classes. For a given data table, several candidate models are usually examined: they may differ in the numbers of clusters or more generally in the number of free parameters of the model. Model selection then becomes a critical issue, for which the tools that have been derived for model-based one-way clustering need to be adapted. We develop here a criterion based on an approximation of the Integrated Classification Likelihood (ICL) of block models, and propose a BIC-like variant following a similar form. The proposed criteria are assessed on simulated data, where their performances are shown to be fairly reliable for medium to large data tables with well-separated clusters.},
  timestamp = {2016-12-01T15:31:14Z},
  booktitle = {2012 {{IEEE}} 12th {{International Conference}} on {{Data Mining Workshops}}},
  author = {Lomet, A. and Govaert, G. and Grandvalet, Y.},
  month = dec,
  year = {2012},
  keywords = {Adaptation models,approximation,Approximation methods,approximation theory,Bayesian methods,BIC,block clustering,co-clustering,collections of physical data,Computational modeling,continuous data,Data models,data table,Gaussian distribution,homogeneous block structures,ICL,integrated classification likelihood,latent block model,model-based one-way clustering,model selection,pattern classification,Probabilistic logic,Robustness,simulated data},
  pages = {147--153},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\46F9QRGA\\6406435.html:text/html}
}

@article{Come2015,
  title = {Model Selection and Clustering in Stochastic Block Models Based on the Exact Integrated Complete Data Likelihood},
  issn = {1471-082X, 1477-0342},
  doi = {10.1177/1471082X15577017},
  abstract = {The stochastic block model (SBM) is a mixture model for the clustering of nodes in networks. The SBM has now been employed for more than a decade to analyze very different types of networks in many scientific fields, including biology and the social sciences. Recently, an analytical expression based on the collapsing of the SBM parameters has been proposed, in combination with a sampling procedure that allows the clustering of the vertices and the estimation of the number of clusters to be performed simultaneously. Although the corresponding algorithm can technically accommodate up to 10 000 nodes and millions of edges, the Markov chain, however, tends to exhibit poor mixing properties, that is, low acceptance rates, for large networks. Therefore, the number of clusters tends to be highly overestimated, even for a very large number of samples. In this article, we rely on a similar expression, which we call the integrated complete data log likelihood, and propose a greedy inference algorithm that focuses on maximizing this exact quantity. This algorithm incurs a smaller computational cost than existing inference techniques for the SBM and can be employed to analyze large networks (several tens of thousands of nodes and millions of edges) with no convergence problems. Using toy datasets, the algorithm exhibits improvements over existing strategies, both in terms of clustering and model selection. An application to a network of blogs related to illustrations and comics is also provided.},
  language = {en},
  timestamp = {2016-12-01T15:33:18Z},
  urldate = {2016-12-01},
  journal = {Statistical Modelling},
  author = {C{\^o}me, Etienne and Latouche, Pierre},
  month = mar,
  year = {2015},
  keywords = {greedy inference,integrated classification likelihood,networks,random graphs,stochastic block models},
  pages = {1471082X15577017},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\F974BTMB\\1471082X15577017.html:text/html}
}

@book{Duda2000a,
  address = {New York; Chichester},
  edition = {2nd Revised edition},
  title = {{Pattern Classification 2e}},
  isbn = {978-0-471-05669-0},
  language = {Anglais},
  timestamp = {2016-12-09T13:51:37Z},
  publisher = {{Wiley-Blackwell}},
  author = {Duda, R. O.},
  month = nov,
  year = {2000}
}

@Online{pahio2005,
  author    = {{pahio}},
  title     = {Summed Numerator and Summed Denominator},
  year      = {2005},
  url       = {https://planetmath.org/SummedNumeratorAndSummedDenominator},
  timestamp = {2017-02-17T10:23:51Z},
}

@article{Srinivasan2006a,
  title = {Source Analysis of {{EEG}} Oscillations Using High-Resolution {{EEG}} and {{MEG}}},
  volume = {159},
  issn = {0079-6123},
  doi = {10.1016/S0079-6123(06)59003-X},
  abstract = {We investigated spatial properties of the source distributions that generate scalp electroencephalographic (EEG) oscillations. The inherent complexity of the spatiotemporal dynamics of EEG oscillations indicates that conceptual models that view source activity as consisting of only of a few ``equivalent dipoles'' are inadequate. We present an approach that uses volume conduction models to characterize the distinct spatial filtering of cortical source activity by average-reference EEG, high-resolution EEG, and magnetoencephalography (MEG). By comparing these three measures, we can make inferences about the sources of EEG oscillations without having to make prior assumptions about the sources. We apply this approach to spontaneous EEG oscillations observed with eyes closed at rest. Both EEG and MEG recordings show robust alpha rhythms over posterior regions of the cortex; however, the dominant frequency of these rhythms varies between EEG and MEG recordings. Frontal alpha and theta rhythms are generated almost exclusively by superficial radial dipole layers that generate robust EEG signals but very little MEG signals; these sources are presumably mainly in the gyral crowns of frontal cortex. MEG and high-resolution EEG estimates of alpha rhythms provide evidence of local tangential and radial sources in the posterior cortex, lying mainly on sulcal and gyral surfaces. Despite the detailed information about local radial and tangential sources potentially afforded by high-resolution EEG and MEG, it is also evident that the alpha and theta rhythms receive contributions from non-local source activity, for instance large dipole layers distributed over lobeal or (potentially) even larger spatial scales.},
  timestamp = {2017-03-23T18:23:32Z},
  urldate = {2017-03-23},
  journal = {Progress in brain research},
  author = {Srinivasan, Ramesh and Winter, William R. and Nunez, Paul L.},
  year = {2006},
  pages = {29--42},
  pmid = {17071222},
  pmcid = {PMC1995013}
}

@article{Leblond2015,
  title = {Identifiability {{Properties}} for {{Inverse Problems}} in {{EEG Data Processing Medical Engineering}} with {{Observability}} and {{Optimization Issues}}},
  volume = {135},
  issn = {0167-8019, 1572-9036},
  doi = {10.1007/s10440-014-9951-7},
  abstract = {We consider inverse problems of source identification in electroencephalography, modelled by elliptic partial differential equations. Being given boundary data that consist in values of the current fl},
  language = {en},
  timestamp = {2017-03-23T18:25:26Z},
  number = {1},
  urldate = {2017-03-23},
  journal = {Acta Applicandae Mathematicae},
  author = {Leblond, Juliette},
  month = feb,
  year = {2015},
  pages = {175--190},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MSNDV54H\\s10440-014-9951-7.html:text/html}
}

@article{Ary1981,
  title = {Location of {{Sources}} of {{Evoked Scalp Potentials}}: {{Corrections}} for {{Skull}} and {{Scalp Thicknesses}}},
  volume = {BME-28},
  issn = {0018-9294},
  shorttitle = {Location of {{Sources}} of {{Evoked Scalp Potentials}}},
  doi = {10.1109/TBME.1981.324817},
  abstract = {The problem of locating the position of the source of evoked potentials from measurements on the surface of the scalp has been examined. It is shown that the position of the source in a head modeled by a sphere surrounded by two concentric shells of differing conductivities representing the skull and the scalp can be inferred from source localization calculations made on a homogeneous model.},
  timestamp = {2017-03-23T18:27:38Z},
  number = {6},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Ary, J. P. and Klein, S. A. and Fender, D. H.},
  month = jun,
  year = {1981},
  keywords = {Bioinformatics,Brain modeling,Conductivity,Electroencephalography,Evoked Potentials,Head,Humans,Nonuniform electric fields,Position measurement,Scalp,Skull,Surface fitting,Systems engineering and theory},
  pages = {447--452},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8NZMAE8B\\4121248.html:text/html}
}

@article{Zakharov2014,
  title = {Limits of Applicability of a Spherical Model for Solving Problems of Electroencephalography},
  volume = {38},
  issn = {0278-6419, 1934-8428},
  doi = {10.3103/S0278641914030091},
  abstract = {Methods are developed for the localization of neuron brain sources with the potential recorded as an electroencephalogram (EEG). A boundary-value problem for a Poisson equation is considered. A bounda},
  language = {en},
  timestamp = {2017-03-23T18:30:15Z},
  number = {3},
  urldate = {2017-03-23},
  journal = {Moscow University Computational Mathematics and Cybernetics},
  author = {Zakharov, E. V. and Zimozdra, R. E.},
  month = jul,
  year = {2014},
  pages = {100--104},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\IMF6T9ZU\\S0278641914030091.html:text/html}
}

@article{Parsons2004,
  title = {Subspace {{Clustering}} for {{High Dimensional Data}}: {{A Review}}},
  volume = {6},
  issn = {1931-0145},
  shorttitle = {Subspace {{Clustering}} for {{High Dimensional Data}}},
  doi = {10.1145/1007730.1007731},
  abstract = {Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces within a dataset. Often in high dimensional data, many dimensions are irrelevant and can mask existing clusters in noisy data. Feature selection removes irrelevant and redundant dimensions by analyzing the entire dataset. Subspace clustering algorithms localize the search for relevant dimensions allowing them to find clusters that exist in multiple, possibly overlapping subspaces. There are two major branches of subspace clustering based on their search strategy. Top-down algorithms find an initial clustering in the full set of dimensions and evaluate the subspaces of each cluster, iteratively improving the results. Bottom-up approaches find dense regions in low dimensional spaces and combine them to form clusters. This paper presents a survey of the various subspace clustering algorithms along with a hierarchy organizing the algorithms by their defining characteristics. We then compare the two main approaches to subspace clustering using empirical scalability and accuracy tests and discuss some potential applications where subspace clustering could be particularly useful.},
  timestamp = {2017-03-24T14:41:57Z},
  number = {1},
  urldate = {2017-03-24},
  journal = {SIGKDD Explor. Newsl.},
  author = {Parsons, Lance and Haque, Ehtesham and Liu, Huan},
  month = jun,
  year = {2004},
  keywords = {clustering survey,high dimensional data,projected clustering,Subspace clustering},
  pages = {90--105}
}

@article{Agrawal2005,
  title = {Automatic {{Subspace Clustering}} of {{High Dimensional Data}}},
  volume = {11},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-005-1396-1},
  abstract = {Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets.},
  language = {en},
  timestamp = {2017-03-24T14:45:08Z},
  number = {1},
  urldate = {2017-03-24},
  journal = {Data Mining and Knowledge Discovery},
  author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
  month = jul,
  year = {2005},
  pages = {5--33},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\QJAEU655\\s10618-005-1396-1.html:text/html}
}

@book{Vidal,
  title = {A {{Tutorial}} on {{Subspace Clustering}}},
  abstract = {The past few years have witnessed an explosion in the availability of data from multiple sources and modalities. For example, millions of cameras have been installed in buildings, streets, airports and cities around the world. This has generated extraordinary advances on how to acquire, compress, store, transmit and process massive amounts of complex high-dimensional data. Many of these advances have relied on the observation that, even though these data sets are high-dimensional, their intrinsic dimension is often much smaller than the dimension of the ambient space. In computer vision, for example, the number of pixels in an image can be rather large, yet most computer vision models use only a few parameters to describe the appearance, geometry and dynamics of a scene. This has motivated the development of a number of techniques for finding a low-dimensional representation},
  timestamp = {2017-03-24T14:50:10Z},
  author = {Vidal, Ren{\'e}},
  file = {Citeseer - Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\VGST26FT\\summary.html:text/html}
}

@article{Nayagam2015,
  title = {Comparative {{Study}} of {{Subspace Clustering Algorithms}}},
  volume = {6},
  timestamp = {2017-03-24T14:58:32Z},
  number = {5},
  journal = {(IJCSIT) International Journal of Computer Science and Information Technologies},
  author = {Nayagam, S.Chitra},
  year = {2015}
}

@book{Bertsekas2003,
  address = {Belmont, Mass},
  title = {Convex {{Analysis}} and {{Optimization}}},
  isbn = {978-1-886529-45-8},
  abstract = {A uniquely pedagogical, insightful, and rigorous treatment of the analytical/geometrical foundations of optimization. Among its special features, the book: 1) Develops rigorously and comprehensively the theory of convex sets and functions, in the classical tradition of Fenchel and Rockafellar 2) Provides a geometric, highly visual treatment of convex and nonconvex optimization problems, including existence of solutions, optimality conditions, Lagrange multipliers, and duality 3) Includes an insightful and comprehensive presentation of minimax theory and zero sum games, and its connection with duality 4) Describes dual optimization, the associated computational methods, including the novel incremental subgradient methods, and applications in linear, quadratic, and integer programming 5) Contains many examples, illustrations, and exercises with complete solutions (about 200 pages) posted on the internet. From the preface:This book focuses on the theory of convex sets and functions, and its connections with a number of topics that span a broad range from continuous to discrete optimization. These topics include Lagrange multiplier theory, Lagrangian and conjugate/Fenchel duality, minimax theory, and nondifferentiable optimization.The book evolved from a set of lecture notes for a graduate course at M.I.T. It is widely recognized that, aside from being an eminently useful subject in engineering, operations research, and economics, convexity is an excellent vehicle for assimilating some of the basic concepts of real analysis within an intuitive geometrical setting. Unfortunately, the subject's coverage in academic curricula is scant and incidental. We believe that at least part of the reason is the shortage of textbooks that are suitable for classroom instruction, particularly for nonmathematics majors. We have therefore tried to make convex analysis accessible to a broader audience by emphasizing its geometrical character, while maintaining mathematical rigor. We have included as many insightful illustrations as possible, and we have used geometric visualization as a principal tool for maintaining the students' interest in mathematical proofs.Our treatment of convexity theory is quite comprehensive, with all major aspects of the subject receiving substantial treatment. The mathematical prerequisites are a course in linear algebra and a course in real analysis in finite dimensional spaces (which is the exclusive setting of the book). A summary of this material, without proofs, is provided in Section 1.1. The coverage of the theory has been significantly extended in the exercises, which represent a major component of the book. Detailed solutions of all the exercises (nearly 200 pages) are internet-posted in the book's www pageSome of the exercises may be attempted by the reader without looking at the solutions, while others are challenging but may be solved by the advanced reader with the assistance of hints. Still other exercises represent substantial theoretical results, and in some cases include new and unpublished research. Readers and instructors should decide for themselves how to make best use of the internet-posted solutions.An important part of our approach has been to maintain a close link between the theoretical treatment of convexity and its application to optimization.},
  language = {English},
  timestamp = {2017-03-31T12:04:19Z},
  publisher = {{Athena Scientific}},
  author = {Bertsekas, Dimitri and Nedic, Angelia},
  month = apr,
  year = {2003}
}

@book{Bossavit2011,
  title = {Computational {{Electromagnetism}}: {{Variational Formulations}}, {{Complementarity}}, {{Edge Elements}}},
  isbn = {978-0-12-388560-9},
  shorttitle = {Computational {{Electromagnetism}}},
  abstract = {Computational Electromagnetism refers to the modern concept of computer-aided analysis, and design, of virtually all electric devices such as motors, machines, transformers, etc, as well as of the equipment inthe currently booming field of telecommunications, such as antennas, radars, etc. The present book is uniquely written to enable the reader- be it a student, a scientist, or a practitioner- to successfully perform important simulation techniques and to design efficient computer software for electromagnetic device analysis. Numerous illustrations, solved exercises, original ideas, and an extensive and up-to-date bibliography make it a valuable reference for both experts and beginners in the field. A researcher and practitioner will find in it information rarely available in other sources, such as on symmetry, bilateral error bounds by complimentarity, edge and face elements, treatment of infinite domains, etc. At the same time, the book is a useful teaching tool for courses in computational techniques in certain fields of physics and electrical engineering. As a self-contained text, it presents an extensive coverage of the most important concepts from Maxwells equations to computer-solvable algebraic systems- for both static, quasi-static, and harmonic high-frequency problems.},
  language = {English},
  timestamp = {2017-04-11T16:24:44Z},
  publisher = {{Academic Press}},
  author = {Bossavit, Alain and Mayergoyz, Isaak D.},
  month = jun,
  year = {2011}
}

@book{Haddar2015,
  address = {Cham},
  edition = {1st ed. 2015 edition},
  title = {Computational {{Electromagnetism}}: {{Cetraro}}, {{Italy}} 2014},
  isbn = {978-3-319-19305-2},
  shorttitle = {Computational {{Electromagnetism}}},
  abstract = {Presenting topics that have not previously been contained in a single volume, this book offers an up-to-date review of computational methods in electromagnetism, with a focus on recent results in the numerical simulation of real-life electromagnetic problems and on theoretical results that are useful in devising and analyzing approximation algorithms. Based on four courses delivered in Cetraro in June 2014, the material covered includes the spatial discretization of Maxwell's equations in a bounded domain, the numerical approximation of the eddy current model in harmonic regime, the time domain integral equation method (with an emphasis on the electric-field integral equation) and an overview of qualitative methods for inverse electromagnetic scattering problems.Assuming some knowledge of the variational formulation of PDEs and of finite element/boundary element methods, the book is suitable for PhD students and researchers interested in numerical approximation of partial differential equations and scientific computing.},
  language = {English},
  timestamp = {2017-04-11T16:26:33Z},
  publisher = {{Springer}},
  author = {Haddar, Houssem and Hiptmair, Ralf and Monk, Peter and Rodr{\'\i}guez, Rodolfo},
  editor = {de Castro, Alfredo Berm{\'u}dez and Valli, Alberto},
  month = jul,
  year = {2015}
}

@book{Tuy2016,
  address = {Cham},
  edition = {2nd ed. 2016 edition},
  title = {Convex {{Analysis}} and {{Global Optimization}}},
  isbn = {978-3-319-31482-2},
  abstract = {This book presents state-of-the-art results and methodologies in modern global optimization, and has been a staple reference for researchers, engineers, advanced students (also in applied mathematics), and practitioners in various fields of engineering. The second edition has been brought up to date and continues to develop a coherent and rigorous theory of deterministic global optimization, highlighting the essential role of convex analysis. The text has been revised and expanded to meet the needs of research, education, and applications for many years to come.Updates for this new edition include:$\cdot$~~~~~~~ Discussion of modern approaches to minimax, fixed point, and equilibrium theorems, and to nonconvex optimization; $\cdot$~~~~~~~ Increased focus on dealing more efficiently with ill-posed problems of global optimization, particularly those with hard constraints; $\cdot$~~~~~~~ Important discussions of decomposition methods for specially structured problems; $\cdot$~~~~~~~ A complete revision of the chapter on nonconvex quadratic programming, in order to encompass the advances made in quadratic optimization since publication of the first edition. $\cdot$~~~~~~~ Additionally, this new edition contains entirely new chapters devoted to monotonic optimization, polynomial optimization and optimization under equilibrium constraints, including bilevel programming, multiobjective programming, and optimization with variational inequality constraint.From the reviews of the first edition:                The book gives a good review of the topic. \ldots{}The text is carefully constructed and well written, the exposition is clear. It leaves a remarkable impression of the concepts, tools and techniques in global optimization. It might also be used as a basis and guideline for lectures on this subject. Students as well as professionals will profitably read and use it.\rule{1em}{1pt}Mathematical Methods of Operations Research, 49:3 (1999)},
  language = {English},
  timestamp = {2017-04-11T16:27:13Z},
  publisher = {{Springer}},
  author = {Tuy, Hoang},
  month = nov,
  year = {2016}
}

@book{Borwein2005,
  address = {New York, NY},
  edition = {2nd edition},
  title = {Convex {{Analysis}} and {{Nonlinear Optimization}}: {{Theory}} and {{Examples}}},
  isbn = {978-0-387-29570-1},
  shorttitle = {Convex {{Analysis}} and {{Nonlinear Optimization}}},
  abstract = {Optimization is a rich and thriving mathematical discipline, and the underlying theory of current computational optimization techniques grows ever more sophisticated. This book aims to provide a concise, accessible account of convex analysis and its applications and extensions, for a broad audience. Each section concludes with an often extensive set of optional exercises. This new edition adds material on semismooth optimization, as well as several new proofs.},
  language = {English},
  timestamp = {2017-04-11T16:28:39Z},
  publisher = {{Springer}},
  author = {Borwein, Jonathan and Lewis, Adrian S.},
  month = nov,
  year = {2005}
}

@book{Bertsekas2003a,
  address = {Belmont, Mass},
  title = {Convex {{Analysis}} and {{Optimization}}},
  isbn = {978-1-886529-45-8},
  abstract = {A uniquely pedagogical, insightful, and rigorous treatment of the analytical/geometrical foundations of optimization. Among its special features, the book: 1) Develops rigorously and comprehensively the theory of convex sets and functions, in the classical tradition of Fenchel and Rockafellar 2) Provides a geometric, highly visual treatment of convex and nonconvex optimization problems, including existence of solutions, optimality conditions, Lagrange multipliers, and duality 3) Includes an insightful and comprehensive presentation of minimax theory and zero sum games, and its connection with duality 4) Describes dual optimization, the associated computational methods, including the novel incremental subgradient methods, and applications in linear, quadratic, and integer programming 5) Contains many examples, illustrations, and exercises with complete solutions (about 200 pages) posted on the internet. From the preface:This book focuses on the theory of convex sets and functions, and its connections with a number of topics that span a broad range from continuous to discrete optimization. These topics include Lagrange multiplier theory, Lagrangian and conjugate/Fenchel duality, minimax theory, and nondifferentiable optimization.The book evolved from a set of lecture notes for a graduate course at M.I.T. It is widely recognized that, aside from being an eminently useful subject in engineering, operations research, and economics, convexity is an excellent vehicle for assimilating some of the basic concepts of real analysis within an intuitive geometrical setting. Unfortunately, the subject's coverage in academic curricula is scant and incidental. We believe that at least part of the reason is the shortage of textbooks that are suitable for classroom instruction, particularly for nonmathematics majors. We have therefore tried to make convex analysis accessible to a broader audience by emphasizing its geometrical character, while maintaining mathematical rigor. We have included as many insightful illustrations as possible, and we have used geometric visualization as a principal tool for maintaining the students' interest in mathematical proofs.Our treatment of convexity theory is quite comprehensive, with all major aspects of the subject receiving substantial treatment. The mathematical prerequisites are a course in linear algebra and a course in real analysis in finite dimensional spaces (which is the exclusive setting of the book). A summary of this material, without proofs, is provided in Section 1.1. The coverage of the theory has been significantly extended in the exercises, which represent a major component of the book. Detailed solutions of all the exercises (nearly 200 pages) are internet-posted in the book's www pageSome of the exercises may be attempted by the reader without looking at the solutions, while others are challenging but may be solved by the advanced reader with the assistance of hints. Still other exercises represent substantial theoretical results, and in some cases include new and unpublished research. Readers and instructors should decide for themselves how to make best use of the internet-posted solutions.An important part of our approach has been to maintain a close link between the theoretical treatment of convexity and its application to optimization.},
  language = {English},
  timestamp = {2017-04-11T16:30:41Z},
  publisher = {{Athena Scientific}},
  author = {Bertsekas, Dimitri and Nedic, Angelia},
  month = apr,
  year = {2003}
}

@book{Pallaschke2010,
  address = {Dordrecht},
  edition = {Softcover reprint of hardcover 1st ed. 1997 edition},
  title = {Foundations of {{Mathematical Optimization}}: {{Convex Analysis}} without {{Linearity}}},
  isbn = {978-90-481-4800-4},
  shorttitle = {Foundations of {{Mathematical Optimization}}},
  abstract = {Many books on optimization consider only finite dimensional spaces. This volume is unique in its emphasis: the first three chapters develop optimization in spaces without linear structure, and the analog of convex analysis is constructed for this case. Many new results have been proved specially for this publication. In the following chapters optimization in infinite topological and normed vector spaces is considered. The novelty consists in using the drop property for weak well-posedness of linear problems in Banach spaces and in a unified approach (by means of the Dolecki approximation) to necessary conditions of optimality. The method of reduction of constraints for sufficient conditions of optimality is presented. The book contains an introduction to non-differentiable and vector optimization.  Audience: This volume will be of interest to mathematicians, engineers, and economists working in mathematical optimization.},
  language = {English},
  timestamp = {2017-04-11T16:31:48Z},
  publisher = {{Springer}},
  author = {Pallaschke, Diethard Ernst and Rolewicz, S.},
  month = dec,
  year = {2010}
}

@book{Kang2006,
  address = {Providence, RI},
  title = {Inverse {{Problems}}, {{Multi}}-{{Scale Analysis}}, and {{Effective Medium Theory}}},
  isbn = {978-0-8218-3968-3},
  abstract = {Recent developments in inverse problems, multi-scale analysis and effective medium theory reveal that these fields share several fundamental concepts. This book is the proceedings of the research conference, "Workshop in Seoul: Inverse Problems, Multi-Scale Analysis and Homogenization," held at Seoul National University, June 22-24, 2005. It highlights the benefits of sharing ideas among these areas, of merging the expertise of scientists working there, and of directing interest towards challenging issues such as imaging nanoscience and biological imaging. Contributions are written by prominent experts and are of interest to researchers and graduate students interested in partial differential equations and applications.},
  language = {English},
  timestamp = {2017-04-11T16:32:53Z},
  publisher = {{American Mathematical Society}},
  author = {Kang, Habib Ammari and Hyeonbae},
  editor = {Ammari, Habib and Kang, Hyeonbae},
  month = sep,
  year = {2006}
}

@book{Ben-Tal2001,
  address = {Philadelphia},
  title = {Lectures on {{Modern Convex Optimization}}: {{Analysis}}, {{Algorithms}}, and {{Engineering Applications}}},
  isbn = {978-0-89871-491-3},
  shorttitle = {Lectures on {{Modern Convex Optimization}}},
  abstract = {Here is a book devoted to well-structured and thus efficiently solvable convex optimization problems, with emphasis on conic quadratic and semidefinite programming. The authors present the basic theory underlying these problems as well as their numerous applications in engineering, including synthesis of filters, Lyapunov stability analysis, and structural design. The authors also discuss the complexity issues and provide an overview of the basic theory of state-of-the-art polynomial time interior point methods for linear, conic quadratic, and semidefinite programming. The book's focus on well-structured convex problems in conic form allows for unified theoretical and algorithmical treatment of a wide spectrum of important optimization problems arising in applications.},
  language = {English},
  timestamp = {2017-04-11T16:34:24Z},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Ben-Tal, Aharon and Nemirovski, Arkadi},
  month = aug,
  year = {2001}
}

@article{Afdideh2016,
  title = {Recovery Guarantees for Mixed Norm $\ell_{p_1,p_2}$ Block Sparse 	Representations},
  doi = {10.1109/EUSIPCO.2016.7760274},
  abstract = {In this work, we propose theoretical and algorithmic-independent recovery
	conditions which guarantee the uniqueness of block sparse recovery
	in general dictionaries through a general mixed norm optimization
	problem. These conditions are derived using the proposed block uncertainty
	principles and block null space property, based on some newly defined
	characterizations of block spark, and (p, p)-block mutual incoherence.
	We show that there is improvement in the recovery condition when
	exploiting the block structure of the representation. In addition,
	the proposed recovery condition extends the similar results for block
	sparse setting by generalizing the criterion for determining the
	active blocks, generalizing the block sparse recovery condition,
	and relaxing some constraints on blocks such as linear independency
	of the columns.},
  timestamp = {2017-06-23T11:39:51Z},
  journal = {24th Eur. Signal Process. Conf.},
  author = {Afdideh, F. and Phlypo, R. and Jutten, C.},
  month = aug,
  year = {2016},
  keywords = {algorithmic-independent recovery condition,block mutual incoherence
	constant,Block Mutual Incoherence Constant (BMIC),block null space
	property,Block Spark,block spark characterization,block sparse recovery
	condition,Block-sparse recovery conditions,Block-sparsity,block uncertainty,Block
	Uncertainty Principle (BUP),Dictionaries,Europe,mixed norm lp1-p2
	block sparse representation,mixed norm optimization problem,optimisation,Optimization,signal
	processing,Signal processing algorithms,signal representation,Sparks,Uncertainty},
  pages = {378-382},
  annote = {2016 24th European Signal Processing Conference (EUSIPCO)}
}

@book{Walter2014,
  title = {Numerical {{Methods}} and {{Optimization}} - {{A Consumer Guide}}},
  abstract = {Initial training in pure and applied sciences tends to present problem-solving as the process of elaborating explicit closed-form solutions from basic...},
  timestamp = {2017-06-08T15:02:25Z},
  urldate = {2017-06-08},
  publisher = {{Springer}},
  author = {Walter, {\'E}ric},
  year = {2014},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\3X99GKGU\\9783319076706.html:text/html}
}

@inproceedings{Rao2012b,
  title = {A Clustering Approach to Optimize Online Dictionary Learning},
  doi = {10.1109/ICASSP.2012.6288126},
  abstract = {Dictionary learning has emerged as a powerful tool for low level image processing tasks such as denoising and inpainting, as well as sparse coding and representation of images. While there has been extensive work on the development of online and offline dictionary learning algorithms to perform the aforementioned tasks, the problem of choosing an appropriate dictionary size is not as widely addressed. In this paper, we introduce a new scheme to reduce and optimize dictionary size in an online setting by synthesizing new atoms from multiple previous ones. We show that this method performs as well as existing offline and online dictionary learning algorithms in terms of representation accuracy while achieving significant speedup in dictionary reconstruction and image encoding times. Our method not only helps in choosing smaller and more representative dictionaries, but also enables learning of more incoherent ones.},
  timestamp = {2017-06-27T14:30:13Z},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Rao, N. and Porikli, F.},
  month = mar,
  year = {2012},
  keywords = {clustering,Clustering algorithms,clustering approach,Complexity theory,Dictionaries,dictionary reconstruction,Encoding,Image coding,Image denoising,image encoding,image inpainting,image reconstruction,image representation,image sparse coding,learning (artificial intelligence),low level image processing tasks,Noise reduction,Online Dictionary Learning,online dictionary learning optimization,optimisation,pattern clustering,representative dictionaries,Training},
  pages = {1293--1296},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XEVR7KIA\\6288126.html:text/html}
}

@inproceedings{Ramirez2010a,
  title = {Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features},
  doi = {10.1109/CVPR.2010.5539964},
  abstract = {A clustering framework within the sparse modeling and dictionary learning setting is introduced in this work. Instead of searching for the set of centroid that best fit the data, as in k-means type of approaches that model the data as distributions around discrete points, we optimize for a set of dictionaries, one for each cluster, for which the signals are best reconstructed in a sparse coding manner. Thereby, we are modeling the data as a union of learned low dimensional subspaces, and data points associated to subspaces spanned by just a few atoms of the same learned dictionary are clustered together. An incoherence promoting term encourages dictionaries associated to different classes to be as independent as possible, while still allowing for different classes to share features. This term directly acts on the dictionaries, thereby being applicable both in the supervised and unsupervised settings. Using learned dictionaries for classification and clustering makes this method robust and well suited to handle large datasets. The proposed framework uses a novel measurement for the quality of the sparse representation, inspired by the robustness of the $\mathscr{l}$1 regularization term in sparse coding. In the case of unsupervised classification and/or clustering, a new initialization based on combining sparse coding with spectral clustering is proposed. This initialization clusters the dictionary atoms, and therefore is based on solving a low dimensional eigen-decomposition problem, being applicable to large datasets. We first illustrate the proposed framework with examples on standard image and speech datasets in the supervised classification setting, obtaining results comparable to the state-of-the-art with this simple approach. We then present experiments for fully unsupervised clustering on extended standard datasets and texture images, obtaining excellent performance.},
  timestamp = {2017-06-27T14:32:26Z},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ramirez, I. and Sprechmann, P. and Sapiro, G.},
  month = jun,
  year = {2010},
  keywords = {Art,Atomic measurements,Clustering algorithms,clustering framework,Dictionaries,dictionary learning,discrete points,image reconstruction,Image restoration,ℓ1 regularization term,learned dictionary,learning (artificial intelligence),Linear approximation,low dimensional eigen-decomposition problem,pattern classification,pattern clustering,Robustness,shared features,Signal processing algorithms,Sparse coding,sparse modeling,sparse representation,spectral clustering,Speech,structured incoherence,unsupervised classification},
  pages = {3501--3508},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NSZG2SCJ\\5539964.html:text/html}
}

@article{Zhang2016c,
  title = {Spectral \#x2013;{{Spatial Feature Learning Using Cluster}}-{{Based Group Sparse Coding}} for {{Hyperspectral Image Classification}}},
  volume = {9},
  issn = {1939-1404},
  doi = {10.1109/JSTARS.2016.2593907},
  abstract = {This paper presents a new spectral-spatial feature learning method for hyperspectral image classification, which integrates spectral and spatial information into group sparse coding (GSC) via clusters, each of which is an adaptive spatial partition of pixels. The clusters derived from the segmentation maps by the mean-shift algorithm are regarded as groups in GSC, where pixels within the same group are simultaneously represented by a sparse linear combination of a few common atoms in a given dictionary, thus enforcing spatial smoothness across the pixels in the same segmentation region to learn a spectral-spatial joint sparse representation. Finally, the recovered sparse representation can be viewed as a new feature and used directly for classification (e.g., by support vector machine). In comparison with other spectral-spatial classification techniques that exploit a fixed neighborhood system and force neighboring pixels to share a common sparsity pattern, the proposed method is more flexible and able to obtain adaptive spatial neighborhood correlations for spectral-spatial joint sparse coding. In addition, we also develop kernel GSC (KGSC) by incorporating the kernel trick into GSC to capture nonlinear relationships. The developed KGSC can also be applied to learning kernel sparse representation under the framework of the proposed spectral-spatial method, leading to a new spectral-spatial kernel sparse representation algorithm. Experimental results on three real hyperspectral datasets indicate that the proposed methods improve classification accuracy and provide distinctive classification maps, especially at small regions and boundaries in an image, compared with other similar approaches.},
  timestamp = {2017-06-27T14:34:22Z},
  number = {9},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  author = {Zhang, X. and Song, Q. and Gao, Z. and Zheng, Y. and Weng, P. and Jiao, L. C.},
  month = sep,
  year = {2016},
  keywords = {adaptive spatial pixel partition,cluster-based group sparse coding,Clustering algorithms,Dictionaries,fixed neighborhood system,force neighboring pixels,geophysical techniques,group sparse coding,Group sparse coding (GSC),GSC,hyperspectral image classification,hyperspectral image segmentation,Hyperspectral imaging,image boundaries,image classification,Image coding,Image segmentation,Kernel,kernel group sparse coding,learning kernel sparse representation,mean-shift algorithm,mean shift (MS),segmentation maps,spatial neighborhood correlations,spatial pixel smoothness,spectral information,spectral–spatial classification,spectral-spatial classification techniques,spectral-spatial feature learning method,spectral-spatial joint sparse coding,spectral-spatial joint sparse representation,spectral-spatial kernel sparse representation algorithm,support vector machine},
  pages = {4142--4159},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\EGWDX3FF\\7547367.html:text/html}
}

@article{Goldenholz2009,
  title = {Mapping the {{Signal}}-{{To}}-{{Noise}}-{{Ratios}} of {{Cortical Sources}} in {{Magnetoencephalography}} and {{Electroencephalography}}},
  volume = {30},
  issn = {1065-9471},
  doi = {10.1002/hbm.20571},
  abstract = {Although magnetoencephalography (MEG) and electroencephalography (EEG) have been available for decades, their relative merits are still debated. We examined regional differences in signal-to-noise-ratios (SNRs) of cortical sources in MEG and EEG. Data from four subjects were used to simulate focal and extended sources located on the cortical surface reconstructed from high-resolution magnetic resonance images. The SNR maps for MEG and EEG were found to be complementary. The SNR of deep sources was larger in EEG than in MEG, whereas the opposite was typically the case for superficial sources. Overall, the SNR maps were more uniform for EEG than for MEG. When using a noise model based on uniformly distributed random sources on the cortex, the SNR in MEG was found to be underestimated, compared with the maps obtained with noise estimated from actual recorded MEG and EEG data. With extended sources, the total area of cortex in which the SNR was higher in EEG than in MEG was larger than with focal sources. Clinically, SNR maps in a patient explained differential sensitivity of MEG and EEG in detecting epileptic activity. Our results emphasize the benefits of recording MEG and EEG simultaneously.},
  timestamp = {2017-07-20T16:22:49Z},
  number = {4},
  journal = {Human brain mapping},
  author = {Goldenholz, Daniel M. and Ahlfors, Seppo P. and H{\"a}m{\"a}l{\"a}inen, Matti S. and Sharon, Dahlia and Ishitobi, Mamiko and Vaina, Lucia M. and Stufflebeam, Steven M.},
  month = apr,
  year = {2009},
  pages = {1077--1086},
  pmid = {18465745},
  pmcid = {PMC2882168}
}

@article{Wens2015,
  title = {A Geometric Correction Scheme for Spatial Leakage Effects in {{MEG}}/{{EEG}} Seed-Based Functional Connectivity Mapping},
  volume = {36},
  issn = {1097-0193},
  doi = {10.1002/hbm.22943},
  abstract = {Spatial leakage effects are particularly confounding for seed-based investigations of brain networks using source-level electroencephalography (EEG) or magnetoencephalography (MEG). Various methods designed to avoid this issue have been introduced but are limited to particular assumptions about its temporal characteristics. Here, we investigate the usefulness of a model-based geometric correction scheme (GCS) to suppress spatial leakage emanating from the seed location. We analyze its properties theoretically and then assess potential advantages and limitations with simulated and experimental MEG data (resting state and auditory-motor task). To do so, we apply Minimum Norm Estimation (MNE) for source reconstruction and use variation of error parameters, statistical gauging of spatial leakage correction and comparison with signal orthogonalization. Results show that the GCS has a local (i.e., near the seed) effect only, in line with the geometry of MNE spatial leakage, and is able to map spatially all types of brain interactions, including linear correlations eliminated after signal orthogonalization. Furthermore, it is robust against the introduction of forward model errors. On the other hand, the GCS can be affected by local overcorrection effects and seed mislocation. These issues arise with signal orthogonalization too, although significantly less extensively, so the two approaches complement each other. The GCS thus appears to be a valuable addition to the spatial leakage correction toolkits for seed-based FC analyses in source-projected MEG/EEG data. Hum Brain Mapp 36:4604\textendash{}4621, 2015. \textcopyright{} 2015 Wiley Periodicals, Inc.},
  language = {en},
  timestamp = {2017-07-20T16:39:38Z},
  number = {11},
  journal = {Human Brain Mapping},
  author = {Wens, Vincent and Marty, Brice and Mary, Alison and Bourguignon, Mathieu and {Op de Beeck}, Marc and Goldman, Serge and Van Bogaert, Patrick and Peigneux, Philippe and De Ti{\`e}ge, Xavier},
  month = nov,
  year = {2015},
  keywords = {dynamic imaging of coherent sources,functional connectivity,Inverse problem,Magnetoencephalography,resting-state networks,spatial leakage},
  pages = {4604--4621},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8J4NKNRE\\abstract.html:text/html}
}

@article{Spruston2008,
  title = {Pyramidal Neurons: Dendritic Structure and Synaptic Integration},
  volume = {9},
  copyright = {\textcopyright{} 2008 Nature Publishing Group},
  issn = {1471-003X},
  shorttitle = {Pyramidal Neurons},
  doi = {10.1038/nrn2286},
  abstract = {Pyramidal neurons are characterized by their distinct apical and basal dendritic trees and the pyramidal shape of their soma. They are found in several regions of the CNS and, although the reasons for their abundance remain unclear, functional studies \textemdash{} especially of CA1 hippocampal and layer V neocortical pyramidal neurons \textemdash{} have offered insights into the functions of their unique cellular architecture. Pyramidal neurons are not all identical, but some shared functional principles can be identified. In particular, the existence of dendritic domains with distinct synaptic inputs, excitability, modulation and plasticity appears to be a common feature that allows synapses throughout the dendritic tree to contribute to action-potential generation. These properties support a variety of coincidence-detection mechanisms, which are likely to be crucial for synaptic integration and plasticity.},
  language = {en},
  timestamp = {2017-07-21T10:01:14Z},
  number = {3},
  urldate = {2017-07-21},
  journal = {Nature Reviews Neuroscience},
  author = {Spruston, Nelson},
  month = mar,
  year = {2008},
  pages = {206--221},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\S27N9IA7\\nrn2286.html:text/html}
}

@book{Malmivuo1995,
  address = {New York, NY},
  title = {{Bioelectromagnetism: Principles and Applications of Bioelectric and Biomagnetic Fields}},
  isbn = {978-0-19-505823-9},
  shorttitle = {{Bioelectromagnetism}},
  abstract = {This book is one of the first to apply engineering science and technology to biological cells and tissues that are electrically conducting and excitable.  It describes the theory and a wide range of applications in both electric and magnetic fields.  The similarities and differences between bioelectricity and biomagnetism are described in detail from the viewpoint of lead field theory. This book will enable readers to understand the properties of existing bioelectric and biomagnetic measurements and stimulation methods, and to design new systems.  It includes carefully drawn illustrations and 500 references, and can be used as a textbook and as a reference.},
  language = {Anglais},
  timestamp = {2017-08-21T08:41:48Z},
  publisher = {{OUP USA}},
  author = {Malmivuo, Jaakko and Plonsey, Robert},
  month = oct,
  year = {1995}
}

@article{Cohen2003,
  title = {Magnetoencephalography ({{Neuromagnetism}})},
  timestamp = {2017-08-21T12:35:48Z},
  journal = {Encyclopedia of Neuroscience},
  author = {Cohen, D. and Halgren, E.},
  year = {2003}
}

@article{Oosterom2012,
  title = {The Inverse Problem of Bioelectricity: An Evaluation},
  volume = {50},
  issn = {0140-0118, 1741-0444},
  shorttitle = {The Inverse Problem of Bioelectricity},
  doi = {10.1007/s11517-012-0941-5},
  abstract = {This invited paper presents a personal view on the current status of the solution to the inverse problem of bioelectricity. Its focus lies on applications in the field of electrocardiography. The topic discussed is also relevant in other medical domains, such as electroencephalography, electroneurography and electromyography. In such domains the methodology involved rests on the same basic principles of physics and electrophysiology as well as on the applied techniques of signal analysis and numerical analysis.},
  language = {en},
  timestamp = {2017-08-21T12:39:04Z},
  number = {9},
  urldate = {2017-08-21},
  journal = {Medical \& Biological Engineering \& Computing},
  author = {van Oosterom, Adriaan},
  month = sep,
  year = {2012},
  pages = {891--902},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\D9RC8ETM\\s11517-012-0941-5.html:text/html}
}

@article{Andino2011,
  title = {Spatiotemporal Scales and Links between Electrical Neuroimaging Modalities},
  volume = {49},
  issn = {0140-0118, 1741-0444},
  doi = {10.1007/s11517-011-0769-4},
  abstract = {Recordings of brain electrophysiological activity provide the most direct reflect of neural function. Information contained in these signals varies as a function of the spatial scale at which recordings are done: from single cell recording to large scale macroscopic fields, e.g., scalp EEG. Microscopic and macroscopic measurements and models in Neuroscience are often in conflict. Solving this conflict might require the developments of a sort of bio-statistical physics, a framework for relating the microscopic properties of individual cells to the macroscopic or bulk properties of neural circuits. Such a framework can only emerge in Neuroscience from the systematic analysis and modeling of the diverse recording scales from simultaneous measurements. In this article we briefly review the different measurement scales and models in modern neuroscience to try to identify the sources of conflict that might ultimately help to create a unified theory of brain electromagnetic fields. We argue that seen the different recording scales, from the single cell to the large scale fields measured by the scalp electroencephalogram, as derived from a unique physical magnitude\textemdash{}the electric potential that is measured in all cases\textemdash{}might help to conciliate microscopic and macroscopic models of neural function as well as the animal and human neuroscience literature.},
  language = {en},
  timestamp = {2017-08-21T12:45:20Z},
  number = {5},
  urldate = {2017-08-21},
  journal = {Medical \& Biological Engineering \& Computing},
  author = {Andino, Sara L. Gonzalez and Perrig, Stephen and Menendez, Rolando Grave de Peralta},
  month = may,
  year = {2011},
  pages = {511--520},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\92BCP8RT\\s11517-011-0769-4.html:text/html}
}

@misc{LopesdaSilva2010,
  title = {Electrophysiological {{Basis}} of {{MEG Signals}} - {{Oxford Scholarship}}},
  abstract = {This chapter focuses on the basic physiological and biophysical aspects of how magnetic signals are generated in the brain. It begins with a brief description of the main features of magnetoencephalography as a method to study brain functions in man. It then discusses the main features of MEG, some basic notions of cellular neurophysiology and biophysics, neuronal models, and the transfer of magnetic signals from the brain to the skull. Answers to frequently asked questions about MEG are provided.},
  timestamp = {2017-08-21T12:56:04Z},
  urldate = {2017-08-21},
  howpublished = {\url{http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780195307238.001.0001/acprof-9780195307238-chapter-001}},
  author = {{Lopes da Silva}, Fernando H.},
  month = jun,
  year = {2010},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7H2AEE6X\\acprof-9780195307238-chapter-001.html:text/html}
}

@article{Okada1997,
  title = {Genesis of {{MEG}} Signals in a Mammalian {{CNS}} Structure},
  volume = {103},
  issn = {0013-4694},
  doi = {10.1016/S0013-4694(97)00043-6},
  abstract = {Neuromagnetic signals of guinea pig hippocampal slices were characterized and compared with the extracellular field potential to elucidate the genesis of magnetoencephalographic signals in a mammalian CNS structure. The spatial distribution of magnetic evoked field (MEF) directed normal to bath surface was similar for transverse CA1, longitudinal CA1 and longitudinal CA3 slices in the presence of 0.1 mM picrotoxin (PTX) which blocks inhibitory synaptic transmission. Their MEFs were produced by currents along the longitudinal axis of the pyramidal cells. Comparisons of the MEF with the laminar potential profile revealed that the MEF was generated by intracellular longitudinal currents. The dipolar component of the intracellular currents was the dominant factor generating the MEF even at a distance of 2 mm from the slice. The MEF from a slice in Ringer's solution without PTX became similar in temporal waveform with time to the MEF in the presence of PTX, indicating the predominance of excitatory connections in generating the MEF and the existence of highly synchronous population activities across the slice even in PTX-free Ringer's solution. The presence of such highly synchronous population activities underlying the MEFs was verified directly with field potentials recorded across the slice. A systematic variation of the stimulation site revealed a characteristic waveform for each site. The variation of the waveform with stimulation site suggested the contribution of many factors, both synaptic and voltage-sensitive conductances, to the overall waveform of the MEF.},
  timestamp = {2017-08-21T12:59:12Z},
  number = {4},
  journal = {Electroencephalography and Clinical Neurophysiology},
  author = {Okada, Yoshio C. and Wu, Jie and Kyuhou, Shinichi},
  month = oct,
  year = {1997},
  keywords = {biomagnetism,CA1,CA3,electroencephalography (EEG),Hippocampus,magnetoencephalography (MEG)},
  pages = {474--485},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\NW5FR332\\S0013469497000436.html:text/html}
}

@article{Jones2009,
  title = {Quantitative {{Analysis}} and {{Biophysically Realistic Neural Modeling}} of the {{MEG Mu Rhythm}}: {{Rhythmogenesis}} and {{Modulation}} of {{Sensory}}-{{Evoked Responses}}},
  volume = {102},
  issn = {0022-3077},
  shorttitle = {Quantitative {{Analysis}} and {{Biophysically Realistic Neural Modeling}} of the {{MEG Mu Rhythm}}},
  doi = {10.1152/jn.00535.2009},
  abstract = {Variations in cortical oscillations in the alpha (7\textendash{}14 Hz) and beta (15\textendash{}29 Hz) range have been correlated with attention, working memory, and stimulus detection. The mu rhythm recorded with magnetoencephalography (MEG) is a prominent oscillation generated by Rolandic cortex containing alpha and beta bands. Despite its prominence, the neural mechanisms regulating mu are unknown. We characterized the ongoing MEG mu rhythm from a localized source in the finger representation of primary somatosensory (SI) cortex. Subjects showed variation in the relative expression of mu-alpha or mu-beta, which were nonoverlapping for roughly 50\% of their respective durations on single trials. To delineate the origins of this rhythm, a biophysically principled computational neural model of SI was developed, with distinct laminae, inhibitory and excitatory neurons, and feedforward (FF, representative of lemniscal thalamic drive) and feedback (FB, representative of higher-order cortical drive or input from nonlemniscal thalamic nuclei) inputs defined by the laminar location of their postsynaptic effects. The mu-alpha component was accurately modeled by rhythmic FF input at approximately 10-Hz. The mu-beta component was accurately modeled by the addition of approximately 10-Hz FB input that was nearly synchronous with the FF input. The relative dominance of these two frequencies depended on the delay between FF and FB drives, their relative input strengths, and stochastic changes in these variables. The model also reproduced key features of the impact of high prestimulus mu power on peaks in SI-evoked activity. For stimuli presented during high mu power, the model predicted enhancement in an initial evoked peak and decreased subsequent deflections. In agreement, the MEG-evoked responses showed an enhanced initial peak and a trend to smaller subsequent peaks. These data provide new information on the dynamics of the mu rhythm in humans and the model provides a novel mechanistic interpretation of this rhythm and its functional significance.},
  timestamp = {2017-08-21T13:08:47Z},
  number = {6},
  journal = {Journal of Neurophysiology},
  author = {Jones, Stephanie R. and Pritchett, Dominique L. and Sikora, Michael A. and Stufflebeam, Steven M. and H{\"a}m{\"a}l{\"a}inen, Matti and Moore, Christopher I.},
  month = dec,
  year = {2009},
  pages = {3554--3572},
  pmid = {19812290},
  pmcid = {PMC2804421}
}

@article{Jones2007,
  title = {Neural {{Correlates}} of {{Tactile Detection}}: {{A Combined Magnetoencephalography}} and {{Biophysically Based Computational Modeling Study}}},
  volume = {27},
  copyright = {Copyright \textcopyright{} 2007 Society for Neuroscience 0270-6474/07/2710751-14\$15.00/0},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Neural {{Correlates}} of {{Tactile Detection}}},
  doi = {10.1523/JNEUROSCI.0482-07.2007},
  abstract = {Previous reports conflict as to the role of primary somatosensory neocortex (SI) in tactile detection. We addressed this question in normal human subjects using whole-head magnetoencephalography (MEG) recording. We found that the evoked signal (0\textendash{}175 ms) showed a prominent equivalent current dipole that localized to the anterior bank of the postcentral gyrus, area 3b of SI. The magnitude and timing of peaks in the SI waveform were stimulus amplitude dependent and predicted perception beginning at $\sim$70 ms after stimulus. To make a direct and principled connection between the SI waveform and underlying neural dynamics, we developed a biophysically realistic computational SI model that contained excitatory and inhibitory neurons in supragranular and infragranular layers. The SI evoked response was successfully reproduced from the intracellular currents in pyramidal neurons driven by a sequence of lamina-specific excitatory input, consisting of output from the granular layer ($\sim$25 ms), exogenous input to the supragranular layers ($\sim$70 ms), and a second wave of granular output ($\sim$135 ms). The model also predicted that SI correlates of perception reflect stronger and shorter-latency supragranular and late granular drive during perceived trials. These findings strongly support the view that signatures of tactile detection are present in human SI and are mediated by local neural dynamics induced by lamina-specific synaptic drive. Furthermore, our model provides a biophysically realistic solution to the MEG signal and can predict the electrophysiological correlates of human perception.},
  language = {en},
  timestamp = {2017-08-21T13:13:01Z},
  number = {40},
  urldate = {2017-08-21},
  journal = {Journal of Neuroscience},
  author = {Jones, Stephanie R. and Pritchett, Dominique L. and Stufflebeam, Steven M. and H{\"a}m{\"a}l{\"a}inen, Matti and Moore, Christopher I.},
  month = oct,
  year = {2007},
  keywords = {computational model,conscious perception,dendritic processes,Magnetoencephalography,network dynamics,Somatosensory Cortex},
  pages = {10751--10764},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\MIDHTM2N\\10751.html:text/html},
  pmid = {17913909}
}

@article{Murakami2003,
  title = {Contribution of Ionic Currents to Magnetoencephalography ({{MEG}}) and Electroencephalography ({{EEG}}) Signals Generated by Guinea-Pig {{CA3}} Slices},
  volume = {553},
  issn = {0022-3751},
  doi = {10.1113/jphysiol.2003.051144},
  abstract = {A mathematical model was used to analyse the contributions of different types of ionic currents in the pyramidal cells of longitudinal CA3 slices to the magnetic fields and field potentials generated by this preparation. Murakami et al. recently showed that a model based on the work of Traub et al. provides a quantitatively accurate account of the basic features of three types of empirical data (magnetic fields outside the slice, extracellular field potentials within the slice and intracellular potentials within the pyramidal neurons) elicited by stimulations of the soma and apical dendrites. This model was used in the present study to compute the net current dipole moment (Q) due to each of the different voltage- and ligand-gated channels in the cells in the presence of fast GABAA inhibition. These values of Q are proportional to the magnetic field and electrical potential far away from the slice. The intrinsic conductances were found to be more important than the synaptic conductances in determining the shape and magnitude of Q. Among the intrinsic conductances, the sodium (gNa) and delayed-rectifier potassium (gK(DR)) channels were found to produce sharp spikes. The high-threshold calcium channel (gCa) and C-type potassium channel (gK(C)) primarily determined the overall current waveforms. The roles of gCa and gK(C) were independent of small perturbations in these channel densities in the apical and basal dendrites. A combination of gNa, gK(DR), gCa, and gK(C) accounted for most of the evoked responses, except for later slow components, which were primarily due to synaptic channels.},
  timestamp = {2017-08-21T13:15:28Z},
  number = {Pt 3},
  journal = {The Journal of Physiology},
  author = {Murakami, Shingo and Hirose, Akira and Okada, Yoshio C},
  month = dec,
  year = {2003},
  pages = {975--985},
  pmid = {14528026},
  pmcid = {PMC2343617}
}

@article{Murakami2006,
  title = {Contributions of Principal Neocortical Neurons to Magnetoencephalography and Electroencephalography Signals},
  volume = {575},
  issn = {0022-3751},
  doi = {10.1113/jphysiol.2006.105379},
  abstract = {A realistically shaped three-dimensional single-neuron model was constructed for each of four principal cell types in the neocortex in order to infer their contributions to magnetoencephalography (MEG) and electroencephalography (EEG) signals. For each cell, the soma was stimulated and the resulting intracellular current was used to compute the current dipole Q for the whole cell or separately for the apical and basal dendrites. The magnitude of Q is proportional to the magnetic field and electrical potential far from the neuron. A train of spikes and depolarization shift in an intracellular burst discharge were seen as spikes and an envelope in Q for the layer V and layer II/III pyramidal cells. The stellate cells lacked the envelope. As expected, the pyramidal cells produced a stronger Q than the stellate cells. The spikes produced by the layer V pyramidal cells (n = 4) varied between -0.78 and 2.97 pA m with the majority of the cells showing a current toward the pia (defined as positive). The basal dendrites, however, produced considerable spike currents. The magnitude and direction of dipole moment are in agreement with the distribution of the dendrites. The spikes in Q for the layer V pyramidal cells were produced by the transient sodium conductance and potassium conductance of delayed rectifier type; the conductances distributed along the dendrites were capable of generating spike propagation, which was seen in Q as the tail of a triphasic wave lasting several milliseconds. The envelope was similar in magnitude (-0.41 to -0.90 pA m) across the four layer V pyramidal cells. The spike and envelope for the layer II/III pyramidal cell were 0.47 and -0.29 pA m, respectively; these values agreed well with empirical and theoretical estimates for guinea pig CA3 pyramidal cells. Spikes were stronger for the layer IV spiny stellate (0.27 pA m) than the layer III aspiny stellate cell (0.06 pA m) along their best orientations. The spikes may thus be stronger than has been previously thought. The Q for a population of stellate cells may be weaker than a linear sum of their individual Q values due to their variable dendritic geometry. The burst discharge by pyramidal cells may be detectable with MEG and EEG when 10 000\textendash{}50 000 cells are synchronously active.},
  timestamp = {2017-08-21T13:16:33Z},
  number = {Pt 3},
  journal = {The Journal of Physiology},
  author = {Murakami, Shingo and Okada, Yoshio},
  month = sep,
  year = {2006},
  pages = {925--936},
  pmid = {16613883},
  pmcid = {PMC1995687}
}

@article{Murakami2002,
  title = {Physiological Origins of Evoked Magnetic Fields and Extracellular Field Potentials Produced by Guinea-Pig {{CA3}} Hippocampal Slices},
  volume = {544},
  issn = {0022-3751},
  doi = {10.1113/jphysiol.2002.027094},
  abstract = {This study examined whether evoked magnetic fields and intra- and extracellular potentials from longitudinal CA3 slices of guinea-pig can be interpreted within a single theoretical framework that incorporates ligand- and voltage-sensitive conductances in the dendrites and soma of the pyramidal cells. The 1991 CA3 mathematical model of R. D. Traub is modified to take into account the asymmetric branching patterns of the apical and basal dendrites of the pyramidal cells. The revised model accounts for the magnitude and waveform of the bi- and triphasic magnetic fields evoked by somatic and apical stimulations, respectively, in the slice in the absence of fast inhibition (blocked by 0.1 mm picrotoxin). The revised model also accounts for selective effects of 4-aminopyridine (4-AP) and tetraethylammonium (TEA), which block the potassium channels of A and C type, respectively, on the slow wave of the magnetic fields. Furthermore, the model correctly predicts the laminar profiles of field potential as well as intracellular potentials in the pyramidal cells produced by two classes of cells - those directly activated and those indirectly (synaptically) activated by the applied external stimulus. The intracellular potentials in this validated model reveal that the spikes and slow waves of the magnetic fields are generated in or near the soma and apical dendrites, respectively. These results demonstrate that a single theoretical framework couched within the modern concepts of cellular physiology provides a unified account of magnetic fields outside the slice, extracellular potentials within the slice and intracellular potentials of the pyramidal cells for CA3.},
  timestamp = {2017-08-21T13:18:48Z},
  number = {Pt 1},
  journal = {The Journal of Physiology},
  author = {Murakami, Shingo and Zhang, Tongsheng and Hirose, Akira and Okada, Yoshio C},
  month = oct,
  year = {2002},
  pages = {237--251},
  pmid = {12356895},
  pmcid = {PMC2290560}
}

@article{Blagoev2007,
  title = {Modelling the Magnetic Signature of Neuronal Tissue},
  volume = {37},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2007.04.033},
  abstract = {Neuronal communication in the brain involves electrochemical currents, which produce magnetic fields. Stimulus-evoked brain responses lead to changes in these fields and can be studied using magneto- and electro-encephalography (MEG/EEG). In this paper we model the spatiotemporal distribution of the magnetic field of a physiologically idealized but anatomically realistic neuron to assess the possibility of using magnetic resonance imaging (MRI) for directly mapping the neuronal currents in the human brain. Our results show that the magnetic field several centimeters from the centre of the neuron is well approximated by a dipole source, but the field close to the neuron is not, a finding particularly important for understanding the possible contrast mechanism underlying the use of MRI to detect and locate these currents. We discuss the importance of the spatiotemporal characteristics of the magnetic field in cortical tissue for evaluating and optimizing an experiment based on this mechanism and establish an upper bound for the expected MRI signal change due to stimulus-induced cortical response. Our simulations show that the expected change of the signal magnitude is 1.6\% and its phase shift is 1$^\circ$. An unexpected finding of this work is that the cortical orientation with respect to the external magnetic field has little effect on the predicted MRI contrast. This encouraging result shows that magnetic resonance contrast directly based on the neuronal currents present in the cortex is theoretically a feasible imaging technique. MRI contrast generation based on neuronal currents depends on the dendritic architecture and we obtained high-resolution optical images of cortical tissue to discuss the spatial structure of the magnetic field in grey matter.},
  timestamp = {2017-08-21T13:21:21Z},
  number = {1},
  journal = {NeuroImage},
  author = {Blagoev, K. B. and Mihaila, B. and Travis, B. J. and Alexandrov, L. B. and Bishop, A. R. and Ranken, D. and Posse, S. and Gasparovic, C. and Mayer, A. and Aine, C. J. and Ulbert, I. and Morita, M. and M{\"u}ller, W. and Connor, J. and Halgren, E.},
  month = aug,
  year = {2007},
  keywords = {MEG,Modelling,MRI,Neural currents,Neuroelectrophysiology},
  pages = {137--148},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\5TGXGKKZ\\S1053811907003308.html:text/html}
}

@article{Cassara2008,
  title = {Realistic Simulations of Neuronal Activity: {{A}} Contribution to the Debate on Direct Detection of Neuronal Currents by {{MRI}}},
  volume = {39},
  issn = {1053-8119},
  shorttitle = {Realistic Simulations of Neuronal Activity},
  doi = {10.1016/j.neuroimage.2007.08.048},
  abstract = {Many efforts have been done in order to preview the properties of the magnetic resonance (MR) signals produced by the neuronal currents using simulations. In this paper, starting with a detailed calculation of the magnetic field produced by the neuronal currents propagating over single hippocampal CA1 pyramidal neurons placed inside a cubic MR voxel of length 1.2 mm, we proceeded on the estimation of the phase and magnitude MR signals. We then extended the results to layers of parallel and synchronous similar neurons and to ensembles of layers, considering different echo times, voxel volumes and neuronal densities. The descriptions of the neurons and of their electrical activity took into account the real neuronal morphologies and the physiology of the neuronal events. Our results concern: (a) the expected time course of the MR signals produced by the neuronal currents in the brain, based on physiological and anatomical properties; (b) the different contributions of post-synaptic potentials and of action potentials to the MR signals; (c) the estimation of the equivalent current dipole and the influence of its orientation with respect to the external magnetic field on the observable MR signal variations; (d) the size of the estimated neuronal current induced phase and magnitude MR signal changes with respect to the echo time, voxel-size and neuronal density. The inclusion of realistic neuronal properties into the simulation introduces new information that can be helpful for the design of MR sequences for the direct detection of neuronal current effects and the testing of bio-electromagnetic models.},
  timestamp = {2017-08-21T13:23:04Z},
  number = {1},
  journal = {NeuroImage},
  author = {Cassar{\`a}, A. M. and Hagberg, G. E. and Bianciardi, M. and Migliore, M. and Maraviglia, B.},
  month = jan,
  year = {2008},
  keywords = {Hippocampus,Magnetic field changes,Neuronal current MRI,Pyramidal cells},
  pages = {87--106},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\8PDRJ2RZ\\S1053811907007677.html:text/html}
}

@article{Cassara2009,
  series = {Proceedings of the International School on Magnetic Resonance and Brain Function},
  title = {Neuronal Current Detection with Low-Field Magnetic Resonance: Simulations and Methods},
  volume = {27},
  issn = {0730-725X},
  shorttitle = {Neuronal Current Detection with Low-Field Magnetic Resonance},
  doi = {10.1016/j.mri.2009.01.015},
  abstract = {The noninvasive detection of neuronal currents in active brain networks [or direct neuronal imaging (DNI)] by means of nuclear magnetic resonance (NMR) remains a scientific challenge. Many different attempts using NMR scanners with magnetic fields $>$1 T (high-field NMR) have been made in the past years to detect phase shifts or magnitude changes in the NMR signals. However, the many physiological (i.e., the contemporarily BOLD effect, the weakness of the neuronal-induced magnetic field, etc.) and technical limitations (e.g., the spatial resolution) in observing the weak signals have led to some contradicting results. In contrast, only a few attempts have been made using low-field NMR techniques. As such, this paper was aimed at reviewing two recent developments in this front. The detection schemes discussed in this manuscript, the resonant mechanism (RM) and the DC method, are specific to NMR instrumentations with main fields below the earth magnetic field (50 $\mu$T), while some even below a few microteslas (ULF-NMR). However, the experimental validation for both techniques, with differentiating sensitivity to the various neuronal activities at specific temporal and spatial resolutions, is still in progress and requires carefully designed magnetic field sensor technology. Additional care should be taken to ensure a stringent magnetic shield from the ambient magnetic field fluctuations. In this review, we discuss the characteristics and prospect of these two methods in detecting neuronal currents, along with the technical requirements on the instrumentation.},
  timestamp = {2017-08-21T13:25:06Z},
  number = {8},
  journal = {Magnetic Resonance Imaging},
  author = {Cassar{\'a}, Antonino Mario and Maraviglia, Bruno and Hartwig, Stefan and Trahms, Lutz and Burghoff, Martin},
  month = oct,
  year = {2009},
  keywords = {DC-MEG,Low-field magnetic resonance,neuronal currents,Simulations},
  pages = {1131--1139},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KCJ8UTXC\\S0730725X09000101.html:text/html}
}

@article{Plonsey1967,
  title = {Considerations of Quasi-Stationarity in Electrophysiological Systems},
  volume = {29},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02476917},
  abstract = {Conditions under which a time varying electromagnetic field problem (such as arises in electrophysiology, electrocardiography, etc.) can be reduced to the conventional quasistatic problem are summarized. These conditions are discussed for typical physiological parameters.},
  language = {en},
  timestamp = {2017-08-21T15:00:59Z},
  number = {4},
  urldate = {2017-08-21},
  journal = {The bulletin of mathematical biophysics},
  author = {Plonsey, Robert and Heppner, Dennis B.},
  month = dec,
  year = {1967},
  pages = {657--664},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\6Z4EXCM7\\BF02476917.html:text/html}
}

@article{Okada1993,
  title = {Empirical Bases for Constraints in Current-Imaging Algorithms},
  volume = {5},
  issn = {0896-0267},
  language = {eng},
  timestamp = {2017-08-21T16:12:56Z},
  number = {4},
  journal = {Brain Topography},
  author = {Okada, Y.},
  year = {1993},
  keywords = {algorithms,Brain,Evoked Potentials,Humans,Magnetoencephalography,Models; Neurological},
  pages = {373--377},
  pmid = {8357710}
}

@article{Hosek1978,
  title = {The {{Contributions}} of {{Intracerebral Currents}} to the {{EEG}} and {{Evoked Potentials}}},
  volume = {BME-25},
  issn = {0018-9294},
  doi = {10.1109/TBME.1978.326337},
  abstract = {Scalp and cortical potentials due to implanted, dipole current sources were measured in the monkey. A four region spherical model of the head was developed and scalp potentials due to theoretical radial dipoles were computed and compared with experimental results. Dipole source locations were chosen to correspond to points along the somatosensory projection pathways to permit comparison of findings with clinical cortical and scalp evoked potential records. Data yielded by the theoretical head model compare well with those obtained experimentally. The results suggest that depth cerebral bioelectric sources can contribute to scalp recorded activity when averaging techniques are used.},
  timestamp = {2017-08-22T09:33:45Z},
  number = {5},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Hosek, R. S. and Sances, A. and Jodat, R. W. and Larson, S. J.},
  month = sep,
  year = {1978},
  keywords = {Animals,Audio recording,bioelectric phenomena,Brain modeling,Cerebral cortex,Delay,Electric Conductivity,Electric potential,Electrodes; Implanted,Electroencephalography,Evoked Potentials,Haplorhini,Humans,Magnetic heads,Scalp,Signal generators},
  pages = {405--413},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\A3TRPMBS\\4122870.html:text/html}
}

@article{deMunck1988,
  title = {The Potential Distribution in a Layered Anisotropic Spheroidal Volume Conductor},
  volume = {64},
  issn = {0021-8979},
  doi = {10.1063/1.341983},
  timestamp = {2017-08-22T09:35:44Z},
  number = {2},
  journal = {Journal of Applied Physics},
  author = {{de Munck}, J. C.},
  month = jul,
  year = {1988},
  pages = {464--470},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\7BHGCN5P\\1.html:text/html}
}

@article{Munck1993,
  title = {A Fast Method to Compute the Potential in the Multisphere Model ({{EEG}} Application)},
  volume = {40},
  issn = {0018-9294},
  doi = {10.1109/10.245635},
  abstract = {A series expansion is derived for the potential distribution, caused by a dipole source in a multilayered sphere with piecewise constant conductivity. When the radial coordinate of the source approaches the radial coordinate of the field point the spherical harmonics expansion converges only very slowly. It is shown how the convergence can be improved by first calculating an asymptotic approximation of the potential and using the so-called addition-subtraction method. Since the asymptotic solution is an approximation of the true solution, it gives some insight on the dependence of the potential on the conductivities. The formulas are given in Cartesian coordinates, so that difficulties with coordinate transformations are avoided. Attention is paid to the (fast) computation of the partial derivatives of the potential, which is useful for inverse algorithms.},
  timestamp = {2017-08-22T09:38:24Z},
  number = {11},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {de Munck, J. C. and Peters, M. J.},
  month = nov,
  year = {1993},
  keywords = {Acceleration,addition-subtraction method,algorithms,Anisotropic magnetoresistance,Anisotropy,bioelectric potentials,Brain modeling,brain models,Conductivity,Conductors,convergence,coordinate transformations,dipole source,Electroencephalography,field point,Geometry,head model,inverse algorithms,inverse problems,Models; Neurological,multilayered sphere,multisphere model,Ocean temperature,partial derivatives,piecewise constant conductivity,potential computation method,potential distribution,radial coordinate,series expansion,Skin,spherical harmonics expansion},
  pages = {1166--1174},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\RXJSPPBD\\245635.html:text/html}
}

@article{Nolte1999,
  title = {Perturbative Analytical Solutions of the Electric Forward Problem for Realistic Volume Conductors},
  volume = {86},
  issn = {0021-8979},
  doi = {10.1063/1.371128},
  timestamp = {2017-08-22T09:41:20Z},
  number = {5},
  journal = {Journal of Applied Physics},
  author = {Nolte, Guido and Curio, Gabriel},
  month = aug,
  year = {1999},
  pages = {2800--2811},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\648GQC5X\\1.html:text/html}
}

@article{Nolte2001,
  title = {Perturbative Analytical Solutions of the Magnetic Forward Problem for Realistic Volume Conductors},
  volume = {89},
  issn = {0021-8979},
  doi = {10.1063/1.1337089},
  timestamp = {2017-08-22T09:42:30Z},
  number = {4},
  journal = {Journal of Applied Physics},
  author = {Nolte, Guido and Fieseler, Thomas and Curio, Gabriel},
  month = jan,
  year = {2001},
  pages = {2360--2369},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\D2TGBAPW\\1.html:text/html}
}

@article{Yan1991,
  title = {Finite-Element Model of the Human Head: Scalp Potentials Due to Dipole Sources},
  volume = {29},
  issn = {0140-0118, 1741-0444},
  shorttitle = {Finite-Element Model of the Human Head},
  doi = {10.1007/BF02442317},
  abstract = {Three-dimensional finite-element models provide a method to study the relationship between human scalp potentials and neural current sources inside the brain. A new formulation of dipole-like current sources is developed here. Finiteelement analyses based on this formulation are carried out for both a threeconcentric-spheres model and a human-head model. Differences in calculated scalp potentials between these two models are studied in the context of the forward and inverse problems in EEG. The effects of the eye orbit structure on surface potential distribution are also studied.},
  language = {en},
  timestamp = {2017-08-22T09:45:07Z},
  number = {5},
  urldate = {2017-08-22},
  journal = {Medical and Biological Engineering and Computing},
  author = {Yan, Y. and Nunez, P. L. and Hart, R. T.},
  month = sep,
  year = {1991},
  pages = {475--481},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\DHKDQG8Q\\BF02442317.html:text/html}
}

@article{Awada1997,
  title = {Computational Aspects of Finite Element Modeling in {{EEG}} Source Localization},
  volume = {44},
  issn = {0018-9294},
  doi = {10.1109/10.605431},
  abstract = {A comparison is made of two different implementations of the finite element method (FEM) for calculating the potential due to dipole sources in electroencephalography (EEG). In one formulation (the direct method) the total potential is the unknown that is solved for and the dipole source is directly incorporated into the model. In the second formulation (the subtraction method) the unknown is the difference between the total potential and the potential due to the same dipole in an infinite region of homogeneous conductivity, corresponding to the region where the dipole is located. Both methods have the same FEM system matrix. However, the subtraction method requires an additional calculation of flux integrations along the edges of the elements in the computation of the right-hand side (RHS) vector. It is shown that the subtraction method is usually more accurate in the forward modeling, provided the flux integrations are computed accurately. Errors in calculating the flux integrations may result in large errors in the forward solution due to the ill-conditioned nature of the FEM system matrix caused by the Neumann boundary condition. To minimize the errors, closed-form expressions for the flux integrations are used for both linear and quadratic triangular elements. It is also found that FEM forward modeling errors may cause false extrema in the least-square objective function obtained from the boundary potential, near boundaries between media of differing conductivity. Multiple initial guesses help eliminate the possibility of the solution getting trapped in these false extrema.},
  timestamp = {2017-08-22T09:47:44Z},
  number = {8},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Awada, K. A. and Jackson, D. R. and Williams, J. T. and Wilton, D. R. and Baumann, S. B. and Papanicolaou, A. C.},
  month = aug,
  year = {1997},
  keywords = {Boundary conditions,boundary potential,Brain,Brain modeling,brain models,closed-form expressions,Closed-form solution,computational aspects,Conductivity,dipole source,dipole sources,direct method,EEG,EEG source localization,Electric Conductivity,Electric potential,Electroencephalography,Error analysis,errors,false extrema,FEM system matrix,finite element analysis,Finite element methods,finite element modeling,flux integrations,forward modeling,homogeneous conductivity,Humans,inverse problems,least-square objective function,linear elements,Magnetic heads,magnetic resonance imaging,matrix algebra,medical signal processing,Models; Neurological,Neumann boundary condition,Poisson Distribution,potential,quadratic triangular elements,RHS vector,Scalp,Sensitivity and Specificity,Skull,subtraction method,total potential},
  pages = {736--752},
  file = {IEEE Xplore Abstract Record:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KKB8B6K3\\605431.html:text/html}
}

@article{Buchner1997,
  title = {Inverse Localization of Electric Dipole Current Sources in Finite Element Models of the Human Head},
  volume = {102},
  issn = {0013-4694},
  doi = {10.1016/S0013-4694(96)95698-9},
  abstract = {The paper describes finite element related procedures for inverse localization of multiple sources in realistically shaped head models. Dipole sources are modeled by placing proper monopole sources on neighboring nodes. Lead field operators are established for dipole sources. Two different strategies for the solution of inverse problems, namely combinatorial optimization techniques and regularization methods are discussed and applied to visually evoked potentials, for which exemplary results are shown. Most of the procedures described are fully automatic and require only proper input preparation. The overall work for the example presented (from EEG recording to visual inspection of the results) can be performed in roughly a week, most of which is waiting time for the computation of the lead field matrix or inverse calculations on a standard and affordable engineering workstation.},
  timestamp = {2017-08-22T09:49:31Z},
  number = {4},
  journal = {Electroencephalography and Clinical Neurophysiology},
  author = {Buchner, Helmut and Knoll, Gunter and Fuchs, Manfred and Rien{\"a}cker, Adrian and Beckmann, Rainer and Wagner, Michael and Silny, Jiri and Pesch, J{\"o}rg},
  month = apr,
  year = {1997},
  keywords = {Combinatorial optimization,Finite element method,Inverse source localization,Simulated annealing,Spatial regularization},
  pages = {267--278},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\JHENGC5T\\S0013469496956989.html:text/html}
}

@article{vandenBroek1998,
  title = {Volume Conduction Effects in {{EEG}} and {{MEG}}},
  volume = {106},
  issn = {0013-4694},
  doi = {10.1016/S0013-4694(97)00147-8},
  abstract = {Volume conductor models that are commonly used to describe the EEG and MEG neglect holes in the skull, lesions, the ventricles, and anisotropic conductivity of the skull. To determine the influence of these features, simulations were carried out using the finite element method. The simulations showed that a hole in the skull will have a large effect on the EEG, and as one of the consequences localisation errors up to 15 mm may occur. The effect on the MEG is negligible. The presence of a lesion may cause the shape and magnitude of the EEG and MEG to change. Hence, a lesion has to be taken into account, if the active neurones are close to it. Moreover, a localisation procedure may fail if the lesion is not included in the volume conductor model. Inclusion of the ventricles in the volume conductor model is necessary only if sources are in their vicinity or if their sizes are unusually large. Anisotropic conductivity of the skull has a smearing effect on the EEG but does not influence the MEG.},
  timestamp = {2017-08-22T09:50:55Z},
  number = {6},
  journal = {Electroencephalography and Clinical Neurophysiology},
  author = {{van den Broek}, S. P and Reinders, F and Donderwinkel, M and Peters, M. J},
  month = jun,
  year = {1998},
  keywords = {Anisotropy,EEG,Holes,MEG,Ventricles,Volume conduction},
  pages = {522--534},
  file = {ScienceDirect Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\GCQPXGFT\\S0013469497001478.html:text/html}
}

@article{Marin1998,
  title = {Influence of Skull Anisotropy for the Forward and Inverse Problem in {{EEG}}: {{Simulation}} Studies Using {{FEM}} on Realistic Head Models},
  volume = {6},
  issn = {1097-0193},
  shorttitle = {Influence of Skull Anisotropy for the Forward and Inverse Problem in {{EEG}}},
  doi = {10.1002/(SICI)1097-0193(1998)6:4<250::AID-HBM5>3.0.CO;2-2},
  abstract = {For the sake of realism in the description of conduction from primary neural currents to scalp potentials, we investigated the influence of skull anisotropy on the forward and inverse problems in brain functional imaging with EEG. At present, all methods available for cortical imaging assume a spherical geometry, or when using realistic head shapes do not consider the anisotropy of head tissues. However, to our knowledge, no study relates the implication of this simplifying hypothesis on the spatial resolution of EEG for source imaging. In this paper, a method using finite elements in a realistic head geometry is implemented and validated. The influence of erroneous conductivity values for the head tissues is presented, and results show that the conductivities of the brain and the skull in the radial orientation are the most critical ones. In the inverse problem, this influence has been evaluated with simulations using a distributed source model with a comparison of two regularization techniques, with the isotropic model working on data sets produced by a nonisotropic model. Regularization with minimum norm priors produces source images with spurious activity, meaning that the errors in the head model totally annihilate any localization ability. But nonlinear regularization allows the accurate recovery of simultaneous spots of activity, while the restoration of very close active regions is profoundly disabled by errors in the head model. We conclude that for robust cortical source imaging with EEG, a realistic head model taking anisotropy of tissues into account should be used. Hum. Brain Mapping 6:250\textendash{}269, 1998. \textcopyright{} 1998 Wiley-Liss, Inc.},
  language = {en},
  timestamp = {2017-08-22T09:53:19Z},
  number = {4},
  journal = {Human Brain Mapping},
  author = {Marin, Gildas and Guerin, Christophe and Baillet, Sylvain and Garnero, Line and Meunier, G{\'e}rard},
  month = jan,
  year = {1998},
  keywords = {Anisotropic conductivity,EEG,FEM,Forward problem,Inverse problem},
  pages = {250--269},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\KR4SMRZE\\abstract.html:text/html}
}

@article{Huang1999,
  title = {A Sensor-Weighted Overlapping-Sphere Head Model and Exhaustive Head Model Comparison for {{MEG}}},
  volume = {44},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/44/2/010},
  abstract = {The spherical head model has been used in magnetoencephalography (MEG) as a simple forward model for calculating the external magnetic fields resulting from neural activity. For more realistic head shapes, the boundary element method (BEM) or similar numerical methods are used, but at greatly increased computational cost. We introduce a sensor-weighted overlapping-sphere (OS) head model for rapid calculation of more realistic head shapes. The volume currents associated with primary neural activity are used to fit spherical head models for each individual MEG sensor such that the head is more realistically modelled as a set of overlapping spheres, rather than a single sphere. To assist in the evaluation of this OS model with BEM and other head models, we also introduce a novel comparison technique that is based on a generalized eigenvalue decomposition and accounts for the presence of noise in the MEG data. With this technique we can examine the worst possible errors for thousands of dipole locations in a realistic brain volume. We test the traditional single-sphere model, three-shell and single-shell BEM, and the new OS model. The results show that the OS model has accuracy similar to the BEM but is orders of magnitude faster to compute.},
  language = {en},
  timestamp = {2017-08-22T09:57:10Z},
  number = {2},
  journal = {Physics in Medicine \& Biology},
  author = {Huang, M. X. and Mosher, J. C. and Leahy, R. M.},
  year = {1999},
  pages = {423}
}

@article{Akalin-Acar2004,
  title = {An Advanced Boundary Element Method ({{BEM}}) Implementation for the Forward Problem of Electromagnetic Source Imaging},
  volume = {49},
  issn = {0031-9155},
  abstract = {The forward problem of electromagnetic source imaging has two components: a numerical model to solve the related integral equations and a model of the head geometry. This study is on the boundary element method (BEM) implementation for numerical solutions and realistic head modelling. The use of second-order (quadratic) isoparametric elements and the recursive integration technique increase the accuracy in the solutions. Two new formulations are developed for the calculation of the transfer matrices to obtain the potential and magnetic field patterns using realistic head models. The formulations incorporate the use of the isolated problem approach for increased accuracy in solutions. If a personal computer is used for computations, each transfer matrix is calculated in 2.2 h. After this pre-computation period, solutions for arbitrary source configurations can be obtained in milliseconds for a realistic head model. A hybrid algorithm that uses snakes, morphological operations, region growing and thresholding is used for segmentation. The scalp, skull, grey matter, white matter and eyes are segmented from the multimodal magnetic resonance images and meshes for the corresponding surfaces are created. A mesh generation algorithm is developed for modelling the intersecting tissue compartments, such as eyes. To obtain more accurate results quadratic elements are used in the realistic meshes. The resultant BEM implementation provides more accurate forward problem solutions and more efficient calculations. Thus it can be the firm basis of the future inverse problem solutions.},
  language = {eng},
  timestamp = {2017-08-22T10:00:49Z},
  number = {21},
  journal = {Physics in Medicine and Biology},
  author = {Akalin-Acar, Zeynep and Gen{\c c}er, Nevzat G.},
  month = nov,
  year = {2004},
  keywords = {algorithms,Brain,Brain Mapping,Computer Simulation,Diagnosis; Computer-Assisted,Electroencephalography,finite element analysis,Head,Humans,Magnetoencephalography,Models; Neurological,Reproducibility of Results,Sensitivity and Specificity},
  pages = {5011--5028},
  pmid = {15584534}
}

@article{Gauraha2016,
  title = {Efficient {{Clustering}} of {{Correlated Variables}} and {{Variable Selection}} in {{High}}-{{Dimensional Linear Models}}},
  abstract = {In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable selection in high dimensional sparse regression models with strongly correlated variables. To handle correlated variables, the concept of clustering or grouping variables and then pursuing model fitting is widely accepted. When the dimension is very high, finding an appropriate group structure is as difficult as the original problem. The ACL is a three-stage procedure where, at the first stage, we use the Lasso(or its adaptive or thresholded version) to do initial selection, then we also include those variables which are not selected by the Lasso but are strongly correlated with the variables selected by the Lasso. At the second stage we cluster the variables based on the reduced set of predictors and in the third stage we perform sparse estimation such as Lasso on cluster representatives or the group Lasso based on the structures generated by clustering procedure. We show that our procedure is consistent and efficient in finding true underlying population group structure(under assumption of irrepresentable and beta-min conditions). We also study the group selection consistency of our method and we support the theory using simulated and pseudo-real dataset examples.},
  timestamp = {2017-10-12T12:05:03Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.03724},
  primaryClass = {cs, stat},
  journal = {arXiv:1603.03724 [cs, stat]},
  author = {Gauraha, Niharika and Parui, Swapan K.},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {arXiv.org Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\SHWZS3QP\\1603.html:text/html}
}

@misc{Oostenveld2011,
  type = {Research article},
  title = {{{FieldTrip}}: {{Open Source Software}} for {{Advanced Analysis}} of {{MEG}}, {{EEG}}, and {{Invasive Electrophysiological Data}}},
  shorttitle = {{{FieldTrip}}},
  abstract = {This paper describes FieldTrip, an open source software package that we developed for the analysis of MEG, EEG, and other electrophysiological data. The software is implemented as a MATLAB toolbox and includes a complete set of consistent and user-friendly high-level functions that allow experimental neuroscientists to analyze experimental data. It includes algorithms for simple and advanced analysis, such as time-frequency analysis using multitapers, source reconstruction using dipoles, distributed sources and beamformers, connectivity analysis, and nonparametric statistical permutation tests at the channel and source level. The implementation as toolbox allows the user to perform elaborate and structured analyses of large data sets using the MATLAB command line and batch scripting. Furthermore, users and developers can easily extend the functionality and implement new algorithms. The modular design facilitates the reuse in other software packages.},
  language = {en},
  timestamp = {2017-12-25T16:41:37Z},
  urldate = {2017-12-25},
  howpublished = {\url{https://www.hindawi.com/journals/cin/2011/156869/}},
  journal = {Computational Intelligence and Neuroscience},
  author = {Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs},
  year = {2011},
  file = {Snapshot:\\\\filesrv4\\home$\\afdidehf\\.windows\\Application Data\\Zotero\\Zotero\\Profiles\\wi3wq94h.default\\zotero\\storage\\XGP3I2D4\\156869.html:application/xhtml+xml},
  pmid = {21253357},
  doi = {10.1155/2011/156869}
}

@misc{SPM,
  title = {{{SPM}} - {{Statistical Parametric Mapping}}},
  timestamp = {2017-12-25T17:01:39Z},
  urldate = {2017-12-25},
  howpublished = {\url{http://www.fil.ion.ucl.ac.uk/spm/}}
}

@PhdThesis{Dossal2005,
  author = {Dossal, C.},
  title  = {Estimation de fonctions g{\'e}om{\'e}triques et d{\'e}convolution},
  year   = {2005},
}

@Book{Hari2017,
  title  = {MEG-EEG Primer},
  year   = {2017},
  author = {Riitta Hari and Aina Puce},
}

@Comment{jabref-meta: databaseType:biblatex;}
